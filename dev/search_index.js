var documenterSearchIndex = {"docs":
[{"location":"modules/DiffEqParamEstim/#DiffEqParamEstim.jl","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"DiffEqParamEstim.jl is a package for simplified parameter estimation with  DifferentialEquations.jl While not as expansive as SciMLSensitivity.jl,  it's a simple and helpful for new users who want to quickly run standard parameter  estimation routines for model callibration on not too large of models (<100 parameters  or ODEs). ","category":"page"},{"location":"modules/DiffEqParamEstim/","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"note: Note\n","category":"page"},{"location":"modules/DiffEqParamEstim/","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"For much larger models and more complex setups (multiple datasets, batching, etc.) see    SciMLSensitivity.","category":"page"},{"location":"modules/DiffEqParamEstim/#Installation","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"To use this functionality, you must install DiffEqParamEstim.jl:","category":"page"},{"location":"modules/DiffEqParamEstim/","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"]add DiffEqParamEstim\nusing DiffEqParamEstim","category":"page"},{"location":"modules/DiffEqDocs/basics/compatibility_chart/#Solver-Compatibility-Chart","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"This chart is for documenting the compatibility of the component solver packages to the common interface. An x means that the option is implemented or the add-on functionality will work with the given solver. A blank means that the option has not been implemented or that a given add-on has not been tested with a given package. If there are any errors in this chart, please file an issue or submit a pull-request.","category":"page"},{"location":"modules/DiffEqDocs/basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"Option OrdinaryDiffEq.jl Sundials.jl ODE.jl ODEInterface.jl LSODA.jl StochasticDiffEq.jl DelayDiffEq.jl DASKR.jl DASSL.jl\nNonlinear Dense (continuous) output x x    x x x \nTolerance control x x x x x x x x x\nAdvanced stepsize control x 0  x 0 x x 0 \nMass Matrices^ x 0  x 0 x x 0 \nAnalytical Jacobians^† x x  x  x x x \nGeneral Performance Overloads^† x 0  0 0 x x 0 \ninternalnorm x 0 x 0 0 x x 0 \nInitial dt x x x x  x x x \nsave_everystep x x x x x x x x \nsaveat x x x x x x x x \ntstops x x  0  x x x \nd_discontinuities x   0  x x  \nisoutofdomain x  x   x x  \nAllows reverse time direction x x x x x x x  \nUnitful numbers x 0  0 0  x 0 \nArbitrary dimension arrays x x x x x x x x x\nComplex numbers p     x p  \nArbitrary precision x 0 x 0 0 x x 0 x\nApproxFun types x 0  0 0  x 0 \nProgress monitoring x     x x  \nIntegrator interface x x  0  x x  \nResizability x 0  0 0 x x 0 \nCache iterator x 0  0 0 x x 0 \nCan choose linear solvers x s    x x s x\nCan choose nonlinear solvers x 0  0 0 x x 0 x\nCan use out of place natively x 0 x 0 0 x x 0 x\nCan use inplace natively x x  x x x x x \nCompatible with DiffEqDevTools x x x x x x x x \nCompatible with ParameterizedFunctions x x x x x x x x \nContinuous Callbacks x x  x  x x  x\nDiscrete Callbacks x x  x  x x  \nMonte Carlo Simulations x x x x x x x x \nParameter Estimation x n n n n x x n x\nParameter Sensitivity Analysis x x x x x  x  \nPlotting and solution handling x x x x x x x x x","category":"page"},{"location":"modules/DiffEqDocs/basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"x: Full compatibility\np: Partial compatibility, only in nonstiff methods unless the Jacobian is provided.\nn: General compatibility, but not compatible with routines which. require being able to autodifferentiate through the entire solver.\n0: Not possible. This is generally due to underlying inflexibility in a wrapped library.\ns: Special, Sundials has its own linear solver choices.\n^: Only stiff (implicit) methods.\n†: For packages with compatibility, no warning is given when a specific algorithm does not need to use this feature.","category":"page"},{"location":"modules/DiffEqDocs/basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"All blank spaces are possible future additions.","category":"page"},{"location":"modules/GlobalSensitivity/#Global-Sensitivity-Analysis","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"Global Sensitivity Analysis","text":"","category":"section"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"Global Sensitivity Analysis (GSA) methods are used to quantify the uncertainty in output of a model w.r.t. the parameters. These methods allow practitioners to measure both parameter's individual contributions and the contribution of their interactions to the output uncertainity.","category":"page"},{"location":"modules/GlobalSensitivity/#Installation","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"Installation","text":"","category":"section"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"To use this functionality, you must install GlobalSensitivity.jl:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"]add GlobalSensitivity\nusing GlobalSensitivity","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"Note: GlobalSensitivity.jl is unrelated to the GlobalSensitivityAnalysis.jl package.","category":"page"},{"location":"modules/GlobalSensitivity/#General-Interface","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"General Interface","text":"","category":"section"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"The general interface for calling a global sensitivity analysis is:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"res = gsa(f, method, param_range; samples, batch=false)","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"where:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"y=f(x) is a function that takes in a single vector and spits out a single vector or scalar. If batch=true, then f takes in a matrix where each row is a set of parameters, and returns a matrix where each row is a the output for the corresponding row of parameters.\nmethod is one of the GSA methods below.\nparam_range is a vector of tuples for the upper and lower bound for the given parameter i.\nsamples is a required keyword argument for the number of samples of parameters for the design matrix. Note that this is not relevant for Fractional Factorial Method and Morris Method.","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"Additionally,","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"For Delta Moment-Independent Method, EASI Method and Regression Method input and output matrix based method as follows is available:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"res = gsa(X, Y, method)","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"where:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"X is the number of parameters * samples matrix with parameter values.\nY is the output dimension * number of samples matrix with out evaluated at X's columns.\nmethod is one of the GSA methods below.","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"For Sobol Method one can use the following design matrices based method instead of parameter range based method discussed earlier:","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"effects = gsa(f, method, A, B; batch=false)","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"where A and B are design matrices with each row being a set of parameters. Note that generate_design_matrices from QuasiMonteCarlo.jl can be used to generate the design matrices.","category":"page"},{"location":"modules/GlobalSensitivity/","page":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","title":"GlobalSensitivity.jl: Global Sensitivity Analysis (GSA)","text":"The descriptions of the available methods can be found in the Methods section. The GSA interface allows for utilizing batched functions with the batch kwarg discussed above for parallel computation of GSA results.","category":"page"},{"location":"modules/Surrogates/lp/#Lp-norm-function","page":"Lp norm","title":"Lp norm function","text":"","category":"section"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"The Lp norm function is defined as: f(x) = sqrtp sum_i=1^d vert x_i vert ^p","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"function f(x,p)\n    return norm(x,p)\nend","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"Let's see a simple 1D case:","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"n = 30\nlb = -5.0\nub = 5.0\np = 1.3\nx = sample(n,lb,ub,SobolSample())\ny = f.(x,p)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"Fitting different Surrogates:","category":"page"},{"location":"modules/Surrogates/lp/","page":"Lp norm","title":"Lp norm","text":"my_pol = PolynomialChaosSurrogate(x,y,lb,ub)\nloba_1 = LobachevskySurrogate(x,y,lb,ub)\nkrig = Kriging(x,y,lb,ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 5), legend=:top)\nplot!(xs,f.(xs,p), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/#Initial-and-Boundary-Conditions-with-sampled/measured-Data","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Initial and boundary conditions are sometimes applied with measured data that is itself pre-discretized. In order to use such data it is recommended to leverage Interpolations.jl, or DataInterpolations.jl, for better dealing with possibly noisy data (currently limited to 1D). To create a callable effectively continuous function, for example (from the Interpolations.jl docs):","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"1D:","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"using Interpolations\n\nA_x = 1.:2.:40.\nA = [log10(x) for x in A_x]\nitp = interpolate(A, BSpline(Cubic(Line(OnGrid()))))\nsitp1 = scale(itp, A_x)\nsitp1(3.) # exactly log10(3.)\nsitp1(3.5) # approximately log10(3.5)","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Multidimensional:","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"using Interpolations \n\nA_x1 = 1:.1:10\nA_x2 = 1:.5:20\nf(x1, x2) = log10(x1+x2)\nA = [f(x1,x2) for x1 in A_x1, x2 in A_x2]\nitp = interpolate(A, BSpline(Cubic(Line(OnGrid()))))\nsitp2 = scale(itp, A_x1, A_x2)\nsitp2(5., 10.) # exactly log10(5 + 10)\nsitp2(5.6, 7.1) # approximately log10(5.6 + 7.1)","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Then, register the functions with ModelingToolkit:","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"sitp1_f(x) = sitp1(x)\nsitp2_f(x, y) = sitp2(x, y)\n@register_symbolic sitp1_f(y)\n@register_symbolic sitp2_f(x, y)","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Then as a BC or IC:","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"bcs = [u(0, x, y) ~ sitp2_f(x, y),\n       u(t, 0, y) ~ sitp1_f(y),\n       ...\n       ]","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"Note that the measured data need not be measured on the same grid as will be generated for the discretization in MethodOfLines.jl, as long as it is defined upon the whole simulation domain it will be automatically re-sampled.","category":"page"},{"location":"modules/MethodOfLines/tutorials/icbc_sampled/","page":"Initial and Boundary Conditions with sampled/measured Data","title":"Initial and Boundary Conditions with sampled/measured Data","text":"If you are using an edge_align grid, your interpolation will need to be defined ±dx/2 above and below the edges of the simulation domain where dx is the step size in the direction of that edge. Extrapolation may prove useful here.","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/#sensitivity","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Sensitivity analysis, or automatic differentiation of the solver, is provided by the DiffEq suite. The model sensitivities are the derivatives of the solution u(t) with respect to the parameters. Specifically, the local sensitivity of the solution to a parameter is defined by how much the solution would change by changes in the parameter, i.e. the sensitivity of the ith independent variable to the jth parameter is fracpartial u_ipartial p_j.","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Sensitivity analysis serves two major purposes. On one hand, the sensitivities are diagnostics of the model which are useful for understand how it will change in accordance to changes in the parameters. But another use is simply because in many cases these derivatives are useful. Sensitivity analysis provides a cheap way to calculate the gradient of the solution which can be used in parameter estimation and other optimization tasks.","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"There are three types of sensitivity analysis. Local forward sensitivity analysis directly gives the gradient of the solution with respect to each parameter along the time series. The computational cost scales like N*M, where N is the number of states and M is the number of parameters. While this gives all of the information, it can be expensive for models with large numbers of parameters. Local adjoint sensitivity analysis solves directly for the gradient of some functional of the solution, such as a cost function or energy functional, in a manner that is cheaper when the number of parameters is large. Global Sensitivity Analysis methods are meant to be used for exploring the sensitivity over a larger domain without calculating derivatives and are covered on a different page.","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/#Installation-and-Usage","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Installation and Usage","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install SciMLSensitivity.jl:","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"]add SciMLSensitivity\nusing SciMLSensitivity","category":"page"},{"location":"modules/DiffEqDocs/analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For complete information on using the sensitivity analyis features, please consult the SciMLSensitivity.jl documentation","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#components","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"The symbolic models of ModelingToolkit can be composed together to easily build large models. The composition is lazy and only instantiated at the time of conversion to numerical models, allowing a more performant way in terms of computation time and memory.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Simple-Model-Composition-Example","page":"Composing Models and Building Reusable Components","title":"Simple Model Composition Example","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"The following is an example of building a model in a library with an optional forcing function, and allowing the user to specify the forcing later. Here, the library author defines a component named decay. The user then builds two decay components and connects them, saying the forcing term of decay1 is a constant while the forcing term of decay2 is the value of the state variable x.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"using ModelingToolkit\n\nfunction decay(;name)\n  @parameters t a\n  @variables x(t) f(t)\n  D = Differential(t)\n  ODESystem([\n      D(x) ~ -a*x + f\n    ];\n    name=name)\nend\n\n@named decay1 = decay()\n@named decay2 = decay()\n\n@parameters t\nD = Differential(t)\nconnected = compose(ODESystem([\n                        decay2.f ~ decay1.x\n                        D(decay1.f) ~ 0\n                      ], t; name=:connected), decay1, decay2)\n\nequations(connected)\n\n#4-element Vector{Equation}:\n# Differential(t)(decay1₊f(t)) ~ 0\n# decay2₊f(t) ~ decay1₊x(t)\n# Differential(t)(decay1₊x(t)) ~ decay1₊f(t) - (decay1₊a*(decay1₊x(t)))\n# Differential(t)(decay2₊x(t)) ~ decay2₊f(t) - (decay2₊a*(decay2₊x(t)))\n\nsimplified_sys = structural_simplify(connected)\n\nequations(simplified_sys)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Now we can solve the system:","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"x0 = [\n  decay1.x => 1.0\n  decay1.f => 0.0\n  decay2.x => 1.0\n]\np = [\n  decay1.a => 0.1\n  decay2.a => 0.2\n]\n\nusing DifferentialEquations\nprob = ODEProblem(simplified_sys, x0, (0.0, 100.0), p)\nsol = solve(prob, Tsit5())\nsol[decay2.f]","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Basics-of-Model-Composition","page":"Composing Models and Building Reusable Components","title":"Basics of Model Composition","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Every AbstractSystem has a system keyword argument for specifying subsystems. A model is the composition of itself and its subsystems. For example, if we have:","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"@named sys = compose(ODESystem(eqs,indepvar,states,ps),subsys)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"the equations of sys is the concatenation of get_eqs(sys) and equations(subsys), the states are the concatenation of their states, etc. When the ODEProblem or ODEFunction is generated from this system, it will build and compile the functions associated with this composition.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"The new equations within the higher level system can access the variables in the lower level system by namespacing via the nameof(subsys). For example, let's say there is a variable x in states and a variable x in subsys. We can declare that these two variables are the same by specifying their equality: x ~ subsys.x in the eqs for sys. This algebraic relationship can then be simplified by transformations like structural_simplify which will be described later.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Numerics-with-Composed-Models","page":"Composing Models and Building Reusable Components","title":"Numerics with Composed Models","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"These composed models can then be directly transformed into their associated SciMLProblem type using the standard constructors. When this is done, the initial conditions and parameters must be specified in their namespaced form. For example:","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"u0 = [\n  x => 2.0\n  subsys.x => 2.0\n]","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Note that any default values within the given subcomponent will be used if no override is provided at construction time. If any values for initial conditions or parameters are unspecified an error will be thrown.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"When the model is numerically solved, the solution can be accessed via its symbolic values. For example, if sol is the ODESolution, one can use sol[x] and sol[subsys.x] to access the respective timeseries in the solution. All other indexing rules stay the same, so sol[x,1:5] accesses the first through fifth values of x. Note that this can be done even if the variable x is eliminated from the system from transformations like alias_elimination or tearing: the variable will be lazily reconstructed on demand.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Variable-scope-and-parameter-expressions","page":"Composing Models and Building Reusable Components","title":"Variable scope and parameter expressions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"In some scenarios, it could be useful for model parameters to be expressed in terms of other parameters, or shared between common subsystems. To facilitate this, ModelingToolkit supports symbolic expressions in default values, and scoped variables.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"With symbolic parameters, it is possible to set the default value of a parameter or initial condition to an expression of other variables.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"# ...\nsys = ODESystem(\n    # ...\n    # directly in the defauls argument\n    defaults=Pair{Num, Any}[\n    x => u,\n    y => σ,\n    z => u-0.1,\n])\n# by assigning to the parameter\nsys.y = u*1.1","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"In a hierarchical system, variables of the subsystem get namespaced by the name of the system they are in. This prevents naming clashes, but also enforces that every state and parameter is local to the subsystem it is used in. In some cases it might be desirable to have variables and parameters that are shared between subsystems, or even global. This can be accomplished as follows.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"@variables a b c d\n\n# a is a local variable\nb = ParentScope(b) # b is a variable that belongs to one level up in the hierarchy\nc = ParentScope(ParentScope(c)) # ParentScope can be nested\nd = GlobalScope(d) # global variables will never be namespaced","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Structural-Simplify","page":"Composing Models and Building Reusable Components","title":"Structural Simplify","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"In many cases, the nicest way to build a model may leave a lot of unnecessary variables. Thus one may want to remove these equations before numerically solving. The structural_simplify function removes these trivial equality relationships and trivial singularity equations, i.e. equations which result in 0~0 expressions, in over-specified systems.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Inheritance-and-Combine","page":"Composing Models and Building Reusable Components","title":"Inheritance and Combine","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Model inheritance can be done in two ways: implicitly or explicitly. First, one can use the extend function to extend a base model with another set of equations, states, and parameters. An example can be found in the acausal components tutorial.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"The explicit way is to shadow variables with equality expressions. For example, let's assume we have three separate systems which we want to compose to a single one. This is how one could explicitly forward all states and parameters to the higher level system:","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"using ModelingToolkit, OrdinaryDiffEq, Plots\n\n## Library code\n\n@parameters t\nD = Differential(t)\n\n@variables S(t), I(t), R(t)\nN = S + I + R\n@parameters β,γ\n\n@named seqn = ODESystem([D(S) ~ -β*S*I/N])\n@named ieqn = ODESystem([D(I) ~ β*S*I/N-γ*I])\n@named reqn = ODESystem([D(R) ~ γ*I])\n\nsir = compose(ODESystem([\n                    S ~ ieqn.S,\n                    I ~ seqn.I,\n                    R ~ ieqn.R,\n                    ieqn.S ~ seqn.S,\n                    seqn.I ~ ieqn.I,\n                    seqn.R ~ reqn.R,\n                    ieqn.R ~ reqn.R,\n                    reqn.I ~ ieqn.I], t, [S,I,R], [β,γ],\n                    defaults = [\n                        seqn.β => β\n                        ieqn.β => β\n                        ieqn.γ => γ\n                        reqn.γ => γ\n                    ], name=:sir), seqn, ieqn, reqn)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Note that the states are forwarded by an equality relationship, while the parameters are forwarded through a relationship in their default values. The user of this model can then solve this model simply by specifying the values at the highest level:","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"sireqn_simple = structural_simplify(sir)\n\nequations(sireqn_simple)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"## User Code\n\nu0 = [seqn.S => 990.0,\n      ieqn.I => 10.0,\n      reqn.R => 0.0]\n\np = [\n    β => 0.5\n    γ => 0.25\n]\n\ntspan = (0.0,40.0)\nprob = ODEProblem(sireqn_simple,u0,tspan,p,jac=true)\nsol = solve(prob,Tsit5())\nsol[reqn.R]","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Tearing-Problem-Construction","page":"Composing Models and Building Reusable Components","title":"Tearing Problem Construction","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Some system types, specifically ODESystem and NonlinearSystem, can be further reduced if structural_simplify has already been applied to them. This is done by using the alternative problem constructors, ODAEProblem and BlockNonlinearProblem respectively. In these cases, the constructor uses the knowledge of the strongly connected components calculated during the process of simplification as the basis for building pre-simplified nonlinear systems in the implicit solving. In summary: these problems are structurally modified, but could be more efficient and more stable.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Components-with-discontinuous-dynamics","page":"Composing Models and Building Reusable Components","title":"Components with discontinuous dynamics","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"When modeling, e.g., impacts, saturations or Coulomb friction, the dynamic equations are discontinuous in either the state or one of its derivatives. This causes the solver to take very small steps around the discontinuity, and sometimes leads to early stopping due to dt <= dt_min. The correct way to handle such dynamics is to tell the solver about the discontinuity by means of a root-finding equation. ODEsystems accept a keyword argument continuous_events","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"ODESystem(eqs, ...; continuous_events::Vector{Equation})\nODESystem(eqs, ...; continuous_events::Pair{Vector{Equation}, Vector{Equation}})","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"where equations can be added that evaluate to 0 at discontinuities.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"To model events that have an effect on the state, provide events::Pair{Vector{Equation}, Vector{Equation}} where the first entry in the pair is a vector of equations describing event conditions, and the second vector of equations describe the effect on the state. The effect equations must be of the form","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"single_state_variable ~ expression_involving_any_variables","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Example:-Friction","page":"Composing Models and Building Reusable Components","title":"Example: Friction","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"The system below illustrates how this can be used to model Coulomb friction","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"using ModelingToolkit, OrdinaryDiffEq, Plots\nfunction UnitMassWithFriction(k; name)\n  @variables t x(t)=0 v(t)=0\n  D = Differential(t)\n  eqs = [\n    D(x) ~ v\n    D(v) ~ sin(t) - k*sign(v) # f = ma, sinusoidal force acting on the mass, and Coulomb friction opposing the movement\n  ]\n  ODESystem(eqs, t; continuous_events=[v ~ 0], name) # when v = 0 there is a discontinuity\nend\n@named m = UnitMassWithFriction(0.7)\nprob = ODEProblem(m, Pair[], (0, 10pi))\nsol = solve(prob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Example:-Bouncing-ball","page":"Composing Models and Building Reusable Components","title":"Example: Bouncing ball","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"In the documentation for DifferentialEquations, we have an example where a bouncing ball is simulated using callbacks which has an affect! on the state. We can model the same system using ModelingToolkit like this","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"@variables t x(t)=1 v(t)=0\nD = Differential(t)\n\nroot_eqs = [x ~ 0]  # the event happens at the ground x(t) = 0\naffect   = [v ~ -v] # the effect is that the velocity changes sign\n\n@named ball = ODESystem([\n    D(x) ~ v\n    D(v) ~ -9.8\n], t; continuous_events = root_eqs => affect) # equation => affect\n\nball = structural_simplify(ball)\n\ntspan = (0.0,5.0)\nprob = ODEProblem(ball, Pair[], tspan)\nsol = solve(prob,Tsit5())\n@assert 0 <= minimum(sol[x]) <= 1e-10 # the ball never went through the floor but got very close\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/#Test-bouncing-ball-in-2D-with-walls","page":"Composing Models and Building Reusable Components","title":"Test bouncing ball in 2D with walls","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"Multiple events? No problem! This example models a bouncing ball in 2D that is enclosed by two walls at y = pm 15.","category":"page"},{"location":"modules/ModelingToolkit/basics/Composition/","page":"Composing Models and Building Reusable Components","title":"Composing Models and Building Reusable Components","text":"@variables t x(t)=1 y(t)=0 vx(t)=0 vy(t)=2\nD = Differential(t)\n\ncontinuous_events = [ # This time we have a vector of pairs\n    [x ~ 0] => [vx ~ -vx]\n    [y ~ -1.5, y ~ 1.5] => [vy ~ -vy]\n]\n\n@named ball = ODESystem([\n    D(x)  ~ vx,\n    D(y)  ~ vy,\n    D(vx) ~ -9.8-0.1vx, # gravity + some small air resistance\n    D(vy) ~ -0.1vy,\n], t; continuous_events)\n\n\nball = structural_simplify(ball)\n\ntspan = (0.0,10.0)\nprob = ODEProblem(ball, Pair[], tspan)\n\nsol = solve(prob,Tsit5())\n@assert 0 <= minimum(sol[x]) <= 1e-10 # the ball never went through the floor but got very close\n@assert minimum(sol[y]) > -1.5 # check wall conditions\n@assert maximum(sol[y]) < 1.5  # check wall conditions\n\ntv = sort([LinRange(0, 10, 200); sol.t])\nplot(sol(tv)[y], sol(tv)[x], line_z=tv)\nvline!([-1.5, 1.5], l=(:black, 5), primary=false)\nhline!([0], l=(:black, 5), primary=false)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/#jump_diffusion_tutorial","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial in DifferentialEquations.jl.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Jump Diffusion equations are stochastic differential equations with discontinuous jumps. These can be written as:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"du = f(upt)dt + sum_jg_j(upt)dW_j(t) + sum_ih_i(upt)dN_i(t)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"where N_i is a Poisson-counter which denotes jumps of size h_i. In this tutorial we will show how to solve problems with even more general jumps. In the special case that g_j = 0 for all j, we'll call the resulting jump-ODE a piecewise deterministic Markov process.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Before running this tutorial please install the following packages if they are not already installed","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"using Pkg\nPkg.add(\"DifferentialEquations\")\nPkg.add(\"Plots\")","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"DifferentialEquations.jl will install JumpProcesses, along with the needed ODE and SDE solvers.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"We then load these packages, and set some plotting defaults, as","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"using DifferentialEquations, Plots\ndefault(; lw = 2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/#Defining-a-ConstantRateJump-Problem","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Defining a ConstantRateJump Problem","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"To start, let's solve an ODE that is coupled to a ConstantRateJump. A jump is defined as being \"constant rate\" if the rate is only dependent on values from other ConstantRateJumps or MassActionJumps (a special type of ConstantRateJump). This means that its rate must not be coupled with time, the solution to the differential equation, or a solution component that is changed by a VariableRateJump. ConstantRateJumps are cheaper to compute than VariableRateJumps, and so should be preferred when mathematically appropriate.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"(Note: if your rate is only \"slightly\" dependent on the solution of the differential equation, then it may be okay to use a ConstantRateJump. Accuracy loss will be related to the percentage that the rate changes over the jump intervals.)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Let's solve the following problem. We will have a linear ODE with a Poisson counter of rate 2 (which is the mean and variance), where at each jump the current solution will be halved. To solve this problem, we first define the ODEProblem:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"function f(du,u,p,t)\n    du[1] = u[1]\n    nothing\nend\n\nprob = ODEProblem(f, [0.2], (0.0, 10.0))","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Notice that, even though our equation is scalar, we define it using the in-place array form. Variable rate jump equations will require this form. Note that for this tutorial we solve a one-dimensional problem, but the same syntax applies for solving a system of differential equations with multiple jumps.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Now we define our rate equation for our jump. Since it's just the constant value 2, we do:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"rate(u, p, t) = 2","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Now we define the affect! of the jump. This is the same as an affect! from a DiscreteCallback, and thus acts directly on the integrator. Therefore, to make it halve the current value of u, we do:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"function affect!(integrator)\n    integrator.u[1] = integrator.u[1] / 2\n    nothing\nend","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Then we build our jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump = ConstantRateJump(rate, affect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Next, we extend our ODEProblem to a JumpProblem by attaching the jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob, Direct(), jump)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"We can now solve this extended problem using any ODE solver:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"sol = solve(jump_prob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/#Variable-Rate-Jumps","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Variable Rate Jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Now let's define a jump with a rate that is dependent on the differential equation via the solution vector. Let's set the rate to be the current value of the solution, that is:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"rate(u,p,t) = u[1]","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Using the same affect! we build a VariableRateJump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump = VariableRateJump(rate, affect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"To make things interesting, let's copy this jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump2 = deepcopy(jump)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"so that way we have two independent jump processes. We now couple these jumps to the ODEProblem:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"which we once again solve using an ODE solver:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"sol = solve(jump_prob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"In this way we have solve a mixed jump-ODE, i.e. a piecewise deterministic Markov process.","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/#Jump-Diffusion","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Jump Diffusion","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"Now we will finally solve the jump diffusion problem. The steps are the same as before, except we now start with a SDEProblem instead of an ODEProblem. Using the same drift function f as before, we add multiplicative noise via:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"function g(du, u, p, t)\n  du[1] = u[1]\n  nothing\nend\n\nprob = SDEProblem(f, g, [0.2], (0.0, 10.0))","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"and couple it to the jumps:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"We then solve it using an SDE algorithm:","category":"page"},{"location":"modules/JumpProcesses/tutorials/jump_diffusion/","page":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","title":"Piecewise Deterministic Markov Processes and Jump Diffusion Equations","text":"sol = solve(jump_prob, SRIW1())\nplot(sol)","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#[Boundary-Conditions](@ref-bcs)","page":"Boundary Conditions","title":"Boundary Conditions","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"What follows is a set of allowable boundary conditions, please note that this is not exhaustive - try your condition and see if it works, the handling is quite general. If it doesn't please post an issue and we'll try to support it. At the moment boundary conditions have to be supplied at the edge of the domain, but there are plans to support conditions embedded in the domain.","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Definitions","page":"Boundary Conditions","title":"Definitions","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"using ModelingToolkit, MethodOfLines, Domainsets\n\n@parameters x y t\n@variables u(..) v(..)\nDt = Differential(t)\nDx = Differential(x)\nDy = Differential(y)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\nx_min = y_min = 0.0\n\nx_max = y_max = 1.0","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Dirichlet","page":"Boundary Conditions","title":"Dirichlet","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"v(t, 0, y) ~ 1.0","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Time-dependant","page":"Boundary Conditions","title":"Time dependant","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"u(t, 0., y) ~ x_min*y+ 0.5t","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Julia-function","page":"Boundary Conditions","title":"Julia function","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"v(t, x, y_max) ~ sin(x)","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#User-defined-function","page":"Boundary Conditions","title":"User defined function","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"alpha = 9\n\nf(t,x,y) = x*y - t\n\nfunction g(x,y) \n    z = sin(x*y)+cos(y)\n    # Note that symbolic conditionals require the use of IfElse.ifelse, or registration\n    return IfElse.ifelse(z > 0, x, 1.0)\nend\n\nu(t,x,y_min) ~ f(t,x,y_min) + alpha/g(x,y_min)","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Registered-User-Defined-Function","page":"Boundary Conditions","title":"Registered User Defined Function","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"alpha = 9\n\nf(t,x,y) = x*y - t\n\nfunction g(x,y) \n    z = sin(x*y)+cos(y)\n    # This function must be registered as it contains a symbolic conditional\n    if z > 0\n        return x\n    else\n        return 1.0\n    end\nend\n\n@register g(x, y)\n\nu(t,x,y_min) ~ f(t,x,y_min) + alpha/g(x,y_min)","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Neumann/Robin","page":"Boundary Conditions","title":"Neumann/Robin","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"v(t, x_min, y) ~ 2. * Dx(v(t, x_min, y))","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Time-dependant-2","page":"Boundary Conditions","title":"Time dependant","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"u(t, x_min, y) ~ x_min*Dy(v(t,x_min,y)) + 0.5t","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Higher-order","page":"Boundary Conditions","title":"Higher order","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"v(t, x, 1.0) ~ sin(x) + Dyy(v(t, x, y_max))","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Time-derivative","page":"Boundary Conditions","title":"Time derivative","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"Dt(u(t, x_min, y)) ~ 0.2","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#User-defined-function-2","page":"Boundary Conditions","title":"User defined function","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"function f(u, v)\n    (u + Dyy(v) - Dy(u))/(1 + v)\nend\n\nDyy(u(t, x, y_min)) ~ f(u(t, x, y_min), v(t, x, y_min)) + 1","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#lhs","page":"Boundary Conditions","title":"0 lhs","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"0 ~ u(t, x, y_max) - Dy(v(t, x, y_max))","category":"page"},{"location":"modules/MethodOfLines/boundary_conditions/#Periodic","page":"Boundary Conditions","title":"Periodic","text":"","category":"section"},{"location":"modules/MethodOfLines/boundary_conditions/","page":"Boundary Conditions","title":"Boundary Conditions","text":"u(t, x_min, y) ~ u(t, x_max, y)\n\nv(t, x, y_max) ~ u(t, x_max, y)","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"using Plots\nfunction f(x,μ,σ)\n    1/sqrt(2 *π*σ^2) * exp(-(x - μ)^2 / (2σ^2))\nend\nμ, σ = [1., 1.7], [0.2, 0.3]\nρ(x) = 0.5*f(x,μ[1],σ[1]) + 0.5*f(x,μ[2],σ[2])\nx = 0:0.01:3\nplot(x,ρ.(x))\nxlabel!(\"x\")\nylabel!(\"rho(x)\")\nusing PolyChaos\ndeg = 4\nmeas = Measure(\"my_GaussMixture\", ρ, (-Inf,Inf), false, Dict(:μ=>μ, :σ=>σ)) # build measure\nop = OrthoPoly(\"my_op\", deg, meas; Nquad = 100,Nrec = 2*deg) # construct orthogonal polynomial\nshowbasis(op, digits=2) # in case you wondered\nT2 = Tensor(2,op) # compute scalar products\nT2num_1 = [ T2.get([i,j]) for i in 0:deg, j in 0:deg]\nusing QuadGK\nT2num_2 = [quadgk(x -> evaluate(i,x,op)*evaluate(j,x,op)*ρ(x),-Inf,Inf)[1] for i in 0:deg, j in 0:deg ]\nT2num_1 - T2num_2","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/#Gaussian-Mixture-Models","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"","category":"section"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"Gaussian mixture models are popular for clustering data. Generally speaking, they are continuous random variables with a special probability density, namely","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"rho(x) = sum_i = 1^n fracw_isqrt2 pi sigma_i^2 exp left( frac(x - mu_i)^22 sigma_i^2 right) quad textwith quad sum_i = 1^n w_i = 1","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"where the pairs of means and standard deviations (mu_i sigma_i), and the weights w_i for all i in  1 dots n  are given. Let's consider a simple example.","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"using Plots\nf(x,μ,σ) = 1 / sqrt(2*π*σ^2) * exp(-(x - μ)^2 / (2σ^2))\nμs, σs, ws = [1., 1.7], [0.2, 0.3], [0.5, 0.5]\nρ(x) = sum(w*f(x, μ, σ) for (μ, σ, w) in zip(μs, σs, ws))\nx = 0:0.01:3;\nplot(x, ρ.(x))\nxlabel!(\"x\"); ylabel!(\"rho(x)\");","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"This looks nice!","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"What are now the polynomials that are orthogonal relative to this specific density?","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"using PolyChaos\ndeg = 4\nmeas = Measure(\"my_GaussMixture\", ρ, (-Inf,Inf), false, Dict(:μ=>μ, :σ=>σ)) # build measure\nop = OrthoPoly(\"my_op\", deg, meas; Nquad = 100, Nrec = 2*deg) # construct orthogonal polynomial\nshowbasis(op, digits=2) # in case you wondered","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"Let's add the quadrature rule and compute the square norms of the basis polynomials.","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"T2 = Tensor(2,op) # compute scalar products\n[ T2.get([i,j]) for i in 0:deg, j in 0:deg ]","category":"page"},{"location":"modules/PolyChaos/gaussian_mixture_model/","page":"Gaussian Mixture Models","title":"Gaussian Mixture Models","text":"Great!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/additional/#Additional-Tutorials","page":"Additional Tutorials","title":"Additional Tutorials","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/additional/","page":"Additional Tutorials","title":"Additional Tutorials","text":"Additional tutorials can be found at SciMLTutorials.jl. These include interactive introductions, optimizing code, modeling examples, and deeper examples for extra features.","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/#Compositional-Modeling-of-Reaction-Systems","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Catalyst supports the construction of models in a compositional fashion, based on ModelingToolkit's subsystem functionality. In this tutorial we'll see how we can construct the earlier repressilator model by composing together three identically repressed genes, and how to use compositional modeling to create compartments.","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/#Compositional-Modeling-Tooling","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling Tooling","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Catalyst supports two ModelingToolkit interfaces for composing multiple ReactionSystems together into a full model. The first mechanism for extending a system is the extend command","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"using Catalyst\nbasern = @reaction_network rn1 begin\n           k, A + B --> C\n         end k\nnewrn = @reaction_network rn2 begin\n        r, C --> A + B\n      end r\n@named rn = extend(newrn, basern)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Here we extended basern with newrn giving a system with all the reactions. Note, if a name is not specified via @named or the name keyword then rn will have the same name as newrn.","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"The second main compositional modeling tool is the use of subsystems. Suppose we now add to basern two subsystems, newrn and newestrn, we get a different result:","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"newestrn = @reaction_network rn3 begin\n            v, A + D --> 2D\n           end v\n@named rn = compose(basern, [newrn, newestrn])","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Here we have created a new ReactionSystem that adds newrn and newestrn as subsystems of basern. The variables and parameters in the sub-systems are considered distinct from those in other systems, and so are namespaced (i.e. prefaced) by the name of the system they come from.","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"We can see the subsystems of a given system by","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"ModelingToolkit.get_systems(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"They naturally form a tree-like structure","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"using Plots, GraphRecipes\nplot(TreePlot(rn), method=:tree, fontsize=12, nodeshape=:ellipse)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"(Image: rn network with subsystems)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"We could also have directly constructed rn using the same reaction as in basern as","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"@parameters k\n@variables t, A(t), B(t), C(t)\nrxs = [Reaction(k, [A,B], [C])]\n@named rn  = ReactionSystem(rxs, t; systems = [newrn, newestrn])","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Catalyst provides several different accessors for getting information from a single system, or all systems in the tree. To get the species, parameters, and equations only within a given system (i.e. ignoring subsystems), we can use","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"ModelingToolkit.get_states(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"ModelingToolkit.get_ps(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"ModelingToolkit.get_eqs(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"To see all the species, parameters and reactions in the tree we can use","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"species(rn)   # or states(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"parameters(rn)  # or reactionparameters(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"reactions(rn)   # or equations(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"If we want to collapse rn down to a single system with no subsystems we can use","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"flatrn = Catalyst.flatten(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"where","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"ModelingToolkit.get_systems(flatrn)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"More about ModelingToolkit's interface for compositional modeling can be found in the ModelingToolkit docs.","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/#Compositional-Model-of-the-Repressilator","page":"Compositional Modeling of Reaction Systems","title":"Compositional Model of the Repressilator","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Let's apply the tooling we've just seen to create the repressilator in a more modular fashion. We start by defining a function that creates a negatively repressed gene, taking the repressor as input","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"function repressed_gene(; R, name)\n    @reaction_network $name begin\n        hillr($R,α,K,n), ∅ --> m\n        (δ,γ), m <--> ∅\n        β, m --> m + P\n        μ, P --> ∅\n    end α K n δ γ β μ\nend","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Here we assume the user will pass in the repressor species as a ModelingToolkit variable, and specify a name for the network. We use Catalyst's interpolation ability to substitute the value of these variables into the DSL (see Interpolation of Julia Variables). To make the repressilator we now make three genes, and then compose them together","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"@variables t, G3₊P(t)\n@named G1 = repressed_gene(; R=ParentScope(G3₊P))\n@named G2 = repressed_gene(; R=ParentScope(G1.P))\n@named G3 = repressed_gene(; R=ParentScope(G2.P))\n@named repressilator = ReactionSystem(t; systems=[G1,G2,G3])","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Notice, in this system each gene is a child node in the system graph of the repressilator","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"plot(TreePlot(repressilator), method=:tree, fontsize=12, nodeshape=:ellipse)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"(Image: repressilator tree plot)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"In building the repressilator we needed to use two new features. First, we needed to create a symbolic variable that corresponds to the protein produced by the third gene before we created the corresponding system. We did this via @variables t, G3₊P(t). We also needed to set the scope where each repressor lived. Here ParentScope(G3₊P), ParentScope(G1.P), and ParentScope(G2.P) signal Catalyst that these variables will come from parallel systems in the tree that have the same parent as the system being constructed (in this case the top-level repressilator system).","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/#Compartment-based-Models","page":"Compositional Modeling of Reaction Systems","title":"Compartment-based Models","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Finally, let's see how we can make a compartment-based model. Let's create a simple eukaryotic gene expression model with negative feedback by protein dimers. Transcription and gene inhibition by the protein dimer occur in the nucleus, translation and dimerization occur in the cytosol, and nuclear import and export reactions couple the two compartments. We'll include volume parameters for the nucleus and cytosol, and assume we are working with species having units of number of molecules. Rate constants will have their common concentration units, i.e. if V denotes the volume of a compartment then","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Reaction Type Example Rate Constant Units Effective rate constant (units of per time)\nZero order varnothing oversetalphato A concentration / time alpha V\nFirst order A oversetbetato B (time)⁻¹ beta\nSecond order A + B oversetgammato C (concentration × time)⁻¹ gammaV","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"In our model we'll therefore add the conversions of the last column to properly account for compartment volumes:","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"# transcription and regulation\nnuc = @reaction_network nuc begin\n        α, G --> G + M\n        (κ₊/V,κ₋), D + G <--> DG\n      end α V κ₊ κ₋\n\n# translation and dimerization\ncyto = @reaction_network cyto begin\n            β, M --> M + P\n            (k₊/V,k₋), 2P <--> D\n            σ, P --> 0\n            μ, M --> 0\n        end β k₊ k₋ V σ μ\n\n# export reactions,\n# γ,δ=probability per time to be exported/imported\nmodel = @reaction_network model begin\n       γ, $(nuc.M) --> $(cyto.M)\n       δ, $(cyto.D) --> $(nuc.D)\n    end γ δ\n@named model = compose(model, [nuc, cyto])","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"A graph of the resulting network is","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"Graph(model)","category":"page"},{"location":"modules/Catalyst/tutorials/compositional_modeling/","page":"Compositional Modeling of Reaction Systems","title":"Compositional Modeling of Reaction Systems","text":"(Image: graph of gene regulation model)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/#Neural-Ordinary-Differential-Equations","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"A neural ODE is an ODE where a neural network defines its derivative function. Thus for example, with the multilayer perceptron neural network Lux.Chain(Lux.Dense(2, 50, tanh), Lux.Dense(50, 2)), we can define a differential equation which is u' = NN(u). This is done simply by the NeuralODE struct. Let's take a look at an example.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/#Copy-Pasteable-Code","page":"Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using Lux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationOptimJL, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\n# Do not plot by default for the documentation\n# Users should change doplot=true to see the plots callbacks\ncallback = function (p, l, pred; doplot = false)\n  println(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\npinit = Lux.ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...; doplot=true)\n\n# use Optimization.jl to solve the problem\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n                                        Optim.BFGS(initial_stepnorm=0.01),\n                                        callback=callback,\n                                        allow_f_increases = false)\n\ncallback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot=true)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"(Image: Neural ODE)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/#Explanation","page":"Neural Ordinary Differential Equations","title":"Explanation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Let's get a time series array from a sprial ODE to train against.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"using Lux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationOptimJL, Random, Plots\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Now let's define a neural network with a NeuralODE layer. First we define the layer. Here we're going to use Lux.Chain, which is a suitable neural network structure for NeuralODEs with separate handling of state variables:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\np, st = Lux.setup(rng, dudt2)\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Note that we can directly use Chains from Flux.jl as well, for example:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"dudt2 = Chain(x -> x.^3,\n              Dense(2, 50, tanh),\n              Dense(50, 2))","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"In our model we used the x -> x.^3 assumption in the model. By incorporating structure into our equations, we can reduce the required size and training time for the neural network, but a good guess needs to be known!","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"From here we build a loss function around it. The NeuralODE has an optional second argument for new parameters which we will use to iteratively change the neural network in our training loop. We will use the L2 loss of the network's output against the time series data:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We define a callback function. In this example we set doplot = false because otherwise it would show every step and overflow the documentation, but for your use case set doplot=true to see a live animation of the training process!.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Callback function to observe training\ncallback = function (p, l, pred; doplot = false)\n  println(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\npinit = Lux.ComponentArray(p)\ncallback(pinit, loss_neuralode(pinit)...)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then train the neural network to learn the ODE.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS. By using the two together, we are able to fit the neural ODE in 9 seconds! (Note, the timing commented out the plotting). You can easily incorporate the procedure below to set up custom optimization problems. For more information on the usage of Optimization.jl, please consult this documentation.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"The x and p variables in the optimization function are different than x and p above. The optimization function runs over the space of parameters of the original problem, so x_optimization == p_original.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Train using the ADAM optimizer\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n\nresult_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"We then complete the training using a different optimizer starting from where ADAM stopped. We do allow_f_increases=false to make the optimization automatically halt when near the minimum.","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"# Retrain using the LBFGS optimizer\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n                                        Optim.BFGS(initial_stepnorm=0.01),\n                                        callback = callback,\n                                        allow_f_increases = false)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"And then we use the callback with doplot=true to see the final plot:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_ode/","page":"Neural Ordinary Differential Equations","title":"Neural Ordinary Differential Equations","text":"callback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot=true)","category":"page"},{"location":"modules/LabelledArrays/#LabelledArrays.jl:-Arrays-with-Label-Goodness","page":"Home","title":"LabelledArrays.jl: Arrays with Label Goodness","text":"","category":"section"},{"location":"modules/LabelledArrays/","page":"Home","title":"Home","text":"LabelledArrays.jl is a package which provides arrays with labels, i.e. they are arrays which map, broadcast, and all of that good stuff, but their components are labelled. For instance, users can name the second component of an array to :second and retrieve it with A.second.","category":"page"},{"location":"modules/LabelledArrays/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/LabelledArrays/","page":"Home","title":"Home","text":"To install LabelledArrays.jl, use the Julia package manager:","category":"page"},{"location":"modules/LabelledArrays/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LabelledArrays\")","category":"page"},{"location":"modules/LabelledArrays/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/LabelledArrays/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#nonlinearsystemsolvers","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"solve(prob::NonlinearProblem,alg;kwargs)","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Solves for f(u)=0 in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This page is solely focused on the methods for nonlinear systems.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#Recommended-Methods","page":"Nonlinear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NewtonRaphson is a good choice for most problems. It is non-allocating on static arrays and thus really well-optimized for small systems, while for large systems it can make use of sparsity patterns for sparse automatic differentiation and sparse linear solving of very large systems. That said, as a classic Newton method, its stability region can be smaller than other methods. NLSolveJL's :trust_region method can be a good choice for high stability, along with CMINPACK.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"For a system which is very non-stiff (i.e., the condition number of the Jacobian is small, or the eigenvalues of the Jacobian are within a few orders of magnitude), then NLSolveJL's :anderson can be a good choice.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#Full-List-of-Methods","page":"Nonlinear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#NonlinearSolve.jl","page":"Nonlinear System Solvers","title":"NonlinearSolve.jl","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"These are the core solvers.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NewtonRaphson(;autodiff=true,chunk_size=12,diff_type=Val{:forward},linsolve=DEFAULT_LINSOLVE): A Newton-Raphson method with swappable nonlinear solvers and autodiff methods for high performance on large and sparse systems. When used on objects like static arrays, this method is non-allocating.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#SciMLNLSolve.jl","page":"Nonlinear System Solvers","title":"SciMLNLSolve.jl","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This is a wrapper package for importing solvers from other packages into this interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"]add SciMLNLSolve\nusing SciMLNLSolve","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"CMINPACK(): A wrapper for using the classic MINPACK method through MINPACK.jl\nNLSolveJL(): A wrapper for NLsolve.jl","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"NLSolveJL(;\n          method=:trust_region,\n          autodiff=:central,\n          store_trace=false,\n          extended_trace=false,\n          linesearch=LineSearches.Static(),\n          linsolve=(x, A, b) -> copyto!(x, A\\b),\n          factor = one(Float64),\n          autoscale=true,\n          m=10,\n          beta=one(Float64),\n          show_trace=false,\n       )","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"Choices for methods in NLSolveJL:","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":":fixedpoint: Fixed-point iteration\n:anderson: Anderson-accelerated fixed-point iteration\n:newton: Classical Newton method with an optional line search\n:trust_region: Trust region Newton method (the default choice)","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"For more information on these arguments, consult the NLsolve.jl documentation.","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/#Sundials.jl","page":"Nonlinear System Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"This is a wrapper package for the SUNDIALS C library, specifically the KINSOL nonlinear solver included in that ecosystem. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"]add Sundials\nusing Sundials","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"KINSOL: The KINSOL method of the SUNDIALS C library","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"KINSOL(;\n    linear_solver = :Dense,\n    jac_upper = 0,\n    jac_lower = 0,\n    userdata = nothing,\n)","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":"The choices for the linear solver are:","category":"page"},{"location":"modules/NonlinearSolve/solvers/NonlinearSystemSolvers/","page":"Nonlinear System Solvers","title":"Nonlinear System Solvers","text":":Dense: A dense linear solver\n:Band: A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense: A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand: A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:Diagonal: This method is specialized for diagonal Jacobians.\n:GMRES: A GMRES method. Recommended first choice Krylov method.\n:BCG: A biconjugate gradient method\n:PCG: A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR: A TFQMR method.\n:KLU: A sparse factorization method. Requires that the user specify a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/#SpeedMapping.jl","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"SpeedMapping accelerates the convergence of a mapping to a fixed point by the Alternating cyclic extrapolation algorithm which can also perform multivariate optimization based on the gradient function.","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"The SpeedMapping algorithm is called by SpeedMappingOpt()","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/#Installation:-OptimizationSpeedMapping.jl","page":"SpeedMapping.jl","title":"Installation: OptimizationSpeedMapping.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"To use this package, install the OptimizationSpeedMapping package:","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"import Pkg; Pkg.add(\"OptimizationSpeedMapping\")","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/#Global-Optimizer","page":"SpeedMapping.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/speedmapping/#Without-Constraint-Equations","page":"SpeedMapping.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"The method in SpeedMapping is performing optimization on problems without constraint equations. Lower and upper constraints set by lb and ub in the OptimizationProblem are optional.","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"If no AD backend is defined via OptimizationFunction the gradient is calculated via SpeedMapping's ForwardDiff AD backend.","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"The Rosenbrock function can be optimized using the SpeedMappingOpt() with and without bound as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/speedmapping/","page":"SpeedMapping.jl","title":"SpeedMapping.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(f, x0, _p)\nsol = solve(prob,SpeedMappingOpt())\n\nprob = OptimizationProblem(f, x0, _p;lb=[0.0,0.0], ub=[1.0,1.0])\nsol = solve(prob,SpeedMappingOpt())","category":"page"},{"location":"modules/MethodOfLines/howitworks/#hiw","page":"How it Works","title":"How it works","text":"","category":"section"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"MethodOfLines.jl makes heavy use of Symbolics.jl and SymbolicUtils.jl, namely it's rule matching features to recognize terms which require particular discretizations.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"See here for the highest level overview of the algorithm.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"Given your discretization and PDESystem, we take each independent variable defined on the space to be discretized and create a corresponding range. We then take each dependant variable and create an array of symbolic variables to represent it in its discretized form. This is stored in a DiscreteSpace object, a useful abstraction.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"We recognize boundary conditions, i.e whether they are on the upper or lower ends of the domain, or periodic here, and use this information to construct the interior of the domain for each equation here. Each PDE is matched to each dependant variable in this step by which variable is highest order in each PDE, with precedance given to time derivatives. This dictates which boundary conditions reduce the size of the interior for which PDE. This is done to ensure that there will be the same number of equations as discrete variable states, so that the system of equations is balanced.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"Next, the boundary conditions are discretized, creating an equation for each point on the boundary in terms of the discretized variables, replacing any space derivatives in the direction of the boundary with their upwind finite difference expressions. This is the place to look to see how this happens.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"After that, the system of PDEs is discretized creating a finite difference equation for each point in their interior. Specific terms are recognized, and the best implemented scheme for these terms dispatched. For example advection terms are discretized with the upwind scheme. There are also special schemes for the nonlinear laplacian and spherical laplacian. See here for how this term matching occurs, note that the order the generated rules are applied is important, with more specific rules applied first to avoid their terms being matched incorrectly by more general rules.  The SymbolicUtils.jl docs are a useful companion here. See here for the practical implementation of the finite difference schemes.","category":"page"},{"location":"modules/MethodOfLines/howitworks/","page":"How it Works","title":"How it Works","text":"Now we have a system of equations which are either ODEs, linear, or nonlinear equations and an equal number of unknowns. See here for the system that is generated for the Brusselator at low point count. The structure of the system is simplified with ModelingToolkit.structural_simplify, and then either an ODEProblem or NonlinearProblem is returned. Under the hood, the ODEProblem generates a fast semidiscretization, written in Julia with RuntimeGeneratedFunctions. See here for an example of the generated code for the Brusselator system at low point count. ","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#PDESystem","page":"PDESystem","title":"PDESystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"PDESystem is the common symbolic PDE specification for the SciML ecosystem. It is currently being built as a component of the ModelingToolkit ecosystem,","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Vision","page":"PDESystem","title":"Vision","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"The vision for the common PDE interface is that a user should only have to specify their PDE once, mathematically, and have instant access to everything as simple as a finite difference method with constant grid spacing, to something as complex as a distributed multi-GPU discontinuous Galerkin method.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"The key to the common PDE interface is a separation of the symbolic handling from the numerical world. All of the discretizers should not \"solve\" the PDE, but instead be a conversion of the mathematical specification to a numerical problem. Preferably, the transformation should be to another ModelingToolkit.jl AbstractSystem, but in some cases this cannot be done or will not be performant, so a SciMLProblem is the other choice.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"These elementary problems, such as solving linear systems Ax=b, solving nonlinear systems f(x)=0, ODEs, etc. are all defined by SciMLBase.jl, which then numerical solvers can all target these common forms. Thus someone who works on linear solvers doesn't necessarily need to be working on a discontinuous Galerkin or finite element library, but instead \"linear solvers that are good for matrices A with properties ...\" which are then accessible by every other discretization method in the common PDE interface.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"Similar to the rest of the AbstractSystem types, transformation and analysis functions will allow for simplifying the PDE before solving it, and constructing block symbolic functions like Jacobians.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Constructors","page":"PDESystem","title":"Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"PDESystem","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#ModelingToolkit.PDESystem","page":"PDESystem","title":"ModelingToolkit.PDESystem","text":"struct PDESystem <: AbstractMultivariateSystem\n\nA system of partial differential equations.\n\nFields\n\neqs\nThe equations which define the PDE\nbcs\nThe boundary conditions\ndomain\nThe domain for the independent variables.\nivs\nThe independent variables\ndvs\nThe dependent variables\nps\nThe parameters\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nconnector_type\ntype: type of the system\n\nsystems\nsystems: The internal systems. These are required to have unique names.\n\nname\nname: the name of the system\n\nExample\n\nusing ModelingToolkit\n\n@parameters x\n@variables t u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC=1\neq  = Dtt(u(t,x)) ~ C^2*Dxx(u(t,x))\n\n# Initial and boundary conditions\nbcs = [u(t,0) ~ 0.,# for all t > 0\n       u(t,1) ~ 0.,# for all t > 0\n       u(0,x) ~ x*(1. - x), #for all 0 < x < 1\n       Dt(u(0,x)) ~ 0. ] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ (0.0,1.0),\n           x ∈ (0.0,1.0)]\n\n@named pde_system = PDESystem(eq,bcs,domains,[t,x],[u])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Domains-(WIP)","page":"PDESystem","title":"Domains (WIP)","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"Domains are specifying by saying indepvar in domain, where indepvar is a single or a collection of independent variables, and domain is the chosen domain type. A 2-tuple can be used to indicate an Interval.  Thus forms for the indepvar can be like:","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"t ∈ (0.0,1.0)\r\n(t,x) ∈ UnitDisk()\r\n[v,w,x,y,z] ∈ VectorUnitBall(5)","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Domain-Types-(WIP)","page":"PDESystem","title":"Domain Types (WIP)","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"Interval(a,b): Defines the domain of an interval from a to b (requires explicit","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"import from DomainSets.jl, but a 2-tuple can be used instead)","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#discretize-and-symbolic_discretize","page":"PDESystem","title":"discretize and symbolic_discretize","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"The only functions which act on a PDESystem are the following:","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"discretize(sys,discretizer): produces the outputted AbstractSystem or SciMLProblem.\nsymbolic_discretize(sys,discretizer): produces a debugging symbolic description of the discretized problem.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Boundary-Conditions-(WIP)","page":"PDESystem","title":"Boundary Conditions (WIP)","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Transformations","page":"PDESystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Analyses","page":"PDESystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/#Discretizer-Ecosystem","page":"PDESystem","title":"Discretizer Ecosystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/#NeuralPDE.jl:-PhysicsInformedNN","page":"PDESystem","title":"NeuralPDE.jl: PhysicsInformedNN","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"NeuralPDE.jl defines the PhysicsInformedNN discretizer which uses a DiffEqFlux.jl neural network to solve the differential equation.","category":"page"},{"location":"modules/ModelingToolkit/systems/PDESystem/#DiffEqOperators.jl:-MOLFiniteDifference-(WIP)","page":"PDESystem","title":"DiffEqOperators.jl: MOLFiniteDifference (WIP)","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/PDESystem/","page":"PDESystem","title":"PDESystem","text":"DiffEqOperators.jl defines the MOLFiniteDifference discretizer which performs a finite difference discretization using the DiffEqOperators.jl stencils. These stencils make use of NNLib.jl for fast operations on semi-linear domains.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#ODE-Tableaus","page":"ODE Tableaus","title":"ODE Tableaus","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#Explicit-Runge-Kutta-Methods","page":"ODE Tableaus","title":"Explicit Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/","page":"ODE Tableaus","title":"ODE Tableaus","text":"constructEuler - Euler's 1st order method.\nconstructHuen() Huen's order 2 method.\nconstructRalston() - Ralston's order 2 method.\nconstructSSPRK22() - Explicit SSP method of order 2 using 2 stages.\nconstructKutta3 - Kutta's classic 3rd order method.\nconstructSSPRK33() - Explicit SSP method of order 3 using 3 stages.\nconstructSSPRK43() - Explicit SSP method of order 3 using 4 stages.\nconstructRK4 - The classic 4th order \"Runge-Kutta\" method.\nconstructRK438Rule - The classic 4th order \"3/8th's Rule\" method.\nconstructSSPRK104() - Explicit SSP method of order 4 using 10 stages.\nconstructBogakiShampine3() - Bogakai-Shampine's 2/3 method.\nconstructRKF4() - Runge-Kutta-Fehlberg 3/4.\nconstructRKF5() - Runge-Kutta-Fehlberg 4/5.\nconstructRungeFirst5() - Runge's first 5th order method.\nconstructCassity5() - Cassity's 5th order method.\nconstructLawson5() - Lawson's 5th order method.\nconstructLutherKonen5 - Luther-Konen's first 5th order method.\nconstructLutherKonen52() - Luther-Konen's second 5th order method.\nconstructLutherKonen53() - Luther-Konen's third 5th order method.\nconstructPapakostasPapaGeorgiou5() - Papakostas and PapaGeorgiou more stable order 5 method.\nconstructPapakostasPapaGeorgiou52() - Papakostas and PapaGeorgiou more efficient order 5 method.\nconstructTsitouras5() - Tsitouras's order 5 method.\nconstructBogakiShampine5() - Bogaki and Shampine's Order 5 method.\nconstructSharpSmart5() - Sharp and Smart's Order 5 method.\nconstructCashKarp() - Cash-Karp method 4/5.\nconstructDormandPrince() - Dormand-Prince 4/5.\nconstructButcher6() - Butcher's first order 6 method.\nconstructButcher62() - Butcher's second order 6 method.\nconstructButcher63() - Butcher's third order 6 method.\nconstructDormandPrince6() - Dormand-Prince's 5/6 method.\nconstructSharpVerner6() Sharp-Verner's 5/6 method.\nconstructVerner916() - Verner's more efficient order 6 method (1991).\nconstructVerner9162() - Verner's second more efficient order 6 method (1991).\nconstructVernerRobust6() - Verner's \"most robust\" order 6 method.\nconstructVernerEfficient6() - Verner's \"most efficient\" order 6 method.\nconstructPapakostas6() - Papakostas's order 6 method.\nconstructLawson6() - Lawson's order 6 method.\nconstructTsitourasPapakostas6() - Tsitouras and Papakostas's order 6 method.\nconstructDormandLockyerMcCorriganPrince6() - the Dormand-Lockyer-McCorrigan-Prince order 6 method.\nconstructTanakaKasugaYamashitaYazaki6A() - Tanaka-Kasuga-Yamashita-Yazaki order 6 method A.\nconstructTanakaKasugaYamashitaYazaki6B() - Tanaka-Kasuga-Yamashita-Yazaki order 6 method B.\nconstructTanakaKasugaYamashitaYazaki6C() - Tanaka-Kasuga-Yamashita-Yazaki order 6 method C.\nconstructTanakaKasugaYamashitaYazaki6D() - Tanaka-Kasuga-Yamashita-Yazaki order 6 method D.\nconstructMikkawyEisa() - Mikkawy and Eisa's order 6 method.\nconstructChummund6() - Chummund's first order 6 method.\nconstructChummund62() - Chummund's second order 6 method.\nconstructHuta6() - Huta's first order 6 method.\nconstructHuta62() - Huta's second order 6 method.\nconstructVerner6() - An old order 6 method attributed to Verner.\nconstructDverk() - The classic DVERK algorithm attributed to Verner.\nconstructClassicVerner6() - A classic Verner order 6 algorithm (1978).\nconstructButcher7() - Butcher's order 7 algorithm.\nconstructClassicVerner7()- A classic Verner order 7 algorithm (1978).\nconstructVernerRobust7() - Verner's \"most robust\" order 7 algorithm.\nconstructTanakaYamashitaStable7() - Tanaka-Yamashita more stable order 7 algorithm.\nconstructTanakaYamashitaEfficient7() - Tanaka-Yamashita more efficient order 7 algorithm.\nconstructSharpSmart7() - Sharp-Smart's order 7 algorithm.\nconstructSharpVerner7() - Sharp-Verner's order 7 algorithm.\nconstructVerner7() - Verner's \"most efficient\" order 7 algorithm.\nconstructVernerEfficient7() - Verner's \"most efficient\" order 7 algorithm.\nconstructClassicVerner8() - A classic Verner order 8 algorithm (1978).\nconstructCooperVerner8() - Cooper-Verner's first order 8 algorithm.\nconstructCooperVerner82() - Cooper-Verner's second order 8 algorithm.\nconstructTsitourasPapakostas8() - Tsitouras-Papakostas order 8 algorithm.\nconstructdverk78() - The classic order 8 DVERK algorithm.\nconstructEnrightVerner8() - Enright-Verner order 8 algorithm.\nconstructCurtis8() - Curtis' order 8 algorithm.\nconstructVerner8() - Verner's \"most efficient\" order 8 algorithm.\nconstructRKF8() - Runge-Kutta-Fehlberg Order 7/8 method.\nconstructDormandPrice8() - Dormand-Prince Order 7/8 method.\nconstructDormandPrince8_64bit() - Dormand-Prince Order 7/8 method. Coefficients are rational approximations good for 64 bits.\nconstructVernerRobust9() - Verner's \"most robust\" order 9 method.\nconstructVernerEfficient9() - Verner's \"most efficient\" order 9 method.\nconstructSharp9() - Sharp's order 9 method.\nconstructTsitouras9() - Tsitouras's first order 9 method.\nconstructTsitouras92() - Tsitouras's second order 9 method.\nconstructCurtis10() - Curtis' order 10 method.\nconstructOno10() - Ono's order 10 method.\nconstructFeagin10Tableau() - Feagin's order 10 method.\nconstructCurtis10() - Curtis' order 10 method.\nconstructBaker10() - Baker's order 10 method.\nconstructHairer10() Hairer's order 10 method.\nconstructFeagin12Tableau() - Feagin's order 12 method.\nconstructOno12() - Ono's order 12 method.\nconstructFeagin14Tableau() Feagin's order 14 method.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#Implicit-Runge-Kutta-Methods","page":"ODE Tableaus","title":"Implicit Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/","page":"ODE Tableaus","title":"ODE Tableaus","text":"constructImplicitEuler - The 1st order Implicit Euler method.\nconstructMidpointRule - The 2nd order Midpoint method.\nconstructTrapezoidalRule - The 2nd order Trapezoidal rule (2nd order LobattoIIIA)\nconstructLobattoIIIA4 - The 4th order LobattoIIIA\nconstructLobattoIIIB2 - The 2nd order LobattoIIIB\nconstructLobattoIIIB4 - The 4th order LobattoIIIB\nconstructLobattoIIIC2 - The 2nd order LobattoIIIC\nconstructLobattoIIIC4 - The 4th order LobattoIIIC\nconstructLobattoIIICStar2 - The 2nd order LobattoIIIC*\nconstructLobattoIIICStar4 - The 4th order LobattoIIIC*\nconstructLobattoIIID2 - The 2nd order LobattoIIID\nconstructLobattoIIID4 - The 4th order LobattoIIID\nconstructRadauIA3 - The 3rd order RadauIA\nconstructRadauIA5 - The 5th order RadauIA\nconstructRadauIIA3 - The 3rd order RadauIIA\nconstructRadauIIA5 - The 5th order RadauIIA","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#Tableau-Methods","page":"ODE Tableaus","title":"Tableau Methods","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/","page":"ODE Tableaus","title":"ODE Tableaus","text":"DiffEqDevTools.stability_region\nOrdinaryDiffEq.ODE_DEFAULT_TABLEAU","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.stability_region","page":"ODE Tableaus","title":"DiffEqDevTools.stability_region","text":"stability_region(z,tab::ODERKTableau)\n\nCalculates the stability function from the tableau at z. Stable if <1.\n\nr(z) = 1 + z bᵀ(I - zA)¹ e\n\nwhere e denotes a vector of ones.\n\n\n\n\n\nstability_region(tab::ODERKTableau; initial_guess=-3.0)\n\nCalculates the length of the stability region in the real axis.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#OrdinaryDiffEq.ODE_DEFAULT_TABLEAU","page":"ODE Tableaus","title":"OrdinaryDiffEq.ODE_DEFAULT_TABLEAU","text":"ODEDEFAULTTABLEAU\n\nSets the default tableau for the ODE solver. Currently Dormand-Prince 4/5.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#Explicit-Tableaus","page":"ODE Tableaus","title":"Explicit Tableaus","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/","page":"ODE Tableaus","title":"ODE Tableaus","text":"DiffEqDevTools.constructEuler\nDiffEqDevTools.constructRalston\nDiffEqDevTools.constructHeun\nDiffEqDevTools.constructKutta3\nOrdinaryDiffEq.constructBS3\nDiffEqDevTools.constructBogakiShampine3\nDiffEqDevTools.constructRK4\nDiffEqDevTools.constructRK438Rule\nDiffEqDevTools.constructRKF4\nDiffEqDevTools.constructRKF5\nDiffEqDevTools.constructCashKarp\nDiffEqDevTools.constructDormandPrince\nOrdinaryDiffEq.constructBS5\nDiffEqDevTools.constructPapakostasPapaGeorgiou5\nDiffEqDevTools.constructPapakostasPapaGeorgiou52\nDiffEqDevTools.constructTsitouras5\nDiffEqDevTools.constructLutherKonen5\nDiffEqDevTools.constructLutherKonen52\nDiffEqDevTools.constructLutherKonen53\nDiffEqDevTools.constructRungeFirst5\nDiffEqDevTools.constructLawson5\nDiffEqDevTools.constructSharpSmart5\nDiffEqDevTools.constructBogakiShampine5\nDiffEqDevTools.constructCassity5\nDiffEqDevTools.constructButcher6\nDiffEqDevTools.constructButcher62\nDiffEqDevTools.constructButcher63\nDiffEqDevTools.constructVernerRobust6\nDiffEqDevTools.constructTanakaKasugaYamashitaYazaki6A\nDiffEqDevTools.constructTanakaKasugaYamashitaYazaki6B\nDiffEqDevTools.constructTanakaKasugaYamashitaYazaki6C\nDiffEqDevTools.constructTanakaKasugaYamashitaYazaki6D\nDiffEqDevTools.constructHuta6\nDiffEqDevTools.constructHuta62\nDiffEqDevTools.constructVerner6\nDiffEqDevTools.constructDormandPrince6\nDiffEqDevTools.constructSharpVerner6\nDiffEqDevTools.constructVern6\nDiffEqDevTools.constructClassicVerner6\nDiffEqDevTools.constructChummund6\nDiffEqDevTools.constructChummund62\nDiffEqDevTools.constructPapakostas6\nDiffEqDevTools.constructLawson6\nDiffEqDevTools.constructTsitourasPapakostas6\nDiffEqDevTools.constructDormandLockyerMcCorriganPrince6\nDiffEqDevTools.constructVernerEfficient6\nDiffEqDevTools.constructMikkawyEisa\nDiffEqDevTools.constructVernerEfficient7\nDiffEqDevTools.constructClassicVerner7\nDiffEqDevTools.constructSharpVerner7\nDiffEqDevTools.constructTanakaYamashitaStable7\nDiffEqDevTools.constructSharpSmart7\nDiffEqDevTools.constructTanakaYamashitaEfficient7\nDiffEqDevTools.constructVernerRobust7\nOrdinaryDiffEq.constructTanYam7\nDiffEqDevTools.constructEnrightVerner7\nDiffEqDevTools.constructDormandPrince8\nDiffEqDevTools.constructRKF8\nDiffEqDevTools.constructCooperVerner8\nDiffEqDevTools.constructCooperVerner82\nDiffEqDevTools.constructTsitourasPapakostas8\nDiffEqDevTools.constructEnrightVerner8\nDiffEqDevTools.constructdverk78\nDiffEqDevTools.constructClassicVerner8\nDiffEqDevTools.constructDormandPrince8_64bit\nDiffEqDevTools.constructCurtis8\nOrdinaryDiffEq.constructTsitPap8\nDiffEqDevTools.constructSharp9\nDiffEqDevTools.constructTsitouras9\nDiffEqDevTools.constructTsitouras92\nDiffEqDevTools.constructVernerEfficient9\nOrdinaryDiffEq.constructVern9\nDiffEqDevTools.constructVerner916\nDiffEqDevTools.constructVerner9162\nDiffEqDevTools.constructVernerRobust9\nDiffEqDevTools.constructFeagin10\nDiffEqDevTools.constructFeagin10Tableau\nDiffEqDevTools.constructOno10\nDiffEqDevTools.constructCurtis10\nDiffEqDevTools.constructHairer10\nDiffEqDevTools.constructBaker10\nDiffEqDevTools.constructFeagin12\nDiffEqDevTools.constructOno12\nDiffEqDevTools.constructFeagin12Tableau\nDiffEqDevTools.constructFeagin14\nDiffEqDevTools.constructFeagin14Tableau","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructEuler","page":"ODE Tableaus","title":"DiffEqDevTools.constructEuler","text":"Euler's method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRalston","page":"ODE Tableaus","title":"DiffEqDevTools.constructRalston","text":"Ralston's Order 2 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructHeun","page":"ODE Tableaus","title":"DiffEqDevTools.constructHeun","text":"Heun's Order 2 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructKutta3","page":"ODE Tableaus","title":"DiffEqDevTools.constructKutta3","text":"Kutta's Order 3 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructBogakiShampine3","page":"ODE Tableaus","title":"DiffEqDevTools.constructBogakiShampine3","text":"constructBogakiShampine3()\n\nConstructs the tableau object for the Bogakai-Shampine Order 2/3 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRK4","page":"ODE Tableaus","title":"DiffEqDevTools.constructRK4","text":"Classic RK4 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRK438Rule","page":"ODE Tableaus","title":"DiffEqDevTools.constructRK438Rule","text":"Classic RK4 3/8's rule method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRKF4","page":"ODE Tableaus","title":"DiffEqDevTools.constructRKF4","text":"Runge-Kutta-Fehberg Order 4/3\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRKF5","page":"ODE Tableaus","title":"DiffEqDevTools.constructRKF5","text":"Runge-Kutta-Fehlberg Order 4/5 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCashKarp","page":"ODE Tableaus","title":"DiffEqDevTools.constructCashKarp","text":"constructCashKarp()\n\nConstructs the tableau object for the Cash-Karp Order 4/5 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructPapakostasPapaGeorgiou5","page":"ODE Tableaus","title":"DiffEqDevTools.constructPapakostasPapaGeorgiou5","text":"S.N. Papakostas and G. PapaGeorgiou higher error more stable\n\nA Family of Fifth-order Runge-Kutta Pairs, by S.N. Papakostas and G. PapaGeorgiou,  Mathematics of Computation,Volume 65, Number 215, July 1996, Pages 1165-1181.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructPapakostasPapaGeorgiou52","page":"ODE Tableaus","title":"DiffEqDevTools.constructPapakostasPapaGeorgiou52","text":"S.N. Papakostas and G. PapaGeorgiou less stable lower error  Strictly better than DP5\n\nA Family of Fifth-order Runge-Kutta Pairs, by S.N. Papakostas and G. PapaGeorgiou,  Mathematics of Computation,Volume 65, Number 215, July 1996, Pages 1165-1181.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTsitouras5","page":"ODE Tableaus","title":"DiffEqDevTools.constructTsitouras5","text":"Runge–Kutta pairs of orders 5(4) using the minimal set of simplifying assumptions,  by Ch. Tsitouras, TEI of Chalkis, Dept. of Applied Sciences, GR34400, Psahna, Greece.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLutherKonen5","page":"ODE Tableaus","title":"DiffEqDevTools.constructLutherKonen5","text":"Luther and Konen's First Order 5 Some Fifth-Order Classical Runge Kutta Formulas, H.A.Luther and H.P.Konen,  Siam Review, Vol. 3, No. 7, (Oct., 1965) pages 551-558.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLutherKonen52","page":"ODE Tableaus","title":"DiffEqDevTools.constructLutherKonen52","text":"Luther and Konen's Second Order 5 Some Fifth-Order Classical Runge Kutta Formulas, H.A.Luther and H.P.Konen,  Siam Review, Vol. 3, No. 7, (Oct., 1965) pages 551-558.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLutherKonen53","page":"ODE Tableaus","title":"DiffEqDevTools.constructLutherKonen53","text":"Luther and Konen's Third Order 5 Some Fifth-Order Classical Runge Kutta Formulas, H.A.Luther and H.P.Konen,  Siam Review, Vol. 3, No. 7, (Oct., 1965) pages 551-558.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRungeFirst5","page":"ODE Tableaus","title":"DiffEqDevTools.constructRungeFirst5","text":"Runge's First Order 5 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLawson5","page":"ODE Tableaus","title":"DiffEqDevTools.constructLawson5","text":"Lawson's 5th order scheme\n\nAn Order Five Runge Kutta Process with Extended Region of Stability, J. Douglas Lawson,  Siam Journal on Numerical Analysis, Vol. 3, No. 4, (Dec., 1966) pages 593-597\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructSharpSmart5","page":"ODE Tableaus","title":"DiffEqDevTools.constructSharpSmart5","text":"Explicit Runge-Kutta Pairs with One More Derivative Evaluation than the Minimum, by P.W.Sharp and E.Smart,  Siam Journal of Scientific Computing, Vol. 14, No. 2, pages. 338-348, March 1993.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructBogakiShampine5","page":"ODE Tableaus","title":"DiffEqDevTools.constructBogakiShampine5","text":"An Efficient Runge-Kutta (4,5) Pair by P.Bogacki and L.F.Shampine  Computers and Mathematics with Applications, Vol. 32, No. 6, 1996, pages 15 to 28\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCassity5","page":"ODE Tableaus","title":"DiffEqDevTools.constructCassity5","text":"Cassity's Order 5 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructButcher6","page":"ODE Tableaus","title":"DiffEqDevTools.constructButcher6","text":"Butcher's First Order 6 method\n\nOn Runge-Kutta Processes of High Order, by J. C. Butcher,  Journal of the Australian Mathematical Society, Vol. 4, (1964), pages 179 to 194\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructButcher62","page":"ODE Tableaus","title":"DiffEqDevTools.constructButcher62","text":"Butcher's Second Order 6 method\n\nOn Runge-Kutta Processes of High Order, by J. C. Butcher,  Journal of the Australian Mathematical Society, Vol. 4, (1964), pages 179 to 194\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructButcher63","page":"ODE Tableaus","title":"DiffEqDevTools.constructButcher63","text":"Butcher's Third Order 6\n\nOn Runge-Kutta Processes of High Order, by J. C. Butcher,  Journal of the Australian Mathematical Society, Vol. 4, (1964), pages 179 to 194\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerRobust6","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerRobust6","text":"From Verner's Website\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6A","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6A","text":"TanakaKasugaYamashitaYazaki Order 6 A\n\nOn the Optimization of Some Eight-stage Sixth-order Explicit Runge-Kutta Method,  by M. Tanaka, K. Kasuga, S. Yamashita and H. Yazaki,  Journal of the Information Processing Society of Japan, Vol. 34, No. 1 (1993), pages 62 to 74.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6B","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6B","text":"constructTanakaKasugaYamashitaYazaki Order 6 B\n\nOn the Optimization of Some Eight-stage Sixth-order Explicit Runge-Kutta Method,  by M. Tanaka, K. Kasuga, S. Yamashita and H. Yazaki,  Journal of the Information Processing Society of Japan, Vol. 34, No. 1 (1993), pages 62 to 74.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6C","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6C","text":"constructTanakaKasugaYamashitaYazaki Order 6 C\n\nOn the Optimization of Some Eight-stage Sixth-order Explicit Runge-Kutta Method,  by M. Tanaka, K. Kasuga, S. Yamashita and H. Yazaki,  Journal of the Information Processing Society of Japan, Vol. 34, No. 1 (1993), pages 62 to 74.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6D","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaKasugaYamashitaYazaki6D","text":"constructTanakaKasugaYamashitaYazaki Order 6 D\n\nOn the Optimization of Some Eight-stage Sixth-order Explicit Runge-Kutta Method,  by M. Tanaka, K. Kasuga, S. Yamashita and H. Yazaki,  Journal of the Information Processing Society of Japan, Vol. 34, No. 1 (1993), pages 62 to 74.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructHuta6","page":"ODE Tableaus","title":"DiffEqDevTools.constructHuta6","text":"Anton Hutas First Order 6 method\n\nUne amélioration de la méthode de Runge-Kutta-Nyström pour la résolution numérique des équations différentielles du premièr ordre, by Anton Huta, Acta Fac. Nat. Univ. Comenian Math., Vol. 1, pages 201-224 (1956).\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructHuta62","page":"ODE Tableaus","title":"DiffEqDevTools.constructHuta62","text":"Anton Hutas Second Order 6 method\n\nUne amélioration de la méthode de Runge-Kutta-Nyström pour la résolution numérique des équations différentielles du premièr ordre, by Anton Huta, Acta Fac. Nat. Univ. Comenian Math., Vol. 1, pages 201-224 (1956).\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVerner6","page":"ODE Tableaus","title":"DiffEqDevTools.constructVerner6","text":"Verner Order 5/6 method\n\nA Contrast of a New RK56 pair with DP56, by Jim Verner,  Department of Mathematics. Simon Fraser University, Burnaby, Canada, 2006.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructDormandPrince6","page":"ODE Tableaus","title":"DiffEqDevTools.constructDormandPrince6","text":"Dormand-Prince Order 5//6 method\n\nP.J. Prince and J. R. Dormand, High order embedded Runge-Kutta formulae, Journal of Computational and Applied Mathematics . 7 (1981), pp. 67-75.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructSharpVerner6","page":"ODE Tableaus","title":"DiffEqDevTools.constructSharpVerner6","text":"Sharp-Verner Order 5/6 method\n\nCompletely Imbedded Runge-Kutta Pairs, by P. W. Sharp and J. H. Verner,  SIAM Journal on Numerical Analysis, Vol. 31, No. 4. (Aug., 1994), pages. 1169 to 1190.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructClassicVerner6","page":"ODE Tableaus","title":"DiffEqDevTools.constructClassicVerner6","text":"EXPLICIT RUNGE-KUTFA METHODS WITH ESTIMATES OF THE LOCAL TRUNCATION ERROR\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructChummund6","page":"ODE Tableaus","title":"DiffEqDevTools.constructChummund6","text":"Chummund's First Order 6 method\n\nA three-dimensional family of seven-step Runge-Kutta methods of order 6, by G. M. Chammud (Hammud), Numerical Methods and programming, 2001, Vol.2, 2001, pages 159-166 (Advanced Computing Scientific journal published by the Research Computing Center of the Lomonosov Moscow State Univeristy)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructChummund62","page":"ODE Tableaus","title":"DiffEqDevTools.constructChummund62","text":"Chummund's Second Order 6 method\n\nA three-dimensional family of seven-step Runge-Kutta methods of order 6, by G. M. Chammud (Hammud), Numerical Methods and programming, 2001, Vol.2, 2001, pages 159-166 (Advanced Computing Scientific journal published by the Research Computing Center of the Lomonosov Moscow State Univeristy)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructPapakostas6","page":"ODE Tableaus","title":"DiffEqDevTools.constructPapakostas6","text":"Papakostas's Order 6\n\nOn Phase-Fitted modified Runge-Kutta Pairs of order 6(5), by Ch. Tsitouras and I. Th. Famelis,  International Conference of Numerical Analysis and Applied Mathematics, Crete, (2006)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLawson6","page":"ODE Tableaus","title":"DiffEqDevTools.constructLawson6","text":"Lawson's Order 6\n\nAn Order 6 Runge-Kutta Process with an Extended Region of Stability, by J. D. Lawson,  Siam Journal on Numerical Analysis, Vol. 4, No. 4 (Dec. 1967) pages 620-625.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTsitourasPapakostas6","page":"ODE Tableaus","title":"DiffEqDevTools.constructTsitourasPapakostas6","text":"Tsitouras-Papakostas's Order 6\n\nCheap Error Estimation for Runge-Kutta methods, by Ch. Tsitouras and S.N. Papakostas, Siam Journal on Scientific Computing, Vol. 20, Issue 6, Nov 1999.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructDormandLockyerMcCorriganPrince6","page":"ODE Tableaus","title":"DiffEqDevTools.constructDormandLockyerMcCorriganPrince6","text":"DormandLockyerMcCorriganPrince Order 6 Global Error Estimation\n\nGlobal Error estimation with Runge-Kutta triples, by J.R.Dormand, M.A.Lockyer, N.E.McCorrigan and P.J.Prince,  Computers and Mathematics with Applications, 18 (1989) pages 835-846.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerEfficient6","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerEfficient6","text":"From Verner's Website\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructMikkawyEisa","page":"ODE Tableaus","title":"DiffEqDevTools.constructMikkawyEisa","text":"Mikkawy-Eisa Order 6\n\nA general four-parameter non-FSAL embedded Runge–Kutta algorithm of orders 6 and 4 in seven stages,  by M.E.A. El-Mikkawy and M.M.M. Eisa,  Applied Mathematics and Computation, Vol. 143, No. 2, (2003) pages 259 to 267.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerEfficient7","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerEfficient7","text":"From Verner's website\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructClassicVerner7","page":"ODE Tableaus","title":"DiffEqDevTools.constructClassicVerner7","text":"EXPLICIT RUNGE-KUTFA METHODS WITH ESTIMATES OF THE LOCAL TRUNCATION ERROR\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructSharpVerner7","page":"ODE Tableaus","title":"DiffEqDevTools.constructSharpVerner7","text":"Completely Imbedded Runge-Kutta Pairs, by P.W.Sharp and J.H.Verner, Siam Journal on Numerical Analysis, Vol.31, No.4. (August 1994) pages 1169-1190.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaYamashitaStable7","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaYamashitaStable7","text":"On the Optimization of Some Nine-Stage Seventh-order Runge-Kutta Method, by M. Tanaka, S. Muramatsu and S. Yamashita, Information Processing Society of Japan, Vol. 33, No. 12 (1992) pages 1512-1526.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructSharpSmart7","page":"ODE Tableaus","title":"DiffEqDevTools.constructSharpSmart7","text":"Explicit Runge-Kutta Pairs with One More Derivative Evaluation than the Minimum, by P.W.Sharp and E.Smart,  Siam Journal of Scientific Computing, Vol. 14, No. 2, pages. 338-348, March 1993.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTanakaYamashitaEfficient7","page":"ODE Tableaus","title":"DiffEqDevTools.constructTanakaYamashitaEfficient7","text":"On the Optimization of Some Nine-Stage Seventh-order Runge-Kutta Method, by M. Tanaka, S. Muramatsu and S. Yamashita, Information Processing Society of Japan, Vol. 33, No. 12 (1992) pages 1512-1526.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerRobust7","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerRobust7","text":"From Verner's website\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructEnrightVerner7","page":"ODE Tableaus","title":"DiffEqDevTools.constructEnrightVerner7","text":"The Relative Efficiency of Alternative Defect Control Schemes for High-Order Continuous Runge-Kutta Formulas  W. H. Enright SIAM Journal on Numerical Analysis, Vol. 30, No. 5. (Oct., 1993), pp. 1419-1445.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructDormandPrince8","page":"ODE Tableaus","title":"DiffEqDevTools.constructDormandPrince8","text":"constructDormandPrice8()\n\nConstructs the tableau object for the Dormand-Prince Order 6/8 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRKF8","page":"ODE Tableaus","title":"DiffEqDevTools.constructRKF8","text":"constructRKF8()\n\nConstructs the tableau object for the Runge-Kutta-Fehlberg Order 7/8 method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCooperVerner8","page":"ODE Tableaus","title":"DiffEqDevTools.constructCooperVerner8","text":"Some Explicit Runge-Kutta Methods of High Order, by G. J. Cooper and J. H. Verner,  SIAM Journal on Numerical Analysis, Vol. 9, No. 3, (September 1972), pages 389 to 405\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCooperVerner82","page":"ODE Tableaus","title":"DiffEqDevTools.constructCooperVerner82","text":"Some Explicit Runge-Kutta Methods of High Order, by G. J. Cooper and J. H. Verner,  SIAM Journal on Numerical Analysis, Vol. 9, No. 3, (September 1972), pages 389 to 405\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTsitourasPapakostas8","page":"ODE Tableaus","title":"DiffEqDevTools.constructTsitourasPapakostas8","text":"Cheap Error Estimation for Runge-Kutta methods, by Ch. Tsitouras and S.N. Papakostas,  Siam Journal on Scientific Computing, Vol. 20, Issue 6, Nov 1999.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructEnrightVerner8","page":"ODE Tableaus","title":"DiffEqDevTools.constructEnrightVerner8","text":"The Relative Efficiency of Alternative Defect Control Schemes for High-Order Continuous Runge-Kutta Formulas  W. H. Enright SIAM Journal on Numerical Analysis, Vol. 30, No. 5. (Oct., 1993), pp. 1419-1445.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructdverk78","page":"ODE Tableaus","title":"DiffEqDevTools.constructdverk78","text":"Jim Verner's \"Maple\" (dverk78)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructClassicVerner8","page":"ODE Tableaus","title":"DiffEqDevTools.constructClassicVerner8","text":"EXPLICIT RUNGE-KUTFA METHODS WITH ESTIMATES OF THE LOCAL TRUNCATION ERROR\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructDormandPrince8_64bit","page":"ODE Tableaus","title":"DiffEqDevTools.constructDormandPrince8_64bit","text":"constructDormandPrice8_64bit()\n\nConstructs the tableau object for the Dormand-Prince Order 6/8 method with the approximated coefficients from the paper. This works until below 64-bit precision.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCurtis8","page":"ODE Tableaus","title":"DiffEqDevTools.constructCurtis8","text":"An Eighth Order Runge-Kutta process with Eleven Function Evaluations per Step, by A. R. Curtis,  Numerische Mathematik, Vol. 16, No. 3 (1970), pages 268 to 277\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructSharp9","page":"ODE Tableaus","title":"DiffEqDevTools.constructSharp9","text":"Journal of Applied Mathematics & Decision Sciences, 4(2), 183-192 (2000),  \"High order explicit Runge-Kutta pairs for ephemerides of the Solar System and the Moon\".\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTsitouras9","page":"ODE Tableaus","title":"DiffEqDevTools.constructTsitouras9","text":"Optimized explicit Runge-Kutta pairs of order 9(8), by Ch. Tsitouras,  Applied Numerical Mathematics, 38 (2001) 123-134.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTsitouras92","page":"ODE Tableaus","title":"DiffEqDevTools.constructTsitouras92","text":"Optimized explicit Runge-Kutta pairs of order 9(8), by Ch. Tsitouras,  Applied Numerical Mathematics, 38 (2001) 123-134.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerEfficient9","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerEfficient9","text":"From Verner's Webiste\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVerner916","page":"ODE Tableaus","title":"DiffEqDevTools.constructVerner916","text":"Verner 1991 First Order 5/6 method\n\nSome Ruge-Kutta Formula Pairs, by J.H.Verner,  SIAM Journal on Numerical Analysis, Vol. 28, No. 2 (April 1991), pages 496 to 511.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVerner9162","page":"ODE Tableaus","title":"DiffEqDevTools.constructVerner9162","text":"Verner 1991 Second Order 5/6 method\n\nSome Ruge-Kutta Formula Pairs, by J.H.Verner,  SIAM Journal on Numerical Analysis, Vol. 28, No. 2 (April 1991), pages 496 to 511.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructVernerRobust9","page":"ODE Tableaus","title":"DiffEqDevTools.constructVernerRobust9","text":"From Verner's Webiste\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructFeagin10","page":"ODE Tableaus","title":"DiffEqDevTools.constructFeagin10","text":"Feagin10 in Tableau form\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructOno10","page":"ODE Tableaus","title":"DiffEqDevTools.constructOno10","text":"Ono10\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructCurtis10","page":"ODE Tableaus","title":"DiffEqDevTools.constructCurtis10","text":"High-order Explicit Runge-Kutta Formulae, Their uses, and Limitations, A.R.Curtis, J. Inst. Maths Applics (1975) 16, 35-55.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructHairer10","page":"ODE Tableaus","title":"DiffEqDevTools.constructHairer10","text":"A Runge-Kutta Method of Order 10, E. Hairer, J. Inst. Maths Applics (1978) 21, 47-59.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructBaker10","page":"ODE Tableaus","title":"DiffEqDevTools.constructBaker10","text":"Tom Baker, University of Teeside. Part of RK-Aid http://www.scm.tees.ac.uk/users/u0000251/research/researcht.htm http://www.scm.tees.ac.uk/users/u0000251/j.r.dormand/t.baker/rk10921m/rk10921m\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructFeagin12","page":"ODE Tableaus","title":"DiffEqDevTools.constructFeagin12","text":"Tableau form of Feagin12\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructOno12","page":"ODE Tableaus","title":"DiffEqDevTools.constructOno12","text":"On the 25 stage 12th order explicit Runge-Kutta method, by Hiroshi Ono. Transactions of the Japan Society for Industrial and applied Mathematics, Vol. 6, No. 3, (2006) pages 177 to 186\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructFeagin14","page":"ODE Tableaus","title":"DiffEqDevTools.constructFeagin14","text":"Tableau form of Feagin14\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#Implicit-Tableaus","page":"ODE Tableaus","title":"Implicit Tableaus","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/tableaus/","page":"ODE Tableaus","title":"ODE Tableaus","text":"DiffEqDevTools.constructImplicitEuler\nDiffEqDevTools.constructMidpointRule\nDiffEqDevTools.constructTrapezoidalRule\nDiffEqDevTools.constructLobattoIIIA4\nDiffEqDevTools.constructLobattoIIIB2\nDiffEqDevTools.constructLobattoIIIB4\nDiffEqDevTools.constructLobattoIIIC2\nDiffEqDevTools.constructLobattoIIIC4\nDiffEqDevTools.constructLobattoIIICStar2\nDiffEqDevTools.constructLobattoIIICStar4\nDiffEqDevTools.constructLobattoIIID2\nDiffEqDevTools.constructLobattoIIID4\nDiffEqDevTools.constructGL2\nDiffEqDevTools.constructGL4\nDiffEqDevTools.constructGL6\nDiffEqDevTools.constructRadauIA3\nDiffEqDevTools.constructRadauIA5\nDiffEqDevTools.constructRadauIIA3\nDiffEqDevTools.constructRadauIIA5","category":"page"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructImplicitEuler","page":"ODE Tableaus","title":"DiffEqDevTools.constructImplicitEuler","text":"Implicit Euler Method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructMidpointRule","page":"ODE Tableaus","title":"DiffEqDevTools.constructMidpointRule","text":"Order 2 Midpoint Method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructTrapezoidalRule","page":"ODE Tableaus","title":"DiffEqDevTools.constructTrapezoidalRule","text":"Order 2 Trapezoidal Rule (LobattoIIIA2)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIIA4","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIIA4","text":"LobattoIIIA Order 4 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIIB2","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIIB2","text":"LobattoIIIB Order 2 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIIB4","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIIB4","text":"LobattoIIIB Order 4 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIIC2","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIIC2","text":"LobattoIIIC Order 2 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIIC4","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIIC4","text":"LobattoIIIC Order 4 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIICStar2","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIICStar2","text":"LobattoIIIC* Order 2 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIICStar4","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIICStar4","text":"LobattoIIIC* Order 4 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIID2","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIID2","text":"LobattoIIID Order 2 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructLobattoIIID4","page":"ODE Tableaus","title":"DiffEqDevTools.constructLobattoIIID4","text":"LobattoIIID Order 4 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructGL2","page":"ODE Tableaus","title":"DiffEqDevTools.constructGL2","text":"Gauss-Legendre Order 2.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructGL4","page":"ODE Tableaus","title":"DiffEqDevTools.constructGL4","text":"Gauss-Legendre Order 4.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructGL6","page":"ODE Tableaus","title":"DiffEqDevTools.constructGL6","text":"Gauss-Legendre Order 6.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRadauIA3","page":"ODE Tableaus","title":"DiffEqDevTools.constructRadauIA3","text":"RadauIA Order 3 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRadauIA5","page":"ODE Tableaus","title":"DiffEqDevTools.constructRadauIA5","text":"RadauIA Order 5 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRadauIIA3","page":"ODE Tableaus","title":"DiffEqDevTools.constructRadauIIA3","text":"RadauIIA Order 3 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDevDocs/internals/tableaus/#DiffEqDevTools.constructRadauIIA5","page":"ODE Tableaus","title":"DiffEqDevTools.constructRadauIIA5","text":"RadauIIA Order 5 method\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/#SDDE-Solvers","page":"SDDE Solvers","title":"SDDE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"solve(prob::AbstractSDDEProblem, alg; kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"Solves the SDDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/#Recommended-Methods","page":"SDDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"The recommended method for SDDE problems are the SDE algorithms. On SDEs you simply reuse the same algorithm as the SDE solver, and StochasticDelayDiffEq.jl will convert it to an SDDE solver. The recommendations for SDDE solvers match those of SDEs, except that only up to strong order 1 is recommended. Note too that order 1 is currently only attainable if there is no delay term in the diffusion function g: delays in the drift function f are compatible with first order convergence. Theoretical issues with higher order methods (1.5+) on SDDEs is currently unknown.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"Note that adaptive time stepping utilizes the same rejection sampling with memory technique as SDEs, but no proof of convergence is known for SDDEs.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/#Example","page":"SDDE Solvers","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"function hayes_modelf(du,u,h,p,t)\n    τ,a,b,c,α,β,γ = p\n    du .= a.*u .+ b .* h(p,t-τ) .+ c\nend\nfunction hayes_modelg(du,u,h,p,t)\n    τ,a,b,c,α,β,γ = p\n    du .= α.*u .+ γ\nend\nh(p,t) = (ones(1) .+ t);\ntspan = (0.,10.)\n\npmul = [1.0,-4.,-2.,10.,-1.3,-1.2, 1.1]\npadd = [1.0,-4.,-2.,10.,-0.0,-0.0, 0.1]\n\nprob = SDDEProblem(hayes_modelf, hayes_modelg, [1.], h, tspan, pmul; constant_lags = (pmul[1],));\nsol = solve(prob,RKMil())","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/#Neural-ODEs-on-GPUs","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Note that the differential equation solvers will run on the GPU if the initial condition is a GPU array. Thus, for example, we can define a neural ODE by hand that runs on the GPU (if no GPU is available, the calculation defaults back to the CPU):","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using DifferentialEquations, Flux, DiffEqFlux, SciMLSensitivity\n\nusing Random\nrng = Random.default_rng()\n\nmodel_gpu = Chain(Dense(2, 50, tanh), Dense(50, 2)) |> gpu\np, re = Flux.destructure(model_gpu)\ndudt!(u, p, t) = re(p)(u)\n\n# Simulation interval and intermediary points\ntspan = (0f0, 10f0)\ntsteps = 0f0:1f-1:10f0\n\nu0 = Float32[2.0; 0.0] |> gpu\nprob_gpu = ODEProblem(dudt!, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Or we could directly use the neural ODE layer function, like:","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"prob_neuralode_gpu = NeuralODE(gpu(model_gpu), tspan, Tsit5(), saveat = tsteps)","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"If one is using Lux.Chain, then the computation takes place on the GPU with f(x,p,st) if x, p and st are on the GPU. This commonly looks like:","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"import Lux\n\ndudt2 = Lux.Chain(x -> x.^3,\n            Lux.Dense(2,50,tanh),\n            Lux.Dense(50,2))\n\nu0 = Float32[2.; 0.] |> gpu\np, st = Lux.setup(rng, dudt2) .|> gpu\n\ndudt2_(u, p, t) = dudt2(u,p,st)[1]\n\n# Simulation interval and intermediary points\ntspan = (0f0, 10f0)\ntsteps = 0f0:1f-1:10f0\n\nprob_gpu = ODEProblem(dudt2_, u0, tspan, p)\n\n# Runs on a GPU\nsol_gpu = solve(prob_gpu, Tsit5(), saveat = tsteps)","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"or via the NeuralODE struct:","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"prob_neuralode_gpu = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nprob_neuralode_gpu(u0,p,st)","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/#Neural-ODE-Example","page":"Neural ODEs on GPUs","title":"Neural ODE Example","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"Here is the full neural ODE example. Note that we use the gpu function so that the same code works on CPUs and GPUs, dependent on using CUDA.","category":"page"},{"location":"modules/DiffEqFlux/examples/GPUs/","page":"Neural ODEs on GPUs","title":"Neural ODEs on GPUs","text":"using Flux, DiffEqFlux, Optimization, OptimizationFlux, Zygote, \n      OrdinaryDiffEq, Plots, CUDA, SciMLSensitivity, Random, ComponentArrays\nCUDA.allowscalar(false) # Makes sure no slow operations are occuring\n\n#rng for Lux.setup\nrng = Random.default_rng()\n# Generate Data\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\n# Make the data into a GPU-based array if the user has a GPU\node_data = gpu(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\ndudt2 = Chain(x -> x.^3, Dense(2, 50, tanh), Dense(50, 2)) |> gpu\nu0 = Float32[2.0; 0.0] |> gpu\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  gpu(prob_neuralode(u0,p))\nend\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n  display(l)\n  # plot current prediction against data\n  plt = scatter(tsteps, Array(ode_data[1,:]), label = \"data\")\n  scatter!(plt, tsteps, Array(pred[1,:]), label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, prob_neuralode.p)\nresult_neuralode = Optimization.solve(optprob,ADAM(0.05),callback = callback,maxiters = 300)","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Overview-of-DifferentialEquations.jl","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"The general workflow for using the package is as follows:","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Define a problem\nSolve the problem\nAnalyze the output","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Defining-Problems","page":"Overview of DifferentialEquations.jl","title":"Defining Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Problems are specified via a type interface. The problem types are designed to contain the necessary information to fully define their associated differential equation. Each problem type has a page explaining their problem type and the special features associated with them. For example, an ordinary differential equation is defined by","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"fracdudt = f(upt)","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"over some time interval tspan with some initial condition u0, and therefore the ODEProblem is defined by those components:","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"prob = ODEProblem(f,u0,tspan)\nprob = ODEProblem(f,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Note that the number types in the solution will match the types you designate in the problem. For example, if one uses Rational{BigInt} for specifying the timespan and BigFloat for specifying the initial condition, then the solution will solve using Rational{BigInt} for the timesteps and BigFloat for the independent variables. A wide variety of number types are compatible with the solvers such as complex numbers, unitful numbers (via Unitful.jl), decimals (via DecFP), dual numbers, and many more which may not have been tested yet (thanks to the power of multiple dispatch!). For information on type-compatibilty, please see the solver pages for the specific problems.","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Solving-the-Problems","page":"Overview of DifferentialEquations.jl","title":"Solving the Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Each type of differential equation has its own problem type which allow the solvers to dispatch to the right methods. The common interface for calling the solvers is:","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"sol = solve(prob,alg;kwargs)","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Into the command, one passes the differential equation problem that they defined prob, optionally choose an algorithm alg (a default is given if not chosen), and change the properties of the solver using keyword arguments. The common arguments which are accepted by most methods is defined in the common solver options manual page. The solver returns a solution object sol which hold all of the details for the solution.","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Analyzing-the-Solution","page":"Overview of DifferentialEquations.jl","title":"Analyzing the Solution","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"With the solution object, you do the analysis as you please! The solution type has a common interface which makes handling the solution similar between the different types of differential equations. Tools such as interpolations are seamlessly built into the solution interface to make analysis easy. This interface is described in the solution handling manual page.","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Plotting functionality is provided by a recipe to Plots.jl. To use plot solutions, simply call the plot(sol) and the plotter will generate appropriate plots. If save_everystep was used, the plotters can generate animations of the solutions to evolution equations using the animate(sol) command. Plots can be customized using all of the keyword arguments provided by Plots.jl. Please see Plots.jl's documentation for more information.","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Add-on-Tools","page":"Overview of DifferentialEquations.jl","title":"Add-on Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"One of the most compelling features of DifferentialEquations.jl is that the common solver interface allows one to build tools which are \"algorithm and problem agnostic\". For example, one of the provided tools allows for performing parameter estimation on ODEProblems. Since the solve interface is the same for the different algorithms, one can use any of the associated solving algorithms. This modular structure allows one to mix and match overarching analysis tools with specialized algorithms to one's problem, leading to high performance with a large feature base. Isn't that the promise of Julia just being fulfilled?","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/#Development-and-Testing-Tools","page":"Overview of DifferentialEquations.jl","title":"Development and Testing Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Lastly, one unique feature of DifferentialEquations.jl is the existence of algorithm development and testing functionality. This suite was designed by researchers in the field of numerical differential equations to both try out new ideas and distribute finalized results to large audiences. The tools for algorithm development allow for easy convergence testing, benchmarking, and higher order analysis (stability plotting, etc.). This is one of the reasons why DifferentialEquations.jl contains many algorithms which are unique and the results of recent publications! Please check out the developer documentation for more information on using the development tools.","category":"page"},{"location":"modules/DiffEqDocs/basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Note that DifferentialEquations.jl allows for distributed development, meaning that algorithms which \"plug-into ecosystem\" don't have to be a part of the major packages. If you are interested in adding your work to the ecosystem, checkout the developer documentation for more information.","category":"page"},{"location":"modules/SciMLBase/#The-SciML-Common-Interface-for-Julia-Equation-Solvers","page":"Home","title":"The SciML Common Interface for Julia Equation Solvers","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"The SciML common interface ties together the numerical solvers of the Julia package ecosystem into a single unified interface. It is designed for maximal efficiency and parallelism, while incorporating essential features for large-scale scientific machine learning such as differentiability, composability, and sparsity.","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"This documentation is made to pool together the docs of the various SciML libraries to paint the overarching picture, establish development norms, and document the shared/common functionality.","category":"page"},{"location":"modules/SciMLBase/#Domains-of-SciML","page":"Home","title":"Domains of SciML","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"The SciML common interface covers the following domains:","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"Linear systems (LinearProblem)\nDirect methods for dense and sparse\nIterative solvers with preconditioning\nNonlinear Systems (NonlinearProblem)\nSystems of nonlinear equations\nScalar bracketing systems\nIntegrals (quadrature) (QuadratureProblem)\nDifferential Equations\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (AbstractDEProblems with callbacks)\nOptimization (OptimizationProblem)\nNonlinear (constrained) optimization\n(Stochastic/Delay/Differential-Algebraic) Partial Differential Equations (PDESystem)\nFinite difference and finite volume methods\nInterfaces to finite element methods\nPhysics-Informed Neural Networks (PINNs)\nIntegro-Differential Equations\nFractional Differential Equations","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"The SciML common interface also includes ModelingToolkit.jl for defining such systems symbolically, allowing for optimizations like automated generation of parallel code, symbolic simplification, and generation of sparsity patterns.","category":"page"},{"location":"modules/SciMLBase/#Extended-SciML-Domain","page":"Home","title":"Extended SciML Domain","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"In addition to the purely numerical representations of mathematical objects, there are also sets of problem types associated with common mathematical algorithms. These are:","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"Data-driven modeling\nDiscrete-time data-driven dynamical systems (DiscreteDataDrivenProblem)\nContinuous-time data-driven dynamical systems (ContinuousDataDrivenProblem)\nSymbolic regression (DirectDataDrivenProblem)\nUncertainty quantification and expected values (ExpectationProblem)","category":"page"},{"location":"modules/SciMLBase/#Inverse-Problems,-Parameter-Estimation,-and-Structural-Identification","page":"Home","title":"Inverse Problems, Parameter Estimation, and Structural Identification","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"We note that parameter estimation and inverse problems are solved directly on their constituant problem types using tools like DiffEqFlux.jl. Thus for example, there is no ODEInverseProblem, and instead ODEProblem is used to find the parameters p that solve the inverse problem.","category":"page"},{"location":"modules/SciMLBase/#Common-Interface-High-Level","page":"Home","title":"Common Interface High Level","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"The SciML interface is common as the usage of arguments is standardized across all of the problem domains. Underlying high level ideas include:","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"All domains use the same interface of defining a AbstractSciMLProblem which is then solved via solve(prob,alg;kwargs), where alg is a AbstractSciMLAlgorithm. The keyword argument namings are standardized across the organization.\nAbstractSciMLProblems are generally defined by a SciMLFunction which can define extra details about a model function, such as its analytical Jacobian, its sparsity patterns and so on.\nThere is an organization-wide method for defining linear and nonlinear solvers used within other solvers, giving maximum control of performance to the user.\nTypes used within the packages are defined by the input types. For example, packages attempt to internally use the type of the initial condition as the type for the state within differential equation solvers.\nsolve calls should be thread-safe and parallel-safe.\ninit(prob,alg;kwargs) returns an iterator which allows for directly iterating over the solution process\nHigh performance is key. Any performance that is not at the top level is considered a bug and should be reported as such.\nAll functions have an in-place and out-of-place form, where the in-place form is made to utilize mutation for high performance on large-scale problems and the out-of-place form is for compatibility with tooling like static arrays and some reverse-mode automatic differentiation systems.","category":"page"},{"location":"modules/SciMLBase/#User-Facing-Solver-Libraries","page":"Home","title":"User-Facing Solver Libraries","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"DifferentialEquations.jl\nMulti-package interface of high performance numerical solvers of differential equations\nModelingToolkit.jl\nThe symbolic modeling package which implements the SciML symbolic common interface.\nLinearSolve.jl\nMulti-package interface for specifying linear solvers (direct, sparse, and iterative), along with tools for caching and preconditioners for use in large-scale modeling.\nNonlinearSolve.jl\nHigh performance numerical solving of nonlinear systems.\nQuadrature.jl\nMulti-package interface for high performance, batched, and parallelized numerical quadrature.\nOptimization.jl\nMulti-package interface for numerical solving of optimization problems.\nNeuralPDE.jl\nPhysics-Informed Neural Network (PINN) package for transforming partial differential equations into optimization problems.\nDiffEqOperators.jl\nAutomated finite difference method (FDM) package for transforming partial differential equations into nonlinear problems and ordinary differential equations.\nDiffEqFlux.jl\nHigh level package for scientific machine learning applications, such as neural and universal differential equations, solving of inverse problems, parameter estimation, nonlinear optimal control, and more.\nDataDrivenDiffEq.jl\nMulti-package interface for data-driven modeling, Koopman dynamic mode decomposition, symbolic regression/sparsification, and automated model discovery.\nDiffEqUncertainty.jl\nExtension to the dynamical modeling tools for performing uncertainty quantification and calculating expectations.","category":"page"},{"location":"modules/SciMLBase/#Interface-Implementation-Libraries","page":"Home","title":"Interface Implementation Libraries","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"SciMLBase.jl\nThe core package defining the interface which is consumed by the modeling and solver packages.\nDiffEqBase.jl\nThe core package defining the extended interface which is consumed by the differential equation solver packages.\nDiffEqSensitivity.jl\nA package which pools together the definition of derivative overloads to define the common sensealg automatic differentiation interface.\nDiffEqNoiseProcess.jl\nA package which defines the stochastic AbstractNoiseProcess interface for the SciML ecosystem.\nRecursiveArrayTools.jl\nA package which defines the underlying AbstractVectorOfArray structure used as the output for all time series results.\nArrayInterface.jl\nThe package which defines the extended AbstractArray interface employed throughout the SciML ecosystem.","category":"page"},{"location":"modules/SciMLBase/#Using-Facing-Modeling-Libraries","page":"Home","title":"Using-Facing Modeling Libraries","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"There are too many to name here and this will be populated when there is time!","category":"page"},{"location":"modules/SciMLBase/#Flowchart-Example-for-PDE-Constrained-Optimal-Control","page":"Home","title":"Flowchart Example for PDE-Constrained Optimal Control","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"The following example showcases how the pieces of the common interface connect to solve a problem that mixes inference, symbolics, and numerics.","category":"page"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"(Image: )","category":"page"},{"location":"modules/SciMLBase/#External-Binding-Libraries","page":"Home","title":"External Binding Libraries","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"diffeqr\nSolving differential equations in R using DifferentialEquations.jl with ModelingToolkit for JIT compilation and GPU-acceleration\ndiffeqpy\nSolving differential equations in Python using DifferentialEquations.jl","category":"page"},{"location":"modules/SciMLBase/#Solver-Libraries","page":"Home","title":"Solver Libraries","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"There are too many to name here. Check out the SciML Organization Github Page for details.","category":"page"},{"location":"modules/SciMLBase/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/SciMLBase/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums (look for the modelingtoolkit tag\nSee also SciML Community page","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/#Derivative-based-Global-Sensitivity-Measure-Method","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"struct DGSM <: GSAMethod\n    crossed::Bool \nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"The keyword arguments for DGSM are as follows:","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"crossed: A string(True/False) which act as indicator for computation of DGSM crossed indices. Defaults to false.","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/#Method-Details","page":"Derivative based Global Sensitivity Measure Method","title":"Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"The DGSM method takes a probability distribution for each of the  parameters and samples are obtained from the distributions to create  random parameter sets. Derivatives of the function being analysed are  then computed at the sampled parameters and specific statistics of those  derivatives are used. The paper by Sobol and Kucherenko  discusses the relationship between the DGSM results, tao and  sigma and the Morris elementary effects and Sobol Indices.","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/#API","page":"Derivative based Global Sensitivity Measure Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"function gsa(f, method::DGSM, dist::AbstractArray; samples::Int, kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"dist: Array of distribution of respective variables. E.g. dist = [Normal(5,6),Uniform(2,3)] for two variables.","category":"page"},{"location":"modules/GlobalSensitivity/methods/dgsm/#Example","page":"Derivative based Global Sensitivity Measure Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/dgsm/","page":"Derivative based Global Sensitivity Measure Method","title":"Derivative based Global Sensitivity Measure Method","text":"using GlobalSensitivity, Test, Distributions\n\nsamples = 2000000\n\nf1(x) = x[1] + 2*x[2] + 6.00*x[3]\ndist1 = [Uniform(4,10),Normal(4,23),Beta(2,3)]\nb =  gsa(f1,DGSM(),dist1,samples=samples)","category":"page"},{"location":"modules/Surrogates/wendland/#Wendland-Surrogate","page":"Wendland","title":"Wendland Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"The Wendland surrogate is a compact surrogate: it allocates much less memory then other surrogates. The coefficients are found using an iterative solver.","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"f = x - exp(-x^2)","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"using Surrogates\nusing Plots","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"n = 40\nlower_bound = 0.0\nupper_bound = 1.0\nf = x -> exp(-x^2)\nx = sample(n,lower_bound,upper_bound,SobolSample())\ny = f.(x)","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/wendland/#Building-Surrogate","page":"Wendland","title":"Building Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"The choice of the right parameter is especially important here: a slight change in ϵ would produce a totally different fit. Try it yourself with this function!","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"my_eps = 0.5\nwend = Wendland(x,y,lower_bound,upper_bound,eps=my_eps)","category":"page"},{"location":"modules/Surrogates/wendland/","page":"Wendland","title":"Wendland","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(wend, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/#Global-Identifiability-of-Differential-Models","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"In this tutorial, let us cover an example problem of querying the ODE for globally identifiable parameters.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/#Input-System","page":"Global Identifiability of Differential Models","title":"Input System","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"Let us consider the following four-dimensional model with two outputs:","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"begincasesx(t) = lm - d  x(t) - beta  x(t)  v(t)\n    y(t) = beta  x(t)  v(t) - a  y(t)\n    v(t) = k  y(t) - u  v(t)\n    w(t) = c  x(t)  y(t)  w(t) - c  q  y(t)  w(t) - b  w(t)\n    z(t) = c  q  y(t)  w(t) - h  z(t)\n    y_1(t) = w(t)\n    y_2(t) = z(t)endcases","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"This model describes HIV dynamics[1]. Let us run a global identifiability check on this model to get the result with probability of correctness being p=0.99. To do this, we will use assess_identifiability(ode, p) function.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"Global identifiability needs information about local identifiability first, hence the function we chose here will take care of that extra step for us.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"using StructuralIdentifiability\n\node = @ODEmodel(\n    x'(t) = lm - d * x(t) - beta * x(t) * v(t),\n    y'(t) = beta * x(t) * v(t) - a * y(t),\n    v'(t) = k * y(t) - u * v(t),\n    w'(t) = c * x(t) * y(t) * w(t) - c * q * y(t) * w(t) - b * w(t),\n    z'(t) = c * q * y(t) * w(t) - h * z(t),\n    y1(t) = w(t),\n    y2(t) = z(t)\n)\nglobal_id = assess_identifiability(ode, 0.99)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"We also note that it's usually inexpensive to obtain the result with higher probability of correctness. For example, taking p=0.9999 in the system above will result only in a slight slowdown.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/#Note-on-the-probability-of-correctness","page":"Global Identifiability of Differential Models","title":"Note on the probability of correctness","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"Currently, the probability of correctness does not include the probability of correctness of the modular reconstruction for Groebner bases.  This probability is ensured by additional check modulo a large prime and can be neglected for practical purposes. However, in the future versions, we plan to  eliminate this possible error.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/global_identifiability/","page":"Global Identifiability of Differential Models","title":"Global Identifiability of Differential Models","text":"[1]: D. Wodarz, M. Nowak, Specific therapy regimes could lead to long-term immunological control of HIV, PNAS December 7, 1999 96 (25) 14464-14469;","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/#systems","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs for Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"In this example, we will solve the PDE system:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\n_t u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with the initial conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"and the boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with physics-informed neural networks.","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/#Solution","page":"Defining Systems of PDEs","title":"Solution","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),\n       Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),\n       0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]\n\nbcs = [u1(0,x) ~ sin(pi*x),\n       u2(0,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       u1(t,0) ~ 0.,\n       u2(t,0) ~ exp(-t),\n       u1(t,1) ~ 0.,\n       u2(t,1) ~ -exp(-t)]\n\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain =[Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1)) for _ in 1:3]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs,bcs,domains,[t,x],[u1(t, x),u2(t, x),u3(t, x)])\nprob = discretize(pdesystem,discretization)\nsym_prob = symbolic_discretize(pdesystem,discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob,BFGS(); callback = callback, maxiters=5000)\n\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/#Direct-Construction-via-symbolic_discretize","page":"Defining Systems of PDEs","title":"Direct Construction via symbolic_discretize","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"One can take apart the pieces and reassemble the loss functions using the symbolic_discretize interface. Here is an example using the components from symbolic_discretize to fully reproduce the discretize optimization:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t,x)) ~ Dxx(u1(t,x)) + u3(t,x)*sin(pi*x),\n       Dtt(u2(t,x)) ~ Dxx(u2(t,x)) + u3(t,x)*cos(pi*x),\n       0. ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x) - exp(-t)]\n\nbcs = [u1(0,x) ~ sin(pi*x),\n       u2(0,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       u1(t,0) ~ 0.,\n       u2(t,0) ~ exp(-t),\n       u1(t,1) ~ 0.,\n       u2(t,1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain =[Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1)) for _ in 1:3]\n@named pdesystem = PDESystem(eqs,bcs,domains,[t,x],[u1(t, x),u2(t, x),u3(t, x)])\n\nstrategy = NeuralPDE.QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bc_loss_functions))\n    return false\nend\n\nloss_functions =  [pde_loss_functions;bc_loss_functions]\n\nfunction loss_function(θ,p)\n    sum(map(l->l(θ) ,loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(prob,OptimizationOptimJL.BFGS(); callback = callback, maxiters=5000)","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/#Solution-Representation","page":"Defining Systems of PDEs","title":"Solution Representation","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Now let's perform some analysis for both the symbolic_discretize and discretize APIs:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using Plots\n\nphi = discretization.phi\nts,xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:3]\n\nanalytic_sol_func(t,x) = [exp(-t)*sin(pi*x), exp(-t)*cos(pi*x), (1+pi^2)*exp(-t)]\nu_real  = [[analytic_sol_func(t,x)[i] for t in ts for x in xs] for i in 1:3]\nu_predict  = [[phi[i]([t,x],minimizers_[i])[1] for t in ts  for x in xs] for i in 1:3]\ndiff_u = [abs.(u_real[i] .- u_predict[i] ) for i in 1:3]\nfor i in 1:3\n    p1 = plot(ts, xs, u_real[i],linetype=:contourf,title = \"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i],linetype=:contourf,title = \"predict\");\n    p3 = plot(ts, xs, diff_u[i],linetype=:contourf,title = \"error\");\n    plot(p1,p2,p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq1)","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq2)","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq3)","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Notice here that the solution is represented in the OptimizationSolution with u as the parameters for the trained neural network. But, for the case where the neural network is from Lux.jl, it's given as a ComponentArray where res.u.depvar.x corresponds to the result for the neural network corresponding to the dependent variable x, i.e. res.u.depvar.u1  are the trained parameters for phi[1] in our example. For simpler indexing, you can use  res.u.depvar[:u1] or res.u.depvar[Symbol(:u,1)] as shown here.","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Subsetting the array also works, but is inelegant.","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(If param_estim == true, then res.u.p are the fit parameters)","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"If Flux.jl is used, then subsetting the array is required. This looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"init_params = [Flux.destructure(c)[1] for c in chain]\nacum =  [0;accumulate(+, length.(init_params))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\nminimizers_ = [res.minimizer[s] for s in sep]","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/#Note:-Solving-Matrices-of-PDEs","page":"Defining Systems of PDEs","title":"Note: Solving Matrices of PDEs","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Also, in addition to vector systems, we can use the matrix form of PDEs:","category":"page"},{"location":"modules/NeuralPDE/tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using ModelingToolkit, NeuralPDE\n@parameters x y\n@variables u[1:2,1:2](..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# Initial and boundary conditions\nbcs = [u[1](x,0) ~ x, u[2](x,0) ~ 2, u[3](x,0) ~ 3, u[4](x,0) ~ 4]\n\n# matrix PDE\neqs  = @. [(Dxx(u_(x,y)) + Dyy(u_(x,y))) for u_ in u] ~ -sin(pi*x)*sin(pi*y)*[0 1; 0 1]\n\nsize(eqs)","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"(Image: SurrogatesLogo)","category":"page"},{"location":"modules/Surrogates/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"A surrogate model is an approximation method that mimics the behavior of a computationally expensive simulation. In more mathematical terms: suppose we are attempting to optimize a function  f(p), but each calculation of  f is very expensive. It may be the case that we need to solve a PDE for each point or use advanced numerical linear algebra machinery, which is usually costly. The idea is then to develop a surrogate model  g which approximates  f by training on previous data collected from evaluations of  f. The construction of a surrogate model can be seen as a three-step process:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Sample selection\nConstruction of the surrogate model\nSurrogate optimization","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"The sampling methods are super important for the behavior of the Surrogate. At the moment they are:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Grid sample\nUniform sample\nSobol sample\nLatin Hypercube sample\nLow discrepancy sample","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"The available surrogates are:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Linear\nRadial Basis\nKriging\nCustom Kriging provided with Stheno\nNeural Network\nSupport Vector Machine\nRandom Forest\nSecond Order Polynomial\nInverse Distance","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"After the surrogate is built, we need to optimize it with respect to some objective function. That is, simultaneously looking for a minimum and sampling the most unknown region. The available optimization methods are:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Stochastic RBF (SRBF)\nLower confidence bound strategy (LCBS)\nExpected improvement (EI)\nDynamic coordinate search (DYCORS)","category":"page"},{"location":"modules/Surrogates/#Multi-output-Surrogates","page":"Overview","title":"Multi-output Surrogates","text":"","category":"section"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"In certain situations, the function being modeled may have a multi-dimensional output space. In such a case, the surrogate models can take advantage of correlations between the observed output variables to obtain more accurate predictions.","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"When constructing the original surrogate, each element of the passed y vector should itself be a vector. For example, the following y are all valid.","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"using Surrogates\nusing StaticArrays\n\nx = sample(5, [0.0; 0.0], [1.0; 1.0], SobolSample())\nf_static = (x) -> StaticVector(x[1], log(x[2]*x[1]))\nf = (x) -> [x, log(x)/2]\n\ny = f_static.(x)\ny = f.(x)","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Currently, the following are implemented as multi-output surrogates:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Radial Basis\nNeural Network (via Flux)\nSecond Order Polynomial\nInverse Distance\nCustom Kriging (via Stheno)","category":"page"},{"location":"modules/Surrogates/#Gradients","page":"Overview","title":"Gradients","text":"","category":"section"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"The surrogates implemented here are all automatically differentiable via Zygote. Because of this property, surrogates are useful models for processes which aren't explicitly differentiable, and can be used as layers in, for instance, Flux models.","category":"page"},{"location":"modules/Surrogates/#Installation","page":"Overview","title":"Installation","text":"","category":"section"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"Surrogates is registered in the Julia General Registry. In the REPL:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"]add Surrogates","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"You can obtain the current master with:","category":"page"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"]add https://github.com/JuliaDiffEq/Surrogates.jl#master","category":"page"},{"location":"modules/Surrogates/#Quick-example","page":"Overview","title":"Quick example","text":"","category":"section"},{"location":"modules/Surrogates/","page":"Overview","title":"Overview","text":"using Surrogates\nnum_samples = 10\nlb = 0.0\nub = 10.0\n\n#Sampling\nx = sample(num_samples,lb,ub,SobolSample())\nf = x-> log(x)*x^2+x^3\ny = f.(x)\n\n#Creating surrogate\nalpha = 2.0\nn = 6\nmy_lobachevsky = LobachevskySurrogate(x,y,lb,ub,alpha=alpha,n=n)\n\n#Approximating value at 5.0\nvalue = my_lobachevsky(5.0)\n\n#Adding more data points\nsurrogate_optimize(f,SRBF(),lb,ub,my_lobachevsky,UniformSample())\n\n#New approximation\nvalue = my_lobachevsky(5.0)","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"#Welded beam function","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"The welded beam function is defined as: f(hlt) = sqrtfraca^2 + b^2 + ablsqrt025(l^2+(h+t)^2) With: a = frac6000sqrt2hl b = frac6000(14 + 05l)*sqrt025(l^2+(h+t)^2)2*0707hl(fracl^212+025*(h+t)^2)","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"It has 3 dimension.","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"function f(x)\n    h = x[1]\n    l = x[2]\n    t = x[3]\n    a = 6000/(sqrt(2)*h*l)\n    b = (6000*(14+0.5*l)*sqrt(0.25*(l^2+(h+t)^2)))/(2*(0.707*h*l*(l^2/12 + 0.25*(h+t)^2)))\n    return (sqrt(a^2+b^2 + l*a*b))/(sqrt(0.25*(l^2+(h+t)^2)))\nend","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"n = 300\nd = 3\nlb = [0.125,5.0,5.0]\nub = [1.,10.,10.]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"modules/Surrogates/welded_beam/","page":"Welded beam function","title":"Welded beam function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nprint(\"MSE Radial: $mse_rad\")\n\nmy_krig = Kriging(x,y,lb,ub)\ny_krig = my_krig.(x_test)\nmse_krig = norm(y_true - y_krig,2)/n_test\nprint(\"MSE Kriging: $mse_krig\")\n\nmy_loba = LobachevskySurrogate(x,y,lb,ub)\ny_loba = my_loba.(x_test)\nmse_rad = norm(y_true - y_loba,2)/n_test\nprint(\"MSE Lobachevsky: $mse_rad\")","category":"page"},{"location":"modules/ModelingToolkit/comparison/#Comparison-of-ModelingToolkit-vs-Equation-Based-and-Block-Modeling-Languages","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/#Comparison-Against-Modelica","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison Against Modelica","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"Both Modelica and ModelingToolkit.jl are acausal modeling languages.\nModelica is a language with many different implementations, such as Dymola and OpenModelica, which have differing levels of performance and can give different results on the same model. Many of the commonly used Modelica compilers are not open source. ModelingToolkit.jl is a language with a single canonical open source implementation.\nAll current Modelica compiler implementations are fixed and not extendable by the users from the Modelica language itself. For example, the Dymola compiler shares its symbolic processing pipeline which is roughly equivalent to the dae_index_lowering and structural_simplify of ModelingToolkit.jl. ModelingToolkit.jl is an open and hackable transformation system which allows users to add new non-standard transformations and control the order of application.\nModelica is a declarative programming language. ModelingToolkit.jl is a declarative symbolic modeling language used from within the Julia programming language. Its programming language semantics, such as loop constructs and conditionals, can be used to more easily generate models.\nModelica is an object-oriented single dispatch language. ModelingToolkit.jl, built on Julia, uses multiple dispatch extensively to simplify code.\nMany Modelica compilers supply a GUI. ModelingToolkit.jl does not.\nModelica can be used to simulate ODE and DAE systems. ModelingToolkit.jl has a much more expansive set of system types, including nonlinear systems, SDEs, PDEs, and more.","category":"page"},{"location":"modules/ModelingToolkit/comparison/#Comparison-Against-Simulink","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison Against Simulink","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"Simulink is a causal modeling environment, whereas ModelingToolkit.jl is an acausal modeling environment. For an overview of the differences, consult academic reviews such as this one. In this sense, ModelingToolkit.jl is more similar to the Simscape sub-environment.\nSimulink is used from MATLAB while ModelingToolkit.jl is used from Julia. Thus any user defined functions have the performance of their host language. For information on the performance differences between Julia and MATLAB, consult open benchmarks which demonstrate Julia as an order of magnitude or more faster in many cases due to its JIT compilation.\nSimulink uses the MATLAB differential equation solvers while ModelingToolkit.jl uses DifferentialEquations.jl. For a systematic comparison between the solvers, consult open benchmarks which demonstrate two orders of magnitude performance advantage for the native Julia solvers across many benchmark problems.\nSimulink comes with a Graphical User Interface (GUI), ModelingToolkit.jl does not.\nSimulink is a proprietary software, meaning users cannot actively modify or extend the software. ModelingToolkit.jl is built in Julia and used in Julia, where users can actively extend and modify the software interactively in the REPL and contribute to its open source repositories.\nSimulink covers ODE and DAE systems. ModelingToolkit.jl has a much more expansive set of system types, including SDEs, PDEs, optimization problems, and more.","category":"page"},{"location":"modules/ModelingToolkit/comparison/#Comparison-Against-CASADI","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison Against CASADI","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"CASADI is written in C++ but used from Python/MATLAB, meaning that it cannot be directly extended by users unless they are using the C++ interface and run a local build of CASADI. ModelingToolkit.jl is both written and used from Julia, meaning that users can easily extend the library on the fly, even interactively in the REPL.\nCASADI includes limited support for Computer Algebra System (CAS) functionality, while ModelingToolkit.jl is built on the full Symbolics.jl CAS.\nCASADI supports DAE and ODE problems via SUNDIALS IDAS and CVODES. ModelingToolkit.jl supports DAE and ODE problems via DifferentialEquations.jl, of which Sundials.jl is <1% of the total available solvers and is outperformed by the native Julia solvers on the vast majority of the benchmark equations. In addition, the DifferentialEquations.jl interface is confederated, meaning that any user can dynamically extend the system to add new solvers to the interface by defining new dispatches of solve.\nCASADI's DAEBuilder does not implement efficiency transformations like tearing which are standard in the ModelingToolkit.jl transformation pipeline.\nCASADI supports special functionality for quadratic programming problems while ModelingToolkit only provides nonlinear programming via OptimizationSystem.\nModelingToolkit.jl integrates with its host language Julia, so Julia code can be automatically converted into ModelingToolkit expressions. Users of CASADI must explicitly create CASADI expressions.","category":"page"},{"location":"modules/ModelingToolkit/comparison/#Comparison-Against-Modia.jl","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison Against Modia.jl","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"Modia.jl uses Julia's expression objects for representing its equations. ModelingToolkit.jl uses Symbolics.jl, and thus the Julia expressions follow Julia semantics and can be manipulated using a computer algebra system (CAS).\nModia's compilation pipeline is similar to the Dymola symbolic processing pipeline with some improvements. ModelingToolkit.jl has an open transformation pipeline that allows for users to extend and reorder transformation passes, where structural_simplify is an adaptation of the Modia.jl-improved alias elimination and tearing algorithms.\nBoth Modia and ModelingToolkit generate DAEProblem and ODEProblem forms for solving with DifferentialEquations.jl.\nModelingToolkit.jl integrates with its host language Julia, so Julia code can be automatically converted into ModelingToolkit expressions. Users of Modia must explicitly create Modia expressions.\nModia covers DAE systems. ModelingToolkit.jl has a much more expansive set of system types, including SDEs, PDEs, optimization problems, and more.","category":"page"},{"location":"modules/ModelingToolkit/comparison/#Comparison-Against-Causal.jl","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison Against Causal.jl","text":"","category":"section"},{"location":"modules/ModelingToolkit/comparison/","page":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","title":"Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages","text":"Causal.jl is a causal modeling environment, whereas ModelingToolkit.jl is an acausal modeling environment. For an overview of the differences, consult academic reviews such as this one.\nBoth ModelingToolkit.jl and Causal.jl use DifferentialEquations.jl as the backend solver library.\nCausal.jl lets one add arbitrary equation systems to a given node, and allow the output to effect the next node. This means an SDE may drive an ODE. These two portions are solved with different solver methods in tandem. In ModelingToolkit.jl, such connections promote the whole system to an SDE. This results in better accuracy and stability, though in some cases it can be less performant.\nCausal.jl, similar to Simulink, breaks algebraic loops via inexact heuristics. ModelingToolkit.jl treats algebraic loops exactly through algebraic equations in the generated model.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/#Automatic-Transformation-of-Nth-Order-ODEs-to-1st-Order-ODEs","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"ModelingToolkit has a system for transformations of mathematical systems. These transformations allow for symbolically changing the representation of the model to problems that are easier to numerically solve. One simple to demonstrate transformation is the ode_order_lowering transformation that sends an Nth order ODE to a 1st order ODE.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"To see this, let's define a second order riff on the Lorenz equations. We utilize the derivative operator twice here to define the second order:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"using ModelingToolkit, OrdinaryDiffEq\r\n\r\n@parameters σ ρ β\r\n@variables t x(t) y(t) z(t)\r\nD = Differential(t)\r\n\r\neqs = [D(D(x)) ~ σ*(y-x),\r\n       D(y) ~ x*(ρ-z)-y,\r\n       D(z) ~ x*y - β*z]\r\n\r\n@named sys = ODESystem(eqs)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"Note that we could've used an alternative syntax for 2nd order, i.e. D = Differential(t)^2 and then E(x) would be the second derivative, and this syntax extends to N-th order. Also, we can use * or ∘ to compose Differentials, like Differential(t) * Differential(x).","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"Now let's transform this into the ODESystem of first order components. We do this by simply calling ode_order_lowering:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"sys = ode_order_lowering(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"Now we can directly numerically solve the lowered system. Note that, following the original problem, the solution requires knowing the initial condition for x', and thus we include that in our input specification:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/higher_order/","page":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","title":"Automatic Transformation of Nth Order ODEs to 1st Order ODEs","text":"u0 = [D(x) => 2.0,\r\n      x => 1.0,\r\n      y => 0.0,\r\n      z => 0.0]\r\n\r\np  = [σ => 28.0,\r\n      ρ => 10.0,\r\n      β => 8/3]\r\n\r\ntspan = (0.0,100.0)\r\nprob = ODEProblem(sys,u0,tspan,p,jac=true)\r\nsol = solve(prob,Tsit5())\r\nusing Plots; plot(sol,vars=(x,y))","category":"page"},{"location":"modules/JumpProcesses/#JumpProcesses.jl","page":"Home","title":"JumpProcesses.jl","text":"","category":"section"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"JumpProcesses.jl, formerly DiffEqJump.jl, provides methods for simulating jump processes, known as stochastic simulation algorithms (SSAs), Doob's method, Gillespie methods, or Kinetic Monte Carlo methods across different fields of science. It also enables the incorporation of jump processes into hybrid jump-ODE and jump-SDE models, including jump diffusions.","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"JumpProcesses is a component package in the SciML ecosystem, and one of the core solver libraries included in DifferentialEquations.jl.","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"The documentation includes","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"a tutorial on simulating basic Poisson processes\na tutorial and details on using JumpProcesses to simulate jump processes via SSAs (i.e. Gillespie methods),\na tutorial on simulating jump-diffusion processes,\na reference on the types of jumps and available simulation methods,\na reference on jump time stepping methods\na FAQ with information on changing parameters between simulations and using callbacks.\nthe JumpProcesses.jl API documentation.","category":"page"},{"location":"modules/JumpProcesses/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"There are two ways to install JumpProcesses.jl. First, users may install the meta DifferentialEquations.jl package, which installs and wraps OrdinaryDiffEq.jl for solving ODEs, StochasticDiffEq.jl for solving SDEs, and JumpProcesses.jl, along with a number of other useful packages for solving models involving ODEs, SDEs and/or jump process. This single install will provide the user with all of the facilities for developing and solving Jump problems.","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"To install the DifferentialEquations.jl package, refer to the following link for complete installation details.","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"If the user wishes to separately install the JumpProcesses.jl library, which is a lighter dependency than DifferentialEquations.jl, then the following code will install JumpProcesses.jl using the Julia package manager:","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"JumpProcesses\")","category":"page"},{"location":"modules/JumpProcesses/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged and #sciml-bridged channels on the Julia Slack\nJuliaDiffEq on Gitter\nthe Julia Discourse forums","category":"page"},{"location":"modules/JumpProcesses/","page":"Home","title":"Home","text":"See also the SciML Community page.","category":"page"},{"location":"modules/MultiScaleArrays/#MultiScaleArrays.jl:-High-Performance-Matrix-Exponentiation-and-Products","page":"Home","title":"MultiScaleArrays.jl: High-Performance Matrix Exponentiation and Products","text":"","category":"section"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"MultiScaleArrays.jl allows you to easily build multiple scale models which are fully compatible with native Julia scientific computing packages like DifferentialEquations.jl or Optim.jl. These models utilize a tree structure to describe phenomena of multiple scales, but the interface allows you to describe equations on different levels, using aggregations from lower levels to describe complex systems. Their structure allows for complex and dynamic models to be developed with only a small performance difference. In the end, they present themselves as an AbstractArray to standard solvers, allowing them to be used in place of a Vector in any appropriately made Julia package.","category":"page"},{"location":"modules/MultiScaleArrays/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"To install MultiScaleArrays.jl, use the Julia package manager:","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"MultiScaleArrays\")","category":"page"},{"location":"modules/MultiScaleArrays/#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"The usage is best described by an example. Here we build a hierarchy where Embryos contain Tissues which contain Populations which contain Cells, and the cells contain proteins whose concentrations are modeled as simply a vector of numbers (it can be anything linearly indexable).","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"using MultiScaleArrays\nstruct Cell{B} <: AbstractMultiScaleArrayLeaf{B}\n    values::Vector{B}\nend\nstruct Population{T<:AbstractMultiScaleArray,B<:Number} <: AbstractMultiScaleArray{B}\n    nodes::Vector{T}\n    values::Vector{B}\n    end_idxs::Vector{Int}\nend\nstruct Tissue{T<:AbstractMultiScaleArray,B<:Number} <: AbstractMultiScaleArray{B}\n    nodes::Vector{T}\n    values::Vector{B}\n    end_idxs::Vector{Int}\nend\nstruct Embryo{T<:AbstractMultiScaleArray,B<:Number} <: AbstractMultiScaleArrayHead{B}\n    nodes::Vector{T}\n    values::Vector{B}\n    end_idxs::Vector{Int}\nend","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"This setup defines a type structure which is both a tree and an array. A picture of a possible version is the following:","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"<img src=\"https://user-images.githubusercontent.com/1814174/27211626-79fe1b9a-520f-11e7-87f1-1cb33da91609.PNG\">","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"Let's build a version of this. Using the constructors we can directly construct leaf types:","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"cell1 = Cell([1.0; 2.0; 3.0])\ncell2 = Cell([4.0; 5.0])","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"and build types higher up in the hierarchy by using the constuct method. The method is construct(T::AbstractMultiScaleArray, nodes, values), though if values is not given it's taken to be empty.","category":"page"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"cell3 = Cell([3.0; 2.0; 5.0])\ncell4 = Cell([4.0; 6.0])\npopulation  = construct(Population, deepcopy([cell1, cell3, cell4]))\npopulation2 = construct(Population, deepcopy([cell1, cell3, cell4]))\npopulation3 = construct(Population, deepcopy([cell1, cell3, cell4]))\ntissue1 = construct(Tissue, deepcopy([population, population2, population3])) # Make a Tissue from Populations\ntissue2 = construct(Tissue, deepcopy([population2, population, population3]))\nembryo = construct(Embryo, deepcopy([tissue1, tissue2])) # Make an embryo from Tissues","category":"page"},{"location":"modules/MultiScaleArrays/#Idea","page":"Home","title":"Idea","text":"","category":"section"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"The idea behind MultiScaleArrays is simple. The *DiffEq solvers (OrdinaryDiffEq.jl, StochasticDiffEq.jl, DelayDiffEq.jl, etc.) and native optimization packages like Optim.jl in their efficient in-place form all work with any Julia-defined AbstractArray which has a linear index. Thus, to define our multiscale model, we develop a type which has an efficient linear index. One can think of representing cells with proteins as each being an array with values for each protein. The linear index of the multiscale model would be indexing through each protein of each cell. With proper index overloads, one can define a type such that a[i] does just that, and thus it will work in the differential equation solvers. MultiScaleArrays.jl takes that further by allowing one to recursively define an arbitrary n-level hierarchical model which has efficient indexing structures. The result is a type which models complex behavior, but the standard differential equation solvers will work directly and efficiently on this type, making it easy to develop novel models without having to re-develop advanced adaptive/stiff/stochastic/etc. solving techniques for each new model.","category":"page"},{"location":"modules/MultiScaleArrays/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/MultiScaleArrays/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/DiffEqBayes/#DiffEqBayes.jl","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl","text":"","category":"section"},{"location":"modules/DiffEqBayes/","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"This repository is a set of extension functionality for estimating the parameters of differential equations using Bayesian methods. It allows the choice of using CmdStan.jl, Turing.jl, DynamicHMC.jl and ApproxBayes.jl to perform a Bayesian estimation of a differential equation problem specified via the DifferentialEquations.jl interface.","category":"page"},{"location":"modules/DiffEqBayes/#Installation","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqBayes/","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"For the Bayesian methods, you must install DiffEqBayes.jl:","category":"page"},{"location":"modules/DiffEqBayes/","page":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","title":"DiffEqBayes.jl: Bayesian Parameter Estimation for Differential Equations","text":"]add DiffEqBayes\nusing DiffEqBayes","category":"page"},{"location":"modules/Catalyst/tutorials/models/#Model-Simulation","page":"Model Simulation","title":"Model Simulation","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"Once created, a reaction network can be used as input to various problem types, which can be solved by DifferentialEquations.jl, and more broadly used within SciML packages.","category":"page"},{"location":"modules/Catalyst/tutorials/models/#Deterministic-simulations-using-ODEs","page":"Model Simulation","title":"Deterministic simulations using ODEs","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"A reaction network can be used as input to an ODEProblem instead of a function, using","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"odeprob = ODEProblem(rn, args...; kwargs...)","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"E.g., a model can be created and solved using:","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"using DiffEqBase, OrdinaryDiffEq\nrn = @reaction_network begin\n  p, ∅ → X\n  d, X → ∅\nend p d\np = [1.0,2.0]\nu0 = [0.1]\ntspan = (0.,1.)\nprob = ODEProblem(rn,u0,tspan,p)\nsol = solve(prob, Tsit5())","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"Here, the order of unknowns in u0 and p matches the order that species and parameters first appear within the DSL. They can also be determined by examining the ordering within the species(rn) and parameters vectors, or accessed more explicitly through the speciesmap(rn) and paramsmap(rn) dictionaries, which map the ModelingToolkit Terms and/or Syms corresponding to each species or parameter to their integer id. Note, if no parameters are given in the @reaction_network, then p does not need to be provided.","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"We can then plot the solution using the solution plotting recipe:","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"using Plots\nplot(sol, lw=2)","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"(Image: models1)","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"To solve for a steady-state starting from the guess u0, one can use","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"using SteadyStateDiffEq\nprob = SteadyStateProblem(rn,u0,p)\nsol = solve(prob, SSRootfind())","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"or","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"prob = SteadyStateProblem(rn,u0,p)\nsol = solve(prob, DynamicSS(Tsit5()))","category":"page"},{"location":"modules/Catalyst/tutorials/models/#Stochastic-simulations-using-SDEs","page":"Model Simulation","title":"Stochastic simulations using SDEs","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"In a similar way an SDE can be created using","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"using StochasticDiffEq\nsdeprob = SDEProblem(rn, args...; kwargs...)","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"In this case the chemical Langevin equations (as derived in Gillespie, J. Chem. Phys. 2000) will be used to generate stochastic differential equations.","category":"page"},{"location":"modules/Catalyst/tutorials/models/#Stochastic-simulations-using-discrete-stochastic-simulation-algorithms","page":"Model Simulation","title":"Stochastic simulations using discrete stochastic simulation algorithms","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"Instead of solving SDEs, one can create a stochastic jump process model using integer copy numbers and a discrete stochastic simulation algorithm (i.e., Gillespie Method or Kinetic Monte Carlo). This can be done using:","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"using JumpProcesses\nrn = @reaction_network begin\n  p, ∅ → X\n  d, X → ∅\nend p d\np = [1.0,2.0]\nu0 = [10]\ntspan = (0.,1.)\ndiscrete_prob = DiscreteProblem(rn, u0, tspan, p)\njump_prob = JumpProblem(rn, discrete_prob, Direct())\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"Here, we used Gillespie's Direct method as the underlying stochastic simulation algorithm. We get:","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"plot(sol, lw=2)","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"(Image: models2)","category":"page"},{"location":"modules/Catalyst/tutorials/models/#[Reaction](@ref)-fields","page":"Model Simulation","title":"Reaction fields","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"Each Reaction within reactions(rn) has a number of subfields. For rx a Reaction we have:","category":"page"},{"location":"modules/Catalyst/tutorials/models/","page":"Model Simulation","title":"Model Simulation","text":"rx.substrates, a vector of ModelingToolkit expressions storing each substrate variable.\nrx.products, a vector of ModelingToolkit expressions storing each product variable.\nrx.substoich, a vector storing the corresponding stoichiometry of each substrate species in rx.substrates.\nrx.prodstoich, a vector storing the corresponding stoichiometry of each product species in rx.products.\nrx.rate, a Number, ModelingToolkit.Sym, or ModelingToolkit expression representing the reaction rate. E.g., for a reaction like k*X, Y --> X+Y, we'd have rate = k*X.\nrx.netstoich, a vector of pairs mapping the ModelingToolkit expression for each species that changes numbers by the reaction to how much it changes. E.g., for k, X + 2Y --> X + W, we'd have rx.netstoich = [Y(t) => -2, W(t) => 1].\nrx.only_use_rate, a boolean that is true if the reaction was made with non-filled arrows and should ignore mass action kinetics. false by default.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/#Automated-Sparse-Analytical-Jacobians","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"In many cases where you have large stiff differential equations, getting a sparse Jacobian can be essential for performance. In this tutorial we will show how to use modelingtoolkitize to regenerate an ODEProblem code with the analytical solution to the sparse Jacobian, along with the sparsity pattern required by DifferentialEquations.jl's solvers to specialize the solving process.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"First let's start out with an implementation of the 2-dimensional Brusselator partial differential equation discretized using finite differences:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"using DifferentialEquations, ModelingToolkit\n\nconst N = 32\nconst xyd_brusselator = range(0,stop=1,length=N)\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\nlimit(a, N) = a == N+1 ? 1 : a == 0 ? N : a\nfunction brusselator_2d_loop(du, u, p, t)\n  A, B, alpha, dx = p\n  alpha = alpha/dx^2\n  @inbounds for I in CartesianIndices((N, N))\n    i, j = Tuple(I)\n    x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\n    ip1, im1, jp1, jm1 = limit(i+1, N), limit(i-1, N), limit(j+1, N), limit(j-1, N)\n    du[i,j,1] = alpha*(u[im1,j,1] + u[ip1,j,1] + u[i,jp1,1] + u[i,jm1,1] - 4u[i,j,1]) +\n                B + u[i,j,1]^2*u[i,j,2] - (A + 1)*u[i,j,1] + brusselator_f(x, y, t)\n    du[i,j,2] = alpha*(u[im1,j,2] + u[ip1,j,2] + u[i,jp1,2] + u[i,jm1,2] - 4u[i,j,2]) +\n                A*u[i,j,1] - u[i,j,1]^2*u[i,j,2]\n    end\nend\np = (3.4, 1., 10., step(xyd_brusselator))\n\nfunction init_brusselator_2d(xyd)\n  N = length(xyd)\n  u = zeros(N, N, 2)\n  for I in CartesianIndices((N, N))\n    x = xyd[I[1]]\n    y = xyd[I[2]]\n    u[I,1] = 22*(y*(1-y))^(3/2)\n    u[I,2] = 27*(x*(1-x))^(3/2)\n  end\n  u\nend\nu0 = init_brusselator_2d(xyd_brusselator)\nprob = ODEProblem(brusselator_2d_loop,u0,(0.,11.5),p)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"Now let's use modelingtoolkitize to generate the symbolic version:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"sys = modelingtoolkitize(prob)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"Now we regenerate the problem using jac=true for the analytical Jacobian and sparse=true to make it sparse:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"sparseprob = ODEProblem(sys,Pair[],(0.,11.5),jac=true,sparse=true)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"Hard? No! How much did that help?","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"using BenchmarkTools\n@btime solve(prob,save_everystep=false) # 51.714 s (7317 allocations: 70.12 MiB)\n@btime solve(sparseprob,save_everystep=false) # 2.880 s (55533 allocations: 885.09 MiB)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"Notice though that the analytical solution to the Jacobian can be quite expensive. Thus in some cases we may only want to get the sparsity pattern. In this case, we can simply do:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/sparse_jacobians/","page":"Automated Sparse Analytical Jacobians","title":"Automated Sparse Analytical Jacobians","text":"sparsepatternprob = ODEProblem(sys,Pair[],(0.,11.5),sparse=true)\n@btime solve(sparsepatternprob,save_everystep=false) # 2.880 s (55533 allocations: 885.09 MiB)","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/#Imposing-Constraints-on-Physics-Informed-Neural-Network-(PINN)-Solutions","page":"Imposing Constraints","title":"Imposing Constraints on Physics-Informed Neural Network (PINN) Solutions","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Let's consider the Fokker-Planck equation:","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"- fracx left  left( alpha x - beta x^3right) p(x)right  + fracsigma^22 frac^2x^2 p(x) = 0  ","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"which must satisfy the normalization condition:","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Delta t  p(x) = 1","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with the boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"p(-22) = p(22) = 0","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with Physics-Informed Neural Networks.","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Integrals, IntegralsCubature\nimport ModelingToolkit: Interval, infimum, supremum\n# the example is taken from this article https://arxiv.org/abs/1910.10503\n@parameters x\n@variables p(..)\nDx = Differential(x)\nDxx = Differential(x)^2\n\nα = 0.3\nβ = 0.5\n_σ = 0.5\nx_0 = -2.2\nx_end = 2.2\n# Discretization\ndx = 0.01\n\neq  = Dx((α*x - β*x^3)*p(x)) ~ (_σ^2/2)*Dxx(p(x))\n\n# Initial and boundary conditions\nbcs = [p(x_0) ~ 0. ,p(x_end) ~ 0.]\n\n# Space and time domains\ndomains = [x ∈ Interval(x_0,x_end)]\n\n# Neural network\ninn = 18\nchain = Lux.Chain(Dense(1,inn,Lux.σ),\n                  Dense(inn,inn,Lux.σ),\n                  Dense(inn,inn,Lux.σ),\n                  Dense(inn,1))\n\nlb = [x_0]\nub = [x_end]\nfunction norm_loss_function(phi,θ,p)\n    function inner_f(x,θ)\n         dx*phi(x, θ) .- 1\n    end\n    prob = IntegralProblem(inner_f, lb, ub, θ)\n    norm2 = solve(prob, HCubatureJL(), reltol = 1e-8, abstol = 1e-8, maxiters =10);\n    abs(norm2[1])\nend\n\ndiscretization = PhysicsInformedNN(chain,\n                                   GridTraining(dx),\n                                   additional_loss=norm_loss_function)\n\n@named pdesystem = PDESystem(eq,bcs,domains,[x],[p(x)])\nprob = discretize(pdesystem,discretization)\nphi = discretization.phi\n\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncb_ = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"additional_loss: \", norm_loss_function(phi,p,nothing))\n    return false\nend\n\nres = Optimization.solve(prob,LBFGS(),callback = cb_,maxiters=400)\nprob = remake(prob,u0=res.u)\nres = Optimization.solve(prob,BFGS(),callback = cb_,maxiters=2000)","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"And some analysis:","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using Plots\nC = 142.88418699042 #fitting param\nanalytic_sol_func(x) = C*exp((1/(2*_σ^2))*(2*α*x^2 - β*x^4))\n\nxs = [infimum(d.domain):dx:supremum(d.domain) for d in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.u)) for x in xs]\n\nplot(xs ,u_real, label = \"analytic\")\nplot!(xs ,u_predict, label = \"predict\")","category":"page"},{"location":"modules/NeuralPDE/tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"(Image: fp)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/#Test-Problems","page":"Test Problems","title":"Test Problems","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"For every problem, one can turn it into a test problem by adding analytical to the DEFunction.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/#No-Analytical-Solution","page":"Test Problems","title":"No Analytical Solution","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"However, in many cases the analytical solution cannot be found, and therefore one uses a low-tolerance calculation as a stand-in for a solution. The JuliaDiffEq ecosystem supports this through the TestSolution type in DiffEqDevTools. There are three constructors. The code is simple, so here it is:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"type TestSolution <: DESolution\n  t\n  u\n  interp\n  dense\nend\n(T::TestSolution)(t) = T.interp(t)\nTestSolution(t,u) = TestSolution(t,u,nothing,false)\nTestSolution(t,u,interp) = TestSolution(t,u,interp,true)\nTestSolution(interp::DESolution) = TestSolution(nothing,nothing,interp,true)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"This acts like a solution. When used in conjunction with apprxtrue:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"appxtrue(sol::AbstractODESolution,sol2::TestSolution)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"you can use it to build a TestSolution from a problem (like ODETestSolution) which holds the errors  If you only give it t and u, then it can only calculate the final error. If the TestSolution has an interpolation, it will define timeseries and dense errors.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"(Note: I would like it so that way the timeseries error will be calculated on the times of sol.t in sol2.t which would act nicely with tstops and when interpolations don't exist, but haven't gotten to it!)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"These can then be passed to other functionality. For example, the benchmarking functions allow one to set appxsol which is a TestSolution for the benchmark solution to calculate errors against, and error_estimate allows one to choose which error estimate to use in the benchmarking (defaults to :final).","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/#Related-Functions","page":"Test Problems","title":"Related Functions","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/","page":"Test Problems","title":"Test Problems","text":"DiffEqDevTools.appxtrue","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/test_problems/#DiffEqDevTools.appxtrue","page":"Test Problems","title":"DiffEqDevTools.appxtrue","text":"appxtrue(sol::AbstractODESolution,sol2::TestSolution)\n\nUses the interpolant from the higher order solution sol2 to approximate errors for sol. If sol2 has no interpolant, only the final error is calculated.\n\n\n\n\n\nappxtrue(sol::AbstractODESolution,sol2::AbstractODESolution)\n\nUses the interpolant from the higher order solution sol2 to approximate errors for sol. If sol2 has no interpolant, only the final error is calculated.\n\n\n\n\n\n","category":"function"},{"location":"modules/MethodOfLines/get_grid/#get*discrete-(@id-get*grid)","page":"Grid and Solution Retrieval","title":"getdiscrete (@id getgrid)","text":"","category":"section"},{"location":"modules/MethodOfLines/get_grid/","page":"Grid and Solution Retrieval","title":"Grid and Solution Retrieval","text":"MethodOfLines.jl exports a helper function get_discrete, which returns a Dict with the keys being the independent and dependent variables, and the values their corresponding discrete grid, and discretized variables used in the discretization. It is used as following:","category":"page"},{"location":"modules/MethodOfLines/get_grid/","page":"Grid and Solution Retrieval","title":"Grid and Solution Retrieval","text":"grid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\n# Retrieve shaped solution\nu_sol = [map(d -> sol[d][i], grid[u(t, x)]) for i in 1:length(sol[t])]","category":"page"},{"location":"modules/DiffEqDocs/types/sdde_types/#sdde_prob","page":"SDDE Problems","title":"SDDE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"SDDEProblem\nSDDEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/sdde_types/#SciMLBase.SDDEProblem","page":"SDDE Problems","title":"SciMLBase.SDDEProblem","text":"Defines a stochastic delay differential equation (SDDE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/sdde_types/\n\nMathematical Specification of a Stochastic Delay Differential Equation (SDDE) Problem\n\nTo define a SDDE Problem, you simply need to give the drift function f, the diffusion function g, the initial condition u_0 at time point t_0, and the history function h which together define a SDDE:\n\ndu = f(uhpt)dt + g(uhpt)dW_t qquad (t geq t_0)\n\nu(t_0) = u_0\n\nu(t) = h(t) qquad (t  t_0)\n\nf should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)) (and g should match). u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.\n\nNote that this functionality should be considered experimental.\n\nFunctional Forms of the History Function\n\nThe history function h can be called in the following ways:\n\nh(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs\n\nNote that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.\n\nDeclaring Lags\n\nLags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate and thus this is recommended.\n\nNeutral, Retarded, and Algebraic Stochastic Delay Differential Equations\n\nNote that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.\n\nNote that algebraic equations can be specified by using a singular mass matrix.\n\nProblem Type\n\nConstructors\n\nSDDEProblem(f,g[, u0], h, tspan[, p]; <keyword arguments>)\nSDDEProblem{isinplace}(f,g[, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nArguments\n\nf: The drift function in the SDDE.\ng: The diffusion function in the SDDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/sdde_types/#SciMLBase.SDDEFunction","page":"SDDE Problems","title":"SciMLBase.SDDEFunction","text":"SDDEFunction{iip,F,G,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,GG,S,O,TCV} <: AbstractSDDEFunction{iip}\n\nA representation of a SDDE function f, defined by:\n\nM du = f(uhpt) dt + g(uhpt) dW_t\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nSDDEFunction{iip,recompile}(f,g;\n                 mass_matrix=I,\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing,\n                 jvp=nothing,\n                 vjp=nothing,\n                 jac_prototype=nothing,\n                 sparsity=jac_prototype,\n                 paramjac = nothing,\n                 syms = nothing,\n                 indepsym = nothing,\n                 colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,h,p,t) or du = f(u,h,p,t). See the section on iip for more details on in-place vs out-of-place handling. The histroy function h acts as an interpolator over time, i.e. h(t) with options matching the solution interface, i.e. h(t; save_idxs = 2).\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,h,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,h,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,h,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,h,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,h,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DDEFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/sdde_types/#Solution-Type","page":"SDDE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"SDDEProblem solutions return an RODESolution. For more information, see the RODE problem definition page for the RODESolution docstring.","category":"page"},{"location":"modules/SciMLOperators/premade_operators/#Premade-SciMLOperators","page":"Premade SciMLOperators","title":"Premade SciMLOperators","text":"","category":"section"},{"location":"modules/SciMLOperators/premade_operators/#Direct-Operator-Definitions","page":"Premade SciMLOperators","title":"Direct Operator Definitions","text":"","category":"section"},{"location":"modules/SciMLOperators/premade_operators/","page":"Premade SciMLOperators","title":"Premade SciMLOperators","text":"ScalarOperator\nSciMLOperators.NullOperator\nMatrixOperator\nDiagonalOperator\nAffineOperator\nFunctionOperator","category":"page"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.ScalarOperator","page":"Premade SciMLOperators","title":"SciMLOperators.ScalarOperator","text":"ScalarOperator(val[; update_func])\n\n(α::ScalarOperator)(a::Number) = α * a\n\nRepresents a time-dependent scalar/scaling operator. The update function is called by update_coefficients! and is assumed to have the following signature:\n\nupdate_func(oldval,u,p,t) -> newval\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.NullOperator","page":"Premade SciMLOperators","title":"SciMLOperators.NullOperator","text":"struct NullOperator{N} <: SciMLOperators.AbstractSciMLLinearOperator{Bool}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.MatrixOperator","page":"Premade SciMLOperators","title":"SciMLOperators.MatrixOperator","text":"MatrixOperator(A[; update_func])\n\nRepresents a time-dependent linear operator given by an AbstractMatrix. The update function is called by update_coefficients! and is assumed to have the following signature:\n\nupdate_func(A::AbstractMatrix,u,p,t) -> [modifies A]\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.DiagonalOperator","page":"Premade SciMLOperators","title":"SciMLOperators.DiagonalOperator","text":"DiagonalOperator(diag, [; update_func])\n\nRepresents a time-dependent elementwise scaling (diagonal-scaling) operation. The update function is called by update_coefficients! and is assumed to have the following signature:\n\nupdate_func(diag::AbstractVector,u,p,t) -> [modifies diag]\n\nWhen diag is an AbstractVector of length N, L=DiagonalOpeator(diag, ...) can be applied to AbstractArrays with size(u, 1) == N. Each column of the u will be scaled by diag, as in LinearAlgebra.Diagonal(diag) * u.\n\nWhen diag is a multidimensional array, L = DiagonalOperator(diag, ...) forms an operator of size (N, N) where N = size(diag, 1) is the leading length of diag. L then is the elementwise-scaling operation on arrays of length(u) = length(diag) with leading length size(u, 1) = N.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.AffineOperator","page":"Premade SciMLOperators","title":"SciMLOperators.AffineOperator","text":"L = AffineOperator(A, B, b[; update_func])\nL(u) = A*u + B*b\n\nRepresents a time-dependent affine operator. The update function is called by update_coefficients! and is assumed to have the following signature:\n\nupdate_func(b::AbstractArray,u,p,t) -> [modifies b]\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.FunctionOperator","page":"Premade SciMLOperators","title":"SciMLOperators.FunctionOperator","text":"Matrix free operators (given by a function)\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#Lazy-Operator-Compositions","page":"Premade SciMLOperators","title":"Lazy Operator Compositions","text":"","category":"section"},{"location":"modules/SciMLOperators/premade_operators/","page":"Premade SciMLOperators","title":"Premade SciMLOperators","text":"SciMLOperators.ScaledOperator\nSciMLOperators.ComposedOperator\nSciMLOperators.AddedOperator\nSciMLOperators.InvertedOperator\nSciMLOperators.InvertibleOperator\nSciMLOperators.AdjointedOperator\nSciMLOperators.TransposedOperator","category":"page"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.ScaledOperator","page":"Premade SciMLOperators","title":"SciMLOperators.ScaledOperator","text":"ScaledOperator\n\n(λ L)*(u) = λ * L(u)\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.ComposedOperator","page":"Premade SciMLOperators","title":"SciMLOperators.ComposedOperator","text":"Lazy operator composition\n\n∘(A, B, C)(u) = A(B(C(u)))\n\nops = (A, B, C)\ncache = (B*C*u , C*u)\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.AddedOperator","page":"Premade SciMLOperators","title":"SciMLOperators.AddedOperator","text":"Lazy operator addition\n\n(A1 + A2 + A3...)u = A1*u + A2*u + A3*u ....\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.InvertedOperator","page":"Premade SciMLOperators","title":"SciMLOperators.InvertedOperator","text":"Lazy Operator Inverse\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/premade_operators/#SciMLOperators.InvertibleOperator","page":"Premade SciMLOperators","title":"SciMLOperators.InvertibleOperator","text":"InvertibleOperator(F)\n\nLike MatrixOperator, but stores a Factorization instead.\n\nSupports left division and ldiv! when applied to an array.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/examples/collocation/#Smoothed-Collocation-for-Fast-Two-Stage-Training","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"One can avoid a lot of the computational cost of the ODE solver by pretraining the neural network against a smoothed collocation of the data. First the example and then an explanation.","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using Lux, DiffEqFlux, OrdinaryDiffEq, SciMLSensitivity, Optimization, OptimizationFlux, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)\nsavefig(\"colloc.png\")\nplot(tsteps,du')\nsavefig(\"colloc_du.png\")\n\ndudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du, _ = dudt2(@view(u[:,i]),p, st)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol)\nsavefig(\"colloc_trained.png\")\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)\nsavefig(\"post_trained.png\")","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/#Generating-the-Collocation","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Generating the Collocation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"The smoothed collocation is a spline fit of the datapoints which allows us to get a an estimate of the approximate noiseless dynamics:","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"using Lux, DiffEqFlux, Optimization, OptimizationFlux, DifferentialEquations, Plots\n\nusing Random\nrng = Random.default_rng()\n\nu0 = Float32[2.0; 0.0]\ndatasize = 300\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\ndata = Array(solve(prob_trueode, Tsit5(), saveat = tsteps)) .+ 0.1randn(2,300)\n\ndu,u = collocate_data(data,tsteps,EpanechnikovKernel())\n\nscatter(tsteps,data')\nplot!(tsteps,u',lw=5)","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"We can then differentiate the smoothed function to get estimates of the derivative at each datapoint:","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"plot(tsteps,du')","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"Because we have (u',u) pairs, we can write a loss function that calculates the squared difference between f(u,p,t) and u' at each point, and find the parameters which minimize this difference:","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"dudt2 = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 50, tanh),\n                  Lux.Dense(50, 2))\n\nfunction loss(p)\n    cost = zero(first(p))\n    for i in 1:size(du,2)\n      _du, _ = dudt2(@view(u[:,i]),p, st)\n      dui = @view du[:,i]\n      cost += sum(abs2,dui .- _du)\n    end\n    sqrt(cost)\nend\n\npinit, st = Lux.setup(rng, dudt2)\n\ncallback = function (p, l)\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nresult_neuralode = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 10000)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\nnn_sol, st = prob_neuralode(u0, result_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol)","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"While this doesn't look great, it has the characteristics of the full solution all throughout the timeseries, but it does have a drift. We can continue to optimize like this, or we can use this as the initial condition to the next phase of our fitting:","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"function predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, data .- pred)\n    return loss\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\n\nnumerical_neuralode = Optimization.solve(optprob,\n                                       ADAM(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\nnn_sol, st = prob_neuralode(u0, numerical_neuralode.u, st)\nscatter(tsteps,data')\nplot!(nn_sol,lw=5)","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"(Image: )","category":"page"},{"location":"modules/DiffEqFlux/examples/collocation/","page":"Smoothed Collocation for Fast Two-Stage Training","title":"Smoothed Collocation for Fast Two-Stage Training","text":"This method then has a good global starting position, making it less prone to local minima and is thus a great method to mix in with other fitting methods for neural ODEs.","category":"page"},{"location":"highlevels/learning_resources/#Curated-Learning,-Teaching,-and-Training-Resouces","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"While the SciML documentation is made to be comprehensive, there will always be good alternative resources. The purpose of this section of the documentation is to highlight the alternative resources which can be helpful for learning how to use the SciML Open Source Software libraries.","category":"page"},{"location":"highlevels/learning_resources/#SciMLTutorials","page":"Curated Learning, Teaching, and Training Resouces","title":"SciMLTutorials","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"SciMLTutorials.jl is an extended set of tutorials for the SciML open source software organization. It contains many complete workflow examples on large-scale problems that may be too large or complex for normal documenation, but good materials for users to learn from.","category":"page"},{"location":"highlevels/learning_resources/#JuliaCon-and-SciMLCon-Videos","page":"Curated Learning, Teaching, and Training Resouces","title":"JuliaCon and SciMLCon Videos","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"Many tutorials and introductions to packages have been taught through previous JuliaCon/SciMLCon workshops and talks. The following is a curated list of such training videos:","category":"page"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"Intro to solving differential equations in Julia\nJuliaCon 2020 | Doing Scientific Machine Learning (SciML) With Julia\nSimulating Big Models in Julia with ModelingToolkit | Workshop | JuliaCon 2021\nStructural Identifiability Tools in Julia: A Tutorial | Ilia Ilmer | SciMLCon 2022\nJuliaCon 2018 | Solving Partial Differential Equations with Julia | Chris Rackauckas","category":"page"},{"location":"highlevels/learning_resources/#SciML-Book:-Parallel-Computing-and-Scientific-Machine-Learning-(SciML):-Methods-and-Applications","page":"Curated Learning, Teaching, and Training Resouces","title":"SciML Book: Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"The book Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications is a compilation of the lecture notes from the MIT Course 18.337J/6.338J: Parallel Computing and Scientific Machine Learning. It contains a walkthrough of many of the methods implemented in the SciML libraries, as well as how to understand much of the functionality at a deeper level. This course was intended for MIT graduate students in engineering, computer science, and mathematics and thus may have a high prerequisite requirement than many other resources.","category":"page"},{"location":"highlevels/learning_resources/#sir-julia:-Various-implementations-of-the-classical-SIR-model-in-Julia","page":"Curated Learning, Teaching, and Training Resouces","title":"sir-julia: Various implementations of the classical SIR model in Julia","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"For those who like to learn by example, the repository sir-julia is a great resource! It showcases how to use the SciML libraries in many different ways to simulate different variations of the classic SIR epidemic model.","category":"page"},{"location":"highlevels/learning_resources/#Other-Books-Featuring-SciML","page":"Curated Learning, Teaching, and Training Resouces","title":"Other Books Featuring SciML","text":"","category":"section"},{"location":"highlevels/learning_resources/","page":"Curated Learning, Teaching, and Training Resouces","title":"Curated Learning, Teaching, and Training Resouces","text":"Nonlinear Dynamics: A Concise Introduction Interlaced with Code\nNumerical Methods for Scientific Computing: The Definitive Manual for Math Geeks\nFundamentals of Numerical Computation\nStatistics with Julia\nStatistical Rethinking with Julia\nThe Koopman Operator in Systems and Control\n\"All simulations have been performed in Julia, with additional Julia packages: LinearAlgebra.jl, Random.jl, Plots.jl, Lasso.jl, DifferentialEquations.jl\"","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/#Ecosystem-Overview","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"So you're looking to help out DifferentialEquations.jl? We'd be happy to have your help. It is recommended you first discuss with some of the developers on the Gitter channel to make sure that you're up-to-date with current developments.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/#The-Common-Interface","page":"Ecosystem Overview","title":"The Common Interface","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"The DiffEq ecosystem is built around the common interface. This is the interface for the solvers:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"__solve(prob,alg;kwargs...)\n__init(prob,alg;kwargs...)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"and the standard methods for dealing with solutions. A higher level solve and init is given by DiffEqBase.jl for functional and distributional intputs. Users build problem types for solvers to act on, and add-on components which use the solution types for higher-level analysis like parameter estimation and sensitivity analysis.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"One can add components at any of these levels to improve the functionality of the system as a whole.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/#Organizational-Setup","page":"Ecosystem Overview","title":"Organizational Setup","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"JuliaDiffEq is setup in a distributed manner to allow developers to retain authoritative control and licensing for their own packages/algorithms, yet contribute to the greater ecosystem. This gives a way for researchers to target a wide audience of users, but not have to fully contribute to public packages or be restricted in licensing. At the center of the ecosystem is DiffEqBase which holds the Problem, Solution, and Algorithm types (the algorithms are defined in DiffEqBase to be accessible by the default_algorithm function. One can opt out of this). Then there's the component solvers, which includes the *DiffEq packages (OrdinaryDiffEq, StochasticDiffEq, etc.) which implement different methods for solve. Then there are the add-on packages, such as the DiffEq* packages (DiffEqParamEstim, DiffEqDevTools) which add functionality to the Problem+solve setup. Lastly, there's DifferentialEquations.jl which is a metapackage which holds all of these pieces together as one cohesive unit.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/ecosystem_overview/","page":"Ecosystem Overview","title":"Ecosystem Overview","text":"If one wants their package to officially join the ecosystem, it will need to be moved to the JuliaDiffEq organization so that maintenance can occur (but the core algorithms will only be managed by the package owners themselves). The Algorithm types can then be moved to DiffEqBase, and after testing the package will be added to the list packages exported by DifferentialEquations.jl and the corresponding documentation.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#DAE-Solvers","page":"DAE Solvers","title":"DAE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#Recommended-Methods","page":"DAE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"For medium to low accuracy small numbers of DAEs in constant mass matrices form, the  Rosenbrock23 and Rodas4 methods are good choices which will get good efficiency if the mass matrix is constant. Rosenbrock23 is better for low accuracy (error tolerance <1e-4) and Rodas4 is better for high accuracy. Another choice at high accuracy is RadauIIA5.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Non-constant mass matrices are not directly supported: users are advised to transform their problem through substitution to a DAE with constant mass matrices.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"If the problem cannot be defined in mass matrix form, the recommended method for performance is IDA from the Sundials.jl package if you are solving problems with Float64. If Julia types are required, currently DFBDF is the best method but still needs more optimizations.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#dae_solve_full","page":"DAE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#Initialization-Schemes","page":"DAE Solvers","title":"Initialization Schemes","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"For all OrdinaryDiffEq.jl methods, an initialization scheme can be set with a common keyword argument initializealg. The choices are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"BrownFullBasicInit: For Index-1 DAEs implicit DAEs and and semi-explicit DAEs in mass matrix form. Keeps the differential variables constant. Requires du0 when used on a DAEProblem.\nShampineCollocationInit: For Index-1 DAEs implicit DAEs and and semi-explicit DAEs in mass matrix form. Changes both the differential and algebraic variables.\nNoInit: Explicitly opts-out of DAE initialization.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#OrdinaryDiffEq.jl-(Implicit-ODE)","page":"DAE Solvers","title":"OrdinaryDiffEq.jl (Implicit ODE)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods from OrdinaryDiffEq are for DAEProblem specifications.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"DImplicitEuler - 1st order A-L and stiffly stable adaptive implicit Euler\nDABDF2 - 2nd order A-L stable adaptive BDF method.\nDFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or IDA in divided differences form.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#OrdinaryDiffEq.jl-(Mass-Matrix)","page":"DAE Solvers","title":"OrdinaryDiffEq.jl (Mass Matrix)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"note: Note\nThe standard Hermite interpolation used for ODE methods in OrdinaryDiffEq.jl is not applicable to the algebraic variables. Thus for the following mass-matrix methods, use the interplation (thus saveat) with caution if the default Hermite interpolation is used. All methods which mention a specialized interpolation (and implicit ODE methods) are safe.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#Rosenbrock-Methods","page":"DAE Solvers","title":"Rosenbrock Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method. with a stiff-aware 3rd order interpolant","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#Rosenbrock-W-Methods","page":"DAE Solvers","title":"Rosenbrock-W Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildy stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#FIRK-Methods","page":"DAE Solvers","title":"FIRK Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"RadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#SDIRK-Methods","page":"DAE Solvers","title":"SDIRK Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"ImplicitEuler - Stage order 1. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - Stage order 1. Symplectic. Good for when symplectic integration is required.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#Multistep-Methods","page":"DAE Solvers","title":"Multistep Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"QNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\nQBDF - An adaptive order quasi-constant timestep BDF method.\nFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or CVODE_BDF in divided differences form.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#dae_solve_sundials","page":"DAE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"]add Sundials\nusing Sundials","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA - This is the IDA method from the Sundials.jl package.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that the constructors for the Sundials algorithms take a main argument:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"linearsolver - This is the linear solver which is used in the Newton iterations. The choices are:\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA() # Newton + Dense solver\nIDA(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nIDA(linear_solver=:BCG) # Biconjugate gradient method                                   ","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"All of the additional options are available. The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA(;linear_solver=:Dense,jac_upper=0,jac_lower=0,krylov_dim=0,\n    max_order = 5,\n    max_error_test_failures = 7,\n    max_nonlinear_iters = 3,\n    nonlinear_convergence_coefficient = 0.33,\n    nonlinear_convergence_coefficient_ic = 0.0033,\n    max_num_steps_ic = 5,\n    max_num_jacs_ic = 4,\n    max_num_iters_ic = 10,\n    max_num_backs_ic = 100,\n    use_linesearch_ic = true,\n    max_convergence_failures = 10,\n    init_all = false,\n    prec = nothing, psetup = nothing)","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"See the Sundials manual for details on the additional options. The option init_all controls the initial condition consistency routine. If the initial conditions are inconsistent (i.e. they do not satisfy the implicit equation), init_all=false means that the algebraic variables and derivatives will be modified in order to satisfy the DAE. If init_all=true, all initial conditions will be modified to satisfy the DAE.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that here prec is a (left) preconditioner function prec(z,r,p,t,y,fy,gamma,delta) where:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"z: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"and psetup is the preconditioner setup function for pre-computing Jacobian information. Where:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"p: the parameters\nt: the current independent variable\nresid: the current residual\nu: the current state\ndu: the current derivative of the state\ngamma: the gamma of W = M - gamma*J","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"psetup is optional when prec is set.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#DASKR.jl","page":"DAE Solvers","title":"DASKR.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"DASKR.jl is not automatically included by DifferentialEquations.jl. To use this algorithm, you will need to install and use the package:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"]add DASKR\nusing DASKR","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"daskr - This is a wrapper for the well-known DASKR algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"All additional options are available. The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"function daskr(;linear_solver=:Dense,\n                  jac_upper=0,jac_lower=0,max_order = 5,\n                  non_negativity_enforcement = 0,\n                  non_negativity_enforcement_array = nothing,\n                  max_krylov_iters = nothing,\n                  num_krylov_vectors = nothing,\n                  max_number_krylov_restarts = 5,\n                  krylov_convergence_test_constant = 0.05,\n                  exclude_algebraic_errors = false)","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Choices for the linear solver are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":":Dense\n:Banded\n:SPIGMR, a Krylov method","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#DASSL.jl","page":"DAE Solvers","title":"DASSL.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"dassl - A native Julia implementation of the DASSL algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/#ODEInterfaceDiffEq.jl","page":"DAE Solvers","title":"ODEInterfaceDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"seulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/#Random-Balance-Design-FAST-Method","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"struct RBDFAST <: GSAMethod  \n    num_harmonics::Int\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"RBDFAST has the following keyword arguments:","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"num_harmonics: Number of harmonics to consider during power spectral density analysis.","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/#Method-Details","page":"Random Balance Design FAST Method","title":"Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"In the Random Balance Designs (RBD) method, similar to eFAST,  N points are selected over a curve in the input space. A fixed frequency  equal to 1 is used for each factor. Then independent random  permutations are applied to the coordinates of the N points in order to  generate the design points. The input model for analysis is evaluated  at each design point and the outputs are reordered such that the design  points are in increasing order with respect to factor Xi. The Fourier  spectrum is calculated on the model output at the frequency 1 and at  its higher harmonics (2, 3, 4, 5, 6) and yields the estimate of the  sensitivity index of factor Xi.","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/#API","page":"Random Balance Design FAST Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"function gsa(f, method::RBDFAST; num_params, N, rng::AbstractRNG = Random.default_rng(), batch = false, kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/rbdfast/#Example","page":"Random Balance Design FAST Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/rbdfast/","page":"Random Balance Design FAST Method","title":"Random Balance Design FAST Method","text":"function linear_batch(X)\n    A= 7\n    B= 0.1\n    @. A*X[1,:]+B*X[2,:]\nend\nfunction linear(X)\n    A= 7\n    B= 0.1\n    A*X[1]+B*X[2]\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nrng = StableRNG(123)\nres1 = gsa(linear,GlobalSensitivity.RBDFAST(),num_params = 4, N=15000)\nres2 = gsa(linear_batch,GlobalSensitivity.RBDFAST(),num_params = 4, batch=true, N=15000)","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#units","page":"Model Validation and Units","title":"Model Validation and Units","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"ModelingToolkit.jl provides extensive functionality for model validation and unit checking. This is done by providing metadata to the variable types and then running the validation functions which identify malformed systems and non-physical equations. This approach provides high performance and compatibility with numerical solvers.","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Assigning-Units","page":"Model Validation and Units","title":"Assigning Units","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Units may assigned with the following syntax. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit, Unitful\r\n@variables t [unit = u\"s\"] x(t) [unit = u\"m\"] g(t) w(t) [unit = \"Hz\"]\r\n\r\n@variables(t, [unit = u\"s\"], x(t), [unit = u\"m\"], g(t), w(t), [unit = \"Hz\"])\r\n\r\n@variables(begin\r\nt, [unit = u\"s\"],\r\nx(t), [unit = u\"m\"],\r\ng(t),\r\nw(t), [unit = \"Hz\"]\r\nend)\r\n\r\n# Simultaneously set default value (use plain numbers, not quantities)\r\n@variable x=10 [unit = u\"m\"]\r\n\r\n# Symbolic array: unit applies to all elements\r\n@variable x[1:3] [unit = u\"m\"]","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Do not use quantities such as  1u\"s\", 1/u\"s\" or u\"1/s\" as these will result in errors; instead use u\"s\", u\"s^-1\", or u\"s\"^-1. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Unit-Validation-and-Inspection","page":"Model Validation and Units","title":"Unit Validation & Inspection","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Unit validation of equations happens automatically when creating a system. However, for debugging purposes one may wish to validate the equations directly using validate.","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"ModelingToolkit.validate","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#ModelingToolkit.validate","page":"Model Validation and Units","title":"ModelingToolkit.validate","text":"Returns true iff units of equations are valid.\n\n\n\n\n\nvalidate(rx::Reaction; info::String = \"\")\n\nCheck that all substrates and products within the given Reaction have the same units, and that the units of the reaction's rate expression are internally consistent (i.e. if the rate involves sums, each term in the sum has the same units).\n\n\n\n\n\nvalidate(rs::ReactionSystem, info::String=\"\")\n\nCheck that all species in the ReactionSystem have the same units, and that the rate laws of all reactions reduce to units of (species units) / (time units).\n\nNotes:\n\nDoes not check subsystems too.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Inside, validate uses get_unit, which may be directly applied to any term. Note that validate will not throw an error in the event of incompatible units, but get_unit will. If you would rather receive a warning instead of an error, use safe_get_unit which will yield nothing in the event of an error. Unit agreement is tested with ModelingToolkit.equivalent(u1,u2). ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"ModelingToolkit.get_unit","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#ModelingToolkit.get_unit","page":"Model Validation and Units","title":"ModelingToolkit.get_unit","text":"Find the unit of a symbolic item.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Example usage below. Note that ModelingToolkit does not force unit conversions to preferred units in the event of nonstandard combinations – it merely checks that the equations are consistent. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit, Unitful\r\n@parameters τ [unit = u\"ms\"]\r\n@variables t [unit = u\"ms\"] E(t) [unit = u\"kJ\"] P(t) [unit = u\"MW\"]\r\nD = Differential(t)\r\neqs = eqs = [D(E) ~ P - E/τ,\r\n                0 ~ P       ]\r\nModelingToolkit.validate(eqs) #Returns true\r\nModelingToolkit.validate(eqs[1]) #Returns true\r\nModelingToolkit.get_unit(eqs[1].rhs) #Returns u\"kJ ms^-1\"","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"An example of an inconsistent system: at present, ModelingToolkit requires that the units of all terms in an equation or sum to be equal-valued (ModelingToolkit.equivalent(u1,u2)), rather that simply dimensionally consistent. In the future, the validation stage may be upgraded to support the insertion of conversion factors into the equations. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit, Unitful\r\n@parameters τ [unit = u\"ms\"]\r\n@variables t [unit = u\"ms\"] E(t) [unit = u\"J\"] P(t) [unit = u\"MW\"]\r\nD = Differential(t)\r\neqs = eqs = [D(E) ~ P - E/τ,\r\n                0 ~ P       ]\r\nModelingToolkit.validate(eqs) #Returns false while displaying a warning message","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#User-Defined-Registered-Functions-and-Types","page":"Model Validation and Units","title":"User-Defined Registered Functions and Types","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"In order to validate user-defined types and registered functions, specialize get_unit.  Single-parameter calls to get_unit expect an object type, while two-parameter calls expect a function type as the first argument, and a vector of arguments as the  second argument.","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit\r\n# Composite type parameter in registered function\r\n@parameters t\r\nD = Differential(t)\r\nstruct NewType\r\n    f\r\nend\r\n@register_symbolic dummycomplex(complex::Num, scalar)\r\ndummycomplex(complex, scalar) = complex.f - scalar\r\n\r\nc = NewType(1)\r\nMT.get_unit(x::NewType) = MT.get_unit(x.f)\r\nfunction MT.get_unit(op::typeof(dummycomplex),args)\r\n    argunits = MT.get_unit.(args)\r\n    MT.get_unit(-,args)\r\nend\r\n\r\nsts = @variables a(t)=0 [unit = u\"cm\"]\r\nps = @parameters s=-1 [unit = u\"cm\"] c=c [unit = u\"cm\"]\r\neqs = [D(a) ~ dummycomplex(c, s);]\r\nsys = ODESystem(eqs, t, [sts...;], [ps...;], name=:sys)\r\nsys_simple = structural_simplify(sys)","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Unitful-Literals","page":"Model Validation and Units","title":"Unitful Literals","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"In order for a function to work correctly during both validation & execution, the function must be unit-agnostic. That is, no unitful literals may be used. Any unitful quantity must either be a parameter or variable. For example, these equations will not validate successfully. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit, Unitful\r\n@variables t [unit = u\"ms\"] E(t) [unit = u\"J\"] P(t) [unit = u\"MW\"]\r\nD = Differential(t)\r\neqs = [D(E) ~ P - E/1u\"ms\"   ]\r\nModelingToolkit.validate(eqs) #Returns false while displaying a warning message\r\n\r\nmyfunc(E) = E/1u\"ms\"\r\neqs = [D(E) ~ P - myfunc(E) ]\r\nModelingToolkit.validate(eqs) #Returns false while displaying a warning message","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Instead, they should be parameterized:","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"using ModelingToolkit, Unitful\r\n@parameters τ [unit = u\"ms\"]\r\n@variables t [unit = u\"ms\"] E(t) [unit = u\"kJ\"] P(t) [unit = u\"MW\"]\r\nD = Differential(t)\r\neqs = [D(E) ~ P - E/τ]\r\nModelingToolkit.validate(eqs) #Returns true\r\n\r\nmyfunc(E,τ) = E/τ \r\neqs = [D(E) ~ P - myfunc(E,τ)]\r\nModelingToolkit.validate(eqs) #Returns true","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"It is recommended not to circumvent unit validation by specializing user-defined functions on Unitful arguments vs. Numbers. This both fails to take advantage of validate for ensuring correctness, and may cause in errors in the future when ModelingToolkit is extended to support eliminating Unitful literals from functions.","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Other-Restrictions","page":"Model Validation and Units","title":"Other Restrictions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Unitful provides non-scalar units such as dBm, °C, etc. At this time, ModelingToolkit only supports scalar quantities. Additionally, angular degrees (°) are not supported because trigonometric functions will treat plain numerical values as radians, which would lead systems validated using degrees to behave erroneously when being solved. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Troubleshooting-and-Gotchas","page":"Model Validation and Units","title":"Troubleshooting & Gotchas","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"If a system fails to validate due to unit issues, at least one warning message will appear, including a line number as well as the unit types and expressions that were in conflict. Some system constructors re-order equations before the unit checking can be done, in which case the equation numbers may be inaccurate. The printed expression that the problem resides in is always correctly shown.","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Symbolic exponents for unitful variables are supported (ex: P^γ in thermodynamics). However, this means that ModelingToolkit cannot reduce such expressions to Unitful.Unitlike subtypes at validation time because the exponent value is not available. In this case ModelingToolkit.get_unit is type-unstable, yielding a symbolic result, which can still be checked for symbolic equality with ModelingToolkit.equivalent. ","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/#Parameter-and-Initial-Condition-Values","page":"Model Validation and Units","title":"Parameter & Initial Condition Values","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Parameter and initial condition values are supplied to problem constructors as plain numbers, with the understanding that they have been converted to the appropriate units. This is done for simplicity of interfacing with optimization solvers. Some helper function for dealing with value maps:","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"remove_units(p::Dict) = Dict(k => Unitful.ustrip(ModelingToolkit.get_unit(k),v) for (k,v) in p)\r\nadd_units(p::Dict) = Dict(k => v*ModelingToolkit.get_unit(k) for (k,v) in p)","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"Recommended usage:","category":"page"},{"location":"modules/ModelingToolkit/basics/Validation/","page":"Model Validation and Units","title":"Model Validation and Units","text":"pars = @parameters τ [unit = u\"ms\"]\r\np = Dict(τ => 1u\"ms\")\r\nODEProblem(sys,remove_units(u0),tspan,remove_units(p))","category":"page"},{"location":"modules/LinearSolve/basics/common_solver_opts/#Common-Solver-Options-(Keyword-Arguments-for-Solve)","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"","category":"section"},{"location":"modules/LinearSolve/basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"While many algorithms have specific arguments within their constructor, the keyword arguments for solve are common across all of the algorithms in order to give composability. These are also the options taken at init time. The following are the options these algorithms take, along with their defaults.","category":"page"},{"location":"modules/LinearSolve/basics/common_solver_opts/#General-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"General Controls","text":"","category":"section"},{"location":"modules/LinearSolve/basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"alias_A: Whether to alias the matrix A or use a copy by default. When true, algorithms like LU-factorization can be faster by reusing the memory via lu!, but care must be taken as the original input will be modified. Default is false.\nalias_b: Whether to alias the matrix b or use a copy by default. When true, algorithms can write and change b upon usage. Care must be taken as the original input will be modified. Default is false.\nverbose: Whether to print extra information. Defaults to false.","category":"page"},{"location":"modules/LinearSolve/basics/common_solver_opts/#Iterative-Solver-Controls","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Iterative Solver Controls","text":"","category":"section"},{"location":"modules/LinearSolve/basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"Error controls are not used by all algorithms. Specifically, direct solves always solve completely. Error controls only apply to iterative solvers.","category":"page"},{"location":"modules/LinearSolve/basics/common_solver_opts/","page":"Common Solver Options (Keyword Arguments for Solve)","title":"Common Solver Options (Keyword Arguments for Solve)","text":"abstol: The absolute tolerance. Defaults to √(eps(eltype(A)))\nreltol: The relative tolerance. Defaults to √(eps(eltype(A)))\nmaxiters: The number of iterations allowed. Defaults to length(prob.b)\nPl,Pr: The left and right preconditioners respectively. For more information see the Preconditioners page.","category":"page"},{"location":"modules/MethodOfLines/nonuniform/#Non-Uniform-Rectilinear-Grids","page":"Non-Uniform Rectilinear Grids","title":"Non-Uniform Rectilinear Grids","text":"","category":"section"},{"location":"modules/MethodOfLines/nonuniform/","page":"Non-Uniform Rectilinear Grids","title":"Non-Uniform Rectilinear Grids","text":"For more information on how to use a non-uniform rectilinear grid, see the docs for MOLFiniteDifference","category":"page"},{"location":"modules/SymbolicNumericIntegration/#SymbolicNumericIntegration.jl","page":"Home","title":"SymbolicNumericIntegration.jl","text":"","category":"section"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"SymbolicNumericIntegration.jl is a hybrid symbolic/numerical integration package that works on the Julia Symbolics expressions.","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"SymbolicNumericIntegration.jl uses a randomized algorithm based on a hybrid of the method of undetermined coefficients and sparse regression and is able to solve a large subset of basic standard integrals (polynomials, exponential/logarithmic, trigonometric and hyperbolic, inverse trigonometric and hyperbolic, rational and square root). The basis of how it works and the theory of integration using the Symbolic-Numeric methods refer to Basis of Symbolic-Numeric Integration.","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"Function integrate returns the integral of a univariate expression with constant real or complex coefficients. integrate returns a tuple with three values. The first one is the solved integral, the second one is the sum of the unsolved terms, and the third value is the residual error. If integrate is successful, the unsolved portion is reported as 0.","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"using Symbolics\nusing SymbolicNumericIntegration\n\n@variables x\n\njulia> integrate(3x^3 + 2x - 5)\n(x^2 + (3//4)*(x^4) - (5x), 0, 0)\n\njulia> integrate((5 + 2x)^-1)\n((1//2)*log((5//2) + x), 0, 0.0)\n\njulia> integrate(1 / (6 + x^2 - (5x)))\n(log(x - 3) - log(x - 2), 0, 3.339372764128952e-16)\n\ny = integrate(1 / (x^2 - 16))\n((1//8)*log(x - 4) - ((1//8)*log(4 + x)), 0, 1.546926788028958e-16)\n\njulia> integrate(x^2/(16 + x^2))\n(x + 4atan((-1//4)*x), 0, 1.3318788420751984e-16)\n\njulia> integrate(x^2/sqrt(4 + x^2))\n((1//2)*x*((4 + x^2)^0.5) - ((2//1)*log(x + sqrt(4 + x^2))), 0, 8.702422633074313e-17)\n\njulia> integrate(x^2*log(x))\n((1//3)*log(x)*(x^3) - ((1//9)*(x^3)), 0, 0)\n\njulia> integrate(x^2*exp(x))\n(2exp(x) + exp(x)*(x^2) - (2x*exp(x)), 0, 0)\n\njulia> integrate(tan(2x))\n((-1//2)*log(cos(2x)), 0, 0)\n\njulia> integrate(sec(x)*tan(x))\n(cos(x)^-1, 0, 0)\n\njulia> integrate(cosh(2x)*exp(x))\n((2//3)*exp(x)*sinh(2x) - ((1//3)*exp(x)*cosh(2x)), 0, 7.073930088880992e-8)\n\njulia> integrate(cosh(x)*sin(x))\n((1//2)*sin(x)*sinh(x) - ((1//2)*cos(x)*cosh(x)), 0, 4.8956233716268386e-17)\n\njulia> integrate(cosh(2x)*sin(3x))\n(0.153845sinh(2x)*sin(3x) - (0.23077cosh(2x)*cos(3x)), 0, 4.9807620877373405e-6)\n\njulia> integrate(log(log(x))*(x^-1))\n(log(x)*log(log(x)) - log(x), 0, 0)\n\njulia> integrate(exp(x^2))\n(0, exp(x^2), Inf)    # as expected!","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"integrate has the form integrate(y; kw...) or integrate(y, x; kw...), where y is the integrand and the optional x is the variable of integration. The keyword parameters are:","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"abstol (default 1e-6): the error tolerance to accept a solution.\nsymbolic (default true): if true, pure symbolic integration is attempted first.\nbypass (default false): if true, the whole expression is considered at once and not per term.\nnum_steps (default 2): one plus the number of expanded basis to check (if num_steps is 1, only the main basis is checked).\nnum_trials (default 5): the number of attempts to solve the integration numerically for each basis set.\nshow_basis (default false): print the basis set, useful for debugging. Only works if verbose is also set.\nhomotopy (default: true as of version 0.7.0): uses the continuous Homotopy operators to generate the integration candidates.\nverbose (default false): if true, prints extra (and voluminous!) debugging information.\nradius (default 1.0): the starting radius to generate random test points.\nopt (default STLSQ(exp.(-10:1:0))): the optimizer passed to sparse_regression!.\nmax_basis (default 110): the maximum number of expression in the basis.\ncomplex_plane (default true): random test points are generated on the complex plane (only over the real axis if complex_plane is false).","category":"page"},{"location":"modules/SymbolicNumericIntegration/#Testing","page":"Home","title":"Testing","text":"","category":"section"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"test/runtests.jl contains a test suite of 160 easy to moderate test integrals (can be run by calling test_integrals). Currently, SymbolicNumericIntegration.jl solves more than 90% of its test suite.","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"Additionally, 12 test suites from the Rule-based Integrator (Rubi) are included in the /test directory. For example, we can test the first one as below (Axiom refers to the format of the test files)","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"  using SymbolicNumericIntegration\n  include(\"test/axiom.jl\")  # note, you may need to use the correct path\n\n  L = convert_axiom(:Apostle)   # can also use L = convert_axiom(1)  \n  test_axiom(L, false; bypass=false, verbose=false, homotopy=true)","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"The test suites description based on the header of the files in the Rubi directory are","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"name id comment\n:Apostle 1 Tom M. Apostol - Calculus, Volume I, Second Edition (1967)\n:Bondarenko 2 Vladimir Bondarenko Integration Problems\n:Bronstein 3 Manuel Bronstein - Symbolic Integration Tutorial (1998)\n:Charlwood 4 Kevin Charlwood - Integration on Computer Algebra Systems (2008)\n:Hearn 5 Anthony Hearn - Reduce Integration Test Package\n:Hebisch 6 Waldek Hebisch - email May 2013\n:Jeffrey 7 David Jeffrey - Rectifying Transformations for Trig Integration (1997)\n:Moses 8 Joel Moses - Symbolic Integration Ph.D. Thesis (1967)\n:Stewart 9 James Stewart - Calculus (1987)\n:Timofeev 10 A. F. Timofeev - Integration of Functions (1948)\n:Welz 11 Martin Welz - posts on Sci.Math.Symbolic\n:Webster 12 Michael Wester","category":"page"},{"location":"modules/SymbolicNumericIntegration/#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"If you use SymbolicNumericIntegration.jl, please cite Symbolic-Numeric Integration of Univariate Expressions based on Sparse Regression:","category":"page"},{"location":"modules/SymbolicNumericIntegration/","page":"Home","title":"Home","text":"@article{Iravanian2022,   \n   author = {Shahriar Iravanian and Carl Julius Martensen and Alessandro Cheli and Shashi Gowda and Anand Jain and Julia Computing and Yingbo Ma and Chris Rackauckas},\n   doi = {10.48550/arxiv.2201.12468},\n   month = {1},\n   title = {Symbolic-Numeric Integration of Univariate Expressions based on Sparse Regression},\n   url = {https://arxiv.org/abs/2201.12468v2},\n   year = {2022},\n}","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"CurrentModule = NeuralOperators","category":"page"},{"location":"modules/NeuralOperators/#NeuralOperators","page":"Home","title":"NeuralOperators","text":"","category":"section"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"(Image: ) (Image: )\nGround Truth Inferenced","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"The demonstration shown above is Navier-Stokes equation learned by the MarkovNeuralOperator with only one time step information. Example can be found in example/FlowOverCircle.","category":"page"},{"location":"modules/NeuralOperators/#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"The package can be installed with the Julia package manager. From the Julia REPL, type ] to enter the Pkg REPL mode and run:","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"pkg> add NeuralOperators","category":"page"},{"location":"modules/NeuralOperators/#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"modules/NeuralOperators/#Fourier-Neural-Operator","page":"Home","title":"Fourier Neural Operator","text":"","category":"section"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"model = Chain(\n    # lift (d + 1)-dimensional vector field to n-dimensional vector field\n    # here, d == 1 and n == 64\n    Dense(2, 64),\n    # map each hidden representation to the next by integral kernel operator\n    OperatorKernel(64=>64, (16, ), FourierTransform, gelu),\n    OperatorKernel(64=>64, (16, ), FourierTransform, gelu),\n    OperatorKernel(64=>64, (16, ), FourierTransform, gelu),\n    OperatorKernel(64=>64, (16, ), FourierTransform),\n    # project back to the scalar field of interest space\n    Dense(64, 128, gelu),\n    Dense(128, 1),\n)","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"Or one can just call:","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"model = FourierNeuralOperator(\n    ch=(2, 64, 64, 64, 64, 64, 128, 1),\n    modes=(16, ),\n    σ=gelu\n)","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"And then train as a Flux model.","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"loss(𝐱, 𝐲) = l₂loss(model(𝐱), 𝐲)\nopt = Flux.Optimiser(WeightDecay(1f-4), Flux.Adam(1f-3))\nFlux.@epochs 50 Flux.train!(loss, params(model), data, opt)","category":"page"},{"location":"modules/NeuralOperators/#DeepONet","page":"Home","title":"DeepONet","text":"","category":"section"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"# tuple of Ints for branch net architecture and then for trunk net,\n# followed by activations for branch and trunk respectively\nmodel = DeepONet((32, 64, 72), (24, 64, 72), σ, tanh)","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"Or specify branch and trunk as separate Chain from Flux and pass to DeepONet","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"branch = Chain(Dense(32, 64, σ), Dense(64, 72, σ))\ntrunk = Chain(Dense(24, 64, tanh), Dense(64, 72, tanh))\nmodel = DeepONet(branch, trunk)","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"You can again specify loss, optimization and training parameters just as you would for a simple neural network with Flux.","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"loss(xtrain, ytrain, sensor) = Flux.Losses.mse(model(xtrain, sensor), ytrain)\nevalcb() = @show(loss(xval, yval, grid))\n\nlearning_rate = 0.001\nopt = Adam(learning_rate)\nparameters = params(model)\nFlux.@epochs 400 Flux.train!(loss, parameters, [(xtrain, ytrain, grid)], opt, cb=evalcb)","category":"page"},{"location":"modules/NeuralOperators/","page":"Home","title":"Home","text":"A more complete example using DeepONet architecture to solve Burgers' equation can be found in the examples.","category":"page"},{"location":"modules/DiffEqNoiseProcess/#DiffEqNoiseProcess.jl:-Noise-Processes-for-Stochastic-Modeling","page":"Home","title":"DiffEqNoiseProcess.jl: Noise Processes for Stochastic Modeling","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"Noise processes are essential in continuous stochastic modeling. The NoiseProcess types are distributionally-exact, meaning they are not solutions of stochastic differential equations and instead are directly generated according to their analytical distributions. These processes are used as the noise term in the SDE and RODE solvers. Additionally, the noise processes themselves can be simulated and solved using the DiffEq common interface (including the Monte Carlo interface).","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"To install DiffEqNoiseProcess.jl, use the Julia package manager:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"DiffEqNoiseProcess\")","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Using-Noise-Processes","page":"Home","title":"Using Noise Processes","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/#Passing-a-Noise-Process-to-a-Problem-Type","page":"Home","title":"Passing a Noise Process to a Problem Type","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"AbstractNoiseProcesses can be passed directly to the problem types to replace the standard Wiener process (Brownian motion) with your choice of noise. To do this, simply construct the noise and pass it to the noise keyword argument:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"μ = 1.0\nσ = 2.0\nW = GeometricBrownianMotionProcess(μ,σ,0.0,1.0,1.0)\n# ...\n# Define f,g,u0,tspan for a SDEProblem\n# ...\nprob = SDEProblem(f,g,u0,tspan,noise=W)","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Basic-Interface","page":"Home","title":"Basic Interface","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"The NoiseProcess acts like a DiffEq solution. For some noise process W, you can get its ith timepoint like W[i] and the associated time W.t[i]. If the NoiseProcess has a bridging distribution defined, it can be interpolated to arbitrary time points using W(t). Note that every interpolated value is saved to the NoiseProcess so that way it can stay distributionally correct. A plot recipe is provided which plots the timeseries.","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Direct-Simulation-of-the-Noise-Process","page":"Home","title":"Direct Simulation of the Noise Process","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"Since the NoiseProcess types are distribution-exact and do not require the stochastic differential equation solvers, many times one would like to directly simulate trajectories from these proecesses. The NoiseProcess has a NoiseProcessProblem type:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"NoiseProblem(noise,tspan)","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"for which solve works. For example, we can simulate a distributionally-exact Geometric Brownian Motion solution by:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"μ = 1.0\nσ = 2.0\nW = GeometricBrownianMotionProcess(μ,σ,0.0,1.0,1.0)\nprob = NoiseProblem(W,(0.0,1.0))\nsol = solve(prob;dt=0.1)","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"solve requires the dt is given, the solution it returns is a NoiseProcess which has stepped through the timespan. Because this follows the common interface, all of the normal functionality works. For example, we can use the Monte Carlo functionality as follows:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"monte_prob = MonteCarloProblem(prob)\nsol = solve(monte_prob;dt=0.1,num_monte=100)","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"simulates 100 Geometric Brownian Motions.","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Direct-Interface","page":"Home","title":"Direct Interface","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"Most of the time, a NoiseProcess is received from the solution of a stochastic or random differential equation, in which case sol.W gives the NoiseProcess and it is already defined along some timeseries. In other cases, NoiseProcess types are directly simulated (see below). However, NoiseProcess types can also be directly acted on. The basic functionality is given by calculate_step! to calculate a future time point, and accept_step! to accept the step. If steps are rejected, the Rejection Sampling with Memory algorithm is applied to keep the solution distributionally exact. This kind of stepping is done via:","category":"page"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"W = WienerProcess(0.0,1.0,1.0)\ndt = 0.1\nW.dt = dt\nu = nothing; p = nothing # for state-dependent distributions\ncalculate_step!(W,dt,u,p)\nfor i in 1:10\n  accept_step!(W,dt,u,p)\nend","category":"page"},{"location":"modules/DiffEqNoiseProcess/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/#diffeq_arrays","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"In many cases, a standard array may not be enough to fully hold the data for a model. Many of the solvers in DifferentialEquations.jl (only the native Julia methods) allow you to solve problems on AbstractArray types which allow you to extend the meaning of an array. This page describes some of the AbstractArray types which can be helpful for modeling differential equations problems.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/#ArrayPartitions","page":"DiffEq-Specific Array Types","title":"ArrayPartitions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"ArrayPartitions in DiffEq are used for heterogeneous arrays. For example, DynamicalODEProblem solvers use them internally to turn the separate parts into a single array. You can construct an ArrayPartition using RecursiveArrayTools.jl:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using RecursiveArrayTools\nA = ArrayPartition(x::AbstractArray...)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"where x is an array of arrays. Then, A will act like a single array, and its broadcast will be type stable, allowing for it to be used inside of the native Julia DiffEq solvers in an efficient way. This is a good way to generate an array which has different units for different parts, or different amounts of precision.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/#Usage","page":"DiffEq-Specific Array Types","title":"Usage","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"An ArrayPartition acts like a single array. A[i] indexes through the first array, then the second, etc. all linearly. But A.x is where the arrays are stored. Thus for","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using RecursiveArrayTools\nA = ArrayPartition(y,z)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"We would have A.x[1]==y and A.x[2]==z. Broadcasting like f.(A) is efficient.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/#Example:-Dynamics-Equations","page":"DiffEq-Specific Array Types","title":"Example: Dynamics Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"In this example we will show using heterogeneous units in dynamics equations. Our arrays will be:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using Unitful, RecursiveArrayTools, DiffEqBase, OrdinaryDiffEq\nusing LinearAlgebra\n\nr0 = [1131.340, -2282.343, 6672.423]u\"km\"\nv0 = [-5.64305, 4.30333, 2.42879]u\"km/s\"\nΔt = 86400.0*365u\"s\"\nμ = 398600.4418u\"km^3/s^2\"\nrv0 = ArrayPartition(r0,v0)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Here, r0 is the initial positions, and v0 are the initial velocities. rv0 is the ArrayPartition initial condition. We now write our update function in terms of the ArrayPartition:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"function f(dy, y, μ, t)\n    r = norm(y.x[1])\n    dy.x[1] .= y.x[2]\n    dy.x[2] .= -μ .* y.x[1] / r^3\nend","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Notice that y.x[1] is the r part of y, and y.x[2] is the v part of y. Using this kind of indexing is type stable, even though the array itself is heterogeneous. Note that one can also use things like 2y or y.+x and the broadcasting will be efficient.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Now to solve our equations, we do the same thing as always in DiffEq:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"prob = ODEProblem(f, rv0, (0.0u\"s\", Δt), μ)\nsol = solve(prob, Vern8())","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/#MultiScaleArrays","page":"DiffEq-Specific Array Types","title":"MultiScaleArrays","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"The multi-scale modeling functionality is provided by MultiScaleArrays.jl. It allows for designing a multi-scale model as an extension of an array, which in turn can be directly used in the native Julia solvers of DifferentialEquations.jl.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"For more information, please see the MultiScaleArrays.jl README.","category":"page"},{"location":"modules/DiffEqOperators/operators/jacobian_vector_product/#Jacobian-Vector-Product-Operators","page":"Jacobian-Vector Product Operators","title":"Jacobian-Vector Product Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/jacobian_vector_product/","page":"Jacobian-Vector Product Operators","title":"Jacobian-Vector Product Operators","text":"JacVecOperator{T}(f,u::AbstractArray,p=nothing,t::Union{Nothing,Number}=nothing;autodiff=true,ishermitian=false,opnorm=true)","category":"page"},{"location":"modules/DiffEqOperators/operators/jacobian_vector_product/","page":"Jacobian-Vector Product Operators","title":"Jacobian-Vector Product Operators","text":"The JacVecOperator is a linear operator J*v where J acts like df/du for some function f(u,p,t). For in-place operations mul!(w,J,v), f is an in-place function f(du,u,p,t).","category":"page"},{"location":"modules/Surrogates/sphere_function/#Sphere-function","page":"Sphere function","title":"Sphere function","text":"","category":"section"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"The sphere function of dimension d is defined as: f(x) = sum_i=1^d x_i^2 with lower bound -10 and upper bound 10.","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"function sphere_function(x)\n    return sum(x.^2)\nend","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"The 1D case is just a simple parabola, let's plot it:","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"n = 20\nlb = -10\nub = 10\nx = sample(n,lb,ub,SobolSample())\ny = sphere_function.(x)\nxs = lb:0.001:ub\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"Fitting RadialSurrogate with different radial basis:","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"rad_1d_linear = RadialBasis(x,y,lb,ub)\nrad_1d_cubic = RadialBasis(x,y,lb,ub,rad = cubicRadial)\nrad_1d_multiquadric = RadialBasis(x,y,lb,ub, rad = multiquadricRadial)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, rad_1d_linear.(xs), label=\"Radial surrogate with linear\", legend=:top)\nplot!(xs, rad_1d_cubic.(xs), label=\"Radial surrogate with cubic\", legend=:top)\nplot!(xs, rad_1d_multiquadric.(xs), label=\"Radial surrogate with multiquadric\", legend=:top)","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"Fitting Lobachevsky Surrogate with different values of hyperparameters alpha:","category":"page"},{"location":"modules/Surrogates/sphere_function/","page":"Sphere function","title":"Sphere function","text":"loba_1 = LobachevskySurrogate(x,y,lb,ub)\nloba_2 = LobachevskySurrogate(x,y,lb,ub,alpha = 1.5, n = 6)\nloba_3 = LobachevskySurrogate(x,y,lb,ub,alpha = 0.3, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2, 120), legend=:top)\nplot!(xs,sphere_function.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky surrogate 1\", legend=:top)\nplot!(xs, loba_2.(xs), label=\"Lobachevsky surrogate 2\", legend=:top)\nplot!(xs, loba_3.(xs), label=\"Lobachevsky surrogate 3\", legend=:top)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Composing-Ordinary-Differential-Equations","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"This is an introductory example for the usage of ModelingToolkit (MTK). It illustrates the basic user-facing functionality by means of some examples of Ordinary Differential Equations (ODE). Some references to more specific documentation are given at appropriate places.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Copy-Pastable-Simplified-Example","page":"Composing Ordinary Differential Equations","title":"Copy-Pastable Simplified Example","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"A much deeper tutorial with forcing functions and sparse Jacobians is all below. But if you want to just see some code and run, here's an example:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"using ModelingToolkit\r\n\r\n@variables t x(t) RHS(t)  # independent and dependent variables\r\n@parameters τ       # parameters\r\nD = Differential(t) # define an operator for the differentiation w.r.t. time\r\n\r\n# your first ODE, consisting of a single equation, indicated by ~\r\n@named fol = ODESystem([ D(x)  ~ (1 - x)/τ])\r\n\r\nusing DifferentialEquations: solve\r\nusing Plots: plot\r\n\r\nprob = ODEProblem(fol, [x => 0.0], (0.0,10.0), [τ => 3.0])\r\nsol = solve(prob)\r\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"(Image: Simulation result of first-order lag element, with right-hand side) Now let's start digging into MTK!","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Your-very-first-ODE","page":"Composing Ordinary Differential Equations","title":"Your very first ODE","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Let us start with a minimal example. The system to be modelled is a","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"first-order lag element:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"dotx = fracf(t) - x(t)tau","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Here, t is the independent variable (time), x(t) is the (scalar) state variable, f(t) is an external forcing function, and tau is a constant parameter. In MTK, this system can be modelled as follows. For simplicity, we first set the forcing function to a constant value.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"using ModelingToolkit\r\n\r\n@variables t x(t)  # independent and dependent variables\r\n@parameters τ       # parameters\r\nD = Differential(t) # define an operator for the differentiation w.r.t. time\r\n\r\n# your first ODE, consisting of a single equation, indicated by ~\r\n@named fol_model = ODESystem(D(x) ~ (1 - x)/τ)\r\n      # Model fol_model with 1 equations\r\n      # States (1):\r\n      #   x(t)\r\n      # Parameters (1):\r\n      #   τ","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Note that equations in MTK use the tilde character (~) as equality sign. Also note that the @named macro simply ensures that the symbolic name matches the name in the REPL. If omitted, you can directly set the name keyword.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"After construction of the ODE, you can solve it using DifferentialEquations.jl:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"using DifferentialEquations\r\nusing Plots\r\n\r\nprob = ODEProblem(fol_model, [x => 0.0], (0.0,10.0), [τ => 3.0])\r\nplot(solve(prob))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"(Image: Simulation result of first-order lag element)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"The initial state and the parameter values are specified using a mapping from the actual symbolic elements to their values, represented as an array of Pairs, which are constructed using the => operator.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Algebraic-relations-and-structural-simplification","page":"Composing Ordinary Differential Equations","title":"Algebraic relations and structural simplification","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"You could separate the calculation of the right-hand side, by introducing an intermediate variable RHS:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"@variables RHS(t)\r\n@named fol_separate = ODESystem([ RHS  ~ (1 - x)/τ,\r\n                                  D(x) ~ RHS ])\r\n      # Model fol_separate with 2 equations\r\n      # States (2):\r\n      #   x(t)\r\n      #   RHS(t)\r\n      # Parameters (1):\r\n      #   τ","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"To directly solve this system, you would have to create a Differential-Algebraic Equation (DAE) problem, since besides the differential equation, there is an additional algebraic equation now. However, this DAE system can obviously be transformed into the single ODE we used in the first example above. MTK achieves this by means of structural simplification:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"fol_simplified = structural_simplify(fol_separate)\r\n\r\nequations(fol_simplified)\r\n      # 1-element Array{Equation,1}:\r\n      #  Differential(t)(x(t)) ~ (τ^-1)*(1 - x(t))\r\n\r\nequations(fol_simplified) == equations(fol_model)\r\n      # true","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"You can extract the equations from a system using equations (and, in the same way, states and parameters). The simplified equation is exactly the same as the original one, so the simulation performance will also be the same. However, there is one difference. MTK does keep track of the eliminated algebraic variables as \"observables\" (see Observables and Variable Elimination). That means, MTK still knows how to calculate them out of the information available in a simulation result. The intermediate variable RHS therefore can be plotted along with the state variable. Note that this has to be requested explicitly, through:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"prob = ODEProblem(fol_simplified, [x => 0.0], (0.0,10.0), [τ => 3.0])\r\nsol = solve(prob)\r\nplot(sol, vars=[x, RHS])","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"(Image: Simulation result of first-order lag element, with right-hand side)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Note that similarly the indexing of the solution works via the names, and so sol[x] gives the timeseries for x, sol[x,2:10] gives the 2nd through 10th values of x matching sol.t, etc. Note that this works even for variables which have been eliminated, and thus sol[RHS] retrieves the values of RHS.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Specifying-a-time-variable-forcing-function","page":"Composing Ordinary Differential Equations","title":"Specifying a time-variable forcing function","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"What if the forcing function (the \"external input\") f(t) is not constant? Obviously, one could use an explicit, symbolic function of time:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"@variables f(t)\r\n@named fol_variable_f = ODESystem([f ~ sin(t), D(x) ~ (f - x)/τ])","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"But often there is time-series data, such as measurement data from an experiment, we want to embed as data in the simulation of a PDE, or as a forcing function on the right-hand side of an ODE – is it is the case here. For this, MTK allows to \"register\" arbitrary Julia functions, which are excluded from symbolic transformations but are just used as-is. So, you could, for example, interpolate a given time series using DataInterpolations.jl. Here, we illustrate this option by a simple lookup (\"zero-order hold\") of a vector of random values:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"value_vector = randn(10)\r\nf_fun(t) = t >= 10 ? value_vector[end] : value_vector[Int(floor(t))+1]\r\n@register_symbolic f_fun(t)\r\n\r\n@named fol_external_f = ODESystem([f ~ f_fun(t), D(x) ~ (f - x)/τ])\r\nprob = ODEProblem(structural_simplify(fol_external_f), [x => 0.0], (0.0,10.0), [τ => 0.75])\r\n\r\nsol = solve(prob)\r\nplot(sol, vars=[x,f])","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"(Image: Simulation result of first-order lag element, step-wise forcing function)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Building-component-based,-hierarchical-models","page":"Composing Ordinary Differential Equations","title":"Building component-based, hierarchical models","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Working with simple one-equation systems is already fun, but composing more complex systems from simple ones is even more fun. Best practice for such a \"modeling framework\" could be to use factory functions for model components:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"function fol_factory(separate=false;name)\r\n    @parameters τ\r\n    @variables t x(t) f(t) RHS(t)\r\n\r\n    eqs = separate ? [RHS ~ (f - x)/τ,\r\n                      D(x) ~ RHS] :\r\n                      D(x) ~(f - x)/τ\r\n\r\n    ODESystem(eqs;name)\r\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Such a factory can then used to instantiate the same component multiple times, but allows for customization:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"@named fol_1 = fol_factory()\r\n@named fol_2 = fol_factory(true) # has observable RHS","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"The @named macro rewrites fol_2 = fol_factory(true) into fol_2 = fol_factory(true,:fol_2). Now, these two components can be used as subsystems of a parent system, i.e. one level higher in the model hierarchy. The connections between the components again are just algebraic relations:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"connections = [ fol_1.f ~ 1.5,\r\n                fol_2.f ~ fol_1.x ]\r\n\r\nconnected = compose(ODESystem(connections,name=:connected), fol_1, fol_2)\r\n      # Model connected with 5 equations\r\n      # States (5):\r\n      #   fol_1₊f(t)\r\n      #   fol_2₊f(t)\r\n      #   fol_1₊x(t)\r\n      #   fol_2₊x(t)\r\n      #   fol_2₊RHS(t)\r\n      # Parameters (2):\r\n      #   fol_1₊τ\r\n      #   fol_2₊τ","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"All equations, variables and parameters are collected, but the structure of the hierarchical model is still preserved. That is, you can still get information about fol_1 by addressing it by connected.fol_1, or its parameter by connected.fol_1.τ. Before simulation, we again eliminate the algebraic variables and connection equations from the system using structural simplification:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"connected_simp = structural_simplify(connected)\r\n      # Model connected with 2 equations\r\n      # States (2):\r\n      #   fol_1₊x(t)\r\n      #   fol_2₊x(t)\r\n      # Parameters (2):\r\n      #   fol_1₊τ\r\n      #   fol_2₊τ\r\n      # Incidence matrix:\r\n      #   [1, 1]  =  ×\r\n      #   [2, 1]  =  ×\r\n      #   [2, 2]  =  ×\r\n      #   [1, 3]  =  ×\r\n      #   [2, 4]  =  ×\r\n\r\nfull_equations(connected_simp)\r\n      # 2-element Array{Equation,1}:\r\n      #  Differential(t)(fol_1₊x(t)) ~ (fol_1₊τ^-1)*(1.5 - fol_1₊x(t))\r\n      #  Differential(t)(fol_2₊x(t)) ~ (fol_2₊τ^-1)*(fol_1₊x(t) - fol_2₊x(t))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"As expected, only the two state-derivative equations remain, as if you had manually eliminated as many variables as possible from the equations. Some observed variables are not expanded unless full_equations is used. As mentioned above, the hierarchical structure is preserved though. So the initial state and the parameter values can be specified accordingly when building the ODEProblem:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"u0 = [ fol_1.x => -0.5,\r\n       fol_2.x => 1.0 ]\r\n\r\np = [ fol_1.τ => 2.0,\r\n      fol_2.τ => 4.0 ]\r\n\r\nprob = ODEProblem(connected_simp, u0, (0.0,10.0), p)\r\nplot(solve(prob))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"(Image: Simulation of connected system (two first-order lag elements in series))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"More on this topic may be found in Composing Models and Building Reusable Components.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Defaults","page":"Composing Ordinary Differential Equations","title":"Defaults","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Often it is a good idea to specify reasonable values for the initial state and the parameters of a model component. Then, these do not have to be explicitly specified when constructing the ODEProblem.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"function unitstep_fol_factory(;name)\r\n    @parameters τ\r\n    @variables t x(t)\r\n    ODESystem(D(x) ~ (1 - x)/τ; name, defaults=Dict(x=>0.0, τ=>1.0))\r\nend\r\n\r\nODEProblem(unitstep_fol_factory(name=:fol),[],(0.0,5.0),[]) |> solve","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Note that the defaults can be functions of the other variables, which is then resolved at the time of the problem construction. Of course, the factory function could accept additional arguments to optionally specify the initial state or parameter values, etc.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Symbolic-and-sparse-derivatives","page":"Composing Ordinary Differential Equations","title":"Symbolic and sparse derivatives","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"One advantage of a symbolic toolkit is that derivatives can be calculated explicitly, and that the incidence matrix of partial derivatives (the \"sparsity pattern\") can also be explicitly derived. These two facts lead to a substantial speedup of all model calculations, e.g. when simulating a model over time using an ODE solver.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"By default, analytical derivatives and sparse matrices, e.g. for the Jacobian, the matrix of first partial derivatives, are not used. Let's benchmark this (prob still is the problem using the connected_simp system above):","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"using BenchmarkTools\r\n\r\n@btime solve($prob, Rodas4());\r\n      # 251.300 μs (873 allocations: 31.18 KiB)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Now have MTK provide sparse, analytical derivatives to the solver. This has to be specified during the construction of the ODEProblem:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"prob_an = ODEProblem(connected_simp, u0, (0.0,10.0), p; jac=true, sparse=true)\r\n\r\n@btime solve($prob_an, Rodas4());\r\n      # 142.899 μs (1297 allocations: 83.96 KiB)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"The speedup is significant. For this small dense model (3 of 4 entries are populated), using sparse matrices is counterproductive in terms of required memory allocations. For large, hierarchically built models, which tend to be sparse, speedup and the reduction of memory allocation can be expected to be substantial. In addition, these problem builders allow for automatic parallelism using the structural information. For more information, see the ODESystem page.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/#Notes-and-pointers-how-to-go-on","page":"Composing Ordinary Differential Equations","title":"Notes and pointers how to go on","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Here are some notes that may be helpful during your initial steps with MTK:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Sometimes, the symbolic engine within MTK is not able to correctly identify the independent variable (e.g. time) out of all variables. In such a case, you usually get an error that some variable(s) is \"missing from variable map\". In most cases, it is then sufficient to specify the independent variable as second argument to ODESystem, e.g. ODESystem(eqs, t).\nA completely macro-free usage of MTK is possible and is discussed in a separate tutorial. This is for package developers, since the macros are only essential for automatic symbolic naming for modelers.\nVector-valued parameters and variables are possible. A cleaner, more consistent treatment of these is work in progress, though. Once finished, this introductory tutorial will also cover this feature.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Where to go next?","category":"page"},{"location":"modules/ModelingToolkit/tutorials/ode_modeling/","page":"Composing Ordinary Differential Equations","title":"Composing Ordinary Differential Equations","text":"Not sure how MTK relates to similar tools and packages? Read Comparison of ModelingToolkit vs Equation-Based and Block Modeling Languages.\nDepending on what you want to do with MTK, have a look at some of the other Symbolic Modeling Tutorials.\nIf you want to automatically convert an existing function to a symbolic representation, you might go through the ModelingToolkitize Tutorials.\nTo learn more about the inner workings of MTK, consider the sections under Basics and System Types.","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/#Training-Neural-Networks-in-Hybrid-Differential-Equations","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"Hybrid differential equations are differential equations with implicit or explicit discontinuities as specified by callbacks. In the following example, explicit dosing times are given for a pharmacometric model and the universal differential equation is trained to uncover the missing dynamical equations.","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"using DiffEqFlux, Flux, DifferentialEquations, Plots\nu0 = Float32[2.; 0.]\ndatasize = 100\ntspan = (0.0f0,10.5f0)\ndosetimes = [1.0,2.0,4.0,8.0]\n\nfunction affect!(integrator)\n    integrator.u = integrator.u.+1\nend\ncb_ = PresetTimeCallback(dosetimes,affect!,save_positions=(false,false))\nfunction trueODEfunc(du,u,p,t)\n    du .= -u\nend\nt = range(tspan[1],tspan[2],length=datasize)\n\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),callback=cb_,saveat=t))\ndudt2 = Flux.Chain(Flux.Dense(2,50,tanh),\n             Flux.Dense(50,2))\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\n\nfunction dudt(du,u,p,t)\n    du[1:2] .= -u[1:2]\n    du[3:end] .= re(p)(u[1:2]) #re(p)(u[3:end])\nend\nz0 = Float32[u0;u0]\nprob = ODEProblem(dudt,z0,tspan)\n\naffect!(integrator) = integrator.u[1:2] .= integrator.u[3:end]\ncallback = PresetTimeCallback(dosetimes,affect!,save_positions=(false,false))\n\nfunction predict_n_ode()\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Tsit5(),u0=z0,p=p,callback=callback,saveat=t,sensealg=ReverseDiffAdjoint()))[1:2,:]\n    #Array(solve(prob,Tsit5(),u0=z0,p=p,saveat=t))[1:2,:]\nend\n\nfunction loss_n_ode()\n    pred = predict_n_ode()\n    loss = sum(abs2,ode_data .- pred)\n    loss\nend\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\n\ncba = function (;doplot=false) #callback function to observe training\n  pred = predict_n_ode()\n  display(sum(abs2,ode_data .- pred))\n  # plot current prediction against data\n  pl = scatter(t,ode_data[1,:],label=\"data\")\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\n  display(plot(pl))\n  return false\nend\ncba()\n\nps = Flux.params(p)\ndata = Iterators.repeated((), 200)\nFlux.train!(loss_n_ode, ps, data, ADAM(0.05), cb = cba)","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"(Image: Hybrid Universal Differential Equation)","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/#Note-on-Sensitivity-Methods","page":"Training Neural Networks in Hybrid Differential Equations","title":"Note on Sensitivity Methods","text":"","category":"section"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"The continuous adjoint sensitivities BacksolveAdjoint, InterpolatingAdjoint, and QuadratureAdjoint are compatible with events for ODEs. BacksolveAdjoint and InterpolatingAdjoint can also handle events for SDEs. Use BacksolveAdjoint if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/hybrid_diffeq/","page":"Training Neural Networks in Hybrid Differential Equations","title":"Training Neural Networks in Hybrid Differential Equations","text":"All methods based on discrete sensitivity analysis via automatic differentiation, like ReverseDiffAdjoint, TrackerAdjoint, or ForwardDiffSensitivity are the methods to use (and ReverseDiffAdjoint is demonstrated above), are compatible with events. This applies to SDEs, DAEs, and DDEs as well.","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/#PhysicsInformedNN-Discretizer-for-PDESystems","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Using the PINNs solver, we can solve general nonlinear PDEs:","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: generalPDE)","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"with suitable boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: bcs)","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"where time t is a special component of x, and Ω contains the temporal domain.","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"PDEs are defined using the ModelingToolkit.jl PDESystem:","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"@named pde_system = PDESystem(eq,bcs,domains,param,var)","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Here, eq is the equation, bcs represents the boundary conditions, param is the parameter of the equation (like [x,y]), and var represents variables (like [u]). For more information, see the ModelingToolkit.jl PDESystem documentation.","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/#The-PhysicsInformedNN-Discretizer","page":"PhysicsInformedNN Discretizer for PDESystems","title":"The PhysicsInformedNN Discretizer","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"NeuralPDE.PhysicsInformedNN\nNeuralPDE.Phi\nSciMLBase.discretize(::PDESystem, ::NeuralPDE.PhysicsInformedNN)","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/#NeuralPDE.PhysicsInformedNN","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PhysicsInformedNN","text":"```julia PhysicsInformedNN(chain,                   strategy;                   initparams = nothing,                   phi = nothing,                   paramestim = false,                   additionalloss = nothing,                   adaptiveloss = nothing,                   logger = nothing,                   log_options = LogOptions(),                   iteration = nothing,                   kwargs...) where {iip}\n\nA discretize algorithm for the ModelingToolkit PDESystem interface which transforms a PDESystem into an OptimizationProblem using the Physics-Informed Neural Networks (PINN) methodology.\n\nPositional Arguments\n\nchain: a vector of Flux.jl or Lux.jl chains with a d-dimensional input and a 1-dimensional output corresponding to each of the dependent variables. Note that this specification respects the order of the dependent variables as specified in the PDESystem.\nstrategy: determines which training strategy will be used. See the Training Strategy documentation for more details.\n\nKeyword Arguments\n\ninit_params: the initial parameters of the neural networks. This should match the specification of the chosen chain library. For example, if a Flux.chain is used, then init_params should match Flux.destructure(chain)[1] in shape. If init_params is not given, then the neural network default parameters are used. Note that for Lux, the default will convert to Float64.\nphi: a trial solution, specified as phi(x,p) where x is the coordinates vector for the dependent variable and p are the weights of the phi function (generally the weights of the neural network defining phi). By default this is generated from the chain. This should only be used to more directly impose functional information in the training problem, for example imposing the boundary condition by the test function formulation.\nadaptive_loss: the choice for the adaptive loss function. See the adaptive loss page for more details. Defaults to no adaptivity.\nadditional_loss: a function additional_loss(phi, θ, p_) where phi are the neural network trial solutions, θ are the weights of the neural network(s), and p_ are the hyperparameters of the OptimizationProblem. If param_estim = true, then θ additionally contains the parameters of the differential equation appended to the end of the vector.\nparam_estim: whether the parameters of the differential equation should be included in the values sent to the additional_loss function. Defaults to true.\nlogger: ?? needs docs\nlog_options: ?? why is this separate from the logger?\niteration: used to control the iteration counter???\nkwargs: Extra keyword arguments which are splatted to the OptimizationProblem on solve.\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/pinns/#NeuralPDE.Phi","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.Phi","text":"An encoding of the test function phi that is used for calculating the PDE value at domain points x\n\nFields:\n\nf: A representation of the chain function. If FastChain, then f(x,p), if Chain then f(p)(x) (from Flux.destructure)\nst: The state of the Lux.AbstractExplicitLayer. If a Flux.Chain then this is nothing. It should be updated on each call.\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/pinns/#SciMLBase.discretize-Tuple{PDESystem, PhysicsInformedNN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.discretize","text":"prob = discretize(pde_system::PDESystem, discretization::PhysicsInformedNN)\n\nTransforms a symbolic description of a ModelingToolkit-defined PDESystem and generates an OptimizationProblem for Optimization.jl whose solution is the solution to the PDE.\n\n\n\n\n\n","category":"method"},{"location":"modules/NeuralPDE/manual/pinns/#symbolic_discretize-and-the-lower-level-interface","page":"PhysicsInformedNN Discretizer for PDESystems","title":"symbolic_discretize and the lower-level interface","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"SciMLBase.symbolic_discretize(::PDESystem, ::NeuralPDE.PhysicsInformedNN)\nNeuralPDE.PINNRepresentation\nNeuralPDE.PINNLossFunctions","category":"page"},{"location":"modules/NeuralPDE/manual/pinns/#SciMLBase.symbolic_discretize-Tuple{PDESystem, PhysicsInformedNN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.symbolic_discretize","text":"prob = symbolic_discretize(pde_system::PDESystem, discretization::PhysicsInformedNN)\n\nsymbolic_discretize is the lower level interface to discretize for inspecting internals. It transforms a symbolic description of a ModelingToolkit-defined PDESystem into a PINNRepresentation which holds the pieces required to build an OptimizationProblem for Optimization.jl whose solution is the solution to the PDE.\n\nFor more information, see discretize and PINNRepresentation.\n\n\n\n\n\n","category":"method"},{"location":"modules/NeuralPDE/manual/pinns/#NeuralPDE.PINNRepresentation","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNRepresentation","text":"PINNRepresentation`\n\nAn internal reprsentation of a physics-informed neural network (PINN). This is the struct used internally and returned for introspection by symbolic_discretize.\n\nFields\n\neqs\nThe equations of the PDE\n\nbcs\nThe boundary condition equations\n\ndomains\nThe domains for each of the independent variables\n\neq_params\n???\n\ndefaults\n???\n\ndefault_p\n???\n\nparam_estim\nWhether parameters are to be appended to the additional_loss\n\nadditional_loss\nThe additional_loss function as provided by the user\n\nadaloss\nThe adaptive loss function\n\ndepvars\nThe dependent variables of the system\n\nindvars\nThe independent variables of the system\n\ndict_indvars\nA dictionary form of the independent variables. Define the structure ???\n\ndict_depvars\nA dictionary form of the dependent variables. Define the structure ???\n\ndict_depvar_input\n???\n\nlogger\nThe logger as provided by the user\n\nmultioutput\nWhether there are multiple outputs, i.e. a system of PDEs\n\niteration\nThe iteration counter used inside of the cost function\n\ninit_params\nThe initial parameters as provided by the user. If the PDE is a system of PDEs, this will be an array of arrays. If Lux.jl is used, then this is an array of ComponentArrays.\n\nflat_init_params\nThe initial parameters as a flattened array. This is the array that is used in the construction of the OptimizationProblem. If a Lux.jl neural network is used, then this flattened form is a ComponentArray. If the equation is a system of equations, then flat_init_params.depvar.x are the parameters for the neural network corresponding to the dependent variable x, and i.e. if depvar[i] == :x then for phi[i]. If param_estim = true, then flat_init_params.p are the parameters and flat_init_params.depvar.x are the neural network parameters, so flat_init_params.depvar.x would be the parameters of the neural network for the dependent variable x if it's a system. If a Flux.jl neural network is used, this is simply an AbstractArray to be indexed and the sizes from the chains must be remembered/stored/used.\n\nphi\nThe representation of the test function of the PDE solution\n\nderivative\nThe function used for computing the derivative\n\nstrategy\nThe training strategy as provided by the user\n\npde_indvars\n???\n\nbc_indvars\n???\n\npde_integration_vars\n???\n\nbc_integration_vars\n???\n\nintegral\n???\n\nsymbolic_pde_loss_functions\nThe PDE loss functions as represented in Julia AST\n\nsymbolic_bc_loss_functions\nThe boundary condition loss functions as represented in Julia AST\n\nloss_functions\nThe PINNLossFunctions, i.e. the generated loss functions\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/pinns/#NeuralPDE.PINNLossFunctions","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNLossFunctions","text":"PINNLossFunctions`\n\nThe generated functions from the PINNRepresentation\n\nFields\n\nbc_loss_functions\nThe boundary condition loss functions\n\npde_loss_functions\nThe PDE loss functions\n\nfull_loss_function\nThe full loss function, combining the PDE and boundary condition loss functions. This is the loss function that is used by the optimizer.\n\nadditional_loss_function\nThe wrapped additional_loss, as pieced together for the optimizer.\n\ndatafree_pde_loss_functions\nThe pre-data version of the PDE loss function\n\ndatafree_bc_loss_functions\nThe pre-data version of the BC loss function\n\n\n\n\n\n","category":"type"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"using PolyChaos, LinearAlgebra\ndeg, n = 4, 20\ns_α, s_β = 2.1, 3.2\nopq = Beta01OrthoPoly(deg, s_α, s_β; Nrec=n, addQuadrature=true)\nnormsq = computeSP2(opq)\nm = 3\nt = Tensor(3,opq)\nt.get([1,2,3])\nT = [ t.get([i1,i2,i3]) for i1=0:dim(opq)-1, i2=0:dim(opq)-1, i3=0:dim(opq)-1]\n#@show normsq == diag(T[:,:,1])\n#@show normsq == diag(T[:,1,:])\n#@show normsq == diag(T[1,:,:])\nt2 = Tensor(2, opq)\n@show normsq == [ t2.get([i, i]) for i in 0:dim(opq)-1]\nusing SpecialFunctions\nsupp = (0, 1)\nw(t) = (t^(s_α-1)*(1-t)^(s_β-1)/SpecialFunctions.beta(s_α,s_β))\nmy_meas = Measure(\"my_meas\", w, supp, false)\nmy_opq = OrthoPoly(\"my_op\", deg, my_meas; Nrec=n, addQuadrature = true)\nmy_normsq = computeSP2(my_opq)\nmy_t = Tensor(m, my_opq)\nmy_T = [ my_t.get([i1,i2,i3]) for i1=0:dim(opq)-1,i2=0:dim(opq)-1,i3=0:dim(opq)-1]\n@show abs.(normsq-my_normsq)\n@show norm(T-my_T)\nmop = MultiOrthoPoly([opq, my_opq], deg)\nmt2 = Tensor(2,mop)\nmt3 = Tensor(3,mop)\nmT2 = [ mt2.get([i,i]) for i=0:dim(mop)-1 ]\nmop.ind\nind_opq = findUnivariateIndices(1,mop.ind)\nind_my_opq = findUnivariateIndices(2,mop.ind)\n@show mT2[ind_opq] - normsq\n@show mT2[ind_my_opq] - my_normsq;","category":"page"},{"location":"modules/PolyChaos/scalar_products/#ComputationOfScalarProducts","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"","category":"section"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"By now, we are able to construct orthogonal polynomials, and to construct quadrature rules for a given nonnegative weight function, respectively. Now we combine both ideas to solve integrals involving the orthogonal polynomials","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"langle phi_i_1 phi_i_2 cdots phi_i_m-1 phi_i_m rangle\n= int phi_i_1(t) phi_i_2(t) cdots phi_i_m-1(t) phi_i_m(t) w(t) mathrmd t","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"both for the univariate and multivariate case. The integrand is a polynomial (possibly multivariate) that can be solved exactly with the appropriate Gauss quadrature rules.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"note: Note\nTo simplify notation we drop the integration interval. It is clear from the context.","category":"page"},{"location":"modules/PolyChaos/scalar_products/#Univariate-Polynomials","page":"Computation of Scalar Products","title":"Univariate Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/scalar_products/#Classical-Polynomials","page":"Computation of Scalar Products","title":"Classical Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Let's begin with a univariate basis for some classical orthogonal polynomial","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"using PolyChaos\ndeg, n = 4, 20\ns_α, s_β = 2.1, 3.2\nopq = Beta01OrthoPoly(deg, s_α, s_β; Nrec=n, addQuadrature=true)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"By setting addQuadrature = true (which is default), an n-point Gauss quadrature rule is create relative to the underlying measure opq.measure, where n is the number of recurrence coefficients stored in opq.α and opq.β.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"To compute the squared norms","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":" phi_k ^2 = langle phi_k phi_k  rangle\n= int phi_k(t) phi_k(t) w(t) mathrmd t","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"of the basis we call computeSP2()","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"normsq = computeSP2(opq)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"For the general case","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"langle phi_i_1 phi_i_2 cdots phi_i_m-1 phi_i_m rangle\n= int phi_i_1(t) phi_i_2(t) cdots phi_i_m-1(t) phi_i_m(t) w(t) mathrmd t","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"there exists a type Tensor that requires only two arguments: the dimension m geq 1, and an AbstractOrthoPoly","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"m = 3\nt = Tensor(3,opq)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"To get the desired entries, Tensor comes with a get() function that is called for some index a in mathbbN_0^m that has the entries a = i_1 i_2 dots i_m. For example","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"t.get([1,2,3])","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Or using comprehension","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"T = [ t.get([i1,i2,i3]) for i1=0:dim(opq)-1, i2=0:dim(opq)-1, i3=0:dim(opq)-1]","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Notice that we can cross-check the results.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"using LinearAlgebra\nnormsq == diag(T[:,:,1]) == diag(T[:,1,:]) == diag(T[1,:,:])","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Also, normsq can be computed analogously in Tensor format","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"t2 = Tensor(2, opq)\nnormsq == [ t2.get([i, i]) for i in 0:dim(opq)-1]","category":"page"},{"location":"modules/PolyChaos/scalar_products/#Arbitrary-Weights","page":"Computation of Scalar Products","title":"Arbitrary Weights","text":"","category":"section"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Of course, the type OrthoPoly can be constructed for arbitrary weights w(t). In this case we have to compute the orthogonal basis and the respective quadrature rule. Let's re-work the above example by hand.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"using SpecialFunctions\nsupp = (0, 1)\nw(t) = (t^(s_α-1)*(1-t)^(s_β-1)/SpecialFunctions.beta(s_α,s_β))\nmy_meas = Measure(\"my_meas\", w, supp, false)\nmy_opq = OrthoPoly(\"my_op\", deg, my_meas; Nrec=n, addQuadrature = true)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Now we can compute the squared norms  phi_k ^2","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"my_normsq = computeSP2(my_opq)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"And the tensor","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"my_t = Tensor(m, my_opq)\nmy_T = [ my_t.get([i1,i2,i3]) for i1=0:dim(opq)-1,i2=0:dim(opq)-1,i3=0:dim(opq)-1]","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Let's compare the results:","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"abs.(normsq-my_normsq)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"norm(T-my_T)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"note: Note\nThe possibility to create quadrature rules for arbitrary weights should be reserved to cases different from classical ones.","category":"page"},{"location":"modules/PolyChaos/scalar_products/#Multivariate-Polynomials","page":"Computation of Scalar Products","title":"Multivariate Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"For multivariate polynomials the syntax for Tensor is very much alike, except that we are dealing with the type MultiOrthoPoly now.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"mop = MultiOrthoPoly([opq, my_opq], deg)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"mt2 = Tensor(2,mop)\nmt3 = Tensor(3,mop)\nmT2 = [ mt2.get([i,i]) for i=0:dim(mop)-1 ]","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"Notice that mT2 carries the elements of the 2-dimensional tensors for the univariate bases opq and my_opq. The encoding is given by the multi-index mop.ind","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"mop.ind","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"To cross-check the results we can distribute the multi-index back to its univariate indices with the help of findUnivariateIndices.","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"ind_opq = findUnivariateIndices(1,mop.ind)\nind_my_opq = findUnivariateIndices(2,mop.ind)","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"mT2[ind_opq] - normsq","category":"page"},{"location":"modules/PolyChaos/scalar_products/","page":"Computation of Scalar Products","title":"Computation of Scalar Products","text":"mT2[ind_my_opq] - my_normsq","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/#Nonlinear-hyperbolic-system-of-PDEs","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We may also solve hyperbolic systems like the following","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracpartial^2upartial t^2 = fracax^n fracpartialpartial x(x^n fracpartial upartial x) + u f(fracuw)  \nfracpartial^2wpartial t^2 = fracbx^n fracpartialpartial x(x^n fracpartial upartial x) + w g(fracuw)  \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where f and g are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nu(0x) = k * j0(ξ(0 x)) + y0(ξ(0 x)) \nu(t0) = k * j0(ξ(t 0)) + y0(ξ(t 0)) \nu(t1) = k * j0(ξ(t 1)) + y0(ξ(t 1)) \nw(0x) = j0(ξ(0 x)) + y0(ξ(0 x)) \nw(t0) = j0(ξ(t 0)) + y0(ξ(t 0)) \nw(t1) = j0(ξ(t 0)) + y0(ξ(t 0)) \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k), j0 and y0 are the Bessel functions, and ξ(t, x) is:","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracsqrtf(k)sqrtfracax^nsqrtfracax^n(t+1)^2 - (x+1)^2\nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We solve this with Neural:","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots\nusing SpecialFunctions\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDx = Differential(x)\nDt = Differential(t)\nDtt = Differential(t)^2\n\n# Constants\na = 16\nb = 16\nn = 0\n\n# Arbitrary functions\nf(x) = x^2\ng(x) = 4 * cos(π * x)\nroot(x) = g(x) - f(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Roots.Bisection())                # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nξ(t, x) = sqrt(f(k)) / sqrt(a) * sqrt(a * (t + 1)^2 - (x + 1)^2)\nθ(t, x) = besselj0(ξ(t, x)) + bessely0(ξ(t, x))                     # Analytical solution to Klein-Gordon equation\nw_analytic(t, x) = θ(t, x)\nu_analytic(t, x) = k * θ(t, x)\n\n# Nonlinear system of hyperbolic equations\neqs = [Dtt(u(t, x)) ~ a / (x^n) * Dx(x^n * Dx(u(t, x))) + u(t, x) * f(u(t, x) / w(t, x)),\n       Dtt(w(t, x)) ~ b / (x^n) * Dx(x^n * Dx(w(t, x))) + w(t, x) * g(u(t, x) / w(t, x))]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n       w(0, x) ~ w_analytic(0, x),\n       u(t, 0) ~ u_analytic(t, 0),\n       w(t, 0) ~ w_analytic(t, 0),\n       u(t, 1) ~ u_analytic(t, 1),\n       w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t,x], [u(t,x),w(t,x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=1000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u,:w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t,x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real  = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict  = [[phi[i]([t,x], minimizers_[i])[1] for t in ts  for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(ts, xs, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"nonlinear_hyperbolic_sol_u$i\")\nend","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"(Image: nonlinear_hyperbolic_sol_u1) (Image: nonlinear_hyperbolic_sol_u2)","category":"page"},{"location":"modules/GlobalSensitivity/methods/fractional/#Fractional-Factorial-Method","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/fractional/","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"FractionalFactorial does not have any keyword arguments.","category":"page"},{"location":"modules/GlobalSensitivity/methods/fractional/#Method-Details","page":"Fractional Factorial Method","title":"Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/fractional/","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"Fractional Factorial method creates a design matrix by utilising Hadamard Matrix and uses it run simulations of the input model. The main effects are then evaluated by dot product between the contrast  for the parameter and the vector of simulation results. The  corresponding main effects and variance, i.e. square of the main effects are returned as results for Fractional Factorial method.","category":"page"},{"location":"modules/GlobalSensitivity/methods/fractional/#API","page":"Fractional Factorial Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/fractional/","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"function gsa(f, method::FractionalFactorial; num_params, p_range = nothing, kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/fractional/#Example","page":"Fractional Factorial Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/fractional/","page":"Fractional Factorial Method","title":"Fractional Factorial Method","text":"using GlobalSensitivity, Test\n\nf = X -> X[1] + 2 * X[2] + 3 * X[3] + 4 * X[7] * X[12]\nres1 = gsa(f,FractionalFactorial(),num_params = 12,N=10)","category":"page"},{"location":"modules/DiffEqDocs/solvers/rode_solve/#RODE-Solvers","page":"RODE Solvers","title":"RODE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/rode_solve/#Recommended-Methods","page":"RODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Currently, the only implemented method is the RandomEM method in StochasticDiffEq.jl. It is strong order alpha for a alpha-Holder continuous noise process.","category":"page"},{"location":"modules/DiffEqDocs/solvers/rode_solve/#Full-List-of-Methods","page":"RODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/rode_solve/#StochasticDiffEq.jl","page":"RODE Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation.","category":"page"},{"location":"modules/DiffEqDocs/solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"RandomEM- The Euler-Maruyama method for RODEs. Strong order matching Holder continuity.","category":"page"},{"location":"modules/DiffEqDocs/solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Example usage:","category":"page"},{"location":"modules/DiffEqDocs/solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"sol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"modules/Optimization/API/optimization_problem/#Defining-OptimizationProblems","page":"Defining OptimizationProblems","title":"Defining OptimizationProblems","text":"","category":"section"},{"location":"modules/Optimization/API/optimization_problem/","page":"Defining OptimizationProblems","title":"Defining OptimizationProblems","text":"OptimizationProblem","category":"page"},{"location":"modules/PoissonRandom/pois_rand/#Poisson-Random-API","page":"Poisson Random API","title":"Poisson Random API","text":"","category":"section"},{"location":"modules/PoissonRandom/pois_rand/","page":"Poisson Random API","title":"Poisson Random API","text":"pois_rand","category":"page"},{"location":"modules/PoissonRandom/pois_rand/#PoissonRandom.pois_rand","page":"Poisson Random API","title":"PoissonRandom.pois_rand","text":"pois_rand(λ)\npois_rand(rng::AbstractRNG, λ)\n\nGenerates Poisson(λ) distributed random numbers using a fast polyalgorithm.\n\nExamples\n\n# Simple Poisson random\npois_rand(λ)\n\n# Using another RNG\nusing RandomNumbers\nrng = Xorshifts.Xoroshiro128Plus()\npois_rand(rng,λ)\n\n\n\n\n\n","category":"function"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/#Parallelized-Morris-and-Sobol-Sensitivity-Analysis-of-an-ODE","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"","category":"section"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's run GSA on the Lotka-Volterra model to and study the sensitivity of the maximum of predator population and the average prey population.","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"using GlobalSensitivity, Statistics, OrdinaryDiffEq #load packages","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"First, let's define our model:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"function f(du,u,p,t)\n  du[1] = p[1]*u[1] - p[2]*u[1]*u[2] #prey\n  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2] #predator\nend\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f,u0,tspan,p)\nt = collect(range(0, stop=10, length=200))","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Now, let's create a function that takes in a parameter set and calculates the maximum of the predator population and the average of the prey population for those parameter values. To do this, we will make use of the remake function, which creates a new ODEProblem, and use the p keyword argument to set the new parameters:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"f1 = function (p)\n  prob1 = remake(prob;p=p)\n  sol = solve(prob1,Tsit5();saveat=t)\n  [mean(sol[1,:]), maximum(sol[2,:])]\nend","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Now, let's perform a Morris global sensitivity analysis on this model. We specify that the parameter range is [1,5] for each of the parameters, and thus call:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m = gsa(f1,Morris(total_num_trajectory=1000,num_trajectory=150),[[1,5],[1,5],[1,5],[1,5]])","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's get the means and variances from the MorrisResult struct.","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m.means\n2×2 Array{Float64,2}:\n 0.474053  0.114922\n 1.38542   5.26094\n\nm.variances\n2×2 Array{Float64,2}:\n 0.208271    0.0317397\n 3.07475   118.103","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"Let's plot the result","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"scatter(m.means[1,:], m.variances[1,:],series_annotations=[:a,:b,:c,:d],color=:gray)\nscatter(m.means[2,:], m.variances[2,:],series_annotations=[:a,:b,:c,:d],color=:gray)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"For the Sobol method, we can similarly do:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"m = gsa(f1,Sobol(),[[1,5],[1,5],[1,5],[1,5]],N=1000)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/#Direct-Use-of-Design-Matrices","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Direct Use of Design Matrices","text":"","category":"section"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"For the Sobol Method, we can have more control over the sampled points by generating design matrices. Doing it in this manner lets us directly specify a quasi-Monte Carlo sampling method for the parameter space. Here we use QuasiMonteCarlo.jl to generate the design matrices as follows:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"using GlobalSensitivity, QuasiMonteCarlo, Plots\nN = 10000\nlb = [1.0, 1.0, 1.0, 1.0]\nub = [5.0, 5.0, 5.0, 5.0]\nsampler = SobolSample()\nA,B = QuasiMonteCarlo.generate_design_matrices(N,lb,ub,sampler)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"and now we tell it to calculate the Sobol indices on these designs for the function f1 we defined in the Lotka Volterra example:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"sobol_result = gsa(f1,Sobol(),A,B)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"We plot the first order and total order Sobol Indices for the parameters (a and b).","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"\np1 = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.ST[1,:],title=\"Total Order Indices prey\",legend=false)\np2 = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.S1[1,:],title=\"First Order Indices prey\",legend=false)\np1_ = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.ST[2,:],title=\"Total Order Indices predator\",legend=false)\np2_ = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.S1[2,:],title=\"First Order Indices predator\",legend=false)\nplot(p1,p2,p1_,p2_)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"(Image: sobolbars)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/#Parallelizing-the-Global-Sensitivity-Analysis","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelizing the Global Sensitivity Analysis","text":"","category":"section"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"In all of the previous examples, f(p) was calculated serially. However, we can parallelize our computations by using the batch interface. In the batch interface, each column p[:,i] is a set of parameters, and we output a column for each set of parameters. Here we showcase using the Ensemble Interface to use EnsembleGPUArray to perform automatic multithreaded-parallelization of the ODE solves.","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"using GlobalSensitivity, QuasiMonteCarlo, OrdinaryDiffEq\n\nfunction f(du,u,p,t)\n  du[1] = p[1]*u[1] - p[2]*u[1]*u[2] #prey\n  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2] #predator\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f,u0,tspan,p)\nt = collect(range(0, stop=10, length=200))\n\nf1 = function (p)\n  prob_func(prob,i,repeat) = remake(prob;p=p[:,i])\n  ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\n  sol = solve(ensemble_prob,Tsit5(),EnsembleThreads();saveat=t,trajectories=size(p,2))\n  # Now sol[i] is the solution for the ith set of parameters\n  out = zeros(2,size(p,2))\n  for i in 1:size(p,2)\n    out[1,i] = mean(sol[i][1,:])\n    out[2,i] = maximum(sol[i][2,:])\n  end\n  out\nend","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"And now to do the parallelized calls we simply add the batch=true keyword argument:","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"sobol_result = gsa(f1,Sobol(),A,B,batch=true)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/parallelized_gsa/","page":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","title":"Parallelized Morris and Sobol Sensitivity Analysis of an ODE","text":"This user-side parallelism thus allows you to take control, and thus for example you can use DiffEqGPU.jl for automated GPU-parallelism of the ODE-based global sensitivity analysis!","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary:-Thermal-Components","page":"Thermal Components","title":"ModelingToolkitStandardLibrary: Thermal Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/","page":"Thermal Components","title":"Thermal Components","text":"CurrentModule = ModelingToolkitStandardLibrary.Thermal","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#Thermal-Utilities","page":"Thermal Components","title":"Thermal Utilities","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/","page":"Thermal Components","title":"Thermal Components","text":"HeatPort\nElement1D","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.HeatPort","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.HeatPort","text":"HeatPort(; name, T_start=273.15 + 20.0, Q_flow_start=0.0)\n\nPort for a thermal system.\n\nParameters:\n\nT_start: [K] Temperature of the port  \nQ_flow_start: [W] Heat flow rate at the port\n\nStates:\n\nT: [K] Temperature of the port  \nQ_flow: [W] Heat flow rate at the port\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.Element1D","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.Element1D","text":"Element1D(;name, dT0=0.0, Q_flow0=0.0)\n\nThis partial model contains the basic connectors and variables to allow heat transfer models to be created that do not  store energy. This model defines and includes equations for the temperature drop across the element, dT, and the heat flow rate through the element from port_a to port_b, Q_flow.\n\nParameters:\n\ndT_start:  [K] Initial temperature difference across the component a.T - b.T\nQ_flow_start: [W] Initial heat flow rate from port a -> port b\n\nStates:\n\ndT:  [K] Temperature difference across the component a.T - b.T\nQ_flow: [W] Heat flow rate from port a -> port b\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#Thermal-Components","page":"Thermal Components","title":"Thermal Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/","page":"Thermal Components","title":"Thermal Components","text":"BodyRadiation\nConvectiveConductor\nConvectiveResistor\nHeatCapacitor\nThermalConductor\nThermalResistor\nThermalCollector","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.BodyRadiation","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.BodyRadiation","text":"BodyRadiation(; name, G)\n\nLumped thermal element for radiation heat transfer.\n\nParameters:\n\nG: [m^2] Net radiation conductance between two surfaces\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.ConvectiveConductor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.ConvectiveConductor","text":"ConvectiveConductor(; name, G)\n\nLumped thermal element for heat convection.\n\nParameters:\n\nG: [W/K] Convective thermal conductance\n\nStates:\n\ndT:  [K] Temperature difference across the component solid.T - fluid.T\nQ_flow: [W] Heat flow rate from solid -> fluid\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.ConvectiveResistor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.ConvectiveResistor","text":"ConvectiveResistor(; name, R)\n\nLumped thermal element for heat convection.\n\nParameters:\n\nR: [K/W] Constant thermal resistance of material\n\nStates:\n\ndT:  [K] Temperature difference across the component solid.T - fluid.T\nQ_flow: [W] Heat flow rate from solid -> fluid\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.HeatCapacitor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.HeatCapacitor","text":"HeatCapacitor(; name, C, T_start=273.15 + 20)\n\nLumped thermal element storing heat\n\nParameters:\n\nC: [J/K] Heat capacity of element (= cp*m)\nT_start: Initial temperature of element\n\nStates:\n\nT: [K] Temperature of element\nder_T: [K/s] Time derivative of temperature\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.ThermalConductor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.ThermalConductor","text":"ThermalConductor(;name, G)\n\nLumped thermal element transporting heat without storing it.\n\nParameters:\n\nG: [W/K] Constant thermal conductance of material\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.ThermalResistor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.ThermalResistor","text":"ThermalResistor(; name, R)\n\nLumped thermal element transporting heat without storing it.\n\nParameters:\n\nR: [K/W] Constant thermal resistance of material\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.ThermalCollector","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.ThermalCollector","text":"ThermalCollector(; name, m=1)\n\nCollects m heat flows\n\nThis is a model to collect the heat flows from m heatports to one single heatport.\n\nParameters:\n\nm: Number of heat ports (e.g. m=2: port_a1, port_a2)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#Thermal-Sensors","page":"Thermal Components","title":"Thermal Sensors","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/","page":"Thermal Components","title":"Thermal Components","text":"RelativeTemperatureSensor\nHeatFlowSensor\nTemperatureSensor","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.RelativeTemperatureSensor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.RelativeTemperatureSensor","text":"RelativeTemperatureSensor(; name)\n\nRelative Temperature sensor.\n\nThe relative temperature port_a.T - port_b.T is determined between the two ports of this component and is provided as  output signal in kelvin.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.HeatFlowSensor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.HeatFlowSensor","text":"HeatFlowSensor(; name)\n\nHeat flow rate sensor.\n\nThis model is capable of monitoring the heat flow rate flowing through this component. The sensed value of heat flow rate is the amount that passes through this sensor while keeping the temperature drop across the sensor zero. This is an ideal  model so it does not absorb any energy and it has no direct effect on the thermal response of a system it is included in. The output signal is positive, if the heat flows from port_a to port_b.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.TemperatureSensor","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.TemperatureSensor","text":"TemperatureSensor(; name)\n\nAbsolute temperature sensor in kelvin.\n\nThis is an ideal absolute temperature sensor which returns the temperature of the connected port in kelvin as an output signal. The sensor itself has no thermal interaction with whatever it is connected to. Furthermore, no thermocouple-like  lags are associated with this sensor model.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#Thermal-Sources","page":"Thermal Components","title":"Thermal Sources","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/","page":"Thermal Components","title":"Thermal Components","text":"FixedHeatFlow\nFixedTemperature ","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.FixedHeatFlow","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.FixedHeatFlow","text":"FixedHeatFlow(; name, Q_flow=1.0, T_ref=293.15, alpha=0.0)\n\nFixed heat flow boundary condition.\n\nThis model allows a specified amount of heat flow rate to be \"injected\" into a thermal system at a given port. The constant amount of heat flow rate Q_flow is given as a parameter. The heat flows into the component to which the component FixedHeatFlow is connected, if parameter Q_flow is positive.\n\nParameters:\n\nQ_flow: [W] Fixed heat flow rate at port\nT_ref: [K] Reference temperature\nalpha: [1/K] Temperature coefficient of heat flow rate\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/thermal/#ModelingToolkitStandardLibrary.Thermal.FixedTemperature","page":"Thermal Components","title":"ModelingToolkitStandardLibrary.Thermal.FixedTemperature","text":"FixedTemperature(; name, T)\n\nFixed temperature boundary condition in kelvin.\n\nThis model defines a fixed temperature T at its port in kelvin, i.e., it defines a fixed temperature as a boundary condition.\n\nParameters:\n\nT: [K] Fixed temperature boundary condition\n\n\n\n\n\n","category":"function"},{"location":"modules/Surrogates/ackley/#Ackley-Function","page":"Ackley function","title":"Ackley Function","text":"","category":"section"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"The Ackley function is defined as: f(x) = -a*exp(-bsqrtfrac1dsum_i=1^d x_i^2) - exp(frac1d sum_i=1^d cos(cx_i)) + a + exp(1) Usually the recommended values are: a =  20, b = 02 and c =  2pi","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"Let's see the 1D case.","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"Now, let's define the Ackley function:","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"function ackley(x)\n    a, b, c = 20.0, -0.2, 2.0*π\n    len_recip = inv(length(x))\n    sum_sqrs = zero(eltype(x))\n    sum_cos = sum_sqrs\n    for i in x\n        sum_cos += cos(c*i)\n        sum_sqrs += i^2\n    end\n    return (-a * exp(b * sqrt(len_recip*sum_sqrs)) -\n            exp(len_recip*sum_cos) + a + 2.71)\nend","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"n = 100\nlb = -32.768\nub = 32.768\nx = sample(n, lb, ub, SobolSample())\ny = ackley.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0,30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"my_rad = RadialBasis(x, y, lb, ub)\nmy_krig = Kriging(x, y, lb, ub)\nmy_loba = LobachevskySurrogate(x, y, lb, ub)","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"scatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, my_krig.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, my_loba.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"The fit looks good. Let's now see if we are able to find the minimum value using optimization methods:","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"surrogate_optimize(ackley,DYCORS(),lb,ub,my_rad,UniformSample())\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(0, 30), legend=:top)\nplot!(xs, ackley.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_rad.(xs), label=\"Radial basis optimized\", legend=:top)","category":"page"},{"location":"modules/Surrogates/ackley/","page":"Ackley function","title":"Ackley function","text":"The DYCORS methods successfully finds the minimum.","category":"page"},{"location":"modules/Surrogates/radials/#Radial-Surrogates","page":"Radials","title":"Radial Surrogates","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"The Radial Basis Surrogate model represents the interpolating function as a linear combination of basis functions, one for each training point. Let's start with something easy to get our hands dirty. I want to build a surrogate for:","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"f(x) = log(x)*x^2+x^3`","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Let's choose the Radial Basis Surrogate for 1D. First of all we have to import these two packages: Surrogates and Plots,","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"We choose to sample f in 30 points between 5 to 25 using sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"f(x) = log(x)*x^2 + x^3\r\nn_samples = 30\r\nlower_bound = 5\r\nupper_bound = 25\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = f.(x)\r\nscatter(x, y, label=\"Sampled Points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", scatter(x, y, label=\"Sampled Points\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/radials/#Building-Surrogate","page":"Radials","title":"Building Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"With our sampled points we can build the Radial Surrogate using the RadialBasis function.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"We can simply calculate radial_surrogate for any value.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"radial_surrogate = RadialBasis(x, y, lower_bound, upper_bound)\r\nval = radial_surrogate(5.4)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"We can also use cubic radial basis functions.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"radial_surrogate = RadialBasis(x, y, lower_bound, upper_bound, cubicRadial)\r\nval = radial_surrogate(5.4)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Currently available radial basis functions are linearRadial (the default), cubicRadial, multiquadricRadial, and thinplateRadial.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Now, we will simply plot radial_surrogate:","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/radials/#Optimizing","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, radial_surrogate, SobolSample())\r\nscatter(x, y, label=\"Sampled points\", legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(radial_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/radials/#Radial-Basis-Surrogate-tutorial-(ND)","page":"Radials","title":"Radial Basis Surrogate tutorial (ND)","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"First of all we will define the Booth function we are going to build surrogate for:","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"f(x) = (x_1 + 2*x_2 - 7)^2 + (2*x_1 + x_2 - 5)^2","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction booth(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    term1 = (x1 + 2*x2 - 7)^2;\r\n    term2 = (2*x1 + x2 - 5)^2;\r\n    y = term1 + term2;\r\nend","category":"page"},{"location":"modules/Surrogates/radials/#Sampling","page":"Radials","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"n_samples = 80\r\nlower_bound = [-5.0, 0.0]\r\nupper_bound = [10.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = booth.(xys);","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"x, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> booth((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> booth((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/radials/#Building-a-surrogate","page":"Radials","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"radial_basis = RadialBasis(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/radials/#Optimizing-2","page":"Radials","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"surrogate_optimize(booth, SRBF(), lower_bound, upper_bound, radial_basis, UniformSample(), maxiters=50)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/radials/","page":"Radials","title":"Radials","text":"p1 = surface(x, y, (x, y) -> radial_basis([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = booth.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> radial_basis([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#Neural-Differential-Equation-Layer-Functions","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layer Functions","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"The following layers are helper functions for easily building neural differential equation architectures in the currently most efficient way. As demonstrated in the tutorials, they do not have to be used since automatic differentiation will just work over solve, but these cover common use cases and choose what's known to be the optimal mode of AD for the respective equation type.","category":"page"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/","page":"Neural Differential Equation Layers","title":"Neural Differential Equation Layers","text":"NeuralODE\nNeuralDSDE\nNeuralSDE\nNeuralCDDE\nNeuralDAE\nNeuralODEMM\nAugmentedNDELayer","category":"page"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralODE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODE","text":"Constructs a continuous-time recurrant neural network, also known as a neural ordinary differential equation (neural ODE), with a fast gradient calculation via adjoints [1]. At a high level this corresponds to solving the forward differential equation, using a second differential equation that propagates the derivatives of the loss backwards in time.\n\nNeuralODE(model,tspan,alg=nothing,args...;kwargs...)\nNeuralODE(model::FastChain,tspan,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇x.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralDSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDSDE","text":"Constructs a neural stochastic differential equation (neural SDE) with diagonal noise.\n\nNeuralDSDE(model1,model2,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\nNeuralDSDE(model1::FastChain,model2::FastChain,tspan,alg=nothing,args...;\n           sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a vector of the same size as the input.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralSDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralSDE","text":"Constructs a neural stochastic differential equation (neural SDE).\n\nNeuralSDE(model1,model2,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralSDE(model1::FastChain,model2::FastChain,tspan,nbrown,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel1: A Chain or FastChain neural network that defines the drift function.\nmodel2: A Chain or FastChain neural network that defines the diffusion function. Should output a matrix that is nbrown x size(x,1).\ntspan: The timespan to be solved on.\nnbrown: The number of Brownian processes\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralCDDE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralCDDE","text":"Constructs a neural delay differential equation (neural DDE) with constant delays.\n\nNeuralCDDE(model,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralCDDE(model::FastChain,tspan,hist,lags,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size [x;x(t-lag_1);...;x(t-lag_n)] and produce and output shaped like x.\ntspan: The timespan to be solved on.\nhist: Defines the history function h(t) for values before the start of the integration.\nlags: Defines the lagged values that should be utilized in the neural network.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralDAE","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralDAE","text":"Constructs a neural differential-algebraic equation (neural DAE).\n\nNeuralDAE(model,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\nNeuralDAE(model::FastChain,constraints_model,tspan,alg=nothing,args...;\n          sensealg=TrackerAdjoint(),kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the derivative function. Should take an input of size x and produce the residual of f(dx,x,t) for only the differential variables.\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to using reverse-mode automatic differentiation via Tracker.jl\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.NeuralODEMM","page":"Neural Differential Equation Layers","title":"DiffEqFlux.NeuralODEMM","text":"Constructs a physically-constrained continuous-time recurrant neural network, also known as a neural differential-algebraic equation (neural DAE), with a mass matrix and a fast gradient calculation via adjoints [1]. The mass matrix formulation is:\n\nMu = f(upt)\n\nwhere M is semi-explicit, i.e. singular with zeros for rows corresponding to the constraint equations.\n\nNeuralODEMM(model,constraints_model,tspan,mass_matrix,alg=nothing,args...;kwargs...)\nNeuralODEMM(model::FastChain,tspan,mass_matrix,alg=nothing,args...;\n          sensealg=InterpolatingAdjoint(autojacvec=SciMLSensitivity.ReverseDiffVJP(true)),\n          kwargs...)\n\nArguments:\n\nmodel: A Chain or FastChain neural network that defines the ̇f(u,p,t)\nconstraints_model: A function constraints_model(u,p,t) for the fixed constaints to impose on the algebraic equations.\ntspan: The timespan to be solved on.\nmass_matrix: The mass matrix associated with the DAE\nalg: The algorithm used to solve the ODE. Defaults to nothing, i.e. the default algorithm from DifferentialEquations.jl. This method requires an implicit ODE solver compatible with singular mass matrices. Consult the DAE solvers documentation for more details.\nsensealg: The choice of differentiation algorthm used in the backpropogation. Defaults to an adjoint method, and with FastChain it defaults to utilizing a tape-compiled ReverseDiff vector-Jacobian product for extra efficiency. Seee the Local Sensitivity Analysis documentation for more details.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/NeuralDELayers/#DiffEqFlux.AugmentedNDELayer","page":"Neural Differential Equation Layers","title":"DiffEqFlux.AugmentedNDELayer","text":"Constructs an Augmented Neural Differential Equation Layer.\n\nAugmentedNDELayer(nde, adim::Int)\n\nArguments:\n\nnde: Any Neural Differential Equation Layer\nadim: The number of dimensions the initial conditions should be lifted\n\nReferences:\n\n[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.\n\n\n\n\n\n","category":"type"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"using PolyChaos, JuMP, MosekTools, LinearAlgebra\nA = [ -1 1 0 0; -1 0 1 0; -1 0 0 1 ; 0 1 -1 0; 0 0 -1 1] # incidence matrix\nNl, N = size(A,1), size(A,2)\nBbr = diagm(0 => -( 2 .+ 10*rand(Nl) )) # line parameters\nΨ = [ zeros(Nl)  -Bbr*A[:,2:end]*inv(A[:,2:end]'*Bbr*A[:,2:end]) ] # PTDF matrix\nCp, Cd = [1 0; 0 0; 0 0; 0 1], [0 0; 1 0; 0 1; 0 0 ] # book-keeping\nNg, Nd = size(Cp,2), size(Cd,2)\nc = 4 .+ 10*rand(Ng) # cost function parameters\nλp, λl = 1.6*ones(Ng), 1.6*ones(Nl) # lambdas for chance constraint reformulations\npmax, pmin = 10*ones(Ng), zeros(Ng) # engineering limits\nplmax, plmin = 10*ones(Nl), -10*ones(Nl) # engineering limits\ndeg = 1\nopq = [Uniform01OrthoPoly(deg; Nrec=5*deg), Uniform01OrthoPoly(deg; Nrec=5*deg)]\nmop = MultiOrthoPoly(opq, deg)\nNpce = mop.dim\nd = zeros(Nd,Npce) # PCE coefficients of load\nd[1,[1,2]] = convert2affinePCE(1., 0.1, mop.uni[1], kind=\"μσ\")\nd[2,[1,3]] = convert2affinePCE(2., 0.2, mop.uni[2], kind=\"μσ\")\nfunction buildSOC(x::Vector,mop::MultiOrthoPoly)\n    t = [ sqrt(Tensor(2,mop).get([i,i])) for i in 0:mop.dim-1 ]\n    (t.*x)[2:end]\nend\nmodel = Model(with_optimizer(Mosek.Optimizer))\n@variable(model, p[i in 1:Ng,j in 1:Npce], base_name=\"p\")\n@constraint(model, energy_balance[j in 1:Npce], sum(p[i,j] for i in 1:Ng) - sum(d[i,j] for i in 1:Nd) == 0)\n@constraint(model, con_pmax[i in 1:Ng], [1/λp[i]*(pmax[i] - mean(p[i,:],mop)); buildSOC(p[i,:],mop)] in SecondOrderCone())\n@constraint(model, con_pmin[i in 1:Ng], [1/λp[i]*(mean(p[i,:],mop) - pmin[i]); buildSOC(p[i,:],mop)] in SecondOrderCone())\npl = Ψ*(Cp*p + Cd*d)\n@constraint(model, con_plmax[i in 1:Nl], [1/λl[i]*(plmax[i] - mean(pl[1,:],mop)); buildSOC(pl[i,:],mop)] in SecondOrderCone())\n@constraint(model, con_plmin[i in 1:Nl], [1/λl[i]*(mean(pl[1,:],mop) - plmin[i]); buildSOC(pl[i,:],mop)] in SecondOrderCone())\n@objective(model, Min, sum( mean(p[i,:],mop)*c[i] for i in 1:Ng) )\noptimize!(model) # here we go\n@assert termination_status(model) == MOI.OPTIMAL \"Model not solved to optimality.\"\npsol, plsol, obj = value.(p), value.(pl), objective_value(model)\np_moments = [ [mean(psol[i,:],mop) var(psol[i,:],mop) ] for i in 1:Ng ]\npbr_moments = [ [mean(plsol[i,:],mop) var(plsol[i,:],mop) ] for i in 1:Nl ]","category":"page"},{"location":"modules/PolyChaos/DCsOPF/#Chance-Constrained-DC-Optimal-Power-Flow","page":"Optimal Power Flow","title":"Chance-Constrained DC Optimal Power Flow","text":"","category":"section"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"The purpose of this tutorial is to show how polynomial chaos can be leveraged to solve optimization problems under uncertainty. Specifically, we study chance-constrained DC optimal power flow as it is presented in this paper.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"We consider the following 4-bus system that has a total of two generators (buses 1 and 3) and two loads (buses 2 and 4):","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"(Image: 4-bus system)","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"We formalize the numbering of the generators (superscript g), loads (superscript d for demand), and branches (superscript br) as follows","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"mathcalN^g =  1 3  mathcalN^d =  2 4  mathcalN^br =  1 2 3 4 5 ","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"With each generator we associate a linear cost with cost coefficient c_i for all i in mathcalN^g. Each generator must adhere to its engineering limits given by (underlinep_i^g  overlinep_i^g ) for all i in mathcalN^g. Also, each line is constrained by its limits (underlinep_i^br overlinep_i^br) for all i in mathcalN^br.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"We model the demand at the buses i in mathcalN^d in terms of uniform distributions with known mean mu_i and standard deviation sigma_i. We concisely write","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"mathsfp_i^d sim mathsfU(mu_i sigma_i) quad forall i in mathcalN^d","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"For simplicity we consider DC conditions. Hence, energy balance reads","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"sum_i in mathcalN^g mathsfp_i^g - sum_i in mathcalN^d mathsfp_i^d = 0","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"and the vector of branch flows is computed from the power transfer distribution factor (PTDF) matrix Psi via","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"mathsfp^br = Psi (C^pmathsfp^g + C^dmathsfp^d)","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"The matrices C^p and C^d map the generators and the loads to the correct buses, respectively.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"We want to solve a chance-constrained optimal power flow problem under DC conditions. According to this paper, we can formulate the problem as undersetmathsfp^goperatornamemin  sum_i in mathcalN_g c_i mathbbE( mathsfp_i^g) subject to","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"sum_i in mathcalN^g mathsfp_i^g - sum_i in mathcalN^d mathsfp_i^d = 0 \nunderlinep_i^g leq mathbbE(mathsfp_i^g) pm lambda_i^g sqrtmathbbV(mathsfp_i^g) leq overlinep_i^g  forall i in mathcalN^g\nunderlinep_i^br leq mathbbE(mathsfp_i^br) pm lambda_i^br sqrtmathbbV(mathsfp_i^br) leq overlinep_i^br forall i in mathcalN^br","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"which minimizes the total expected generation cost subject to the DC power flow equations and chance-constrained engineering limits.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Let's solve the problem using PolyChaos and JuMP, using Mosek as a solver.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"using PolyChaos, JuMP, MosekTools, LinearAlgebra","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Let's define system-specific quantities such as the incidence matrix and the branch flow parameters. From these we can compute the PTDF matrix Psi (assuming the slack is at bus 1).","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"A = [ -1 1 0 0; -1 0 1 0; -1 0 0 1 ; 0 1 -1 0; 0 0 -1 1] # incidence matrix\nNl, N = size(A,1), size(A,2)\nBbr = diagm(0 => -( 2 .+ 10*rand(Nl) )) # line parameters\nΨ = [ zeros(Nl)  -Bbr*A[:,2:end]*inv(A[:,2:end]'*Bbr*A[:,2:end]) ] # PTDF matrix","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Now we can continue the remaining ingredients that specify our systems:","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Cp, Cd = [1 0; 0 0; 0 0; 0 1], [0 0; 1 0; 0 1; 0 0 ] # book-keeping\nNg, Nd = size(Cp,2), size(Cd,2)\nc = 4 .+ 10*rand(Ng) # cost function parameters\nλp, λl = 1.6*ones(Ng), 1.6*ones(Nl) # lambdas for chance constraint reformulations\npmax, pmin = 10*ones(Ng), zeros(Ng) # engineering limits\nplmax, plmin = 10*ones(Nl), -10*ones(Nl) # engineering limits","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"We specify the uncertainty using PolyChaos:","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"deg = 1\nopq = [Uniform01OrthoPoly(deg; Nrec=5*deg), Uniform01OrthoPoly(deg; Nrec=5*deg)]\nmop = MultiOrthoPoly(opq, deg)\nNpce = mop.dim","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"It remains to specify the PCE coefficients, for which we will use convert2affine.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"d = zeros(Nd,Npce) # PCE coefficients of load\nd[1,[1,2]] = convert2affinePCE(1., 0.1, mop.uni[1], kind=\"μσ\")\nd[2,[1,3]] = convert2affinePCE(2., 0.2, mop.uni[2], kind=\"μσ\")","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Now, let's put it all into an optimization problem, specifically a second-order cone program. To build the second-order cone constraints we define a helper function buildSOC.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"function buildSOC(x::Vector,mop::MultiOrthoPoly)\n    t = [ sqrt(Tensor(2,mop).get([i,i])) for i in 0:mop.dim-1 ]\n    (t.*x)[2:end]\nend","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Finally, let's use JuMP to formulate and then solve the problem. We use Mosek to solve the problem; for academic use there are free license.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"model = Model(with_optimizer(Mosek.Optimizer))\n@variable(model, p[i in 1:Ng,j in 1:Npce], base_name=\"p\")\n@constraint(model, energy_balance[j in 1:Npce], sum(p[i,j] for i in 1:Ng) - sum(d[i,j] for i in 1:Nd) == 0)\n@constraint(model, con_pmax[i in 1:Ng], [1/λp[i]*(pmax[i] - mean(p[i,:],mop)); buildSOC(p[i,:],mop)] in SecondOrderCone())\n@constraint(model, con_pmin[i in 1:Ng], [1/λp[i]*(mean(p[i,:],mop) - pmin[i]); buildSOC(p[i,:],mop)] in SecondOrderCone())\npl = Ψ*(Cp*p + Cd*d)\n@constraint(model, con_plmax[i in 1:Nl], [1/λl[i]*(plmax[i] - mean(pl[1,:],mop)); buildSOC(pl[i,:],mop)] in SecondOrderCone())\n@constraint(model, con_plmin[i in 1:Nl], [1/λl[i]*(mean(pl[1,:],mop) - plmin[i]); buildSOC(pl[i,:],mop)] in SecondOrderCone())\n@objective(model, Min, sum( mean(p[i,:],mop)*c[i] for i in 1:Ng) )\noptimize!(model) # here we go","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Let's extract the numerical values of the optimal solution.","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"@assert termination_status(model) == MOI.OPTIMAL \"Model not solved to optimality.\"\npsol, plsol, obj = value.(p), value.(pl), objective_value(model)","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Great, we've solved the problem. How do we now make sense of the solution? For instance, we can look at the moments of the generated power:","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"p_moments = [ [mean(psol[i,:],mop) var(psol[i,:],mop) ] for i in 1:Ng ]","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"Simiarly, we can study the moments for the branch flows:","category":"page"},{"location":"modules/PolyChaos/DCsOPF/","page":"Optimal Power Flow","title":"Optimal Power Flow","text":"pbr_moments = [ [mean(plsol[i,:],mop) var(plsol[i,:],mop) ] for i in 1:Nl ]","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#sde_solve","page":"SDE Solvers","title":"SDE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Recommended-Methods","page":"SDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For most Ito diagonal and scalar noise problems where a good amount of accuracy is required and mild stiffness may be an issue, the SOSRI algorithm should do well. If the problem has additive noise, then SOSRA will be the optimal algorithm. At low tolerances (<1e-4?) SRA3 will be more efficient, though SOSRA is more robust to stiffness. For commutative noise, RKMilCommute is a strong order 1.0 method which utilizes the commutivity property to greatly speed up the stochastic iterated integral approximation and can choose between Ito and Stratonovich. For non-commutative noise, difficult problems usually require adaptive time stepping in order to be efficient. In this case, LambaEM and LambaEulerHeun are adaptive and handle general non-diagonal problems (for Ito and Stratonovich interpretations respectively). If adaptivity isn't necessary, the EM and EulerHeun are good choices (for Ito and Stratonovich interpretations respectively).","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For stiff problems with additive noise, the high order adaptive method SKenCarp is highly preferred and will solve problems with similar efficiency as ODEs. If possible, stiff problems should be converted to make use of this additive noise solver. If the noise term is large/stiff, then the split-step methods are required in order for the implicit methods to be stable. For Ito in this case, use ISSEM and for Stratonovich use ISSEulerHeun. These two methods can handle any noise form.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"If the noise term is not too large, for stiff problems with diagonal noise, ImplicitRKMil is the most efficient method and can choose between Ito and Stratonovich. For each of the theta methods, the parameter theta can be chosen. The default is theta=1/2 which will not dampen numerical oscillations and thus is symmetric (and almost symplectic) and will lead to less error when noise is sufficiently small. However, theta=1/2 is not L-stable in the drift term, and thus one can receive more stability (L-stability in the drift term) with theta=1, but with a tradeoff of error efficiency in the low noise case. In addition, the option symplectic=true will turns these methods into an implicit Midpoint extension which is symplectic in distribution but has an accuracy tradeoff.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"If only an estimation for the expected value of the solution is required, i.e., if one is only interested in an accurate draw from the distribution induced by a given SDE, the use of high weak order solvers is recommended. Specifically, DRI1 is preferred for a high number of Wiener processes. The weak stochastic Runge-Kutta solvers with weak order 2 due to Roessler are adaptive. All other high weak order solvers currently require a fixed step size.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Special-Noise-Forms","page":"SDE Solvers","title":"Special Noise Forms","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Some solvers are for specialized forms of noise. Diagonal noise is the default setup. Non-diagonal noise is specified via setting noise_rate_prototype to a matrix in the SDEProblem type. A special form of non-diagonal noise, commutative noise, occurs when the noise satisfies the following condition:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"sum_i=1^d g_ij_1(tx) fracpartial g_kj_2(tx)partial x_i = sum_i=1^d g_ij_2(tx) fracpartial g_kj_1(tx)partial x_i","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"for every j_1j_2 and k. Additive noise is when g(tu)=g(t), i.e. is independent of u. Multiplicative noise is g_i(tu)=a_i u.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Iterated-Integral-Approximations","page":"SDE Solvers","title":"Iterated Integral Approximations","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The difficulty of higher strong order integrators stems from the presence of iterated stochastic integrals","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"I(h) = int_0^hint_0^sdW^1_tdW^2_s","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"in these schemes.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The approximation of these iterated integrals can be avoided, if the diffusion matrix satisfies the special commutativity condition given above. Because of this, many methods are only applicable to problems that satisfy the commutativity condition. In other words, many methods can only handle specific noise cases, like diagonal noise or commutative noise, because of how this iterated integral approximation is computed.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"However, the methods for general SDEs, like RKMilGeneral, perform a direct approximation of the iterated integrals. For those methods, the algorithms have an ii_approx keyword argument that allows one to specify the method for the approximation. The choices are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"IICommutative: a simplification of the integral which assumes the noise commutativity property. If used on a non-commutative noise problem this will limit the strong convergence to 0.5.\nIILevyArea: computes the iterated integrals based on an approximation of the Levy area using the LevyArea.jl package: Kastner, F. and Rößler, A., arXiv: 2201.08424 Kastner, F. and Rößler, A., LevyArea.jl, 10.5281/ZENODO.5883748. The package supports the schemes: Fourier(), Milstein(), Wiktorsson(),MronRoe(). The optimal algorithm is automatically selected based on the dimension of the Brownian process and the step size. By passing a specific scheme, e.g. ii_approx=Fourier() methods can be manually selected. One must be careful when using the Levy area approximations in conjunction with adaptivity (adaptive=true) because the Levy area approximations draw random numbers that do not reflect the random numbers taken in a previous rejected step. This leads to a bias that increases with an increasing number of rejected steps.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Example: RKMilGeneral(;ii_approx=IILevyArea()).","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Special-Keyword-Arguments","page":"SDE Solvers","title":"Special Keyword Arguments","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"save_noise: Determines whether the values of W are saved whenever the timeseries is saved. Defaults to true.\ndelta: The delta adaptivity parameter for the natural error estimator. Determines the balance between drift and diffusion error. For more details, see the publication.\nseed: Sets the seed for the random number generator. This overrides any seed set in the SDEProblem.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Full-List-of-Methods","page":"SDE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#StochasticDiffEq.jl","page":"SDE Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation. Orders are given in terms of strong order.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Nonstiff-Methods","page":"SDE Solvers","title":"Nonstiff Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"EM- The Euler-Maruyama method. Strong Order 0.5 in the Ito sense. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with large diffusion eigenvalues is improved. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEM- A modified Euler-Maruyama method with adaptive time stepping with an error estimator based on Lamba and Rackauckas. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with   large diffusion eigenvalues is improved. Strong Order 0.5 in the Ito sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nEulerHeun - The Euler-Heun method. Strong Order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEulerHeun - A modified Euler-Heun method with adaptive time stepping with an error estimator based on Lamba due to Rackauckas. Strong order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nRKMil - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method. Defaults to solving the Ito problem, but RKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Only handles scalar and diagonal noise.†\nRKMilCommute - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method for commutative noise problems. Defaults to solving the Ito problem, but RKMilCommute(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Uses a 1.5/2.0 error estimate for adaptive time stepping.†\nRKMilGeneral(;interpretation=:Ito, ii_approx=IILevyArea() - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method for general non-commutative noise problems. Allows for a choice of interpretation between :Ito and :Stratonovich. Allows for a choice of iterated integral approximation.\nWangLi3SMil_A - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_B - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_C - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_D - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_E - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_F - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nSRA - Adaptive strong order 1.5 methods for additive Ito and Stratonovich SDEs. Default tableau is for SRA1. Can handle diagonal, non-diagonal and scalar additive noise.\nSRI - Adaptive strong order 1.5 methods for diagonal/scalar Ito SDEs. Default tableau is for SRIW1.\nSRIW1 - Adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs.†\nSRIW2 - Adaptive strong order 1.5 and weak order 3.0 for diagonal/scalar Ito SDEs.†\nSOSRI - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSOSRI2 - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSRA1 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA2 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA3 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 3. Can handle non-diagonal and scalar additive noise.†\nSOSRA - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†\nSOSRA2 - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Example usage:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"sol = solve(prob,SRIW1())","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"3-stage Milstein Methods WangLi3SMil_A, WangLi3SMil_B, WangLi3SMil_D, WangLi3SMil_E and WangLi3SMil_F are currently implemented for 1-dimensional and diagonal noise only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Tableau-Controls","page":"SDE Solvers","title":"Tableau Controls","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For SRA and SRI, the following option is allowed:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"tableau: The tableau for an :SRA or :SRI algorithm. Defaults to SRIW1 or SRA1.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#S-ROCK-Methods","page":"SDE Solvers","title":"S-ROCK Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SROCK1 - is a fixed step size stabilized explicit method for stiff problems. Defaults to solving th Ito problem but SROCK1(interpretation=:Stratonovich) can make it solve the Stratonovich problem. Strong order of convergence is 0.5 and weak order 1, but is optimised to get order 1 in case os scalar/diagonal noise.\nSROCKEM - is fixed step Euler-Mayurama with first order ROCK stabilization thus can handle stiff problems. Only for Ito problems. Defaults to strong and weak order 1.0, but can solve with weak order 0.5 as SROCKEM(strong_order_1=false). This method can handle 1-dimensional, diagonal and multi-dimensional noise.\nSROCK2 - is a weak second order and strong first order fixed step stabilized method for stiff Ito problems.This method can handle 1-dimensional, diagonal and multi-dimensional noise.\nSKSROCK - is fixed step stabilized explicit method for stiff Ito problems. Strong order 0.5 and weak order 1. This method has a better stability domain then SROCK1. Also it allows special post-processing techniques in case of ergodic dynamical systems, in the context of ergodic Brownian dynamics, to achieve order 2 accuracy. SKSROCK(;post_processing=true) will make use of post processing. By default it doesn't use post processing. Post processing is optional and under development. The rest of the method is completely functional and can handle 1-dimensional, diagonal and multi-dimensional noise.  \nTangXiaoSROCK2 - is a fixed step size stabilized expicit method for stiff problems. Only for Ito problems. Weak order of 2 and strog order of 1. Has 5 versions with different stability domains which can be used as TangXiaoSROCK2(version_num=i) where i is 1-5. Under Development.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Stiff-Methods","page":"SDE Solvers","title":"Stiff Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal,Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Derivative-Based-Methods","page":"SDE Solvers","title":"Derivative-Based Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The following methods require analytic derivatives of the diffusion term.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"PCEuler - The predictor corrector euler method. Strong Order 0.5 in the Ito sense. Requires the ggprime function, which is defined as\n  textggprime^k(tx) = sum_j=1^m sum_i=1^d g_ij(tx) fracpartial g_kj(tx)partial x_i\nThis can also be understood more intuitively in vector/matrix form as,\ntextggprime(tx) = sum_j=1^m barmathcalJvec g^(j)(tx) vec g^(j)(tx)\nwhere vec g^(j) is the noise vector for the j'th noise channel and barmathcalJ is the Jacobian of the j'th   noise vector.\nThe default settings for the drift implicitness is theta=0.5 and the diffusion implicitness is eta=0.5.  ","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#High-Weak-Order-Methods","page":"SDE Solvers","title":"High Weak Order Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Note that none of the following methods are adaptive.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SimplifiedEM - A simplified Euler-Maruyama method with weak order 1.0 and fixed step size. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nDRI1 - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nDRI1NM - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle non-mixing diagonal (i.e., du[k] = f(u[k])) and scalar additive noise.†  \nRI1 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI3 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI5 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI6 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI1WM - Fixed step weak order 1.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI2WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nRDI3WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nRDI4WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS1 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS2 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nPL1WM - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nPL1WMA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle additive noise.†         \nNON - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 4). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nSIEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.\nSIEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.   \nSMEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.  \nSMEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.              ","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#StochasticCompositeAlgorithm","page":"SDE Solvers","title":"StochasticCompositeAlgorithm","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"One unique feature of StochasticDiffEq.jl is the StochasticCompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is StochasticCompositeAlgorithm(algtup,choice_function) where algtup is a tuple of StochasticDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses EM() but switches to RKMil() whenever dt is too small:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"choice_function(integrator) = (Int(integrator.dt<0.001) + 1)\nalg_switch = StochasticCompositeAlgorithm((EM(),RKMil()),choice_function)","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The choice_function takes in an integrator and thus all of the features available in the Integrator Interface can be used in the choice function.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#SimpleDiffEq.jl","page":"SDE Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"This setup provides access to simplified versions of a few SDE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to StochasticDiffEq.jl.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SimpleEM - A fixed timestep solve method for Euler-Maruyama. Only works with non-colored Gaussian noise.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Note that this setup is not automatically included with DifferentialEquaitons.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"]add SimpleDiffEq\nusing SimpleDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#BridgeDiffEq.jl","page":"SDE Solvers","title":"BridgeDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquaitons.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Pkg.clone(\"https://github.com/JuliaDiffEq/BridgeDiffEq.jl\")\nusing BridgeDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"BridgeEuler - Strong order 0.5 Euler-Maruyama method for Ito equations.†\nBridgeHeun - Strong order 0.5 Euler-Heun method for Stratonovich equations.†\nBridgeSRK - Strong order 1.0 derivative-free stochastic Runge-Kutta method for scalar (<:Number) Ito equations.†","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/#Notes","page":"SDE Solvers","title":"Notes","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take much more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/#nonauto_dynamical_prob","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Non-autonomous linear ODEs show up in a lot of scientific problems where the differential equation lives on a manifold such as Lie Group. In these situations, specialized solvers can be utilized to enforce physical bounds on the solution and enhance the solving.","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/#Mathematical-Specification-of-a-Non-autonomous-Linear-ODE","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Mathematical Specification of a Non-autonomous Linear ODE","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"These algorithms require a Non-autonomous linear ODE of the form:","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"u^prime = A(upt)u","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Where A is an AbstractDiffEqOperator that is  multiplied against u. Many algorithms specialize on the form of A,  such as A being a constant or A being only time-dependent (A(t)). ","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/#Construction","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Construction","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Creating a non-autonomous linear ODE is the same as an ODEProblem, except f is represented by an AbstractDiffEqOperator (note: this means that any standard ODE solver can also be applied to problems written in this form). As an example:","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"function update_func(A,u,p,t)\n    A[1,1] = cos(t)\n    A[2,1] = sin(t)\n    A[1,2] = -sin(t)\n    A[2,2] = cos(t)\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (10, 50.))","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"defines a quasi-linear ODE u^prime = A(t)u where the components of A are the given functions. Using that formulation, we can see that the general form is u^prime = A(upt)u, for example:","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = 1\n    A[1,2] = -2*(1 - cos(u[2]) - u[2]*sin(u[2]))\n    A[2,2] = 0\nend","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"has a state-dependent linear operator. Note that many other AbstractDiffEqOperators can be used and DiffEqArrayOperator is just one version that represents A via a matrix (other choices are matrix-free).","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Note that if A is a constant, then it is sufficient to supply A directly without an update_func.","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/#Note-About-Affine-Equations","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Note About Affine Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Note that the affine equation","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"u^prime = A(upt)u + g(upt)","category":"page"},{"location":"modules/DiffEqDocs/types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"can be written as a linear form by extending the size of the system by one to have a constant term of 1. This is done by extending A with a new row, containing only zeros, and giving this new state an initial value of 1. Then extend A to have a new column containing the values of g(u,p,t). In this way, these types of equations can be handled by these specialized integrators.","category":"page"},{"location":"modules/StructuralIdentifiability/utils/elimination/#Elimination","page":"Elimination","title":"Elimination","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/elimination/","page":"Elimination","title":"Elimination","text":"Pages=[\"elimination.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/elimination/","page":"Elimination","title":"Elimination","text":"Modules = [StructuralIdentifiability]\nPages   = [\"elimination.jl\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/elimination/#StructuralIdentifiability.Bezout_matrix-Union{Tuple{P}, Tuple{P, P, P}} where P<:AbstractAlgebra.MPolyElem","page":"Elimination","title":"StructuralIdentifiability.Bezout_matrix","text":"Bezout_matrix(f, g, var_elim)\n\nCompute the Bezout matrix of two polynomials f, g with respect to var_elim\n\nInputs:\n\nf - first polynomial\ng - second polynomial\nvar_elim - variable, of which f and g are considered as polynomials\n\nOutput:\n\nM::MatrixElem - The Bezout matrix\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/elimination/#StructuralIdentifiability.Sylvester_matrix-Union{Tuple{P}, Tuple{P, P, P}} where P<:AbstractAlgebra.MPolyElem","page":"Elimination","title":"StructuralIdentifiability.Sylvester_matrix","text":"Sylvester_matrix(f, g, var_elim)\n\nCompute the Sylvester matrix of two polynomials f, g with respect to var_elim Inputs:\n\nf - first polynomial\ng - second polynomial\nvar_elim - variable, of which f and g are considered as polynomials\n\nOutput:\n\nM::MatrixElem - The Sylvester matrix\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/elimination/#StructuralIdentifiability.choose-Union{Tuple{P}, Tuple{Vector{P}, Any}} where P<:(AbstractAlgebra.MPolyElem{<:AbstractAlgebra.FieldElem})","page":"Elimination","title":"StructuralIdentifiability.choose","text":"choose(polys, generic_point_generator)\n\nInput:\n\npolys - an array of distinct irreducible polynomials in the same ring\ngeneric_point_generator - a generic point generator as described above for one of polys\n\nOutput:\n\nthe polynomial that vanishes at the generic_point_generator\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/elimination/#StructuralIdentifiability.eliminate_var-Union{Tuple{P}, Tuple{P, P, P, Any}} where P<:(AbstractAlgebra.MPolyElem{<:AbstractAlgebra.FieldElem})","page":"Elimination","title":"StructuralIdentifiability.eliminate_var","text":"eliminate_var(f, g, var_elim, generic_point_generator)\n\nEliminate variable from a pair of polynomials\n\nInput:\n\nf and g - polynomials\nvar_elim - variable to be eliminated\ngeneric_point_generator - a generic point generator object for the factor     of the resultant of f and g of interest\n\nOutput:\n\npolynomial - the desired factor of the resultant of f and g\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/elimination/#StructuralIdentifiability.simplify_matrix-Union{Tuple{AbstractAlgebra.MatElem{P}}, Tuple{P}} where P<:AbstractAlgebra.MPolyElem","page":"Elimination","title":"StructuralIdentifiability.simplify_matrix","text":"simplify_matrix(M)\n\nEliminate GCD of entries of every row and column\n\nInput:\n\nM::MatrixElem - matrix to be simplified\n\nOutput:\n\nM::MatrixElem - Simplified matrix\nextra_factors::Vector{AbstractAlgebra.MPolyElem} - array of GCDs eliminated from M.\n\n\n\n\n\n","category":"method"},{"location":"modules/HighDimPDE/MLP/#mlp","page":"The MLP algorithm","title":"The MLP algorithm","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"MLP.jl\"]","category":"page"},{"location":"modules/HighDimPDE/MLP/#HighDimPDE.MLP","page":"The MLP algorithm","title":"HighDimPDE.MLP","text":"MLP(; M=4, L=4, K=10, mc_sample = NoSampling())\n\nMulti level Picard algorithm.\n\nArguments\n\nL: number of Picard iterations (Level),\nM: number of Monte Carlo integrations (at each level l, M^(L-l)integrations),\nK: number of Monte Carlo integrations for the non local term    \nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non local term. \n\nCan be UniformSampling(a,b), NormalSampling(σ_sampling), or NoSampling (by default).\n\n\n\n\n\n","category":"type"},{"location":"modules/HighDimPDE/MLP/#HighDimPDE.solve-Tuple{PIDEProblem, MLP}","page":"The MLP algorithm","title":"HighDimPDE.solve","text":"solve(prob::PIDEProblem,\n    alg::MLP;\n    multithreading=true,\n    verbose=false)\n\nReturns a PIDESolution object.\n\nArguments\n\nmultithreading : if true, distributes the job over all the threads available.\nverbose: print information over the iterations.\n\n\n\n\n\n","category":"method"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP, for Multi-Level Picard iterations, reformulates the PDE problem as a fixed point equation through the Feynman Kac formula. ","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"It relies on Picard iterations to find the fixed point, \nreducing the complexity of the numerical approximation of the time integral through a multilvel Monte Carlo approach.","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm overcomes the curse of dimensionality, with a computational complexity that grows polynomially in the number of dimension (see M. Hutzenthaler et al. 2020).","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"warning: `MLP` can only approximate the solution on a single point\nMLP only works for PIDEProblem with x0_sample = NoSampling(). If you want to solve over an entire domain, you definitely want to check the DeepSplitting algorithm.","category":"page"},{"location":"modules/HighDimPDE/MLP/#The-general-idea","page":"The MLP algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Consider the PDE","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R. ","category":"page"},{"location":"modules/HighDimPDE/MLP/#Picard-Iterations","page":"The MLP algorithm","title":"Picard Iterations","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm observes that the Feynman Kac formula can be viewed as a fixed point equation, i.e. u = phi(u). Introducing a sequence (u_k) defined as u_0 = g and ","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_l+1 = phi(u_l)","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"the Banach fixed-point theorem ensures that the sequence converges to the true solution u. Such a technique is known as Picard iterations.","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The time integral term is evaluated by a Monte-Carlo integration","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"u_L  = frac1Msum_i^M left f(X^x(i)_t - s_(l i) u_L-1(T-s_i X^x( i)_t - s_(l i))) + u(0 X^x(i)_t - s_(l i)) right","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"But the MLP uses an extra trick to lower the computational cost of the iteration. ","category":"page"},{"location":"modules/HighDimPDE/MLP/#Telescope-sum","page":"The MLP algorithm","title":"Telescope sum","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The MLP algorithm uses a telescope sum ","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = phi(u_L-1) = phi(u_L-1) - phi(u_L-2) + phi(u_L-2) - phi(u_L-3) + dots \n= sum_l=1^L-1 phi(u_l-1) - phi(u_l-2)\nendaligned","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"As l grows, the term phi(u_l-1) - phi(u_l-2) becomes smaller - and demands more calculations. The MLP algorithm uses this fact by evaluating the integral term at level l with M^L-l samples.","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nL corresponds to the level of the approximation, i.e. u approx u_L\nM characterises the number of samples for the monte carlo approximation of the time integral","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Overall, MLP can be summarised by the following formula","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i^M^L-l left f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i))) + mathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))right\n\nqquad + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Note that the superscripts (l i) indicate the independence of the random variables X across levels.","category":"page"},{"location":"modules/HighDimPDE/MLP/#Nonlocal-PDEs","page":"The MLP algorithm","title":"Nonlocal PDEs","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"MLP can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"partial_t u = mu(t x) nabla_x u(t x) + frac12 sigma^2(t x) Delta u(t x) + int_Omegaf(x y u(tx) u(ty))dy","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"beginaligned\nu_L = sum_l=1^L-1 frac1M^L-lsum_i=1^M^L-l frac1Ksum_j=1^K  bigg f(X^x(l i)_t - s_(l i) Z^(lj) u(T-s_(l i) X^x(l i)_t - s_(l i)) u(T-s_li Z^(lj))) + \nqquad \nmathbf1_N(l) f(X^x(l i)_t - s_(l i) u(T-s_(l i) X^x(l i)_t - s_(l i)))bigg + frac1M^Lsum_i^M^L u(0 X^x(l i)_t)\nendaligned","category":"page"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"tip: Tip\nIn practice, if you have a non-local model you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K. K characterises the number of samples for the Monte Carlo approximation of the last term.\nmc_sample characterises the distribution of the Z variables","category":"page"},{"location":"modules/HighDimPDE/MLP/#References","page":"The MLP algorithm","title":"References","text":"","category":"section"},{"location":"modules/HighDimPDE/MLP/","page":"The MLP algorithm","title":"The MLP algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBecker, S., Braunwarth, R., Hutzenthaler, M., Jentzen, A., von Wurstemberger, P., Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations. arXiv (2020)","category":"page"},{"location":"modules/DiffEqBayes/methods/#Bayesian-Methods","page":"Methods","title":"Bayesian Methods","text":"","category":"section"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"The following methods require DiffEqBayes.jl:","category":"page"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"]add DiffEqBayes\nusing DiffEqBayes","category":"page"},{"location":"modules/DiffEqBayes/methods/#stan_inference","page":"Methods","title":"stan_inference","text":"","category":"section"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"stan_inference(prob::ODEProblem,t,data,priors = nothing;alg=:rk45,\n               num_samples=1000, num_warmups=1000, reltol=1e-3,\n               abstol=1e-6, maxiter=Int(1e5),likelihood=Normal,\n               vars=(StanODEData(),InverseGamma(2,3)))","category":"page"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"stan_inference uses Stan.jl to perform the Bayesian inference. The Stan installation process is required to use this function. t is the array of time and data is the array where the first dimension (columns) corresponds to the array of system values. priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. alg is a choice between :rk45 and :bdf, the two internal integrators of Stan. num_samples is the number of samples to take per chain, and num_warmups is the number of MCMC warmup steps. abstol and reltol are the keyword arguments for the internal integrator. likelihood is the likelihood distribution to use with the arguments from vars, and vars is a tuple of priors for the distributions of the likelihood hyperparameters. The special value StanODEData() in this tuple denotes the position that the ODE solution takes in the likelihood's parameter list.","category":"page"},{"location":"modules/DiffEqBayes/methods/#turing_inference","page":"Methods","title":"turing_inference","text":"","category":"section"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"function turing_inference(prob::DiffEqBase.DEProblem,alg,t,data,priors;\n                              likelihood_dist_priors, likelihood, num_samples=1000,\n                              sampler = Turing.NUTS(num_samples, 0.65), syms, kwargs...)","category":"page"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"turing_inference uses Turing.jl to perform its parameter inference. prob can be any DEProblem with a corresponding alg choice. t is the array of time points and data is the set of observations for the differential equation system at time point t[i] (or higher dimensional). priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. num_samples is the number of samples per MCMC chain. The extra kwargs are given to the internal differential equation solver.","category":"page"},{"location":"modules/DiffEqBayes/methods/#dynamichmc_inference","page":"Methods","title":"dynamichmc_inference","text":"","category":"section"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"dynamichmc_inference(prob::DEProblem,alg,t,data,priors,transformations;\n                      σ = 0.01,ϵ=0.001,initial=Float64[])","category":"page"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"dynamichmc_inference uses DynamicHMC.jl to  perform the bayesian parameter estimation. prob can be any DEProblem, data is the set  of observations for our model which is to be used in the Bayesian Inference process. priors represent the  choice of prior distributions for the parameters to be determined, passed as an array of Distributions.jl distributions. t is the array of time points. transformations  is an array of Tranformations imposed for constraining the  parameter values to specific domains. initial values for the parameters can be passed, if not passed the means of the  priors are used. ϵ can be used as a kwarg to pass the initial step size for the NUTS algorithm.","category":"page"},{"location":"modules/DiffEqBayes/methods/#abc_inference","page":"Methods","title":"abc_inference","text":"","category":"section"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"abc_inference(prob::DEProblem, alg, t, data, priors; ϵ=0.001,\n     distancefunction = euclidean, ABCalgorithm = ABCSMC, progress = false,\n     num_samples = 500, maxiterations = 10^5, kwargs...)","category":"page"},{"location":"modules/DiffEqBayes/methods/","page":"Methods","title":"Methods","text":"abc_inference uses ApproxBayes.jl which uses Approximate Bayesian Computation (ABC) to perform its parameter inference. prob can be any DEProblem with a corresponding alg choice. t is the array of time points and data[:,i] is the set of observations for the differential equation system at time point t[i] (or higher dimensional). priors is an array of prior distributions for each parameter, specified via a Distributions.jl type. num_samples is the number of posterior samples. ϵ is the target distance between the data and simulated data. distancefunction is a distance metric specified from the Distances.jl package, the default is euclidean. ABCalgorithm is the ABC algorithm to use, options are ABCSMC or ABCRejection from ApproxBayes.jl, the default is the former which is more efficient. maxiterations is the maximum number of iterations before the algorithm terminates. The extra kwargs are given to the internal differential equation solver.","category":"page"},{"location":"modules/FFTW/#FFTW.jl","page":"FFTW.jl","title":"FFTW.jl","text":"","category":"section"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"(Image: CI) (Image: Coveralls)","category":"page"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"(Image: ) (Image: )","category":"page"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"This package provides Julia bindings to the FFTW library for fast Fourier transforms (FFTs), as well as functionality useful for signal processing. These functions were formerly a part of Base Julia.","category":"page"},{"location":"modules/FFTW/#Usage-and-documentation","page":"FFTW.jl","title":"Usage and documentation","text":"","category":"section"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"]add FFTW\nusing FFTW\nfft([0; 1; 2; 1])","category":"page"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"returns","category":"page"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"4-element Array{Complex{Float64},1}:\n  4.0 + 0.0im\n -2.0 + 0.0im\n  0.0 + 0.0im\n -2.0 + 0.0im","category":"page"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"The documentation of generic FFT functionality can be found in the AbstractFFTs.jl package. Additional functionalities supported by the FFTW library are documented in the present package.","category":"page"},{"location":"modules/FFTW/#MKL","page":"FFTW.jl","title":"MKL","text":"","category":"section"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"Alternatively, the FFTs in Intel's Math Kernel Library (MKL) can be used by running FFTW.set_provider!(\"mkl\"). MKL will be provided through MKL_jll. This change of provider is persistent and has to be done only once, i.e., the package will use MKL when building and updating.  Note however that MKL provides only a subset of the functionality provided by FFTW. See Intel's documentation for more information about potential differences or gaps in functionality.  In case MKL does not fit the needs (anymore), FFTW.set_provider!(\"fftw\") allows to revert the change of provider.","category":"page"},{"location":"modules/FFTW/#License","page":"FFTW.jl","title":"License","text":"","category":"section"},{"location":"modules/FFTW/","page":"FFTW.jl","title":"FFTW.jl","text":"The FFTW library will be downloaded on versions of Julia where it is no longer distributed as part of Julia. Note that FFTW is licensed under GPLv2 or higher (see its license file), but the bindings to the library in this package, FFTW.jl, are licensed under MIT. This means that code using the FFTW library via the FFTW.jl bindings is subject to FFTW's licensing terms. Code using alternative implementations of the FFTW API, such as MKL's FFTW3 interface are instead subject to the alternative's license. If you distribute a derived or combined work, i.e. a program that links to and is distributed with the FFTW library, then that distribution falls under the terms of the GPL. If you just distribute source code that links to FFTW.jl, and users have to download FFTW or MKL to provide the backend, then the GPL probably doesn't have much effect on you.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#sensitivity_diffeq","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"SciMLSensitivity.jl's high level interface allows for specifying a  sensitivity algorithm (sensealg) to control the method by which solve is differentiated in an automatic differentiation (AD) context by a compatible AD library. The underlying algorithms then  use the direct interface methods, like ODEForwardSensitivityProblem  and adjoint_sensitivities, to compute the derivatives without  requiring the user to do any of the setup.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Current AD libraries whose calls are captured by the sensitivity system are:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Zygote.jl\nDiffractor.jl","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Using-and-Controlling-Sensitivity-Algorithms-within-AD","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Using and Controlling Sensitivity Algorithms within AD","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Take for example this simple differential equation solve on Lotka-Volterra:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"using SciMLSensitivity, OrdinaryDiffEq, Zygote\n\nfunction fiip(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5())\nloss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\ndu0,dp = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"This will compute the gradient of the loss function \"sum of the values of the solution to the ODE at timepoints dt=0.1\" using an adjoint method, where du0 is the derivative of the loss function with respect to the initial condition and dp is the derivative of the loss function with respect to the parameters.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Because the gradient is calculated by Zygote.gradient and Zygote.jl is one of the compatible AD libraries, this derivative calculation will be captured by the sensealg system, and one of SciMLSensitivity.jl's adjoint overloads will be used to compute the derivative. By default, if the sensealg keyword argument is not defined, then a smart polyalgorithm is used to automatically determine the most appropriate method for a given equation.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Likewise, the sensealg argument can be given to directly control the method by which the derivative is computed. For example:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"loss(u0,p) = sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1))\ndu0,dp = Zygote.gradient(loss,u0,p)","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Choosing-a-Sensitivity-Algorithm","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Choosing a Sensitivity Algorithm","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"There are two classes of algorithms: the continuous sensitivity analysis methods, and the discrete sensitivity analysis methods (direct automatic differentiation). Generally:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Continuous sensitivity analysis are more efficient while the discrete  sensitivity analysis is more stable (full discussion is in the appendix of that paper)\nContinuous sensitivity analysis methods only support a subset of equations, which currently includes:\nODEProblem (with mass matrices for differential-algebraic equations (DAEs)\nSDEProblem\nSteadyStateProblem / NonlinearProblem\nDiscrete sensitivity analysis methods only support a subset of algorithms, namely, the pure Julia solvers which are written generically.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis in this arxiv paper. A general rule of thumb is:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"ForwardDiffSensitivity is the fastest for differential equations with small numbers of parameters (<100) and can be used on any differential equation solver that is native Julia. If the chosen ODE solver is not compatible with direct automatic differentiation, ForwardSensitivty may be used instead.\nAdjoint senstivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using QuadratureAdjoint is the fastest but uses the most memory, BacksolveAdjoint uses the least memory but on very stiff problems it may be unstable and require a lot of checkpoints, while InterpolatingAdjoint is in the middle, allowing checkpointing to control total memory use.\nThe methods which use direct automatic differentiation (ReverseDiffAdjoint, TrackerAdjoint, ForwardDiffSensitivity, and ZygoteAdjoint) support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers.\nFor non-ODEs with large numbers of parameters, TrackerAdjoint in out-of-place form may be the best performer on GPUs, and ReverseDiffAdjoint\nTrackerAdjoint is able to use a TrackedArray form with out-of-place functions du = f(u,p,t) but requires an Array{TrackedReal} form for f(du,u,p,t) mutating du. The latter has much more overhead, and should be avoided if possible. Thus if solving non-ODEs with lots of parameters, using TrackerAdjoint with an out-of-place definition may be the current best option.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"note: Note\nCompatibility with direct automatic differentiation algorithms (ForwardDiffSensitivity, ReverseDiffAdjoint, etc.) can be queried using the  SciMLBase.isautodifferentiable(::SciMLAlgorithm) trait function.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"If the chosen algorithm is a continuous sensitivity analysis algorithm, then an autojacvec argument can be given for choosing how the Jacobian-vector product (J*v) or vector-Jacobian product (J'*v) calculation is computed. For the forward sensitivity methods, autojacvec=true is the most efficient, though autojacvec=false is slightly less accurate but very close in efficiency. For adjoint methods it's more complicated and dependent on the way that the user's f function is implemented:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"EnzymeVJP() is the most efficient if it's applicable on your equation.\nIf your function has no branching (no if statements) but uses mutation, ReverseDiffVJP(true) will be the most efficient after Enzyme. Otherwise ReverseDiffVJP(), but you may wish to proceed with eliminating mutation as without compilation enabled this can be slow.\nIf your on the CPU or GPU and your function is very vectorized and has no mutation, choose ZygoteVJP().\nElse fallback to TrackerVJP() if Zygote does not support the function.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Special-Notes-on-Non-ODE-Differential-Equation-Problems","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Special Notes on Non-ODE Differential Equation Problems","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"While all of the choices are compatible with ordinary differential equations, specific notices apply to other forms:","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Differential-Algebraic-Equations","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Differential-Algebraic Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"We note that while all 3 are compatible with index-1 DAEs via the derivation in the universal differential equations paper (note the reinitialization), we do not recommend BacksolveAdjoint one DAEs because the stiffness inherent in these problems tends to cause major difficulties with the accuracy of the backwards solution due to reinitialization of the algebraic variables.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Stochastic-Differential-Equations","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Stochastic Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"We note that all of the adjoints except QuadratureAdjoint are applicable to stochastic differential equations.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Delay-Differential-Equations","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Delay Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"We note that only the discretize-then-optimize methods are applicable to delay differential equations. Constant lag and variable lag delay differential equation parameters can be estimated, but the lag times themselves are unable to be estimated through these automatic differentiation techniques.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Hybrid-Equations-(Equations-with-events/callbacks)-and-Jump-Equations","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Hybrid Equations (Equations with events/callbacks) and Jump Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"ForwardDiffSensitivity can differentiate code with callbacks when convert_tspan=true. ForwardSensitivity is not compatible with hybrid equations. The shadowing methods are  not compatible with callbacks. All methods based on discrete adjoint sensitivity analysis  via automatic differentiation, like ReverseDiffAdjoint, TrackerAdjoint, or  QuadratureAdjoint are fully compatible with events. This applies to ODEs, SDEs, DAEs,  and DDEs. The continuous adjoint sensitivities BacksolveAdjoint, InterpolatingAdjoint, and QuadratureAdjoint are compatible with events for ODEs. BacksolveAdjoint and InterpolatingAdjoint can also handle events for SDEs. Use BacksolveAdjoint if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Manual-VJPs","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Manual VJPs","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Note that when defining your differential equation the vjp can be manually overwritten by providing the AbstractSciMLFunction definition with  a vjp(u,p,t) that returns a tuple f(u,p,t),v->J*v in the form of  ChainRules.jl. When this is done, the choice of ZygoteVJP will utilize your VJP function during the internal steps of the adjoint. This is useful for models where automatic differentiation may have trouble producing optimal code. This can be paired with  ModelingToolkit.jl for producing hyper-optimized, sparse, and parallel VJP functions utilizing the automated symbolic conversions.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Sensitivity-Algorithms","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"The following algorithm choices exist for sensealg. See the sensitivity mathematics page for more details on the definition of the methods.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"ForwardSensitivity\nForwardDiffSensitivity\nBacksolveAdjoint\nInterpolatingAdjoint\nQuadratureAdjoint\nReverseDiffAdjoint\nTrackerAdjoint\nZygoteAdjoint\nForwardLSS\nAdjointLSS\nNILSS\nNILSAS","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ForwardSensitivity","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ForwardSensitivity","text":"ForwardSensitivity{CS,AD,FDT} <: AbstractForwardSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE. When used within adjoint differentiation (i.e. via Zygote), this will cause forward differentiation of the solve call within the reverse-mode automatic differentiation environment.\n\nConstructor\n\nfunction ForwardSensitivity(;\n                            chunk_size=0,autodiff=true,\n                            diff_type=Val{:central},\n                            autojacvec=autodiff,\n                            autojacmat=false)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation in the internal sensitivity algorithm computations. Default is true.\nchunk_size: Chunk size for forward mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\nautojacvec: Calculate the Jacobian-vector product via automatic differentiation with special seeding.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\n\nFurther details:\n\nIf autodiff=true and autojacvec=true, then the one chunk J*v forward-mode directional derivative calculation trick is used to compute the product without constructing the Jacobian (via ForwardDiff.jl).\nIf autodiff=false and autojacvec=true, then the numerical direction derivative trick (f(x+epsilon*v)-f(x))/epsilon is used to compute J*v without constructing the Jacobian.\nIf autodiff=true and autojacvec=false, then the Jacobian is constructed via chunked forward-mode automatic differentiation (via ForwardDiff.jl).\nIf autodiff=false and autojacvec=false, then the Jacobian is constructed via finite differences via FiniteDiff.jl.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems without callbacks (events).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ForwardDiffSensitivity","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ForwardDiffSensitivity","text":"ForwardDiffSensitivity{CS,CTS} <: AbstractForwardSensitivityAlgorithm{CS,Nothing,Nothing}\n\nAn implementation of discrete forward sensitivity analysis through ForwardDiff.jl. When used within adjoint differentiation (i.e. via Zygote), this will cause forward differentiation of the solve call within the reverse-mode automatic differentiation environment.\n\nConstructor\n\nForwardDiffSensitivity(;chunk_size=0,convert_tspan=nothing)\n\nKeyword Arguments\n\nchunk_size: the chunk size used by ForwardDiff for computing the Jacobian, i.e. the number of simultaneous columns computed.\nconvert_tspan: whether to convert time to also be Dual valued. By default this is nothing which will only convert if callbacks are found. Conversion is required in order to accurately differentiate callbacks (hybrid equations).\n\nSciMLProblem Support\n\nThis sensealg supports any SciMLProblems, provided that the solver algorithms is SciMLBase.isautodifferentiable. Note that ForwardDiffSensitivity can accurately differentiate code with callbacks only when convert_tspan=true.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.BacksolveAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.BacksolveAdjoint","text":"BacksolveAdjoint{CS,AD,FDT,VJP} <: AbstractAdjointSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory (O(1) memory). Checkpointing stabilization is included for additional numerical stability over the naive implementation.\n\nConstructor\n\nBacksolveAdjoint(;chunk_size=0,autodiff=true,\n                  diff_type=Val{:central},\n                  autojacvec=nothing,\n                  checkpointing=true, noisemixing=false)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nautojacvec: Calculate the vector-Jacobian product (J'*v) via automatic differentiation with special seeding. The default is true. The total set of choices are:\nfalse: the Jacobian is constructed via FiniteDiff.jl\ntrue: the Jacobian is constructed via ForwardDiff.jl\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp.\nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function.\ncheckpointing: whether checkpointing is enabled for the reverse pass. Defaults to true.\nnoisemixing: Handle noise processes that are not of the form du[i] = f(u[i]). For example, to compute the sensitivities of an SDE with diagonal diffusion\nfunction g_mixing!(du,u,p,t)\n  du[1] = p[3]*u[1] + p[4]*u[2]\n  du[2] = p[3]*u[1] + p[4]*u[2]\n  nothing\nend\ncorrectly, noisemixing=true must be enabled. The default is false.\n\nFor more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.\n\nApplicability of Backsolve and Caution\n\nWhen BacksolveAdjoint is applicable, it is a fast method and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:\n\nusing Sundials\nfunction lorenz(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz,u0,tspan)\nsol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)\nprob2 = ODEProblem(lorenz,sol[end],(100.0,0.0))\nsol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)\n@show sol[end]-u0 #[-3.22091, -1.49394, 21.3435]\n\nThus one should check the stability of the backsolve on their type of problem before enabling this method. Additionally, using checkpointing with backsolve can be a low memory way to stabilize it.\n\nFor more details on this topic, see Stiff Neural Ordinary Differential Equations.\n\nCheckpointing\n\nTo improve the numerical stability of the reverse pass, BacksolveAdjoint includes a checkpointing feature. If sol.u is a time series, then whenever a time sol.t is hit while reversing, a callback will replace the reversing ODE portion with sol.u[i]. This nudges the solution back onto the appropriate trajectory and reduces the numerical caused by drift.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems, SDEProblems, and RODEProblems. This sensealg supports callback functions (events).\n\nReferences\n\nODE:  Rackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar,  R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations  for Scientific Machine Learning,\tarXiv:2001.04385\n\nHindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R.  and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and  differential/algebraic equation solvers, ACM Transactions on Mathematical  Software (TOMS), 31, pp:363–396 (2005)\n\nChen, R.T.Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D. K.,  Neural ordinary differential equations. In Advances in neural information processing  systems, pp. 6571–6583 (2018)\n\nPontryagin, L. S. and Mishchenko, E.F. and Boltyanskii, V.G. and Gamkrelidze, R.V.  The mathematical theory of optimal processes. Routledge, (1962)\n\nRackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J.  and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and  continuous sensitivity analysis for derivatives of differential equation solutions,  arXiv:1812.01892\n\nDAE:  Cao, Y. and Li, S. and Petzold, L. and Serban, R., Adjoint sensitivity analysis  for differential-algebraic equations: The adjoint DAE system and its numerical  solution, SIAM journal on scientific computing 24 pp: 1076-1089 (2003)\n\nSDE:  Gobet, E. and Munos, R., Sensitivity Analysis Using Ito-Malliavin Calculus and  Martingales, and Application to Stochastic Optimal Control,  SIAM Journal on control and optimization, 43, pp. 1676-1713 (2005)\n\nLi, X. and Wong, T.-K. L.and Chen, R. T. Q. and Duvenaud, D.,  Scalable Gradients for Stochastic Differential Equations,  PMLR 108, pp. 3870-3882 (2020), http://proceedings.mlr.press/v108/li20i.html\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.InterpolatingAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.InterpolatingAdjoint","text":"InterpolatingAdjoint{CS,AD,FDT,VJP} <: AbstractAdjointSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled it will only require the memory to interpolate between checkpoints.\n\nConstructor\n\nfunction InterpolatingAdjoint(;chunk_size=0,autodiff=true,\n                               diff_type=Val{:central},\n                               autojacvec=nothing,\n                               checkpointing=false, noisemixing=false)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nautojacvec: Calculate the vector-Jacobian product (J'*v) via automatic differentiation with special seeding. The default is true. The total set of choices are:\nfalse: the Jacobian is constructed via FiniteDiff.jl\ntrue: the Jacobian is constructed via ForwardDiff.jl\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp.\nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function.\ncheckpointing: whether checkpointing is enabled for the reverse pass. Defaults to false.\nnoisemixing: Handle noise processes that are not of the form du[i] = f(u[i]). For example, to compute the sensitivities of an SDE with diagonal diffusion\nfunction g_mixing!(du,u,p,t)\n  du[1] = p[3]*u[1] + p[4]*u[2]\n  du[2] = p[3]*u[1] + p[4]*u[2]\n  nothing\nend\ncorrectly, noisemixing=true must be enabled. The default is false.\n\nFor more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.\n\nCheckpointing\n\nTo reduce the memory usage of the reverse pass, InterpolatingAdjoint includes a checkpointing feature. If sol is dense, checkpointing is ignored and the continuous solution is used for calculating u(t) at arbitrary time points. If checkpointing=true and sol is not dense, then dense intervals between sol.t[i] and sol.t[i+1] are reconstructed on-demand for calculating u(t) at arbitrary time points. This reduces the total memory requirement to only the cost of holding the dense solution over the largest time interval (in terms of number of required steps). The total compute cost is no more than double the original forward compute cost.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems, SDEProblems, and RODEProblems. This sensealg supports callbacks (events).\n\nReferences\n\nRackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar,  R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations  for Scientific Machine Learning,\tarXiv:2001.04385\n\nHindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R.  and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and  differential/algebraic equation solvers, ACM Transactions on Mathematical  Software (TOMS), 31, pp:363–396 (2005)\n\nRackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J.  and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and  continuous sensitivity analysis for derivatives of differential equation solutions,  arXiv:1812.01892\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.QuadratureAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.QuadratureAdjoint","text":"QuadratureAdjoint{CS,AD,FDT,VJP} <: AbstractAdjointSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature. The inner quadrature uses a ReverseDiff vjp if autojacvec, and compile=false by default but can compile the tape under the same circumstances as ReverseDiffVJP.\n\nThis method is O(n^3 + p) for stiff / implicit equations (as opposed to the O((n+p)^3) scaling of BacksolveAdjoint and InterpolatingAdjoint), and thus is much more compute efficient. However, it requires holding a dense reverse pass and is thus memory intensive.\n\nConstructor\n\nfunction QuadratureAdjoint(;chunk_size=0,autodiff=true,\n                            diff_type=Val{:central},\n                            autojacvec=nothing,abstol=1e-6,\n                            reltol=1e-3,compile=false)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nautojacvec: Calculate the vector-Jacobian product (J'*v) via automatic differentiation with special seeding. The default is true. The total set of choices are:\nfalse: the Jacobian is constructed via FiniteDiff.jl\ntrue: the Jacobian is constructed via ForwardDiff.jl\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp.\nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function.\nabstol: absolute tolerance for the quadrature calculation\nreltol: relative tolerance for the quadrature calculation\ncompile: whether to compile the vjp calculation for the integrand calculation. See ReverseDiffVJP for more details.\n\nFor more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems. This sensealg supports events (callbacks).\n\nReferences\n\nRackauckas, C. and Ma, Y. and Martensen, J. and Warner, C. and Zubov, K. and Supekar,  R. and Skinner, D. and Ramadhana, A. and Edelman, A., Universal Differential Equations  for Scientific Machine Learning,\tarXiv:2001.04385\n\nHindmarsh, A. C. and Brown, P. N. and Grant, K. E. and Lee, S. L. and Serban, R.  and Shumaker, D. E. and Woodward, C. S., SUNDIALS: Suite of nonlinear and  differential/algebraic equation solvers, ACM Transactions on Mathematical  Software (TOMS), 31, pp:363–396 (2005)\n\nRackauckas, C. and Ma, Y. and Dixit, V. and Guo, X. and Innes, M. and Revels, J.  and Nyberg, J. and Ivaturi, V., A comparison of automatic differentiation and  continuous sensitivity analysis for derivatives of differential equation solutions,  arXiv:1812.01892\n\nKim, S., Ji, W., Deng, S., Ma, Y., & Rackauckas, C. (2021). Stiff neural ordinary  differential equations. Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(9), 093122.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ReverseDiffAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ReverseDiffAdjoint","text":"ReverseDiffAdjoint <: AbstractAdjointSensitivityAlgorithm{nothing,true,nothing}\n\nAn implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\n\nConstructor\n\nReverseDiffAdjoint()\n\nSciMLProblem Support\n\nThis sensealg supports any DEProblem if the algorithm is SciMLBase.isautodifferentiable. Requires that the state variables are CPU-based Array types.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.TrackerAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.TrackerAdjoint","text":"TrackerAdjoint <: AbstractAdjointSensitivityAlgorithm{nothing,true,nothing}\n\nAn implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\n\nConstructor\n\nTrackerAdjoint()\n\nSciMLProblem Support\n\nThis sensealg supports any DEProblem if the algorithm is SciMLBase.isautodifferentiable Compatible with a limited subset of AbstractArray types for u0, including CuArrays.\n\nwarn: Warn\nTrackerAdjoint is incompatible with Stiff ODE solvers using forward-mode automatic differentiation for the Jacobians. Thus for example, TRBDF2() will error. Instead, use autodiff=false, i.e. TRBDF2(autodiff=false). This will only remove the forward-mode automatic differentiation of the Jacobian construction, not the reverse-mode AD usage, and thus performance will still be nearly the same, though Jacobian accuracy may suffer which could cause more steps to be required.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ZygoteAdjoint","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ZygoteAdjoint","text":"ZygoteAdjoint <: AbstractAdjointSensitivityAlgorithm{nothing,true,nothing}\n\nAn implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver.\n\nConstructor\n\nZygoteAdjoint()\n\nSciMLProblem Support\n\nCurrently fails on almost every solver.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ForwardLSS","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ForwardLSS","text":"ForwardLSS{CS,AD,FDT,RType,gType} <: AbstractShadowingSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of the discrete, forward-mode least squares shadowing (LSS) method. LSS replaces the ill-conditioned initial value probem (ODEProblem) for chaotic systems by a well-conditioned least-squares problem. This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the ODEProblem. The computational cost of LSS scales as (number of states x number of time steps). Converges to the correct sensitivity at a rate of T^(-1/2), where T is the time of the trajectory. See NILSS() and NILSAS() for a more efficient non-intrusive formulation.\n\nConstructor\n\nForwardLSS(;\n          chunk_size=0,autodiff=true,\n          diff_type=Val{:central},\n          LSSregularizer=TimeDilation(10.0,0.0,0.0),\n          g=nothing)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nLSSregularizer: Using LSSregularizer, one can choose between three different regularization routines. The default choice is TimeDilation(10.0,0.0,0.0).\nCosWindowing(): cos windowing of the time grid, i.e. the time grid (saved time steps) is transformed using a cosine.\nCos2Windowing(): cos^2 windowing of the time grid.\nTimeDilation(alpha::Number,t0skip::Number,t1skip::Number): Corresponds to a time dilation. alpha controls the weight. t0skip and t1skip indicate the times truncated at the beginnning and end of the trajectory, respectively.\ng: instantaneous objective function of the long-time averaged objective.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems. This sensealg does not support events (callbacks). This sensealg assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.\n\nReferences\n\nWang, Q., Hu, R., and Blonigan, P. Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations. Journal of Computational Physics, 267, 210-224 (2014).\n\nWang, Q., Convergence of the Least Squares Shadowing Method for Computing Derivative of Ergodic Averages, SIAM Journal on Numerical Analysis, 52, 156–170 (2014).\n\nBlonigan, P., Gomez, S., Wang, Q., Least Squares Shadowing for sensitivity analysis of turbulent fluid flows, in: 52nd Aerospace Sciences Meeting, 1–24 (2014).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.AdjointLSS","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.AdjointLSS","text":"AdjointLSS{CS,AD,FDT,RType,gType} <: AbstractShadowingSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of the discrete, adjoint-mode least square shadowing method. LSS replaces the ill-conditioned initial value probem (ODEProblem) for chaotic systems by a well-conditioned least-squares problem. This allows for computing sensitivities of long-time averaged quantities with respect to the parameters of the ODEProblem. The computational cost of LSS scales as (number of states x number of time steps). Converges to the correct sensitivity at a rate of T^(-1/2), where T is the time of the trajectory. See NILSS() and NILSAS() for a more efficient non-intrusive formulation.\n\nConstructor\n\nAdjointLSS(;\n          chunk_size=0,autodiff=true,\n          diff_type=Val{:central},\n          LSSRegularizer=TimeDilation(10.0,0.0,0.0),\n          g=nothing)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nLSSregularizer: Using LSSregularizer, one can choose between different regularization routines. The default choice is TimeDilation(10.0,0.0,0.0).\nTimeDilation(alpha::Number,t0skip::Number,t1skip::Number): Corresponds to a time dilation. alpha controls the weight. t0skip and t1skip indicate the times truncated at the beginnning and end of the trajectory, respectively. The default value for t0skip and t1skip is zero(alpha).\ng: instantaneous objective function of the long-time averaged objective.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems. This sensealg does not support events (callbacks). This sensealg assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.\n\nReferences\n\nWang, Q., Hu, R., and Blonigan, P. Least squares shadowing sensitivity analysis of chaotic limit cycle oscillations. Journal of Computational Physics, 267, 210-224 (2014).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.NILSS","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.NILSS","text":"struct NILSS{CS,AD,FDT,RNG,nType,gType} <: AbstractShadowingSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of the forward-mode, continuous non-intrusive least squares shadowing method. NILSS allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an ODEProblem by constraining the computation to the unstable subspace. NILSS employs the continuous-time ForwardSensitivity method as tangent solver. To avoid an exponential blow-up of the (homogenous and inhomogenous) tangent solutions, the trajectory should be divided into sufficiently small segments, where the tangent solutions are rescaled on the interfaces. The computational and memory cost of NILSS scale with the number of unstable (positive) Lyapunov exponents (instead of the number of states as in the LSS method). NILSS avoids the explicit construction of the Jacobian at each time step and thus should generally be preferred (for large system sizes) over ForwardLSS.\n\nConstructor\n\nNILSS(nseg, nstep; nus = nothing,\n                   rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n                   chunk_size=0,autodiff=true,\n                   diff_type=Val{:central},\n                   autojacvec=autodiff,\n                   g=nothing)\n\nArguments\n\nnseg: Number of segments on full time interval on the attractor.\nnstep: number of steps on each segment.\n\nKeyword Arguments\n\nnus: Dimension of the unstable subspace. Default is nothing. nus must be smaller or equal to the state dimension (length(u0)). With the default choice, nus = length(u0) - 1 will be set at compile time.\nrng: (Pseudo) random number generator. Used for initializing the homogenous tangent states (w). Default is Xorshifts.Xoroshiro128Plus(rand(UInt64)).\nautodiff: Use automatic differentiation in the internal sensitivity algorithm computations. Default is true.\nchunk_size: Chunk size for forward mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\nautojacvec: Calculate the Jacobian-vector product via automatic differentiation with special seeding.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\ng: instantaneous objective function of the long-time averaged objective.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems. This sensealg does not support events (callbacks). This sensealg assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.\n\nReferences\n\nNi, A., Blonigan, P. J., Chater, M., Wang, Q., Zhang, Z., Sensitivity analy- sis on chaotic dynamical system by Non-Intrusive Least Square Shadowing (NI-LSS), in: 46th AIAA Fluid Dynamics Conference, AIAA AVIATION Forum (AIAA 2016-4399), American Institute of Aeronautics and Astronautics, 1–16 (2016).\n\nNi, A., and Wang, Q. Sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Shadowing (NILSS). Journal of Computational Physics 347, 56-77 (2017).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.NILSAS","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.NILSAS","text":"NILSAS{CS,AD,FDT,RNG,SENSE,gType} <: AbstractShadowingSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of the adjoint-mode, continuous non-intrusive adjoint least squares shadowing method. NILSAS allows for computing sensitivities of long-time averaged quantities with respect to the parameters of an ODEProblem by constraining the computation to the unstable subspace. NILSAS employs SciMLSensitivity.jl's continuous adjoint sensitivity methods on each segment to compute (homogenous and inhomogenous) adjoint solutions. To avoid an exponential blow-up of the adjoint solutions, the trajectory should be divided into sufficiently small segments, where the adjoint solutions are rescaled on the interfaces. The computational and memory cost of NILSAS scale with the number of unstable, adjoint Lyapunov exponents (instead of the number of states as in the LSS method). NILSAS avoids the explicit construction of the Jacobian at each time step and thus should generally be preferred (for large system sizes) over AdjointLSS. NILSAS is favourable over NILSS for many parameters because NILSAS computes the gradient with respect to multiple parameters with negligible additional cost.\n\nConstructor\n\nNILSAS(nseg, nstep, M=nothing; rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n                                adjoint_sensealg = BacksolveAdjoint(autojacvec=ReverseDiffVJP()),\n                                chunk_size=0,autodiff=true,\n                                diff_type=Val{:central},\n                                g=nothing\n                                )\n\nArguments\n\nnseg: Number of segments on full time interval on the attractor.\nnstep: number of steps on each segment.\nM: number of homogenous adjoint solutions. This number must be bigger or equal than the number of (positive, adjoint) Lyapunov exponents. Default is nothing.\n\nKeyword Arguments\n\nrng: (Pseudo) random number generator. Used for initializing the terminate conditions of the homogenous adjoint states (w). Default is Xorshifts.Xoroshiro128Plus(rand(UInt64)).\nadjoint_sensealg: Continuous adjoint sensitivity method to compute homogenous and inhomogenous adjoint solutions on each segment. Default is BacksolveAdjoint(autojacvec=ReverseDiffVJP()).\nautojacvec: Calculate the vector-Jacobian product (J'*v) via automatic\ndifferentiation with special seeding. The default is true. The total set of choices are:\nfalse: the Jacobian is constructed via FiniteDiff.jl\ntrue: the Jacobian is constructed via ForwardDiff.jl\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp.\nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function.\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\ng: instantaneous objective function of the long-time averaged objective.\n\nSciMLProblem Support\n\nThis sensealg only supports ODEProblems. This sensealg does not support events (callbacks). This sensealg assumes that the objective is a long-time averaged quantity and ergodic, i.e. the time evolution of the system behaves qualitatively the same over infinite time independent of the specified initial conditions, such that only the sensitivity with respect to the parameters is of interest.\n\nReferences\n\nNi, A., and Talnikar, C., Adjoint sensitivity analysis on chaotic dynamical systems by Non-Intrusive Least Squares Adjoint Shadowing (NILSAS). Journal of Computational Physics 395, 690-709 (2019).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Vector-Jacobian-Product-(VJP)-Choices","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Vector-Jacobian Product (VJP) Choices","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"ZygoteVJP\nEnzymeVJP\nTrackerVJP\nReverseDiffVJP","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ZygoteVJP","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ZygoteVJP","text":"ZygoteVJP <: VJPChoice\n\nUses Zygote.jl to compute vector-Jacobian products. Tends to be the fastest VJP method if the ODE/DAE/SDE/DDE is written with mostly vectorized  functions (like neural networks and other layers from Flux.jl) and the f functions is given out-of-place. If the f function is in-place, then Zygote.Buffer arrays are used internally which can greatly reduce the performance of the VJP method.\n\nConstructor\n\nZygoteVJP(;allow_nothing=false)\n\nKeyword arguments:\n\nallow_nothing: whether nothings should be implicitly converted to zeros. In Zygote, the derivative of a function with respect to p which does not use p in any possible calculation is given a derivative of nothing instead of zero. By default, this nothing is caught in order to throw an informative error message about a potentially unintentional misdefined function. However, if this was intentional, setting allow_nothing=true will remove the error message.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.EnzymeVJP","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.EnzymeVJP","text":"EnzymeVJP <: VJPChoice\n\nUses Enzyme.jl to compute vector-Jacobian products. Is the fastest VJP whenever applicable, though Enzyme.jl currently has low coverage over the Julia programming language, for example restricting the user's defined f function to not do things like require garbage collection or calls to BLAS/LAPACK. However, mutation is supported, meaning that in-place f with fully mutating non-allocating code will work with Enzyme (provided no high level calls to C like BLAS/LAPACK are used) and this will be the most efficient adjoint implementation.\n\nConstructor\n\nEnzymeVJP(compile=false)\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.TrackerVJP","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.TrackerVJP","text":"TrackerVJP <: VJPChoice\n\nUses Tracker.jl to compute the vector-Jacobian products. If f is in-place, then it uses a array of structs formulation to do scalarized reverse mode, while if f is out-of-place then it uses an array-based reverse mode.\n\nNot as efficient as ReverseDiffVJP, but supports GPUs when doing array-based reverse mode.\n\nConstructor\n\nTrackerVJP(;allow_nothing=false)\n\nKeyword arguments:\n\nallow_nothing: whether non-tracked values should be implicitly converted to zeros. In Tracker, the derivative of a function with respect to p which does not use p in any possible calculation is given an untracked return instead of zero. By default, this nothing Trackedness is caught in order to throw an informative error message about a potentially unintentional misdefined function. However, if this was intentional, setting allow_nothing=true will remove the error message.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#SciMLSensitivity.ReverseDiffVJP","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"SciMLSensitivity.ReverseDiffVJP","text":"ReverseDiffVJP{compile} <: VJPChoice\n\nUses ReverseDiff.jl to compute the vector-Jacobian products. If f is in-place, then it uses a array of structs formulation to do scalarized reverse mode, while if f is out-of-place then it uses an array-based reverse mode.\n\nUsually the fastest when scalarized operations exist in the f function (like in scientific machine learning applications like Universal Differential Equations) and the boolean compilation is enabled (i.e. ReverseDiffVJP(true)), if EnzymeVJP fails on a given choice of f.\n\nDoes not support GPUs (CuArrays).\n\nConstructor\n\nReverseDiffVJP(compile=false)\n\nKeyword Arguments\n\ncompile: Whether to cache the compilation of the reverse tape. This heavily increases the performance of the method but requires that the f function of the ODE/DAE/SDE/DDE has no branching.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#More-Details-on-Sensitivity-Algorithm-Choices","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"More Details on Sensitivity Algorithm Choices","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"The following section describes a bit more details to consider when choosing a sensitivity algorithm.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Optimize-then-Discretize","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Optimize-then-Discretize","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"The original neural ODE paper popularized optimize-then-discretize with O(1) adjoints via backsolve. This is the methodology BacksolveAdjoint When training non-stiff neural ODEs, BacksolveAdjoint with ZygoteVJP is generally the fastest method. Additionally, this method does not require storing the values of any intermediate points and is thus the most memory efficient. However, BacksolveAdjoint is prone to instabilities whenever the Lipschitz constant is sufficiently large, like in stiff equations, PDE discretizations, and many other contexts, so it is not used by default. When training a neural ODE for machine learning applications, the user should try BacksolveAdjoint and see if it is sufficiently accurate on their problem. More details on this topic can be found in  Stiff Neural Ordinary Differential Equations","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"Note that DiffEqFlux's implementation of BacksolveAdjoint includes an extra feature BacksolveAdjoint(checkpointing=true) which mixes checkpointing with BacksolveAdjoint. What this method does is that, at saveat points, values from the forward pass are saved. Since the reverse solve should numerically be the same as the forward pass, issues with divergence of the reverse pass are mitigated by restarting the reverse pass at the saveat value from the forward pass. This reduces the divergence and can lead to better gradients at the cost of higher memory usage due to having to save some values of the forward pass. This can stabilize the adjoint in some applications, but for highly stiff applications the divergence can be too fast for this to work in practice.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"To avoid the issues of backwards solving the ODE, InterpolatingAdjoint and QuadratureAdjoint utilize information from the forward pass. By default these methods utilize the continuous solution provided by DifferentialEquations.jl in the calculations of the adjoint pass. QuadratureAdjoint uses this to build a continuous function for the solution of adjoint equation and then performs an adaptive quadrature via Quadrature.jl, while InterpolatingAdjoint appends the integrand to the ODE so it's computed simultaneously to the Lagrange multiplier. When memory is not an issue, we find that the QuadratureAdjoint approach tends to be the most efficient as it has a significantly smaller adjoint differential equation and the quadrature converges very fast, but this form requires holding the full continuous solution of the adjoint which can be a significant burden for large parameter problems. The InterpolatingAdjoint is thus a compromise between memory efficiency and compute efficiency, and is in the same spirit as CVODES.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"However, if the memory cost of the InterpolatingAdjoint is too high, checkpointing can be used via InterpolatingAdjoint(checkpointing=true). When this is used, the checkpoints default to sol.t of the forward pass (i.e. the saved timepoints usually set by saveat). Then in the adjoint, intervals of sol.t[i-1] to sol.t[i] are re-solved in order to obtain a short interpolation which can be utilized in the adjoints. This at most results in two full solves of the forward pass, but dramatically reduces the computational cost while being a low-memory format. This is the preferred method for highly stiff equations when memory is an issue, i.e. stiff PDEs or large neural DAEs.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"For forward-mode, the ForwardSensitivty is the version that performs the optimize-then-discretize approach. In this case, autojacvec corresponds to the method for computing J*v within the forward sensitivity equations, which is either true or false for whether to use Jacobian-free forward-mode AD (via ForwardDiff.jl) or Jacobian-free numerical differentiation.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/#Discretize-then-Optimize","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Discretize-then-Optimize","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"In this approach the discretization is done first and then optimization is done on the discretized system. While traditionally this can be done discrete sensitivity analysis, this is can be equivalently done by automatic differentiation on the solver itself. ReverseDiffAdjoint performs reverse-mode automatic differentiation on the solver via ReverseDiff.jl, ZygoteAdjoint performs reverse-mode automatic differentiation on the solver via Zygote.jl, and TrackerAdjoint performs reverse-mode automatic differentiation on the solver via Tracker.jl. In addition, ForwardDiffSensitivty performs forward-mode automatic differentiation on the solver via ForwardDiff.jl.","category":"page"},{"location":"modules/SciMLSensitivity/manual/differential_equation_sensitivities/","page":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Differential Equations with Automatic Differentiation (AD)","text":"We note that many studies have suggested that this approach produces more accurate gradients than the optimize-than-discretize approach","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/#shadowing_methods","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"Let us define the instantaneous objective g(up) which depends on the state u and the parameter p of the differential equation. Then, if the objective is a long-time average quantity","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"langle g rangle_ = lim_T rightarrow  langle g rangle_T","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"where","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"langle g rangle_T = frac1T int_0^T g(up) textdt","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"under the assumption of ergodicity, langle g rangle_ only depends on p.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"In the case of chaotic systems, the trajectories diverge with O(1) error]. This can be seen, for instance, when solving the Lorenz system at 1e-14 tolerances with 9th order integrators and a small machine-epsilon perturbation:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"using OrdinaryDiffEq, SciMLSensitivity, Zygote\n\nfunction lorenz!(du, u, p, t)\n  du[1] = 10 * (u[2] - u[1])\n  du[2] = u[1] * (p[1] - u[3]) - u[2]\n  du[3] = u[1] * u[2] - (8 // 3) * u[3]\nend\n\np = [28.0]\ntspan = (0.0, 100.0)\nu0 = [1.0, 0.0, 0.0]\nprob = ODEProblem(lorenz!, u0, tspan, p)\nsol = solve(prob, Vern9(), abstol = 1e-14, reltol = 1e-14)\nsol2 = solve(prob, Vern9(), abstol = 1e-14 + eps(Float64), reltol = 1e-14)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"(Image: Chaotic behavior of the Lorenz system)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"More formally, such chaotic behavior can be analyzed using tools from uncertainty quantification. This effect of diverging trajectories is known as the butterfly effect and can be formulated as \"most (small) perturbations on initial conditions or parameters lead to new trajectories diverging exponentially fast from the original trajectory\".","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"The latter statement can be roughly translated to the level of sensitivity calculation as follows: \"For most initial conditions, the (homogeneous) tangent solutions grow exponentially fast.\"","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"To compute derivatives of an objective langle g rangle_ with respect to the parameters p of a chaotic systems, one thus encounters that \"traditional\" forward and adjoint sensitivity methods diverge because the tangent space diverges with a rate given by the Lyapunov exponent. Taking the average of these derivative can then also fail, i.e., one finds that the average derivative is not the derivative of the average.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"Although numerically computed chaotic trajectories diverge from the true/original trajectory, the shadowing theorem guarantees that there exists an errorless trajectory with a slightly different initial condition that stays near (\"shadows\") the numerically computed one, see, e.g, the blog post or the non-intrusive least squares shadowing paper for more details. Essentially, the idea is to replace the ill-conditioned ODE by a well-conditioned optimization problem. Shadowing methods use the shadowing theorem within a renormalization procedure to distill the long-time effect from the joint observation of the long-time and the butterfly effect. This allows us to accurately compute derivatives w.r.t. the long-time average quantities.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"The following sensealg choices exist","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"ForwardLSS(;LSSregularizer=TimeDilation(10.0,0.0,0.0),g=nothing,ADKwargs...): An implementation of the forward least square shadowing method. For LSSregularizer, one can choose between two different windowing options, TimeDilation (default) with weight 10.0 and CosWindowing, and Cos2Windowing.\nAdjointLSS(;LSSRegularizer=TimeDilation(10.0, 0.0, 0.0),g=nothing,ADKwargs...): An implementation of the adjoint-mode least square shadowing method. 10.0 controls the weight of the time dilation term in AdjointLSS.\nNILSS(nseg,nstep;nus=nothing,rng=Xorshifts.Xoroshiro128Plus(rand(UInt64)),g=nothing,ADKwargs...):   An implementation of the non-intrusive least squares shadowing (NILSS) method. Here, nseg is the number of segments, nstep is the number of steps per segment, and nus is the number of unstable Lyapunov exponents.\nNILSAS(nseg,nstep,M=nothing;rng =Xorshifts.Xoroshiro128Plus(rand(UInt64)),         adjoint_sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP()),g=nothing,ADKwargs...):   An implementation of the non-intrusive least squares adjoint shadowing (NILSAS) method. nseg is the number of segments. nstep is the number of steps per segment, M >= nus + 1 has to be provided, where nus is the number of unstable covariant Lyapunov vectors.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"Recommendation: Since the computational and memory costs of NILSS() scale with the number of positive (unstable) Lyapunov, it is typically less expensive than ForwardLSS(). AdjointLSS() and NILSAS() are favorable for a large number of system parameters.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"As an example, for the Lorenz system with g(u,p,t) = u[3], i.e., the z coordinate, as the instantaneous objective, we can use the direct interface by passing ForwardLSS as the sensealg:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"function lorenz!(du,u,p,t)\n  du[1] = p[1]*(u[2]-u[1])\n  du[2] = u[1]*(p[2]-u[3]) - u[2]\n  du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\np = [10.0, 28.0, 8/3]\n\ntspan_init = (0.0,30.0)\ntspan_attractor = (30.0,50.0)\nu0 = rand(3)\nprob_init = ODEProblem(lorenz!,u0,tspan_init,p)\nsol_init = solve(prob_init,Tsit5())\nprob_attractor = ODEProblem(lorenz!,sol_init[end],tspan_attractor,p)\n\ng(u,p,t) = u[end]\n\nfunction G(p)\n  _prob = remake(prob_attractor,p=p)\n  _sol = solve(_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=0.01,sensealg=ForwardLSS(g=g))\n  sum(getindex.(_sol.u,3))\nend\ndp1 = Zygote.gradient(p->G(p),p)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"Alternatively, we can define the ForwardLSSProblem and solve it via shadow_forward as follows:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/chaotic_ode/","page":"Sensitivity analysis for chaotic systems (shadowing methods)","title":"Sensitivity analysis for chaotic systems (shadowing methods)","text":"sol_attractor = solve(prob_attractor, Vern9(), abstol=1e-14, reltol=1e-14)\nlss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(g=g))\nresfw = shadow_forward(lss_problem)","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#Partial-Differential-Equation-(PDE)-Solvers-Overview","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#NeuralPDE.jl:-Physics-Informed-Neural-Network-(PINN)-PDE-Solvers","page":"Partial Differential Equation (PDE) Solvers Overview","title":"NeuralPDE.jl: Physics-Informed Neural Network (PINN) PDE Solvers","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"NeuralPDE.jl is a partial differential equation solver library which uses physics-informed neural networks (PINNs) to solve the equations. It uses the ModelingToolkit.jl symbolic PDESystem as its input and can handle a wide variety of equation types, including systems of partial differential equations, partial differential-algebraic equations, and integro-differential equations. Its benefit is its flexibility, and it can be used to easily generate surrogate solutions over entire parameter ranges. However, its downside is solver speed: PINN solvers tend to be a lot slower than other methods for solving PDEs.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#MethodOflines.jl:-Automated-Finite-Difference-Method-(FDM)","page":"Partial Differential Equation (PDE) Solvers Overview","title":"MethodOflines.jl: Automated Finite Difference Method (FDM)","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"MethodOflines.jl is a partial differential equation solver library which automates the discretization of PDEs via the finite difference method. It uses the ModelingToolkit.jl symbolic PDESystem as its input, and generates AbstractSystems and SciMLProblems whose numerical solution gives the solution to the PDE.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#FEniCS.jl:-Wrappers-for-the-Finite-Element-Method-(FEM)","page":"Partial Differential Equation (PDE) Solvers Overview","title":"FEniCS.jl: Wrappers for the Finite Element Method (FEM)","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"FEniCS.jl is a wrapper for the popular FEniCS finite element method library.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#HighDimPDE.jl:-High-dimensional-PDE-Solvers","page":"Partial Differential Equation (PDE) Solvers Overview","title":"HighDimPDE.jl:  High-dimensional PDE Solvers","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"HighDimPDE.jl is a partial differential equation solver library which implements algorithms that break down the curse of dimensionality  to solve the equations. It implements deep-learning based and Picard-iteration based methods to approximately solve high-dimensional, nonlinear, non-local PDEs in up to 10,000 dimensions.  Its cons are accuracy: high-dimensional solvers are stochastic, and might result in wrong solutions if the solver meta-parameters are not appropriate.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#NeuralOperators.jl:-(Fourier)-Neural-Operators-and-DeepONets-for-PDE-Solving","page":"Partial Differential Equation (PDE) Solvers Overview","title":"NeuralOperators.jl: (Fourier) Neural Operators and DeepONets for PDE Solving","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"NeuralOperators.jl is a library for operator learning based PDE solvers. This includes techniques like:","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"Fourier Neural Operators (FNO)\nDeep Operator Networks (DeepONets)\nMarkov Neural Operators (MNO)","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"Currently its connection to PDE solving must be specified manually, though an interface for ModelingToolkit PDESystems is in progress.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#DiffEqOperators.jl:-Operators-for-Finite-Difference-Method-(FDM)-Discretizations","page":"Partial Differential Equation (PDE) Solvers Overview","title":"DiffEqOperators.jl: Operators for Finite Difference Method (FDM) Discretizations","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"DiffEqOperators.jl is a library for defining finite difference operators to easily perform manual FDM semi-discretizations of partial differential equations. This library is fairly incomplete and most cases should receive better performance using MethodOflines.jl.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#Third-Party-Libraries-to-Note","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/#ApproxFun.jl:-Automated-Spectral-Discretizations","page":"Partial Differential Equation (PDE) Solvers Overview","title":"ApproxFun.jl: Automated Spectral Discretizations","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"ApproxFun.jl is a package for approximating functions in basis sets. One particular use case is with spectral basis sets, such as Chebyshev functions and Fourier decompositions, making it easy to represent spectral and pseudospectral discretizations of partial differential equations as ordinary differential equations for the SciML equation solvers.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#Gridap.jl:-Julia-Basd-Tools-for-Finite-Element-Discretizations","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Gridap.jl: Julia-Basd Tools for Finite Element Discretizations","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"Gridap.jl is a package for grid-based approximation of partial differential equations, particularly notable for its use of conforming and nonconforming finite element (FEM) discretizations.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#Trixi.jl:-Adaptive-High-Order-Numerical-Simulations-of-Hyperbolic-Equations","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Trixi.jl: Adaptive High-Order Numerical Simulations of Hyperbolic Equations","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"Trixi.jl is a package for numerical simulation of hyperbolic conservation laws, i.e. a large set of hyperbolic partial differential equations, which interfaces and uses the SciML ordinary differential equation solvers.","category":"page"},{"location":"highlevels/partial_differential_equation_solvers/#VoronoiFVM.jl:-Tools-for-the-Voronoi-Finite-Volume-Discretizations","page":"Partial Differential Equation (PDE) Solvers Overview","title":"VoronoiFVM.jl: Tools for the Voronoi Finite Volume Discretizations","text":"","category":"section"},{"location":"highlevels/partial_differential_equation_solvers/","page":"Partial Differential Equation (PDE) Solvers Overview","title":"Partial Differential Equation (PDE) Solvers Overview","text":"VoronoiFVM.jl is a library for generating FVM discretizations of systems of PDEs. It interfaces with many of the SciML equation solver libraries to allow for ease of discretization and flexibility in the solver choice.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#stiff","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"This tutorial is for getting into the extra features for solving large stiff ordinary differential equations in an efficient manner. Solving stiff ordinary differential equations requires specializing the linear solver on properties of the Jacobian in order to cut down on the mathcalO(n^3) linear solve and the mathcalO(n^2) back-solves. Note that these same functions and controls also extend to stiff SDEs, DDEs, DAEs, etc. This tutorial is for large-scale models, such as those derived for semi-discretizations of partial differential equations (PDEs). For example, we will use the stiff Brusselator partial differential equation (BRUSS).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"note: Note\nThis tutorial is for advanced users to dive into advanced features! DifferentialEquations.jl automates most of this usage, so we recommend users try solve(prob) with the automatic algorithm first!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Definition-of-the-Brusselator-Equation","page":"Solving Large Stiff Equations","title":"Definition of the Brusselator Equation","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"note: Note\nFeel free to skip this section: it simply defines the example problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"The Brusselator PDE is defined as follows:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"beginalign\r\nfracpartial upartial t = 1 + u^2v - 44u + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2) + f(x y t)\r\nfracpartial vpartial t = 34u - u^2v + alpha(fracpartial^2 vpartial x^2 + fracpartial^2 vpartial y^2)\r\nendalign","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"where","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"f(x y t) = begincases\r\n5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11 \r\n0  quad textelse\r\nendcases","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"and the initial conditions are","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"beginalign\r\nu(x y 0) = 22cdot (y(1-y))^32 \r\nv(x y 0) = 27cdot (x(1-x))^32\r\nendalign","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"with the periodic boundary condition","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"beginalign\r\nu(x+1yt) = u(xyt) \r\nu(xy+1t) = u(xyt)\r\nendalign","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"on a timespan of t in 0115.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"To solve this PDE, we will discretize it into a system of ODEs with the finite difference method. We discretize u and v into arrays of the values at each time point: u[i,j] = u(i*dx,j*dy) for some choice of dx/dy, and same for v. Then our ODE is defined with U[i,j,k] = [u v]. The second derivative operator, the Laplacian, discretizes to become a tridiagonal matrix with [1 -2 1] and a 1 in the top right and bottom left corners. The nonlinear functions are then applied at each point in space (they are broadcast). Use dx=dy=1/32.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"The resulting ODEProblem definition is:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using DifferentialEquations, LinearAlgebra, SparseArrays\r\n\r\nconst N = 32\r\nconst xyd_brusselator = range(0,stop=1,length=N)\r\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\r\nlimit(a, N) = a == N+1 ? 1 : a == 0 ? N : a\r\nfunction brusselator_2d_loop(du, u, p, t)\r\n  A, B, alpha, dx = p\r\n  alpha = alpha/dx^2\r\n  @inbounds for I in CartesianIndices((N, N))\r\n    i, j = Tuple(I)\r\n    x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\r\n    ip1, im1, jp1, jm1 = limit(i+1, N), limit(i-1, N), limit(j+1, N), limit(j-1, N)\r\n    du[i,j,1] = alpha*(u[im1,j,1] + u[ip1,j,1] + u[i,jp1,1] + u[i,jm1,1] - 4u[i,j,1]) +\r\n                B + u[i,j,1]^2*u[i,j,2] - (A + 1)*u[i,j,1] + brusselator_f(x, y, t)\r\n    du[i,j,2] = alpha*(u[im1,j,2] + u[ip1,j,2] + u[i,jp1,2] + u[i,jm1,2] - 4u[i,j,2]) +\r\n                A*u[i,j,1] - u[i,j,1]^2*u[i,j,2]\r\n    end\r\nend\r\np = (3.4, 1., 10., step(xyd_brusselator))\r\n\r\nfunction init_brusselator_2d(xyd)\r\n  N = length(xyd)\r\n  u = zeros(N, N, 2)\r\n  for I in CartesianIndices((N, N))\r\n    x = xyd[I[1]]\r\n    y = xyd[I[2]]\r\n    u[I,1] = 22*(y*(1-y))^(3/2)\r\n    u[I,2] = 27*(x*(1-x))^(3/2)\r\n  end\r\n  u\r\nend\r\nu0 = init_brusselator_2d(xyd_brusselator)\r\nprob_ode_brusselator_2d = ODEProblem(brusselator_2d_loop,u0,(0.,11.5),p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Choosing-Jacobian-Types","page":"Solving Large Stiff Equations","title":"Choosing Jacobian Types","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"When one is using an implicit or semi-implicit differential equation solver, the Jacobian must be built at many iterations and this can be one of the most expensive steps. There are two pieces that must be optimized in order to reach maximal efficiency when solving stiff equations: the sparsity pattern and the construction of the Jacobian. The construction is filling the matrix J with values, while the sparsity pattern is what J to use.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"The sparsity pattern is given by a prototype matrix, the jac_prototype, which will be copied to be used as J. The default is for J to be a Matrix, i.e. a dense matrix. However, if you know the sparsity of your problem, then you can pass a different matrix type. For example, a SparseMatrixCSC will give a sparse matrix. Other sparse matrix types include:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Bidiagonal\nTridiagonal\nSymTridiagonal\nBandedMatrix (BandedMatrices.jl)\nBlockBandedMatrix (BlockBandedMatrices.jl)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"DifferentialEquations.jl will internally use this matrix type, making the factorizations faster by using the specialized forms.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Declaring-a-Sparse-Jacobian-with-Automatic-Sparsity-Detection","page":"Solving Large Stiff Equations","title":"Declaring a Sparse Jacobian with Automatic Sparsity Detection","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Jacobian sparsity is declared by the jac_prototype argument in the ODEFunction. Note that you should only do this if the sparsity is high, for example, 0.1% of the matrix is non-zeros, otherwise the overhead of sparse matrices can be higher than the gains from sparse differentiation!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"One of the useful companion tools for DifferentialEquations.jl is Symbolics.jl. This allows for automatic declaration of Jacobian sparsity types. To see this in action, we can give an example du and u and call jacobian_sparsity on our function with the example arguments and it will kick out a sparse matrix with our pattern, that we can turn into our jac_prototype.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using Symbolics\r\ndu0 = copy(u0)\r\njac_sparsity = Symbolics.jacobian_sparsity((du,u)->brusselator_2d_loop(du,u,p,0.0),du0,u0)\r\n\r\n2048×2048 SparseArrays.SparseMatrixCSC{Bool, Int64} with 12288 stored entries:\r\n⠻⣦⡀⠀⠀⠀⠀⠈⠳⣄⠀⠀⠀⠀⠀⠀\r\n⠀⠈⠻⣦⡀⠀⠀⠀⠀⠈⠳⣄⠀⠀⠀⠀\r\n⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠈⠳⣄⠀⠀\r\n⡀⠀⠀⠀⠀⠈⠻⣦⠀⠀⠀⠀⠀⠈⠳⣄\r\n⠙⢦⡀⠀⠀⠀⠀⠀⠻⣦⡀⠀⠀⠀⠀⠈\r\n⠀⠀⠙⢦⡀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀\r\n⠀⠀⠀⠀⠙⢦⡀⠀⠀⠀⠀⠈⠻⣦⡀⠀\r\n⠀⠀⠀⠀⠀⠀⠙⢦⡀⠀⠀⠀⠀⠈⠻⣦","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Notice Julia gives a nice print out of the sparsity pattern. That's neat, and would be tedious to build by hand! Now we just pass it to the ODEFunction like as before:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"f = ODEFunction(brusselator_2d_loop;jac_prototype=float.(jac_sparsity))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Build the ODEProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"prob_ode_brusselator_2d_sparse = ODEProblem(f,u0,(0.,11.5),p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Now let's see how the version with sparsity compares to the version without:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using BenchmarkTools # for @btime\r\n@btime solve(prob_ode_brusselator_2d,TRBDF2(),save_everystep=false) # 2.771 s (5452 allocations: 65.73 MiB)\r\n@btime solve(prob_ode_brusselator_2d_sparse,TRBDF2(),save_everystep=false) # 680.612 ms (37905 allocations: 359.34 MiB)\r\n@btime solve(prob_ode_brusselator_2d_sparse,KenCarp47(linsolve=KLUFactorization()),save_everystep=false) # 342.017 ms (65150 allocations: 158.99 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Note that depending on the properties of the sparsity pattern, one may want to try alternative linear solvers such as TRBDF2(linsolve = KLUFactorization()) or TRBDF2(linsolve = UMFPACKFactorization()).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Using-Jacobian-Free-Newton-Krylov","page":"Solving Large Stiff Equations","title":"Using Jacobian-Free Newton-Krylov","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"A completely different way to optimize the linear solvers for large sparse matrices is to use a Krylov subpsace method. This requires choosing a linear solver for changing to a Krylov method. To swap the linear solver out, we use the linsolve command and choose the GMRES linear solver.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"@btime solve(prob_ode_brusselator_2d,KenCarp47(linsolve=KrylovJL_GMRES()),save_everystep=false)\r\n# 707.439 ms (173868 allocations: 31.07 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Notice that this acceleration does not require the definition of a sparsity pattern and can thus be an easier way to scale for large problems. For more information on linear solver choices, see the linear solver documentation. linsolve choices are any valid LinearSolve.jl solver.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"note: Note\nSwitching to a Krylov linear solver will automatically change the ODE solver into Jacobian-free mode, dramatically reducing the memory required. This can be overridden by adding concrete_jac=true to the algorithm.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Adding-a-Preconditioner","page":"Solving Large Stiff Equations","title":"Adding a Preconditioner","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Any LinearSolve.jl-compatible preconditioner can be used as a preconditioner in the linear solver interface. To define preconditioners, one must define a precs function in compatible stiff ODE solvers which returns the left and right preconditioners, matrices which approximate the inverse of W = I - gamma*J used in the solution of the ODE. An example of this with using IncompleteLU.jl is as follows:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using IncompleteLU\r\nfunction incompletelu(W,du,u,p,t,newW,Plprev,Prprev,solverdata)\r\n  if newW === nothing || newW\r\n    Pl = ilu(convert(AbstractMatrix,W), τ = 50.0)\r\n  else\r\n    Pl = Plprev\r\n  end\r\n  Pl,nothing\r\nend\r\n\r\n# Required due to a bug in Krylov.jl: https://github.com/JuliaSmoothOptimizers/Krylov.jl/pull/477\r\nBase.eltype(::IncompleteLU.ILUFactorization{Tv,Ti}) where {Tv,Ti} = Tv\r\n\r\n@time solve(prob_ode_brusselator_2d_sparse,KenCarp47(linsolve=KrylovJL_GMRES(),precs=incompletelu,concrete_jac=true),save_everystep=false);\r\n# 174.386 ms (61756 allocations: 61.38 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Notice a few things about this preconditioner. This preconditioner uses the sparse Jacobian, and thus we set concrete_jac=true to tell the algorithm to generate the Jacobian (otherwise, a Jacobian-free algorithm is used with GMRES by default). Then newW = true whenever a new W matrix is computed, and newW=nothing during the startup phase of the solver. Thus we do a check newW === nothing || newW and when true, it's only at these points when we we update the preconditioner, otherwise we just pass on the previous version. We use convert(AbstractMatrix,W) to get the concrete W matrix (matching jac_prototype, thus SpraseMatrixCSC) which we can use in the preconditioner's definition. Then we use IncompleteLU.ilu on that sparse matrix to generate the preconditioner. We return Pl,nothing to say that our preconditioner is a left preconditioner, and that there is no right preconditioning.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"This method thus uses both the Krylov solver and the sparse Jacobian. Not only that, it is faster than both implementations! IncompleteLU is fussy in that it requires a well-tuned τ parameter. Another option is to use AlgebraicMultigrid.jl which is more automatic. The setup is very similar to before:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using AlgebraicMultigrid\r\nfunction algebraicmultigrid(W,du,u,p,t,newW,Plprev,Prprev,solverdata)\r\n  if newW === nothing || newW\r\n    Pl = aspreconditioner(ruge_stuben(convert(AbstractMatrix,W)))\r\n  else\r\n    Pl = Plprev\r\n  end\r\n  Pl,nothing\r\nend\r\n\r\n# Required due to a bug in Krylov.jl: https://github.com/JuliaSmoothOptimizers/Krylov.jl/pull/477\r\nBase.eltype(::AlgebraicMultigrid.Preconditioner) = Float64\r\n\r\n@btime solve(prob_ode_brusselator_2d_sparse,KenCarp47(linsolve=KrylovJL_GMRES(),precs=algebraicmultigrid,concrete_jac=true),save_everystep=false);\r\n# 372.528 ms (61179 allocations: 160.82 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"or with a Jacobi smoother:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"function algebraicmultigrid2(W,du,u,p,t,newW,Plprev,Prprev,solverdata)\r\n  if newW === nothing || newW\r\n    A = convert(AbstractMatrix,W)\r\n    Pl = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(A, presmoother = AlgebraicMultigrid.Jacobi(rand(size(A,1))), postsmoother = AlgebraicMultigrid.Jacobi(rand(size(A,1)))))\r\n  else\r\n    Pl = Plprev\r\n  end\r\n  Pl,nothing\r\nend\r\n\r\n@btime solve(prob_ode_brusselator_2d_sparse,KenCarp47(linsolve=KrylovJL_GMRES(),precs=algebraicmultigrid2,concrete_jac=true),save_everystep=false);\r\n# 293.476 ms (65714 allocations: 170.23 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"For more information on the preconditioner interface, see the linear solver documentation.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Sundials-Specific-Handling","page":"Solving Large Stiff Equations","title":"Sundials-Specific Handling","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"While much of the setup makes the transition to using Sundials automatic, there are some differences between the pure Julia implementations and the Sundials implementations which must be taken note of. These are all detailed in the Sundials solver documentation, but here we will highlight the main details which one should make note of.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Defining a sparse matrix and a Jacobian for Sundials works just like any other package. The core difference is in the choice of the linear solver. With Sundials, the linear solver choice is done with a Symbol in the linear_solver from a preset list. Particular choices of note are :Band for a banded matrix and :GMRES for using GMRES. If you are using Sundials, :GMRES will not require defining the JacVecOperator, and instead will always make use of a Jacobian-Free Newton Krylov (with numerical differentiation). Thus on this problem we could do:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using Sundials\r\n@btime solve(prob_ode_brusselator_2d,CVODE_BDF(),save_everystep=false) # 13.280 s (51457 allocations: 2.43 MiB)\r\n# Simplest speedup: use :LapackDense\r\n@btime solve(prob_ode_brusselator_2d,CVODE_BDF(linear_solver=:LapackDense),save_everystep=false) # 2.024 s (51457 allocations: 2.43 MiB)\r\n# GMRES Version: Doesn't require any extra stuff!\r\n@btime solve(prob_ode_brusselator_2d,CVODE_BDF(linear_solver=:GMRES),save_everystep=false) # 213.800 ms (58353 allocations: 2.64 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Notice that using sparse matrices with Sundials requires an analytical Jacobian function. We will use ModelingToolkit.jl's modelingtoolkitize to automatically generate this:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using ModelingToolkit\r\nprob_ode_brusselator_2d_mtk = ODEProblem(modelingtoolkitize(prob_ode_brusselator_2d_sparse),[],(0.0,11.5),jac=true,sparse=true);\r\n@btime solve(prob_ode_brusselator_2d_mtk,CVODE_BDF(linear_solver=:KLU),save_everystep=false) # 493.908 ms (1358 allocations: 5.79 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/#Using-Preconditioners-with-Sundials","page":"Solving Large Stiff Equations","title":"Using Preconditioners with Sundials","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Details for setting up a preconditioner with Sundials can be found at the Sundials solver page. Sundials algorithms are very different from the standard Julia-based algorithms in that they require the user does all handling of the Jacobian matrix. To do this, you must define a psetup function that sets up the preconditioner and then a prec function that is the action of the preconditioner on a vector. For the psetup function, we need to first compute the W = I - gamma*J matrix before computing the preconditioner on it. For the ILU example above, this is done for Sundials like:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"using LinearAlgebra\r\nu0 = prob_ode_brusselator_2d_mtk.u0\r\np  = prob_ode_brusselator_2d_mtk.p\r\nconst jaccache = prob_ode_brusselator_2d_mtk.f.jac(u0,p,0.0)\r\nconst W = I - 1.0*jaccache\r\n\r\nprectmp = ilu(W, τ = 50.0)\r\nconst preccache = Ref(prectmp)\r\n\r\nfunction psetupilu(p, t, u, du, jok, jcurPtr, gamma)\r\n  if jok\r\n    prob_ode_brusselator_2d_mtk.f.jac(jaccache,u,p,t)\r\n    jcurPtr[] = true\r\n\r\n    # W = I - gamma*J\r\n    @. W = -gamma*jaccache\r\n    idxs = diagind(W)\r\n    @. @view(W[idxs]) = @view(W[idxs]) + 1\r\n\r\n    # Build preconditioner on W\r\n    preccache[] = ilu(W, τ = 5.0)\r\n  end\r\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"Then the preconditioner action is to simply use the ldiv! of the generated preconditioner:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"function precilu(z,r,p,t,y,fy,gamma,delta,lr)\r\n  ldiv!(z,preccache[],r)\r\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"We then simply pass these functions to the Sundials solver with a choice of prec_side=1 to indicate that it is a left-preconditioner:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"@btime solve(prob_ode_brusselator_2d_sparse,CVODE_BDF(linear_solver=:GMRES,prec=precilu,psetup=psetupilu,prec_side=1),save_everystep=false);\r\n# 87.176 ms (17717 allocations: 77.08 MiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"And similarly for algebraic multigrid:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/advanced_ode_example/","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"prectmp2 = aspreconditioner(ruge_stuben(W, presmoother = AlgebraicMultigrid.Jacobi(rand(size(W,1))), postsmoother = AlgebraicMultigrid.Jacobi(rand(size(W,1)))))\r\nconst preccache2 = Ref(prectmp2)\r\nfunction psetupamg(p, t, u, du, jok, jcurPtr, gamma)\r\n  if jok\r\n    prob_ode_brusselator_2d_mtk.f.jac(jaccache,u,p,t)\r\n    jcurPtr[] = true\r\n\r\n    # W = I - gamma*J\r\n    @. W = -gamma*jaccache\r\n    idxs = diagind(W)\r\n    @. @view(W[idxs]) = @view(W[idxs]) + 1\r\n\r\n    # Build preconditioner on W\r\n    preccache2[] = aspreconditioner(ruge_stuben(W, presmoother = AlgebraicMultigrid.Jacobi(rand(size(W,1))), postsmoother = AlgebraicMultigrid.Jacobi(rand(size(W,1)))))\r\n  end\r\nend\r\n\r\nfunction precamg(z,r,p,t,y,fy,gamma,delta,lr)\r\n  ldiv!(z,preccache2[],r)\r\nend\r\n\r\n@btime solve(prob_ode_brusselator_2d_sparse,CVODE_BDF(linear_solver=:GMRES,prec=precamg,psetup=psetupamg,prec_side=1),save_everystep=false);\r\n# 136.431 ms (30682 allocations: 275.68 MiB)","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/#Solving-Linear-Systems-in-Julia","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"","category":"section"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"A linear system Au=b is specified by defining an AbstractMatrix A, or by providing a matrix-free operator for performing A*x operations via the function A(u,p,t) out-of-place and A(du,u,p,t) for in-place. For the sake of simplicity, this tutorial will only showcase concrete matrices.","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"The following defines a matrix and a LinearProblem which is subsequently solved by the default linear solver.","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"using LinearSolve\n\nA = rand(4,4)\nb = rand(4)\nprob = LinearProblem(A, b)\nsol = solve(prob)\nsol.u\n\n#=\n4-element Vector{Float64}:\n  0.3784870087078603\n  0.07275749718047864\n  0.6612816064734302\n -0.10598367531463938\n=#","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Note that solve(prob) is equivalent to solve(prob,nothing) where nothing denotes the choice of the default linear solver. This is equivalent to the Julia built-in A\\b, where the solution is recovered via sol.u. The power of this package comes into play when changing the algorithms. For example, IterativeSolvers.jl has some nice methods like GMRES which can be faster in some cases. With LinearSolve.jl, there is one interface and changing linear solvers is simply the switch of the algorithm choice:","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"sol = solve(prob,IterativeSolversJL_GMRES())\n\n#=\nu: 4-element Vector{Float64}:\n  0.37848700870786\n  0.07275749718047908\n  0.6612816064734302\n -0.10598367531463923\n=#","category":"page"},{"location":"modules/LinearSolve/tutorials/linear/","page":"Solving Linear Systems in Julia","title":"Solving Linear Systems in Julia","text":"Thus a package which uses LinearSolve.jl simply needs to allow the user to pass in an algorithm struct and all wrapped linear solvers are immediately available as tweaks to the general algorithm.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/#The-DiffEq-Internals","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"The DiffEq solvers, OrdinaryDiffEq, StochasticDiffEq, FiniteElementDiffEq, etc. all follow a similar scheme which leads to rapid development and high performance. This portion of the documentation explains how the algorithms are written.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/#Developing-New-Solver-Algorithms","page":"The DiffEq Internals","title":"Developing New Solver Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"The easiest way to get started would be to add new solver algorithms. This is a pretty simple task as there are tools which put you right into the \"hot loop\". For example, take a look at the ODE solver code. The mode solve(::ODEProblem,::OrdinaryDiffEqAlgorithm) is glue code to a bunch of solver algorithms. The algorithms which are coded in DifferentialEquations.jl can be found in src/integrators.jl. The actual step is denoted by the perform_step!(integrator) function. For example, take a look at the Midpoint method's implementation:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"@inline function perform_step!(integrator,cache::MidpointConstantCache,f=integrator.f)\n  @unpack t,dt,uprev,u,k = integrator\n  halfdt = dt/2\n  k = integrator.fsalfirst\n  k = f(t+halfdt,uprev+halfdt*k)\n  u = uprev + dt*k\n  integrator.fsallast = f(u,p,t+dt) # For interpolation, then FSAL'd\n  integrator.k[1] = integrator.fsalfirst\n  integrator.k[2] = integrator.fsallast\n  @pack integrator = t,dt,u\nend","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"The available items are all unloaded from the integrator in the first line. fsalfirst inherits the value of fsallast on the last line. The algorithm is written in this form so that way the derivative of both endpoints is defined, allowing the vector integrator.k to determine a  Hermite interpolation polynomial (in general, the k values for each algorithm form the interpolating polynomial). Other than that, the algorithm is as basic as it gets for the Midpoint method, making sure to set fsallast at the last line. The results are then packaged back into the integrator to mutate the state.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"Note the name ConstantCache. In OrdinaryDiffEq.jl, the Algorithm types are used for holding type information about which solver to choose, and its the \"cache\" types which then hold the internal caches and state. ConstantCache types are for non in-place calculations f(u,p,t) and Cache types (like MidpointCache) include all of the internal arrays. Their constructor is specified in the cache.jl file. The cache (and the first fsalfirst) is initialized in the initialize! function next to the cache's perform_step! function.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"The main inner loop can be summarized by the solve! command:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"@inbounds while !isempty(integrator.opts.tstops)\n   while integrator.tdir*integrator.t < integrator.tdir*top(integrator.opts.tstops)\n     loopheader!(integrator)\n     @ode_exit_conditions\n     perform_step!(integrator,integrator.cache)\n     loopfooter!(integrator)\n     if isempty(integrator.opts.tstops)\n       break\n     end\n   end\n   handle_tstop!(integrator)\n end\n postamble!(integrator)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"The algorithm runs until tstop is empty. It hits the loopheader! in order to accept/reject the previous step and choose a new dt. This is done at the top so that way the iterator interface happens \"mid-step\" instead of \"post-step\". In here the algorithm is also chosen for the switching algorithms.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"Then the perform_step! is called. (The exit conditions throw an error if necessary.) Afterwards, the loopfooter! is used to calculate new timesteps, save, and apply the callbacks. If a value of tstops is hit, the algorithm breaks out of the inner-most loop to save the value.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"Adding algorithms to the other problems is very similar, just in a different package.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/#Extras","page":"The DiffEq Internals","title":"Extras","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"If the method is a FSAL method then it needs to be set via isfsal and fsalfirst should be defined before the loop, with fsallast what's pushed up to fsalfirst upon a successful step. See :DP5 for an example.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"If tests fail due to units (i.e. Unitful), don't worry. I would be willing to fix that up. To do so, you have to make sure you keep separate your rateTypes and your uTypes since the rates from f will have units of u but divided by a unit of time. If you simply try to write these into u, the units part will fail (normally you have to multiply by a dt).","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/diffeq_internals/","page":"The DiffEq Internals","title":"The DiffEq Internals","text":"If you want to access the value of u at the second-last time point, you can use uprev2 value. But in order to copy uprev value to uprev2 after each timestep, you need to make alg_extrapolates(alg::Your_Alg) = true in the alg_utils.jl file.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Automated-Index-Reduction-of-DAEs","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"In many cases one may accidentally write down a DAE that is not easily solvable by numerical methods. In this tutorial we will walk through an example of a pendulum which accidentally generates an index-3 DAE, and show how to use the modelingtoolkitize to correct the model definition before solving.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Copy-Pastable-Example","page":"Automated Index Reduction of DAEs","title":"Copy-Pastable Example","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"using ModelingToolkit\nusing LinearAlgebra\nusing OrdinaryDiffEq\nusing Plots\n\nfunction pendulum!(du, u, p, t)\n    x, dx, y, dy, T = u\n    g, L = p\n    du[1] = dx\n    du[2] = T*x\n    du[3] = dy\n    du[4] = T*y - g\n    du[5] = x^2 + y^2 - L^2\n    return nothing\nend\npendulum_fun! = ODEFunction(pendulum!, mass_matrix=Diagonal([1,1,1,1,0]))\nu0 = [1.0, 0, 0, 0, 0]\np = [9.8, 1]\ntspan = (0, 10.0)\npendulum_prob = ODEProblem(pendulum_fun!, u0, tspan, p)\ntraced_sys = modelingtoolkitize(pendulum_prob)\npendulum_sys = structural_simplify(dae_index_lowering(traced_sys))\nprob = ODAEProblem(pendulum_sys, [], tspan)\nsol = solve(prob, Tsit5(),abstol=1e-8,reltol=1e-8)\nplot(sol, vars=states(traced_sys))","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Explanation","page":"Automated Index Reduction of DAEs","title":"Explanation","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Attempting-to-Solve-the-Equation","page":"Automated Index Reduction of DAEs","title":"Attempting to Solve the Equation","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"In this tutorial we will look at the pendulum system:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"beginaligned\n    x^prime = v_x\n    v_x^prime = Tx\n    y^prime = v_y\n    v_y^prime = Ty - g\n    0 = x^2 + y^2 - L^2\nendaligned","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"As a good DifferentialEquations.jl user, one would follow the mass matrix DAE tutorial to arrive at code for simulating the model:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"using OrdinaryDiffEq, LinearAlgebra\nfunction pendulum!(du, u, p, t)\n    x, dx, y, dy, T = u\n    g, L = p\n    du[1] = dx; du[2] = T*x\n    du[3] = dy; du[4] = T*y - g\n    du[5] = x^2 + y^2 - L^2\nend\npendulum_fun! = ODEFunction(pendulum!, mass_matrix=Diagonal([1,1,1,1,0]))\nu0 = [1.0, 0, 0, 0, 0]; p = [9.8, 1]; tspan = (0, 10.0)\npendulum_prob = ODEProblem(pendulum_fun!, u0, tspan, p)\nsolve(pendulum_prob,Rodas4())","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"However, one will quickly be greeted with the unfortunate message:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"┌ Warning: First function call produced NaNs. Exiting.\n└ @ OrdinaryDiffEq C:\\Users\\accou\\.julia\\packages\\OrdinaryDiffEq\\yCczp\\src\\initdt.jl:76\n┌ Warning: Automatic dt set the starting dt as NaN, causing instability.\n└ @ OrdinaryDiffEq C:\\Users\\accou\\.julia\\packages\\OrdinaryDiffEq\\yCczp\\src\\solve.jl:485\n┌ Warning: NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.\n└ @ SciMLBase C:\\Users\\accou\\.julia\\packages\\SciMLBase\\DrPil\\src\\integrator_interface.jl:325","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"Did you implement the DAE incorrectly? No. Is the solver broken? No.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Understanding-DAE-Index","page":"Automated Index Reduction of DAEs","title":"Understanding DAE Index","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"It turns out that this is a property of the DAE that we are attempting to solve. This kind of DAE is known as an index-3 DAE. For a complete discussion of DAE index, see this article. Essentially the issue here is that we have 4 differential variables (x, v_x, y, v_y) and one algebraic variable T (which we can know because there is no D(T) term in the equations). An index-1 DAE always satisfies that the Jacobian of the algebraic equations is non-singular. Here, the first 4 equations are differential equations, with the last term the algebraic relationship. However, the partial derivative of x^2 + y^2 - L^2 w.r.t. T is zero, and thus the Jacobian of the algebraic equations is the zero matrix and thus it's singular. This is a very quick way to see whether the DAE is index 1!","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"The problem with higher order DAEs is that the matrices used in Newton solves are singular or close to singular when applied to such problems. Because of this fact, the nonlinear solvers (or Rosenbrock methods) break down, making them difficult to solve. The classic paper DAEs are not ODEs goes into detail on this and shows that many methods are no longer convergent when index is higher than one. So it's not necessarily the fault of the solver or the implementation: this is known.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"But that's not a satisfying answer, so what do you do about it?","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Transforming-Higher-Order-DAEs-to-Index-1-DAEs","page":"Automated Index Reduction of DAEs","title":"Transforming Higher Order DAEs to Index-1 DAEs","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"It turns out that higher order DAEs can be transformed into lower order DAEs. If you differentiate the last equation two times and perform a substitution, you can arrive at the following set of equations:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"beginaligned\nx^prime = v_x \nv_x^prime = x T \ny^prime = v_y \nv_y^prime = y T - g \n0 = 2 left(v_x^2 + v_y^2 + y ( y T - g ) + T x^2 right)\nendaligned","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"Note that this is mathematically-equivalent to the equation that we had before, but the Jacobian w.r.t. T of the algebraic equation is no longer zero because of the substitution. This means that if you wrote down this version of the model it will be index-1 and solve correctly! In fact, this is how DAE index is commonly defined: the number of differentiations it takes to transform the DAE into an ODE, where an ODE is an index-0 DAE by substituting out all of the algebraic relationships.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/#Automating-the-Index-Reduction","page":"Automated Index Reduction of DAEs","title":"Automating the Index Reduction","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"However, requiring the user to sit there and work through this process on potentially millions of equations is an unfathomable mental overhead. But, we can avoid this by using methods like the Pantelides algorithm for automatically performing this reduction to index 1. While this requires the ModelingToolkit symbolic form, we use modelingtoolkitize to transform the numerical code into symbolic code, run dae_index_lowering lowering, then transform back to numerical code with ODEProblem, and solve with a numerical solver. Let's try that out:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"traced_sys = modelingtoolkitize(pendulum_prob)\npendulum_sys = structural_simplify(dae_index_lowering(traced_sys))\nprob = ODEProblem(pendulum_sys, Pair[], tspan)\nsol = solve(prob, Rodas4())\n\nusing Plots\nplot(sol, vars=states(traced_sys))","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"Note that plotting using states(traced_sys) is done so that any variables which are symbolically eliminated, or any variable reorderings done for enhanced parallelism/performance, still show up in the resulting plot and the plot is shown in the same order as the original numerical code.","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"Note that we can even go a little bit further. If we use the ODAEProblem constructor, we can remove the algebraic equations from the states of the system and fully transform the index-3 DAE into an index-0 ODE which can be solved via an explicit Runge-Kutta method:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"traced_sys = modelingtoolkitize(pendulum_prob)\npendulum_sys = structural_simplify(dae_index_lowering(traced_sys))\nprob = ODAEProblem(pendulum_sys, Pair[], tspan)\nsol = solve(prob, Tsit5(),abstol=1e-8,reltol=1e-8)\nplot(sol, vars=states(traced_sys))","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize_index_reduction/","page":"Automated Index Reduction of DAEs","title":"Automated Index Reduction of DAEs","text":"And there you go: this has transformed the model from being too hard to solve with implicit DAE solvers, to something that is easily solved with explicit Runge-Kutta methods for non-stiff equations.","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"using PolyChaos\nd = 6\nmyops = Dict()\npolynames = [\"gaussian\", \"beta01\", \"uniform01\", \"logistic\"]\ngaussian = GaussOrthoPoly(d);\nmyops[\"gaussian\"] = gaussian\nα, β = 1.3, 2.2\nbeta01 = Beta01OrthoPoly(d,α,β);\nmyops[\"beta01\"] = beta01\nuniform01 = Uniform01OrthoPoly(d);\nmyops[\"uniform01\"] = uniform01\nlogistic = LogisticOrthoPoly(d);\nmyops[\"logistic\"] = logistic;\nmyops\npoints, degrees = randn(10), 0:2:d\n[ evaluate(degree,points,gaussian) for degree in degrees ]\nμ, σ = 2., 0.2\npce_gaussian = convert2affinePCE(μ,σ,gaussian)\na, b = -0.3, 1.2\nconvert2affinePCE(a,b,uniform01)\npce_uniform = convert2affinePCE(μ,σ,uniform01;kind=\"μσ\")\nconvert2affinePCE(a,b,beta01)\npce_beta = convert2affinePCE(μ,σ,beta01; kind=\"μσ\")\na1, a2 = μ, sqrt(3)*σ/pi\npce_logistic = convert2affinePCE(a1,a2,logistic)\nmean(pce_gaussian,myops[\"gaussian\"]), std(pce_gaussian,myops[\"gaussian\"])\nmean(pce_uniform,myops[\"uniform01\"]), std(pce_uniform,myops[\"uniform01\"])\nmean(pce_beta,myops[\"beta01\"]), std(pce_beta,myops[\"beta01\"])\nmean(pce_logistic,myops[\"logistic\"]), std(pce_logistic,myops[\"logistic\"])\nusing Statistics\nN = 1000\nξ_gaussian = sampleMeasure(N,myops[\"gaussian\"])\nsamples_gaussian = evaluatePCE(pce_gaussian,ξ_gaussian,myops[\"gaussian\"])\nξ_uniform = sampleMeasure(N,myops[\"uniform01\"])\nsamples_uniform = evaluatePCE(pce_uniform,ξ_uniform,myops[\"uniform01\"])\nξ_beta = sampleMeasure(N,myops[\"beta01\"])\nsamples_beta = evaluatePCE(pce_beta,ξ_beta,myops[\"beta01\"])\nξ_logistic = sampleMeasure(N,myops[\"logistic\"])\nsamples_logistic = evaluatePCE(pce_logistic,ξ_logistic,myops[\"logistic\"])\n","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#CommonRandomVariables","page":"Basic Usage","title":"Common Random Variables","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Polynomial chaos expansion (PCE) is a Hilbert space technique for random variables with finite variance. Mathematically equivalent to Fourier series expansions for periodic signals, PCE allows to characterize a random variable in terms of its PCE coefficients (aka Fourier coefficients). That is, the PCE of a random variable mathsfx is given by","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"mathsfx = sum_i=0^L x_i phi_i","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"where x_i are the so-called PCE coefficients, and phi_i are the orthogonal polynomials that are orthogonal relative to the probability density function of mathsfx.","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"This tutorial walks you through the PCE of common random variables, namely Gaussian (gaussian), Beta (beta01), Uniform(uniform01), Logistic (logistic), and shows how they are implemented in PolyChaos.","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Construction-of-Basis","page":"Basic Usage","title":"Construction of Basis","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"using PolyChaos","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"The orthogonal polynomials are constructed using the OrthoPoly-type (here of degree at most d). For canonical measures special constructors are implemented:","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"d = 6\n\nmyops = Dict()\npolynames = [\"gaussian\", \"beta01\", \"uniform01\", \"logistic\"]\n\n# gaussian\ngaussian = GaussOrthoPoly(d);\nmyops[\"gaussian\"] = gaussian\n\n# beta01\nα, β = 1.3, 2.2\nbeta01 = Beta01OrthoPoly(d,α,β);\nmyops[\"beta01\"] = beta01\n\n# uniform01\nuniform01 = Uniform01OrthoPoly(d);\nmyops[\"uniform01\"] = uniform01\n\n# logistic\nlogistic = LogisticOrthoPoly(d);\nmyops[\"logistic\"] = logistic;\n\nmyops","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"For example, let's evaluate the Gaussian basis polynomials at some points","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"points, degrees = randn(10), 0:2:d\n\n[ evaluate(degree,points,gaussian) for degree in degrees ]","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Finding-PCE-Coefficients","page":"Basic Usage","title":"Finding PCE Coefficients","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Having constructed the orthogonal bases, the question remains how to find the PCE coefficients for the common random variables. Every random variable can be characterized exactly by two PCE coefficients. For a Gaussian random variable, this is familiar: the mean and the variance suffice to describe a Gaussian random variable entirely. The same is true for any random variable of finite variance given the right basis. The function convert2affinePCE provides the first two PCE coefficients (hence the name affine) for the common random variables.","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Gaussian","page":"Basic Usage","title":"Gaussian","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Given the Gaussian random variable mathsfx sim mathcalN(mu sigma^2) with sigma  0, the affine PCE coefficients are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"# Gaussian\nμ, σ = 2., 0.2\npce_gaussian = convert2affinePCE(μ,σ,gaussian)","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Uniform","page":"Basic Usage","title":"Uniform","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Given the uniform random variable mathsfx sim mathcalU(a b) with finite support ab, the affine PCE coefficients are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"a, b = -0.3, 1.2\nconvert2affinePCE(a,b,uniform01)","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Instead, if the expected value and standard deviation are known, the affine PCE coefficients of the uniform random variable are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"pce_uniform = convert2affinePCE(μ,σ,uniform01;kind=\"μσ\")\n# notice that the zero-order coefficient IS equivalent to the expected value μ","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Beta","page":"Basic Usage","title":"Beta","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Given the Beta random variable mathsfx sim mathcalB(a b alpha beta) with finite support ab and shape parameters alpha beta  0, the affine PCE coefficients are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"convert2affinePCE(a,b,beta01)","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Instead, if the expected value and standard deviation are known, the affine PCE coefficients of the uniform random variable are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"pce_beta = convert2affinePCE(μ,σ,beta01; kind=\"μσ\")","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Logistic","page":"Basic Usage","title":"Logistic","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Given the logstic random variable mathsfx sim mathcalL(a_1a_2) where a_20 with the probability density function","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"rho(t) = frac14 a_2  operatornamesech^2 left(fract-a_12a_2right)","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"the affine PCE coefficients of the uniform random variable are","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"a1, a2 = μ, sqrt(3)*σ/pi\npce_logistic = convert2affinePCE(a1,a2,logistic)","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Moments","page":"Basic Usage","title":"Moments","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"It is a key feature of PCE to compute moments from the PCE coefficients alone; no sampling is required.","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Gaussian-2","page":"Basic Usage","title":"Gaussian","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"mean(pce_gaussian,myops[\"gaussian\"]), std(pce_gaussian,myops[\"gaussian\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Uniform-2","page":"Basic Usage","title":"Uniform","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"mean(pce_uniform,myops[\"uniform01\"]), std(pce_uniform,myops[\"uniform01\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Beta-2","page":"Basic Usage","title":"Beta","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"mean(pce_beta,myops[\"beta01\"]), std(pce_beta,myops[\"beta01\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Logistic-2","page":"Basic Usage","title":"Logistic","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"mean(pce_logistic,myops[\"logistic\"]), std(pce_logistic,myops[\"logistic\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Sampling","page":"Basic Usage","title":"Sampling","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Having found the PCE coefficients, it may be useful to sample the random variables. That means, find N realizations of the random variable that obey the random variable's probability density function. This is done in two steps:","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Draw N samples from the measure (sampleMeasure()), and then\nEvaluate the basis polynomials and multiply times the PCE coefficients, i.e. sum_i=0^L x_i phi_i(xi_j) where xi_j is the j-th sample from the measure (evaluatePCE()).","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"Both steps are combined in the function samplePCE().","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Gaussian-3","page":"Basic Usage","title":"Gaussian","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"using Statistics\nN = 1000\nξ_gaussian = sampleMeasure(N,myops[\"gaussian\"])\nsamples_gaussian = evaluatePCE(pce_gaussian,ξ_gaussian,myops[\"gaussian\"])\n# samplePCE(N,pce_gaussian,myops[\"gaussian\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Uniform-3","page":"Basic Usage","title":"Uniform","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"ξ_uniform = sampleMeasure(N,myops[\"uniform01\"])\nsamples_uniform = evaluatePCE(pce_uniform,ξ_uniform,myops[\"uniform01\"])\n# samples_uniform = samplePCE(N,pce_uniform,myops[\"uniform01\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Beta-3","page":"Basic Usage","title":"Beta","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"ξ_beta = sampleMeasure(N,myops[\"beta01\"])\nsamples_beta = evaluatePCE(pce_beta,ξ_beta,myops[\"beta01\"])\n# samples_beta = samplePCE(N,pce_beta,myops[\"beta01\"])","category":"page"},{"location":"modules/PolyChaos/pce_tutorial/#Logistic-3","page":"Basic Usage","title":"Logistic","text":"","category":"section"},{"location":"modules/PolyChaos/pce_tutorial/","page":"Basic Usage","title":"Basic Usage","text":"ξ_logistic = sampleMeasure(N,myops[\"logistic\"])\nsamples_logistic = evaluatePCE(pce_logistic,ξ_logistic,myops[\"logistic\"])\n# samples_logistic = samplePCE(N,pce_logistic,myops[\"logistic\"])","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/#Generalized-Likelihood-Inference","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"In this example we will demo the likelihood-based approach to parameter fitting. First let's generate a dataset to fit. We will re-use the Lotka-Volterra equation but in this case fit just two parameters.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"f1 = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3.0 * u[2] + u[1]*u[2]\nend\np = [1.5,1.0]\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(f1,u0,tspan,p)\nsol = solve(prob1,Tsit5())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This is a function with two parameters, [1.5,1.0] which generates the same ODE solution as before. This time, let's generate 100 datasets where at each point adds a little bit of randomness:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0,stop=10,length=200))\nfunction generate_data(sol,t)\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(sol,t) for i in 1:100]))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"here with t we measure the solution at 200 evenly spaced points. Thus aggregate_data is a 2x200x100 matrix where aggregate_data[i,j,k] is the ith component at time j of the kth dataset. What we first want to do is get a matrix of distributions where distributions[i,j] is the likelihood of component i at take j. We can do this via fit_mle on a chosen distributional form. For simplicity we choose the Normal distribution. aggregate_data[i,j,:] is the array of points at the given component and time, and thus we find the distribution parameters which fits best at each time point via:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Distributions\ndistributions = [fit_mle(Normal,aggregate_data[i,j,:]) for i in 1:2, j in 1:200]","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Notice for example that we have:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"julia> distributions[1,1]\nDistributions.Normal{Float64}(μ=1.0022440583676806, σ=0.009851964521952437)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"that is, it fit the distribution to have its mean just about where our original solution was and the variance is about how much noise we added to the dataset. This this is a good check to see that the distributions we are trying to fit our parameters to makes sense.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Note that in this case the Normal distribution was a good choice, and in many cases it's a nice go-to choice, but one should experiment with other choices of distributions as well. For example, a TDist can be an interesting way to incorporate robustness to outliers since low degrees of free T-distributions act like Normal distributions but with longer tails (though fit_mle does not work with a T-distribution, you can get the means/variances and build appropriate distribution objects yourself).","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Once we have the matrix of distributions, we can build the objective function corresponding to that distribution fit:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using DiffEqParamEstim\nobj = build_loss_objective(prob1,Tsit5(),LogLikeLoss(t,distributions),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"First let's use the objective function to plot the likelihood landscape:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Plots; plotly()\nprange = 0.5:0.1:5.0\nheatmap(prange,prange,[obj([j,i]) for i in prange, j in prange],\n        yscale=:log10,xlabel=\"Parameter 1\",ylabel=\"Parameter 2\",\n        title=\"Likelihood Landscape\")","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 2 Parameter Likelihood)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Recall that this is the negative loglikelihood and thus the minimum is the maximum of the likelihood. There is a clear valley where the first parameter is 1.5, while the second parameter's likelihood is more muddled. By taking a one-dimensional slice:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"plot(prange,[obj([1.5,i]) for i in prange],lw=3,\n     title=\"Parameter 2 Likelihood (Parameter 1 = 1.5)\",\n     xlabel = \"Parameter 2\", ylabel = \"Objective Function Value\")","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"we can see that there's still a clear minimum at the true value. Thus we will use the global optimizers from BlackBoxOptim.jl to find the values. We set our search range to be from 0.5 to 5.0 for both of the parameters and let it optimize:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using BlackBoxOptim\nbound1 = Tuple{Float64, Float64}[(0.5, 5),(0.5, 5)]\nresult = bboptimize(obj;SearchRange = bound1, MaxSteps = 11e3)\n\nStarting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},B\nlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound\n{BlackBoxOptim.RangePerDimSearchSpace}}\n0.00 secs, 0 evals, 0 steps\n0.50 secs, 1972 evals, 1865 steps, improv/step: 0.266 (last = 0.2665), fitness=-737.311433781\n1.00 secs, 3859 evals, 3753 steps, improv/step: 0.279 (last = 0.2913), fitness=-739.658421879\n1.50 secs, 5904 evals, 5799 steps, improv/step: 0.280 (last = 0.2830), fitness=-739.658433715\n2.00 secs, 7916 evals, 7811 steps, improv/step: 0.225 (last = 0.0646), fitness=-739.658433715\n2.50 secs, 9966 evals, 9861 steps, improv/step: 0.183 (last = 0.0220), fitness=-739.658433715\n\nOptimization stopped after 11001 steps and 2.7839999198913574 seconds\nTermination reason: Max number of steps (11000) reached\nSteps per second = 3951.50873439296\nFunction evals per second = 3989.2242527195904\nImprovements/step = 0.165\nTotal function evaluations = 11106\n\n\nBest candidate found: [1.50001, 1.00001]\n\nFitness: -739.658433715","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This shows that it found the true parameters as the best fit to the likelihood.","category":"page"},{"location":"highlevels/interfaces/#The-SciML-Interface-Libraries","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"","category":"section"},{"location":"highlevels/interfaces/#SciMLBase.jl:-The-SciML-Common-Interface","page":"The SciML Interface Libraries","title":"SciMLBase.jl: The SciML Common Interface","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"SciMLBase.jl defines the core interfaces of the SciML libraries, such as the definitions of abstract types like SciMLProblem, along with their instantiations like ODEProblem. While SciMLBase.jl is insufficient to solve any equations, it holds all of the equation definitions, and thus downstream libraries which wish to allow for using SciML solvers without depending on any solvers can directly depend on SciMLBase.jl.","category":"page"},{"location":"highlevels/interfaces/#SciMLOperators.jl:-The-AbstractSciMLOperator-Interface","page":"The SciML Interface Libraries","title":"SciMLOperators.jl: The AbstractSciMLOperator Interface","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"SciMLOperators.jl defines the interface for how matrix-free linear and affine operators are defined and used throughout the SciML ecosystem.","category":"page"},{"location":"highlevels/interfaces/#DiffEqNoiseProcess.jl:-The-SciML-Common-Noise-Interface","page":"The SciML Interface Libraries","title":"DiffEqNoiseProcess.jl: The SciML Common Noise Interface","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"DiffEqNoiseProcess.jl defines the common interface for stochastic noise processes used by the equation solvers of the SciML ecosystem.","category":"page"},{"location":"highlevels/interfaces/#CommonSolve.jl:-The-Common-Definition-of-Solve","page":"The SciML Interface Libraries","title":"CommonSolve.jl: The Common Definition of Solve","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"CommonSolve.jl is the library that defines the solve, solve!, and init interfaces which are used throughout all of the SciML equation solvers. It's defined as an extremely lightweight library so that other ecosystems can build off of the same solve definition without clashing with SciML when both export.","category":"page"},{"location":"highlevels/interfaces/#Static.jl:-A-Shared-Interface-for-Static-Compile-Time-Computation","page":"The SciML Interface Libraries","title":"Static.jl: A Shared Interface for Static Compile-Time Computation","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"Static.jl is a set of statically parameterized types for performing operations in a statically-defined (compiler-optimized) way with respect to values. ","category":"page"},{"location":"highlevels/interfaces/#DiffEqBase.jl:-A-Library-of-Shared-Components-for-Differential-Equation-Solvers","page":"The SciML Interface Libraries","title":"DiffEqBase.jl: A Library of Shared Components for Differential Equation Solvers","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"DiffEqBase.jl is the core shared component of the DifferentialEquations.jl ecosystem. It's not intended for non-developer users to interface directly with, instead it's used for the common functionality for uniformity of implementation between the solver libraries.","category":"page"},{"location":"highlevels/interfaces/#Third-Party-Libraries-to-Note","page":"The SciML Interface Libraries","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/interfaces/#ArrayInterface.jl:-Extensions-to-the-Julia-AbstractArray-Interface","page":"The SciML Interface Libraries","title":"ArrayInterface.jl: Extensions to the Julia AbstractArray Interface","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"ArrayInterface.jl are traits and functions which extend the Julia Base AbstractArray interface, giving a much larger set of queries to allow for writing high-performance generic code over all array types. For example, functions include can_change_size to know if an AbstractArray type is compatible with resize!, fast_scalar_indexing to know whether direct scalar indexing A[i] is optimized, and functions like findstructralnz to get the structural non-zeros of arbtirary sparse and structured matrices.","category":"page"},{"location":"highlevels/interfaces/#Adapt.jl:-Conversion-to-Allow-Chip-Generic-Programs","page":"The SciML Interface Libraries","title":"Adapt.jl: Conversion to Allow Chip-Generic Programs","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"Adapt.jl makes it possible to write code that is generic to the compute devices, i.e. code that works on both CPUs and GPUs. It defines the adapt function which acts like convert(T, x), but without the restriction of returning a T. This allows you to  \"convert\" wrapper types like Adjoint to be GPU compatible (for example) without throwing away the wrapper.","category":"page"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"Example usage:","category":"page"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"adapt(CuArray, ::Adjoint{Array})::Adjoint{CuArray}","category":"page"},{"location":"highlevels/interfaces/#AbstractFFTs.jl:-High-Level-Shared-Interface-for-Fast-Fourier-Transformation-Libraries","page":"The SciML Interface Libraries","title":"AbstractFFTs.jl: High Level Shared Interface for Fast Fourier Transformation Libraries","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"AbstractFFTs.jl defines the common interface for  Fast Fourier Transformations (FFTs) in Julia. Similar to SciMLBase.jl, AbstractFFTs.jl is not a solver library but instead a shared API which is extended by solver libraries such as FFTW.jl. Code written using AbstractFFTs.jl can be made compatible with FFT libraries without having an explicit dependency on a solver.","category":"page"},{"location":"highlevels/interfaces/#GPUArrays.jl:-Common-Interface-for-GPU-Based-Array-Types","page":"The SciML Interface Libraries","title":"GPUArrays.jl: Common Interface for GPU-Based Array Types","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"GPUArrays.jl defines the shared higher-level operations for GPU-based array types like CUDA.jl's CuArray and AMDGPU.jl's ROCmArray. Packages in SciML use the designation x isa AbstractGPUArray in order to find out if a user's operation is on the GPU and specialize computations.","category":"page"},{"location":"highlevels/interfaces/#RecipesBase.jl:-Standard-Plotting-Recipe-Interface","page":"The SciML Interface Libraries","title":"RecipesBase.jl: Standard Plotting Recipe Interface","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"RecipesBase.jl defines the common interface for plotting recipes, composable transformations of Julia data types into simpler data types for  visualization with libraries such as Plots.jl and Makie.jl. SciML libraries attempt to always include plot recipes wherever possible for ease of visualization.","category":"page"},{"location":"highlevels/interfaces/#Tables.jl:-Common-Interface-for-Tablular-Data-Types","page":"The SciML Interface Libraries","title":"Tables.jl: Common Interface for Tablular Data Types","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"Tables.jl is a common interface for defining tabular data structures, such as DataFrames.jl. SciML's libraries extend the Tables.jl interface to allow for automated conversions into data frame libraries without explicit dependence on any singular implementation.","category":"page"},{"location":"highlevels/interfaces/#EllipsisNotation.jl:-Implementation-of-Ellipsis-Array-Slicing","page":"The SciML Interface Libraries","title":"EllipsisNotation.jl: Implementation of Ellipsis Array Slicing","text":"","category":"section"},{"location":"highlevels/interfaces/","page":"The SciML Interface Libraries","title":"The SciML Interface Libraries","text":"EllipsisNotation.jl defines the ellipsis array slicing notation for Julia. It uses .. as a catch all for \"all dimensions\", allow for indexing like [..,1] to mean \"[:,:,:,1]` on four dimensional arrays, in a way that is generic to the number of dimensions in the underlying array.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#callback_library","page":"Callback Library","title":"Callback Library","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"DiffEqCallbacks.jl provides a library of various helpful callbacks which can be used with any component solver which implements the callback interface. It adds the following callbacks which are available to users of DifferentialEquations.jl.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Manifold-Conservation-and-Projection","page":"Callback Library","title":"Manifold Conservation and Projection","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In many cases, you may want to declare a manifold on which a solution lives. Mathematically, a manifold M is defined by a function g as the set of points where g(u)=0. An embedded manifold can be a lower dimensional object which constrains the solution. For example, g(u)=E(u)-C where E is the energy of the system in state u, meaning that the energy must be constant (energy preservation). Thus by defining the manifold the solution should live on, you can retain desired properties of the solution.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"It is a consequence of convergence proofs both in the deterministic and stochastic cases that post-step projection to manifolds keep the same convergence rate (stochastic requires a truncation in the proof, details details), thus any algorithm can be easily extended to conserve properties. If the solution is supposed to live on a specific manifold or conserve such property, this guarantees the conservation law without modifying the convergence properties.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"ManifoldProjection(g; nlsolve=NLSOLVEJL_SETUP(), save=true, autonomous=numargs(g)==2, nlopts=Dict{Symbol,Any}())","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"g: The residual function for the manifold. This is an inplace function of form g(resid, u) or g(resid, u, p, t) which writes to the residual resid the  difference from the manifold components. Here, it is assumed that resid is of the same shape as u.\nnlsolve: A nonlinear solver as defined in the nlsolve format.\nsave: Whether to do the save after the callback is applied. Standard saving is unchanged.\nautonomous: Whether g is an autonomous function of the form g(resid, u).\nnlopts: Optional arguments to nonlinear solver which can be any of the NLsolve keywords.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Example","page":"Callback Library","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Here we solve the harmonic oscillator:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"u0 = ones(2)\nfunction f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -u[1]\nend\nprob = ODEProblem(f,u0,(0.0,100.0))","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"However, this problem is supposed to conserve energy, and thus we define our manifold to conserve the sum of squares:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function g(resid,u,p,t)\n  resid[1] = u[2]^2 + u[1]^2 - 2\n  resid[2] = 0\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"To build the callback, we just call","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"cb = ManifoldProjection(g)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Using this callback, the Runge-Kutta method Vern7 conserves energy. Note that the standard saving occurs after the step and before the callback, and thus we set save_everystep=false to turn off all standard saving and let the callback save after the projection is applied.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"sol = solve(prob,Vern7(),save_everystep=false,callback=cb)\n@test sol[end][1]^2 + sol[end][2]^2 ≈ 2","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"(Image: manifold_projection)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Saveat-Warning","page":"Callback Library","title":"Saveat Warning","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Note that the ManifoldProjection callback modifies the endpoints of the integration intervals and thus breaks assumptions of internal interpolations. Because of this, the values for given by saveat will not be order-matching. However, the interpolation error can be proportional to the change by the projection, so if the projection is making small changes then one is still safe. However, if there are large changes from each projection, you should consider only saving at stopping/projection times. To do this, set tstops to the same values as saveat. There is a performance hit by doing so because now the integrator is forced to stop at every saving point, but this is guerenteed to match the order of the integrator even with the ManifoldProjection.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#AutoAbstol","page":"Callback Library","title":"AutoAbstol","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Many problem solving environments such as MATLAB provide a way to automatically adapt the absolute tolerance to the problem. This helps the solvers automatically \"learn\" what appropriate limits are. Via the callback interface, DiffEqCallbacks.jl implements a callback AutoAbstol which has the same behavior as the MATLAB implementation, that is the absolute tolerance starts and at each iteration it is set to the maximum value that the state has thus far reached times the relative tolerance. If init_curmax is zero, then the initial value is determined by the abstol of the solver. Otherwise this is the initial value for the current maximum abstol.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"To generate the callback, use the constructor:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"AutoAbstol(save=true;init_curmax=0.0)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#PositiveDomain","page":"Callback Library","title":"PositiveDomain","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Especially in biology and other natural sciences, a desired property of dynamical systems is the positive invariance of the positive cone, i.e. non-negativity of variables at time t_0 ensures their non-negativity at times t geq t_0 for which the solution is defined. However, even if a system satisfies this property mathematically it can be difficult for ODE solvers to ensure it numerically, as these MATLAB examples show.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In order to deal with this problem one can specify isoutofdomain=(u,p,t) -> any(x -> x < 0, u) as additional solver option, which will reject any step that leads to non-negative values and reduce the next time step. However, since this approach only rejects steps and hence calculations might be repeated multiple times until a step is accepted, it can be computationally expensive.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Another approach is taken by a PositiveDomain callback in DiffEqCallbacks.jl, which is inspired by Shampine's et al. paper about non-negative ODE solutions. It reduces the next step by a certain scale factor until the extrapolated value at the next time point is non-negative with a certain tolerance. Extrapolations are cheap to compute but might be inaccurate, so if a time step is changed it is additionally reduced by a safety factor of 0.9. Since extrapolated values are only non-negative up to a certain tolerance and in addition actual calculations might lead to negative values, also any negative values at the current time point are set to 0. Hence by this callback non-negative values at any time point are ensured in a computationally cheap way, but the quality of the solution depends on how accurately extrapolations approximate next time steps.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Please note that the system should be defined also outside the positive domain, since even with these approaches negative variables might occur during the calculations. Moreover, one should follow Shampine's et. al. advice and set the derivative x_i of a negative component x_i to max 0 f_i(x t), where t denotes the current time point with state vector x and f_i is the i-th component of function f in an ODE system x = f(x t).","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor-2","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PositiveDomain(u=nothing; save=true, abstol=nothing, scalefactor=nothing)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"u: A prototype of the state vector of the integrator. A copy of it is saved and extrapolated values are written to it. If it is not specified every application of the callback allocates a new copy of the state vector.\nsave: Whether to do the standard saving (applied after the callback).\nabstol: Tolerance up to which negative extrapolated values are accepted. Element-wise tolerances are allowed. If it is not specified every application of the callback uses the current absolute tolerances of the integrator.\nscalefactor: Factor by which an unaccepted time step is reduced. If it is not specified time steps are halved.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#GeneralDomain","page":"Callback Library","title":"GeneralDomain","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"A GeneralDomain callback in DiffEqCallbacks.jl generalizes the concept of a PositiveDomain callback to arbitrary domains. Domains are specified by in-place functions g(u, resid) or g(t, u, resid) that calculate residuals of a state vector u at time t relative to that domain. As for PositiveDomain, steps are accepted if residuals of the extrapolated values at the next time step are below a certain tolerance. Moreover, this callback is automatically coupled with a ManifoldProjection that keeps all calculated state vectors close to the desired domain, but in contrast to a PositiveDomain callback the nonlinear solver in a ManifoldProjection can not guarantee that all state vectors of the solution are actually inside the domain. Thus a PositiveDomain callback should in general be preferred.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor-3","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function GeneralDomain(g, u=nothing; nlsolve=NLSOLVEJL_SETUP(), save=true,\n                       abstol=nothing, scalefactor=nothing, autonomous=numargs(g)==2,\n                       nlopts=Dict(:ftol => 10*eps()))","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"g: The residual function for the domain. This is an inplace function of form g(resid, u, p, t) which writes to the residual the difference from the domain.\nu: A prototype of the state vector of the integrator and the residuals. Two copies of it are saved, and extrapolated values and residuals are written to them. If it is not specified every application of the callback allocates two new copies of the state vector.\nnlsolve: A nonlinear solver as defined in the nlsolve format which is passed to a ManifoldProjection.\nsave: Whether to do the standard saving (applied after the callback).\nabstol: Tolerance up to which residuals are accepted. Element-wise tolerances are allowed. If it is not specified every application of the callback uses the current absolute tolerances of the integrator.\nscalefactor: Factor by which an unaccepted time step is reduced. If it is not specified time steps are halved.\nautonomous: Whether g is an autonomous function of the form g(u, resid).\nnlopts: Optional arguments to nonlinear solver of a ManifoldProjection which can be any of the NLsolve keywords. The default value of ftol = 10*eps() ensures that convergence is only declared if the infinite norm of residuals is very small and hence the state vector is very close to the domain.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Stepsize-Limiters","page":"Callback Library","title":"Stepsize Limiters","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In many cases there is a known maximal stepsize for which the computation is stable and produces correct results. For example, in hyperbolic PDEs one normally needs to ensure that the stepsize stays below some Delta t_FE determined by the CFL condition. For nonlinear hyperbolic PDEs this limit can be a function dtFE(u,p,t) which changes throughout the computation. The stepsize limiter lets you pass a function which will adaptively limit the stepsizes to match these constraints.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor-4","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"StepsizeLimiter(dtFE;safety_factor=9//10,max_step=false,cached_dtcache=0.0)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"dtFE: The function for the maximal timestep, called as dtFE(u,p,t) using the previous values of u, p, and t.\nsafety_factor: The factor below the true maximum that will be stepped to which defaults to 9//10.\nmax_step: Makes every step equal to safety_factor*dtFE(u,p,t) when the solver is set to adaptive=false.\ncached_dtcache: Should be set to match the type for time when not using Float64 values.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#FunctionCallingCallback","page":"Callback Library","title":"FunctionCallingCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The function calling callback lets you define a function func(u,t,integrator) which gets calls at the time points of interest. The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"  FunctionCallingCallback(func;\n                 funcat=Vector{Float64}(),\n                 func_everystep=isempty(funcat),\n                 func_start = true,\n                 tdir=1)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"func(u, t, integrator) is the function to be called.\nfuncat values that the function is sure to be evaluated at.\nfunc_everystep whether to call the function after each integrator step.\nfunc_start whether the function is called the initial condition.\ntdir should be sign(tspan[end]-tspan[1]). It defaults to 1 and should   be adapted if tspan[1] > tspan[end].","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#saving_callback","page":"Callback Library","title":"SavingCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The saving callback lets you define a function save_func(u, t, integrator) which returns quantities of interest that shall be saved.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor-5","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"SavingCallback(save_func, saved_values::SavedValues;\n               saveat=Vector{eltype(saved_values.t)}(),\n               save_everystep=isempty(saveat),\n               tdir=1)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"save_func(u, t, integrator) returns the quantities which shall be saved. Note that this should allocate the output (not as a view to u).\nsaved_values::SavedValues is the types that save_func will return, i.e. save_func(u, t, integrator)::savevalType. It's specified via SavedValues(typeof(t),savevalType), i.e. give the type for time and the type that save_func will output (or higher compatible type).\nsaveat mimicks saveat in solve from solve.\nsave_everystep mimicks save_everystep from solve.\nsave_start mimicks save_start from solve.\ntdir should be sign(tspan[end]-tspan[1]). It defaults to 1 and should be adapted if tspan[1] > tspan[end].","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The outputted values are saved into saved_values. Time points are found via saved_values.t and the values are saved_values.saveval.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Example-2","page":"Callback Library","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In this example we will solve a matrix equation and at each step save a tuple of values which contains the current trace and the norm of the matrix. We build the SavedValues cache to use Float64 for time and Tuple{Float64,Float64} for the saved values, and then call the solver with the callback.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"using DiffEqCallbacks, OrdinaryDiffEq, LinearAlgebra\nprob = ODEProblem((du,u,p,t) -> du .= u, rand(4,4), (0.0,1.0))\nsaved_values = SavedValues(Float64, Tuple{Float64,Float64})\ncb = SavingCallback((u,t,integrator)->(tr(u),norm(u)), saved_values)\nsol = solve(prob, Tsit5(), callback=cb)\n\nprint(saved_values.saveval)\n#=\nTuple{Float64,Float64}[(2.23186, 2.49102), (2.46675, 2.75318), (3.16138, 3.52847), (4.42011, 4.93337), (6.06683, 6.77129)]\n=#","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Note that the values are retrieved from the cache as .saveval, and the time points are found as .t. If we want to control the saved times, we use saveat in the callback. The save controls like saveat act analogously to how they act in the solve function.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"saved_values = SavedValues(Float64, Tuple{Float64,Float64})\ncb = SavingCallback((u,t,integrator)->(tr(u),norm(u)), saved_values, saveat=0.0:0.1:1.0)\nsol = solve(prob, Tsit5(), callback=cb)\nprint(saved_values.saveval)\nprint(saved_values.t)\n\n#=\nTuple{Float64,Float64}[(2.23186, 2.49102), (2.46659, 2.753), (2.726, 3.04254), (3.0127, 3.36253),\n(3.32955, 3.71617), (3.67972, 4.107), (4.06672, 4.53893), (4.49442, 5.0163), (4.9671, 5.54387),\n(5.48949, 6.12692), (6.06683, 6.77129)]\n[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n=#","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#PresetTimeCallback","page":"Callback Library","title":"PresetTimeCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PresetTimeCallback is a callback that adds callback affect! calls at preset times. No playing around with tstops or anything is required: this callback adds the triggers for you to make it automatic.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PresetTimeCallback(tstops,user_affect!;\n                            initialize = DiffEqBase.INITIALIZE_DEFAULT,\n                            filter_tstops = true,\n                            kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"tstops: the times for the affect! to trigger at.\nuser_affect!: an affect!(integrator) function to use at the time points.\nfilter_tstops: Whether to filter out tstops beyond the end of the integration timespan. Defaults to true. If false, then tstops can extend the interval of integration.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#IterativeCallback","page":"Callback Library","title":"IterativeCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"IterativeCallback is a callback to be used to iteratively apply some effect. For example, if given the first effect at t₁, you can define t₂ to apply the next effect.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"A IterativeCallback is constructed as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function IterativeCallback(time_choice, user_affect!,tType = Float64;\n                           initial_affect = false, kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where time_choice(integrator) determines the time of the next callback and user_affect! is the effect applied to the integrator at the stopping points. If nothing is returned for the time choice then the iterator ends. initial_affect is whether to apply the affect at t=0 which defaults to false. kwargs are keyword arguments accepted by the DiscreteCallback constructor.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#PeriodicCallback","page":"Callback Library","title":"PeriodicCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PeriodicCallback can be used when a function should be called periodically in terms of integration time (as opposed to wall time), i.e. at t = tspan[1], t = tspan[1] + Δt, t = tspan[1] + 2Δt, and so on. This callback can, for example, be used to model a digital controller for an analog system, running at a fixed rate.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#Constructor-6","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PeriodicCallback(f, Δt::Number; initial_affect = false, kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where f is the function to be called periodically, Δt is the period, initial_affect is whether to apply the affect at t=0 which defaults to false, and kwargs are keyword arguments accepted by the DiscreteCallback constructor (see the DiscreteCallback section).","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/#TerminateSteadyState","page":"Callback Library","title":"TerminateSteadyState","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"TerminateSteadyState can be used to solve the problem for the steady-state by running the solver until the derivatives of the problem converge to 0 or tspan[2] is reached. This is an alternative approach to root finding (see the Steady State Solvers section). The constructor of this callback is:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"TerminateSteadyState(abstol = 1e-8, reltol = 1e-6, test = allDerivPass)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where abstol and reltol are the absolute and relative tolerance, respectively. These tolerances may be specified as scalars or as arrays of the same length as the states of the problem. test represents the function that evaluates the condition for termination. The default condition is that all derivatives should become smaller than abstol and the states times reltol. The user can pass any other function to implement a different termination condition. Such function should take four arguments: integrator (see Integrator Interface for details), abstol and reltol.","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/#ode_prob","page":"ODE Problems","title":"ODE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"ODEProblem\nODEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/#SciMLBase.ODEProblem","page":"ODE Problems","title":"SciMLBase.ODEProblem","text":"Defines an ordinary differential equation (ODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/ode_types/\n\nMathematical Specification of an ODE Problem\n\nTo define an ODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nM fracdudt = f(upt)\n\nThere are two different ways of specifying f:\n\nf(du,u,p,t): in-place. Memory-efficient when avoiding allocations. Best option for most cases unless mutation is not allowed. \nf(u,p,t): returning du. Less memory-efficient way, particularly suitable when mutation is not allowed (e.g. with certain automatic differentiation packages such as Zygote).\n\nu₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFor the mass matrix M, see the documentation of ODEFunction.\n\nProblem Type\n\nConstructors\n\nODEProblem can be constructed by first building an ODEFunction or by simply passing the ODE right-hand side to the constructor. The constructors are:\n\nODEProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...)\nODEProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the ODEFunction documentation.\n\nFields\n\nf: The function in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters.\nkwargs: The keyword arguments passed onto the solves.\n\nExample Problems\n\nExample problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_ode_linear, you can do something like:\n\n#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/ode_types/#SciMLBase.ODEFunction","page":"ODE Problems","title":"SciMLBase.ODEFunction","text":"ODEFunction{iip,F,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,S2,O,TCV} <: AbstractODEFunction{iip}\n\nA representation of an ODE function f, defined by:\n\nM fracdudt = f(upt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nODEFunction{iip,recompile}(f;\n                           mass_matrix=I,\n                           analytic=nothing,\n                           tgrad=nothing,\n                           jac=nothing,\n                           jvp=nothing,\n                           vjp=nothing,\n                           jac_prototype=nothing,\n                           sparsity=jac_prototype,\n                           paramjac = nothing,\n                           syms = nothing,\n                           indepsym = nothing,\n                           colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\niip is the the optional boolean for determining whether a given function is written to be used in-place or out-of-place. In-place functions are f!(du,u,p,t) where the return is ignored and the result is expected to be mutated into the value of du. Out-of-place functions are du=f(u,p,t).\n\nNormally this is determined automatically by looking at the method table for f and seeing the maximum number of arguments in available dispatches. For this reason, the constructor ODEFunction(f) generally works (but is type-unstable). However, for type-stability or to enforce correctness, this option is passed via ODEFunction{true}(f).\n\nrecompile: Controlling Compilation and Specialization\n\nThe recompile parameter controls whether the ODEFunction will fully specialize on the typeof(f). This causes recompilation of the solver for each new f function, but gives the maximum compiler information and runtime speed. By default recompile = true. If recompile = false, the ODEFunction uses Any type parameters for each of the functions, allowing for the reuse of compilation caches but adding a dynamic dispatch at the f call sites, potentially leading to runtime regressions.\n\nOverriding the true default is done by passing a second type parameter after iip, for example ODEFunction{true,false}(f) is an in-place function with no recompilation specialization.\n\nFields\n\nThe fields of the ODEFunction type directly match the names of the inputs.\n\nMore Details on Jacobians\n\nThe following example creates an inplace ODEFunction whose jacobian is a Diagonal:\n\nusing LinearAlgebra\nf = (du,u,p,t) -> du .= t .* u\njac = (J,u,p,t) -> (J[1,1] = t; J[2,2] = t; J)\njp = Diagonal(zeros(2))\nfun = ODEFunction(f; jac=jac, jac_prototype=jp)\n\nNote that the integrators will always make a deep copy of fun.jac_prototype, so there's no worry of aliasing.\n\nIn general the jacobian prototype can be anything that has mul! defined, in particular sparse matrices or custom lazy types that support mul!. A special case is when the jac_prototype is a AbstractDiffEqLinearOperator, in which case you do not need to supply jac as it is automatically set to update_coefficients!. Refer to the AbstractSciMLOperators documentation for more information on setting up time/parameter dependent operators.\n\nExamples\n\nDeclaring Explicit Jacobians for ODEs\n\nThe most standard case, declaring a function for a Jacobian is done by overloading the function f(du,u,p,t) with an in-place updating function for the Jacobian: f_jac(J,u,p,t) where the value type is used for dispatch. For example, take the LotkaVolterra model:\n\nfunction f(du,u,p,t)\n  du[1] = 2.0 * u[1] - 1.2 * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\n\nTo declare the Jacobian we simply add the dispatch:\n\nfunction f_jac(J,u,p,t)\n  J[1,1] = 2.0 - 1.2 * u[2]\n  J[1,2] = -1.2 * u[1]\n  J[2,1] = 1 * u[2]\n  J[2,2] = -3 + u[1]\n  nothing\nend\n\nThen we can supply the Jacobian with our ODE as:\n\nff = ODEFunction(f;jac=f_jac)\n\nand use this in an ODEProblem:\n\nprob = ODEProblem(ff,ones(2),(0.0,10.0))\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/ode_types/#Solution-Type","page":"ODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"ODESolution","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/#SciMLBase.ODESolution","page":"ODE Problems","title":"SciMLBase.ODESolution","text":"struct ODESolution{T, N, uType, uType2, DType, tType, rateType, P, A, IType, DE} <: SciMLBase.AbstractODESolution{T, N, uType}\n\nRepresentation of the solution to an ordinary differential equation defined by an ODEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://diffeq.sciml.ai/stable/basics/solution/\n\nFields\n\nu: the representation of the ODE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation.\nt: the time points corresponding to the saved values of the ODE solution.\nprob: the original ODEProblem that was solved.\nalg: the algorithm type used by the solver.\ndestats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully (sol.retcode === :Success), whether it terminated due to a user-defined callback (sol.retcode === :Terminated), or whether it exited due to an error. For more details, see the return code section of the DifferentialEquations.jl documentation.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/ode_types/#Example-Problems","page":"ODE Problems","title":"Example Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"To use a sample problem, such as prob_ode_linear, you can do something like:","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"CurrentModule = ODEProblemLibrary","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"prob_ode_linear\nprob_ode_2Dlinear\nprob_ode_bigfloatlinear\nprob_ode_bigfloat2Dlinear\nprob_ode_large2Dlinear\nprob_ode_2Dlinear_notinplace\nprob_ode_lotkavoltera\nprob_ode_fitzhughnagumo\nprob_ode_threebody\nprob_ode_pleiades\nprob_ode_vanderpol\nprob_ode_vanderpol_stiff\nprob_ode_rober\nprob_ode_rigidbody\nprob_ode_hires\nprob_ode_orego\nprob_ode_pollution\nprob_ode_nonlinchem\nprob_ode_brusselator_1d\nprob_ode_brusselator_2d\nprob_ode_filament\nprob_ode_thomas\nprob_ode_lorenz\nprob_ode_aizawa\nprob_ode_dadras\nprob_ode_chen\nprob_ode_rossler\nprob_ode_rabinovich_fabrikant\nprob_ode_sprott\nprob_ode_hindmarsh_rose","category":"page"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_linear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_linear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s. The parameter is α\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloatlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloatlinear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloat2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloat2Dlinear","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_large2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_large2Dlinear","text":"100x100 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear_notinplace","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear_notinplace","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\non Float64. Purposefully not in-place as a test.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lotkavoltera","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lotkavoltera","text":"Lotka-Voltera Equations (Non-stiff)\n\nfracdxdt = ax - bxy\n\nfracdydt = -cy + dxy\n\nwith initial condition x=y=1\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_fitzhughnagumo","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_fitzhughnagumo","text":"Fitzhugh-Nagumo (Non-stiff)\n\nfracdvdt = v - fracv^33 - w + I_est\n\nτ fracdwdt = v + a -bw\n\nwith initial condition v=w=1\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_threebody","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_threebody","text":"The ThreeBody problem as written by Hairer: (Non-stiff)\n\nfracdy₁dt = y₁ + 2fracdy₂dt - barμfracy₁+μD₁ - μfracy₁-barμD₂\n\nfracdy₂dt = y₂ - 2fracdy₁dt - barμfracy₂D₁ - μfracy₂D₂\n\nD₁ = ((y₁+μ)^2 + y₂^2)^32\n\nD₂ = ((y₁-barμ)^2+y₂^2)^32\n\nμ = 0012277471\n\nbarμ =1-μ\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on t₀ = 00 and T = 170652165601579625588917206249 Periodic with that setup.\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pleiades","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pleiades","text":"Pleiades Problem (Non-stiff)\n\nfracd^2xᵢdt^2 = sum_ji mⱼ(xⱼ-xᵢ)rᵢⱼ\n\nfracd^2yᵢdt^2 = sum_ji mⱼ(yⱼ-yᵢ)rᵢⱼ\n\nwhere\n\nrᵢⱼ = ((xᵢ-xⱼ)^2 + (yᵢ-yⱼ)^2)^32\n\nand initial conditions are\n\nx₁(0) = 3\n\nx₂(0) = 3\n\nx₃(0) = -1\n\nx₄(0) = -3\n\nx₅(0) = 2\n\nx₆(0) = -2\n\nx₇(0) = 2\n\ny₁(0) = 3\n\ny₂(0) = -3\n\ny₃(0) = 2\n\ny₄(0) = 0\n\ny₅(0) = 0\n\ny₆(0) = -4\n\ny₇(0) = 4\n\nand with fracdxᵢ(0)dt=fracdyᵢ(0)dt=0 except for\n\nfracdx₆(0)dt = 175\n\nfracdx₇(0)dt = -15\n\nfracdy₄(0)dt = -125\n\nfracdy₅(0)dt = 1\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 3.\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol","text":"Van der Pol Equations\n\nfracdxdt = y\n\nfracdydt = μ((1-x^2)y -x)\n\nwith μ=10 and u_0=0sqrt3\n\nNon-stiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol_stiff","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol_stiff","text":"Van der Pol Equations\n\nfracdxdt = y\n\nfracdydt = μ((1-x^2)y -x)\n\nwith μ=10^6 and u_0=0sqrt3\n\nStiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rober","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rober","text":"The Robertson biochemical reactions: (Stiff)\n\nfracdy₁dt = -k₁y₁+k₃y₂y₃\n\nfracdy₂dt =  k₁y₁-k₂y₂^2-k₃y₂y₃\n\nfracdy₃dt =  k₂y₂^2\n\nwhere k₁=004, k₂=3times10^7, k₃=10^4. For details, see:\n\nHairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on 01e11\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rigidbody","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rigidbody","text":"Rigid Body Equations (Non-stiff)\n\nfracdy₁dt  = I₁y₂y₃\n\nfracdy₂dt  = I₂y₁y₃\n\nfracdy₃dt  = I₃y₁y₂\n\nwith I₁=-2, I₂=125, and I₃=-12.\n\nThe initial condition is y=100009.\n\nFrom Solving Differential Equations in R by Karline Soetaert\n\nor Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 20.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hires","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hires","text":"Hires Problem (Stiff)\n\nIt is in the form of \n\nfracdydt = f(y)\n\nwith\n\n y(0)=y_0 quad y in ℝ^8 quad 0  t  3218122\n\nwhere f is defined by\n\nf(y) = beginpmatrix 171y_1  +043y_2  +832y_3  +00007y_4   171y_1  875y_2     1003y_3  +043y_4  +0035y_5    832y_2  +171y_3  112y_4    1745y_5  +043y_6  +043y_7    280y_6y_8  +069y_4  +171y_5  043y_6  +069y_7  280y_6y_8  181y_7     280y_6y_8  +181y_7    endpmatrix\n\nReference: demohires.pdf   Notebook: Hires.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_orego","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_orego","text":"Orego Problem (Stiff)\n\nIt is in the form of fracdydt=f(y) quad y(0)=y0 with\n\ny in ℝ^3 quad 0  t  360\n\nwhere f is defined by\n\nf(y) = beginpmatrix s(y_2 - y_1(1-qy_1-y_2))  (y_3 - y_2(1+y_1))s  w(y_1-y_3) endpmatrix\n\nwhere s=7727, w=0161 and q=837510^-6.\n\nReference: demoorego.pdf Notebook: Orego.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pollution","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pollution","text":"Pollution Problem (Stiff)\n\nThis IVP is a stiff system of 20 non-linear Ordinary Differential Equations. It is in the form of \n\nfracdydt=f(y)\n\nwith\n\ny(0)=y0 quad y in ℝ^20 quad 0  t  60\n\nwhere f is defined by\n\nf(y) = beginpmatrix -sum_j110142324 r_j + sum_j23911122225 r_j  -r_2 - r_3 - r_9 - r_12 + r_1 + r_21  -r_15 + r_1 + r_17 + r_19 + r_22  -r_2 - r_16 - r_17 - r_23 + r_15  -r_3 + 2r_4 + r_6 + r_7 + r_13 + r_20  -r_6 - r_8 - r_14 - r_20 + r_3 + 2r_18  -r_4 - r_5 - r_6 + r_13  r_4 + r_5 + r_6 + r_7  -r_7 - r_8  -r_12 + r_7 + r_9  -r_9 - r_10 + r_8 + r_11  r_9  -r_11 + r_10  -r_13 + r_12  r_14  -r_18 - r_19 + r_16  -r_20  r_20  -r21 - r_22 - r_24 + r_23 + r_25  -r_25 + r_24 endpmatrix\n\nwith the initial condition of\n\ny0 = (0 02 0 004 0 0 01 03 001 0 0 0 0 0 0 0 0007 0 0 0)^T\n\nAnalytical Jacobian is included.\n\nReference: pollu.pdf Notebook: Pollution.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_nonlinchem","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_nonlinchem","text":"Nonlinear system of reactions with an analytical solution\n\nfracdy_1dt = -y_1\n\nfracdy_2dt = y_1 - y_2^2\n\nfracdy_3dt = y_2^2\n\nwith initial condition y=100 on a time span of t in (020)\n\nFrom\n\nLiu, L. C., Tian, B., Xue, Y. S., Wang, M., & Liu, W. J. (2012). Analytic solution  for a nonlinear chemistry system of ordinary differential equations. Nonlinear  Dynamics, 68(1-2), 17-21.\n\nThe analytical solution is implemented, allowing easy testing of ODE solvers.\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_1d","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_1d","text":"1D Brusselator\n\nfracpartial upartial t = A + u^2v - (B+1)u + alphafracpartial^2 upartial x^2\n\nfracpartial vpartial t = Bu - u^2v + alphafracpartial^2 upartial x^2\n\nand the initial conditions are\n\nu(x0) = 1+sin(2π x)\n\nv(x0) = 3\n\nwith the boundary condition\n\nu(0t) = u(1t) = 1\n\nv(0t) = v(1t) = 3\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 6\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_2d","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_2d","text":"2D Brusselator\n\nfracpartial upartial t = 1 + u^2v - 44u + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2) + f(x y t)\n\nfracpartial vpartial t = 34u - u^2v + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2)\n\nwhere\n\nf(x y t) = begincases 5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11  0  quad textelse endcases\n\nand the initial conditions are\n\nu(x y 0) = 22cdot y(1-y)^32\n\nv(x y 0) = 27cdot x(1-x)^32\n\nwith the periodic boundary condition\n\nu(x+1yt) = u(xyt)\n\nu(xy+1t) = u(xyt)\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 152\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_filament","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_filament","text":"Filament PDE Discretization\n\nNotebook: Filament.ipynb\n\nIn this problem is a real-world biological model from a paper entitled Magnetic dipole with a flexible tail as a self-propelling microdevice. It is a system of PDEs representing a Kirchhoff model of an elastic rod, where the equations of motion are given by the Rouse approximation with free boundary conditions.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_thomas","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_thomas","text":"Thomas' cyclically symmetric attractor equations\n\nbeginalign\nfracdx(t)dt =  - b xleft( t right) + sinleft( yleft( t right) right) \nfracdy(t)dt =  - b yleft( t right) + sinleft( zleft( t right) right) \nfracdz(t)dt =  - b zleft( t right) + sinleft( xleft( t right) right)\nendalign\n\n\nReference\n\nWikipedia\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lorenz","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lorenz","text":"Lorenz equations\n\nbeginalign\nfracdx(t)dt = sigma left(  - xleft( t right) + yleft( t right) right) \nfracdy(t)dt =  - yleft( t right) + left( rho - zleft( t right) right) xleft( t right) \nfracdz(t)dt = xleft( t right) yleft( t right) - beta zleft( t right)\nendalign\n\n\nReference\n\nWikipedia\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_aizawa","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_aizawa","text":"Aizawa equations\n\nbeginalign\nfracdx(t)dt = left(  - b + zleft( t right) right) xleft( t right) - d yleft( t right) \nfracdy(t)dt = d xleft( t right) + left(  - b + zleft( t right) right) yleft( t right) \nfracdz(t)dt = c - frac13 left( zleft( t right) right)^3 + a zleft( t right) - left( 1 + e zleft( t right) right) left( left( xleft( t right) right)^2 + left( yleft( t right) right)^2 right) + left( xleft( t right) right)^3 f zleft( t right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_dadras","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_dadras","text":"Dadras equations\n\nbeginalign\nfracdx(t)dt =  - a xleft( t right) + b yleft( t right) zleft( t right) + yleft( t right) \nfracdy(t)dt = c yleft( t right) - xleft( t right) zleft( t right) + zleft( t right) \nfracdz(t)dt = d xleft( t right) yleft( t right) - e zleft( t right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_chen","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_chen","text":"chen equations\n\nbeginalign\nfracdx(t)dt = a left(  - xleft( t right) + yleft( t right) right) \nfracdy(t)dt = c yleft( t right) + left( c - a right) xleft( t right) - xleft( t right) zleft( t right) \nfracdz(t)dt = xleft( t right) yleft( t right) - b zleft( t right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rossler","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rossler","text":"rossler equations\n\nbeginalign\nfracdx(t)dt =  - yleft( t right) - zleft( t right) \nfracdy(t)dt = a yleft( t right) + xleft( t right) \nfracdz(t)dt = b + left(  - c + xleft( t right) right) zleft( t right)\nendalign\n\n\nReference Wikipedia\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rabinovich_fabrikant","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rabinovich_fabrikant","text":"rabinovich_fabrikant equations\n\nbeginalign\nfracdx(t)dt = b xleft( t right) + left( -1 + left( xleft( t right) right)^2 + zleft( t right) right) yleft( t right) \nfracdy(t)dt = left( 1 - left( xleft( t right) right)^2 + 3 zleft( t right) right) xleft( t right) + b yleft( t right) \nfracdz(t)dt =  - 2 left( a + xleft( t right) yleft( t right) right) zleft( t right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_sprott","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_sprott","text":"sprott equations\n\nbeginalign\nfracdx(t)dt = xleft( t right) zleft( t right) + a xleft( t right) yleft( t right) + yleft( t right) \nfracdy(t)dt = 1 + yleft( t right) zleft( t right) - left( xleft( t right) right)^2 b \nfracdz(t)dt =  - left( xleft( t right) right)^2 - left( yleft( t right) right)^2 + xleft( t right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hindmarsh_rose","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hindmarsh_rose","text":"hindmarsh_rose equations\n\nbeginalign\nfracdx(t)dt = i - zleft( t right) + left( xleft( t right) right)^2 b - left( xleft( t right) right)^3 a + yleft( t right) \nfracdy(t)dt = c - yleft( t right) - left( xleft( t right) right)^2 d \nfracdz(t)dt = r left( s left(  - xr + xleft( t right) right) - zleft( t right) right)\nendalign\n\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"modules/NeuralPDE/tutorials/ode/#Solving-ODEs-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for ODEs","title":"Solving ODEs with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"note: Note\nIt is highly recommended you first read the solving ordinary differential equations with DifferentialEquations.jl tutorial before reading this tutorials.","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"This tutorial is an introduction to using physics-informed neural networks (PINNs) for solving ordinary differential equations (ODEs). In contrast to the later parts of this documentation which use the symbolic interface, here we will focus on the simplified NNODE which uses the ODEProblem specification for the ODE. Mathematically the ODEProblem defines a problem:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = f(upt)","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (t_0t_f) with an initial condition u(t_0) = u_0. With physics-informed neural networks, we choose a neural network architecture NN to represent the solution u and seek to find parameters p such that NN' = f(NN,p,t) for all points in the domain. When this is satisfied sufficiently closely, then NN is thus a solution to the differential equation.","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/#Solving-an-ODE-with-NNODE","page":"Introduction to NeuralPDE for ODEs","title":"Solving an ODE with NNODE","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Let's solve the simple ODE:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = cos(2pi t)","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (01) and u_0 = 0 with NNODE. First we define the ODEProblem as we would with any other DifferentialEquations.jl solver. This looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using NeuralPDE, Flux, OptimizationOptimisers\n\nlinear(u, p, t) = cos(2pi * t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0, tspan)","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now, to define the NNODE solver, we must choose a neural network architecture. To do this, we will use the Flux.jl library to define a multilayer perceptron (MLP) with one hidden layer of 5 nodes and a sigmoid activation function. This looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"chain = Flux.Chain(Dense(1, 5, σ), Dense(5, 1))","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now we must choose an optimizer to define the NNODE solver. A common choice is ADAM, with a tunable rate parameter which we will set to 0.1. In general, this rate parameter should be decreased if the solver's loss tends to be unsteady (sometimes rise \"too much\"), but should be as large as possible for efficnecy. Thus the definition of the NNODE solver is as follows:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"opt = OptimizationOptimisers.Adam(0.1)\nalg = NeuralPDE.NNODE(chain, opt)","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Once these pieces are together, we call solve just like with any other ODEProblem solver. Let's turn on verbose so we can see the loss over time during the training process:","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"sol = solve(prob, NeuralPDE.NNODE(chain, opt), verbose=true, abstol=1f-6, maxiters=200)","category":"page"},{"location":"modules/NeuralPDE/tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"And that's it: the neural network solution was computed by training the neural network and returned in the standard DifferentialEquations.jl ODESolution format. For more information on handling the solution, consult the DifferentialEquations.jl solution handling section","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/#Smoluchowski-Coagulation-Equation","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"This tutorial shows how to programmatically construct a ReactionSystem corresponding to the chemistry underlying the Smoluchowski coagulation model using ModelingToolkit/Catalyst. A jump process version of the model is then constructed from the ReactionSystem, and compared to the model's analytical solution obtained by the method of Scott (see also 3).","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"The Smoluchowski coagulation equation describes a system of reactions in which monomers may collide to form dimers, monomers and dimers may collide to form trimers, and so on. This models a variety of chemical/physical processes, including polymerization and flocculation.","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"We begin by importing some necessary packages.","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"using ModelingToolkit, Catalyst, LinearAlgebra\nusing DiffEqBase, JumpProcesses\nusing Plots, SpecialFunctions","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"Suppose the maximum cluster size is N. We assume an initial concentration of monomers, Nₒ, and let uₒ denote the initial number of monomers in the system. We have nr total reactions, and label by V the bulk volume of the system (which plays an important role in the calculation of rate laws since we have bimolecular reactions). Our basic parameters are then","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"## Parameter\nN = 10                       # maximum cluster size\nVₒ = (4π/3)*(10e-06*100)^3   # volume of a monomers in cm³\nNₒ = 1e-06/Vₒ                # initial conc. = (No. of init. monomers) / bulk volume\nuₒ = 10000                   # No. of monomers initially\nV = uₒ/Nₒ                    # Bulk volume of system in cm³\n\ninteg(x) = Int(floor(x))\nn        = integ(N/2)\nnr       = N%2 == 0 ? (n*(n + 1) - n) : (n*(n + 1)) # No. of forward reactions","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"The Smoluchowski coagulation equation Wikipedia page illustrates the set of possible reactions that can occur. We can easily enumerate the pairs of multimer reactants that can combine when allowing a maximal cluster size of N monomers. We initialize the volumes of the reactant multimers as volᵢ and volⱼ","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# possible pairs of reactant multimers\npair = []\nfor i = 2:N\n    push!(pair,[1:integ(i/2)  i .- (1:integ(i/2))])\nend\npair = vcat(pair...)\nvᵢ = @view pair[:,1]   # Reactant 1 indices\nvⱼ = @view pair[:,2]   # Reactant 2 indices\nvolᵢ = Vₒ*vᵢ           # cm⁻³\nvolⱼ = Vₒ*vⱼ           # cm⁻³\nsum_vᵢvⱼ = @. vᵢ + vⱼ  # Product index","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"We next specify the rates (i.e. kernel) at which reactants collide to form products. For simplicity, we allow a user-selected additive kernel or constant kernel. The constants(B and C) are adopted from Scott's paper 2","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# set i to  1 for additive kernel, 2  for constant\ni = 1\nif i==1\n    B = 1.53e03                # s⁻¹\n    kv = @. B*(volᵢ + volⱼ)/V  # dividing by volume as its a bi-molecular reaction chain\nelseif i==2\n    C = 1.84e-04               # cm³ s⁻¹\n    kv = fill(C/V, nr)\nend","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"We'll store the reaction rates in pars as Pairs, and set the initial condition that only monomers are present at t=0 in u₀map.","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# state variables are X, pars stores rate parameters for each rx\n@parameters t\n@variables k[1:nr]  X[1:N](t)\npars = Pair.(collect(k), kv)\n\n# time-span\nif i == 1\n    tspan = (0. ,2000.)\nelseif i == 2\n    tspan = (0. ,350.)\nend\n\n # initial condition of monomers\nu₀    = zeros(Int64, N)\nu₀[1] = uₒ\nu₀map = Pair.(collect(X), u₀)   # map variable to its initial value","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"Here we generate the reactions programmatically. We systematically create Catalyst Reactions for each possible reaction shown in the figure on Wikipedia. When vᵢ[n] == vⱼ[n], we set the stoichiometric coefficient of the reactant multimer to two.","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# vector to store the Reactions in\nrx = []\nfor n = 1:nr\n    # for clusters of the same size, double the rate\n    if (vᵢ[n] == vⱼ[n])\n        push!(rx, Reaction(k[n], [X[vᵢ[n]]], [X[sum_vᵢvⱼ[n]]], [2], [1]))\n    else\n        push!(rx, Reaction(k[n], [X[vᵢ[n]], X[vⱼ[n]]], [X[sum_vᵢvⱼ[n]]],\n                           [1, 1], [1]))\n    end\nend\n@named rs = ReactionSystem(rx, t, collect(X), collect(k))","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"We now convert the ReactionSystem into a ModelingToolkit.JumpSystem, and solve it using Gillespie's direct method. For details on other possible solvers (SSAs), see the DifferentialEquations.jl documentation","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# solving the system\njumpsys = convert(JumpSystem, rs)\ndprob   = DiscreteProblem(jumpsys, u₀map, tspan, pars)\njprob   = JumpProblem(jumpsys, dprob, Direct(), save_positions=(false,false))\njsol    = solve(jprob, SSAStepper(), saveat = tspan[2]/30)","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"Lets check the results for the first three polymers/cluster sizes. We compare to the analytical solution for this system:","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"# Results for first three polymers...i.e. monomers, dimers and trimers\nv_res = [1;2;3]\n\n# comparison with analytical solution\n# we only plot the stochastic solution at a small number of points\n# to ease distinguishing it from the exact solution\nt   = jsol.t\nsol = zeros(length(v_res), length(t))\nif i == 1\n    ϕ = @. 1 - exp(-B*Nₒ*Vₒ*t)\n    for j in v_res\n        sol[j,:] = @. Nₒ*(1 - ϕ)*(((j*ϕ)^(j-1))/gamma(j+1))*exp(-j*ϕ)\n    end\nelseif i == 2\n    ϕ = @. (C*Nₒ*t)\n    for j in v_res\n        sol[j,:] = @. 4Nₒ*((ϕ^(j-1))/((ϕ + 2)^(j+1)))\n    end\nend\n\n# plotting normalised concentration vs analytical solution\ndefault(lw=2, xlabel=\"Time (sec)\")\nscatter(ϕ, jsol(t)[1,:]/uₒ, label=\"X1 (monomers)\", markercolor=:blue)\nplot!(ϕ, sol[1,:]/Nₒ, line = (:dot,4,:blue), label=\"Analytical sol--X1\")\n\nscatter!(ϕ, jsol(t)[2,:]/uₒ, label=\"X2 (dimers)\", markercolor=:orange)\nplot!(ϕ, sol[2,:]/Nₒ, line = (:dot,4,:orange), label=\"Analytical sol--X2\")\n\nscatter!(ϕ, jsol(t)[3,:]/uₒ, label=\"X3 (trimers)\", markercolor=:purple)\nplot!(ϕ, sol[3,:]/Nₒ, line = (:dot,4,:purple), label=\"Analytical sol--X3\",\n      ylabel = \"Normalized Concentration\")","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"For the additive kernel we find","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"(Image: additive_kernel)","category":"page"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/#Sources","page":"Smoluchowski Coagulation Equation","title":"Sources","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/generating_reactions_programmatically/","page":"Smoluchowski Coagulation Equation","title":"Smoluchowski Coagulation Equation","text":"https://en.wikipedia.org/wiki/Smoluchowskicoagulationequation\nScott, W. T. (1968). Analytic Studies of Cloud Droplet Coalescence I, Journal of Atmospheric Sciences, 25(1), 54-65. Retrieved Feb 18, 2021, from https://journals.ametsoc.org/view/journals/atsc/25/1/1520-046919680250054asocdc20co2.xml\nIan J. Laurenzi, John D. Bartels, Scott L. Diamond, A General Algorithm for Exact Simulation of Multicomponent Aggregation Processes, Journal of Computational Physics, Volume 177, Issue 2, 2002, Pages 418-449, ISSN 0021-9991, https://doi.org/10.1006/jcph.2002.7017.","category":"page"},{"location":"modules/DiffEqBayes/examples/#Bayesian-Inference-Examples","page":"Examples","title":"Bayesian Inference Examples","text":"","category":"section"},{"location":"modules/DiffEqBayes/examples/#Stan","page":"Examples","title":"Stan","text":"","category":"section"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"Like in the previous examples, we set up the Lotka-Volterra system and generate data.","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"f1 = @ode_def begin\n  dx = a*x - b*x*y\n  dy = -c*y + d*x*y\nend a b c d\np = [1.5,1.0,3.0,1.0]\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\nprob1 = ODEProblem(f1,u0,tspan,p)\nsol = solve(prob1,Tsit5())\nt = collect(range(1,stop=10,length=10))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"Here we now give Stan an array of prior distributions for our parameters. Since the parameters of our differential equation must be positive, we utilize truncated Normal distributions to make sure that is satisfied in the result:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"priors = [truncated(Normal(1.5,0.1),0,2),truncated(Normal(1.0,0.1),0,1.5),\n          truncated(Normal(3.0,0.1),0,4),truncated(Normal(1.0,0.1),0,2)]","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"We then give these to the inference function.","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result = stan_inference(prob1,t,data,priors;\n                                 num_samples=100,num_warmup=500,\n                                 vars = (StanODEData(),InverseGamma(4,1)))","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"InverseGamma(4,1) is our starting estimation for the variance hyperparameter of the default Normal distribution. The result is a Mamba.jl chain object. We can pull out the parameter values via:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"theta1 = bayesian_result.chain_results[:,[\"theta.1\"],:]\ntheta2 = bayesian_result.chain_results[:,[\"theta.2\"],:]\ntheta3 = bayesian_result.chain_results[:,[\"theta.3\"],:]\ntheta4 = bayesian_result.chain_results[:,[\"theta.4\"],:]","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"From these chains we can get our estimate for the parameters via:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"mean(theta1.value[:,:,1])","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"We can get more of a description via:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"Mamba.describe(bayesian_result.chain_results)\n\n# Result\n\nIterations = 1:100\nThinning interval = 1\nChains = 1,2,3,4\nSamples per chain = 100\n\nEmpirical Posterior Estimates:\n                  Mean         SD        Naive SE        MCSE         ESS\n         lp__ -6.15472697 1.657551334 0.08287756670 0.18425029767  80.9314979\naccept_stat__  0.90165904 0.125913744 0.00629568721 0.02781181930  20.4968668\n   stepsize__  0.68014975 0.112183047 0.00560915237 0.06468790087   3.0075188\n  treedepth__  2.68750000 0.524911975 0.02624559875 0.10711170182  24.0159141\n n_leapfrog__  6.77000000 4.121841086 0.20609205428 0.18645821695 100.0000000\n  divergent__  0.00000000 0.000000000 0.00000000000 0.00000000000         NaN\n     energy__  9.12245750 2.518330231 0.12591651153 0.32894488320  58.6109941\n     sigma1.1  0.57164997 0.128579363 0.00642896816 0.00444242658 100.0000000\n     sigma1.2  0.58981422 0.131346442 0.00656732209 0.00397310122 100.0000000\n       theta1  1.50237077 0.008234095 0.00041170473 0.00025803930 100.0000000\n       theta2  0.99778276 0.009752574 0.00048762870 0.00009717115 100.0000000\n       theta3  3.00087782 0.009619775 0.00048098873 0.00020301023 100.0000000\n       theta4  0.99803569 0.008893244 0.00044466218 0.00040886528 100.0000000\n      theta.1  1.50237077 0.008234095 0.00041170473 0.00025803930 100.0000000\n      theta.2  0.99778276 0.009752574 0.00048762870 0.00009717115 100.0000000\n      theta.3  3.00087782 0.009619775 0.00048098873 0.00020301023 100.0000000\n      theta.4  0.99803569 0.008893244 0.00044466218 0.00040886528 100.0000000\n\nQuantiles:\n                  2.5%        25.0%      50.0%      75.0%       97.5%\n         lp__ -10.11994750 -7.0569000 -5.8086150 -4.96936500 -3.81514375\naccept_stat__   0.54808912  0.8624483  0.9472840  0.98695850  1.00000000\n   stepsize__   0.57975100  0.5813920  0.6440120  0.74276975  0.85282400\n  treedepth__   2.00000000  2.0000000  3.0000000  3.00000000  3.00000000\n n_leapfrog__   3.00000000  7.0000000  7.0000000  7.00000000 15.00000000\n  divergent__   0.00000000  0.0000000  0.0000000  0.00000000  0.00000000\n     energy__   5.54070300  7.2602200  8.7707000 10.74517500 14.91849500\n     sigma1.1   0.38135240  0.4740865  0.5533195  0.64092575  0.89713635\n     sigma1.2   0.39674703  0.4982615  0.5613655  0.66973025  0.88361407\n       theta1   1.48728600  1.4967650  1.5022750  1.50805500  1.51931475\n       theta2   0.97685115  0.9914630  0.9971435  1.00394250  1.01765575\n       theta3   2.98354100  2.9937575  3.0001450  3.00819000  3.02065950\n       theta4   0.97934128  0.9918495  0.9977415  1.00430750  1.01442975\n      theta.1   1.48728600  1.4967650  1.5022750  1.50805500  1.51931475\n      theta.2   0.97685115  0.9914630  0.9971435  1.00394250  1.01765575\n      theta.3   2.98354100  2.9937575  3.0001450  3.00819000  3.02065950\n      theta.4   0.97934128  0.9918495  0.9977415  1.00430750  1.01442975","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"More extensive information about the distributions is given by the plots:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"plot_chain(bayesian_result)","category":"page"},{"location":"modules/DiffEqBayes/examples/#Turing","page":"Examples","title":"Turing","text":"","category":"section"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"This case we will build off of the Stan example. Note that turing_inference does not require the use of the @ode_def macro like Stan does, but it will still work with macro-defined functions. Thus, using the same setup as before, we simply give the setup to:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result = turing_inference(prob1,Tsit5(),t,data,priors;num_samples=500)","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"The result is a MCMCChains.jl chains object. The chain for the first parameter is then given by:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result[\"theta[1]\"]","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"Summary statistics can be also be accessed:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"using StatsBase\ndescribe(bayesian_result)","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"The chain can be analysed by the trace plots and other plots obtained by:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"using StatsPlots\nplot(bayesian_result)","category":"page"},{"location":"modules/DiffEqBayes/examples/#DynamicHMC","page":"Examples","title":"DynamicHMC","text":"","category":"section"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"We can use DynamicHMC.jl as the backend for sampling with the dynamic_inference function. It supports any DEProblem, priors can be passed as an array of Distributions.jl distributions, passing initial values is optional and in case where the user has a firm understanding of the domain the parameter values will lie in, transformations can be used to pass an array of constraints for the parameters as an array of Transformations.","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result_hmc = dynamichmc_inference(prob1, Tsit5(), t, data, [Normal(1.5, 1)], [bridge(ℝ, ℝ⁺, )])","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"A tuple with summary statistics and the chain values is returned. The chain for the ith parameter is given by:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result_hmc[1][i]","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"For accessing the various summary statistics:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"DynamicHMC.NUTS_statistics(bayesian_result_dynamic[2])","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"Some details about the NUTS sampler can be obtained from:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"bayesian_result_dynamic[3]","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"In case of dynamic_inference the trace plots for the ith parameter can be obtained by:","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"plot(bayesian_result_hmc[1][i])","category":"page"},{"location":"modules/DiffEqBayes/examples/","page":"Examples","title":"Examples","text":"For a better idea of the summary statistics and plotting you can take a look at the benchmarks.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#jump_problem_type","page":"Jump types and JumpProblem","title":"Jump Problems","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/#Mathematical-Specification-of-an-problem-with-jumps","page":"Jump types and JumpProblem","title":"Mathematical Specification of an problem with jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Jumps are defined as a Poisson process which changes states at some rate. When there are multiple possible jumps, the process is a compound Poisson process. On its own, a jump equation is a continuous-time Markov Chain where the time to the next jump is exponentially distributed as calculated by the rate. This type of process, known in biology as \"Gillespie discrete stochastic simulations\" and modeled by the Chemical Master Equation (CME), is the same thing as adding jumps to a DiscreteProblem. However, any differential equation can be extended by jumps as well. For example, we have an ODE with jumps, denoted by","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"fracdudt = f(upt) + sum_ic_i(upt)p_i(t)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"where p_i is a Poisson counter of rate lambda_i(upt). Extending a stochastic differential equation to have jumps is commonly known as a Jump Diffusion, and is denoted by","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"du = f(upt)dt + sum_jg_j(ut)dW_j(t) + sum_ic_i(upt)dp_i(t)","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Types-of-Jumps:-Regular,-Variable,-Constant-Rate-and-Mass-Action","page":"Jump types and JumpProblem","title":"Types of Jumps: Regular, Variable, Constant Rate and Mass Action","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"A RegularJump is a set of jumps that do not make structural changes to the underlying equation. These kinds of jumps only change values of the dependent variable (u) and thus can be treated in an inexact manner. Other jumps, such as those which change the size of u, require exact handling which is also known as time-adaptive jumping. These can only be specified as a ConstantRateJump, MassActionJump, or a VariableRateJump.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"We denote a jump as variable rate if its rate function is dependent on values which may change between constant rate jumps. For example, if there are multiple jumps whose rates only change when one of them occur, than that set of jumps is a constant rate jump. If a jump's rate depends on the differential equation, time, or by some value which changes outside of any constant rate jump, then it is denoted as variable.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"A MassActionJump is a specialized representation for a collection of constant rate jumps that can each be interpreted as a standard mass action reaction. For systems comprised of many mass action reactions, using the MassActionJump type will offer improved performance. Note, only one MassActionJump should be defined per JumpProblem; it is then responsible for handling all mass action reaction type jumps. For systems with both mass action jumps and non-mass action jumps, one can create one MassActionJump to handle the mass action jumps, and create a number of ConstantRateJumps to handle the non-mass action jumps.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"RegularJumps are optimized for regular jumping algorithms like tau-leaping and hybrid algorithms. ConstantRateJumps and MassActionJumps are optimized for SSA algorithms. ConstantRateJumps, MassActionJumps and VariableRateJumps can be added to standard DiffEq algorithms since they are simply callbacks, while RegularJumps require special algorithms.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Defining-a-Regular-Jump","page":"Jump types and JumpProblem","title":"Defining a Regular Jump","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"The constructor for a RegularJump is:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"RegularJump(rate,c,numjumps;mark_dist = nothing)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"rate(out,u,p,t) is the function which computes the rate for every regular jump process\nc(du,u,p,t,counts,mark) is calculates the update given counts number of jumps for each jump process in the interval.\nnumjumps is the number of jump processes, i.e. the number of rate equations and the number of counts\nmark_dist is the distribution for the mark.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Defining-a-Constant-Rate-Jump","page":"Jump types and JumpProblem","title":"Defining a Constant Rate Jump","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"ConstantRateJump(rate,affect!)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"rate(u,p,t) is a function which calculates the rate given the time and the state.\naffect!(integrator) is the effect on the equation, using the integrator interface.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Defining-a-Mass-Action-Jump","page":"Jump types and JumpProblem","title":"Defining a Mass Action Jump","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"The constructor for a MassActionJump is:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"MassActionJump(reactant_stoich, net_stoich; scale_rates = true, param_idxs=nothing)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"reactant_stoich is a vector whose kth entry is the reactant stoichiometry of the kth reaction. The reactant stoichiometry for an individual reaction is assumed to be represented as a vector of Pairs, mapping species id to stoichiometric coefficient.\nnet_stoich is assumed to have the same type as reactant_stoich; a vector whose kth entry is the net stoichiometry of the kth reaction. The net stoichiometry for an individual reaction is again represented as a vector of Pairs, mapping species id to the net change in the species when the reaction occurs.\nscale_rates is an optional parameter that specifies whether the rate constants correspond to stochastic rate constants in the sense used by Gillespie, and hence need to be rescaled. The default, scale_rates=true, corresponds to rescaling the passed in rate constants. See below.\nparam_idxs is a vector of the indices within the parameter vector, p, that correspond to the rate constant for each jump.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Notes for Mass Action Jumps","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"When using MassActionJump the default behavior is to assume rate constants correspond to stochastic rate constants in the sense used by Gillespie (J. Comp. Phys., 1976, 22 (4)). This means that for a reaction such as 2A oversetkrightarrow B, the jump rate function constructed by MassActionJump would be k*A*(A-1)/2!. For a trimolecular reaction like 3A oversetkrightarrow B the rate function would be k*A*(A-1)*(A-2)/3!. To avoid having the reaction rates rescaled (by 1/2 and 1/6 for these two examples), one can pass the MassActionJump constructor the optional named parameter scale_rates=false, i.e. use\nMassActionJump(reactant_stoich, net_stoich; scale_rates = false, param_idxs)\nZero order reactions can be passed as reactant_stoichs in one of two ways. Consider the varnothing oversetkrightarrow A reaction with rate k=1:\np = [1.]\nreactant_stoich = [[0 => 1]]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nAlternatively one can create an empty vector of pairs to represent the reaction:\np = [1.]\nreactant_stoich = [Vector{Pair{Int,Int}}()]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nFor performance reasons, it is recommended to order species indices in stoichiometry vectors from smallest to largest. That is\nreactant_stoich = [[1 => 2, 3 => 1, 4 => 2], [2 => 2, 3 => 2]]\nis preferred over\nreactant_stoich = [[3 => 1, 1 => 2, 4 = > 2], [3 => 2, 2 => 2]]","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Defining-a-Variable-Rate-Jump","page":"Jump types and JumpProblem","title":"Defining a Variable Rate Jump","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"The constructor for a VariableRateJump is:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"VariableRateJump(rate,affect!;\n                   idxs = nothing,\n                   rootfind=true,\n                   save_positions=(true,true),\n                   interp_points=10,\n                   abstol=1e-12,reltol=0)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Note that this is the same as defining a ContinuousCallback, except that instead of the condition function, you provide a rate(u,p,t) function for the rate at a given time and state.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Defining-a-Jump-Problem","page":"Jump types and JumpProblem","title":"Defining a Jump Problem","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"To define a JumpProblem, you must first define the basic problem. This can be a DiscreteProblem if there is no differential equation, or an ODE/SDE/DDE/DAE if you would like to augment a differential equation with jumps. Denote this previously defined problem as prob. Then the constructor for the jump problem is:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"JumpProblem(prob,aggregator::Direct,jumps::JumpSet;\n            save_positions = typeof(prob) <: AbstractDiscreteProblem ? (false,true) : (true,true))","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"The aggregator is the method for aggregating the constant jumps. These are defined below. jumps is a JumpSet which is just a gathering of jumps. Instead of passing a JumpSet, one may just pass a list of jumps themselves. For example:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"JumpProblem(prob,aggregator,jump1,jump2)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"and the internals will automatically build the JumpSet. save_positions is the save_positions argument built by the aggregation of the constant rate jumps.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Note that a JumpProblem/JumpSet can only have 1 RegularJump (since a RegularJump itself describes multiple processes together). Similarly, it can only have one MassActionJump (since it also describes multiple processes together).","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Constant-Rate-Jump-Aggregators","page":"Jump types and JumpProblem","title":"Constant Rate Jump Aggregators","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Constant rate jump aggregators are the methods by which constant rate jumps, including MassActionJumps, are lumped together. This is required in all algorithms for both speed and accuracy. The current methods are:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Direct: the Gillespie Direct method SSA.\nRDirect: A variant of Gillespie's Direct method that uses rejection to sample the next reaction.\nDirectCR: The Composition-Rejection Direct method of Slepoy et al. For large networks and linear chain-type networks it will often give better performance than Direct. (Requires dependency graph, see below.)\nDirectFW: the Gillespie Direct method SSA with FunctionWrappers. This aggregator uses a different internal storage format for collections of ConstantRateJumps.\nFRM: the Gillespie first reaction method SSA. Direct should generally offer better performance and be preferred to FRM.\nFRMFW: the Gillespie first reaction method SSA with FunctionWrappers.\nNRM: The Gibson-Bruck Next Reaction Method. For some reaction network  structures this may offer better performance than Direct (for example,  large, linear chains of reactions). (Requires dependency graph, see below.)\nRSSA: The Rejection SSA (RSSA) method of Thanh et al. With RSSACR, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nRSSACR: The Rejection SSA (RSSA) with Composition-Rejection method of Thanh et al. With RSSA, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nSortingDirect: The Sorting Direct Method of McCollum et al. It will usually offer performance as good as Direct, and for some systems can offer substantially better performance. (Requires dependency graph, see below.)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"To pass the aggregator, pass the instantiation of the type. For example:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"JumpProblem(prob,Direct(),jump1,jump2)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"will build a problem where the constant rate jumps are solved using Gillespie's Direct SSA method.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Constant-Rate-Jump-Aggregators-Requiring-Dependency-Graphs","page":"Jump types and JumpProblem","title":"Constant Rate Jump Aggregators Requiring Dependency Graphs","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Italicized constant rate jump aggregators require the user to pass a dependency graph to JumpProblem. DirectCR, NRM and SortingDirect require a jump-jump dependency graph, passed through the named parameter dep_graph. i.e.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"JumpProblem(prob,DirectCR(),jump1,jump2; dep_graph=your_dependency_graph)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"For systems with only MassActionJumps, or those generated from a Catalyst reaction_network, this graph will be auto-generated. Otherwise you must construct the dependency graph manually. Dependency graphs are represented as a Vector{Vector{Int}}, with the ith vector containing the indices of the jumps for which rates must be recalculated when the ith jump occurs. Internally, all MassActionJumps are ordered before ConstantRateJumps (with the latter internally ordered in the same order they were passed in).","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"RSSA and RSSACR require two different types of dependency graphs, passed through the following JumpProblem kwargs:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"vartojumps_map - A Vector{Vector{Int}} mapping each variable index, i, to a set of jump indices. The jump indices correspond to jumps with rate functions that depend on the value of u[i].\njumptovars_map - A Vector{Vector{Int}}  mapping each jump index to a set  of variable indices. The corresponding variables are those that have their  value, u[i], altered when the jump occurs.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"For systems generated from a Catalyst reaction_network these will be auto-generated. Otherwise you must explicitly construct and pass in these mappings.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Recommendations-for-Constant-Rate-Jumps","page":"Jump types and JumpProblem","title":"Recommendations for Constant Rate Jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"For representing and aggregating constant rate jumps","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"Use a MassActionJump to handle all jumps that can be represented as mass action reactions. This will generally offer the fastest performance.\nUse ConstantRateJumps for any remaining jumps.\nFor a small number of jumps, < ~10, Direct will often perform as well as the other aggregators.\nFor > ~10 jumps SortingDirect will often offer better performance than Direct.\nFor large numbers of jumps with sparse chain like structures and similar jump rates, for example continuous time random walks, RSSACR, DirectCR and then NRM often have the best performance.\nFor very large networks, with many updates per jump, RSSA and RSSACR will often substantially outperform the other methods.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"In general, for systems with sparse dependency graphs if Direct is slow, one of SortingDirect, RSSA or RSSACR will usually offer substantially better performance. See DiffEqBenchmarks.jl for benchmarks on several example networks.","category":"page"},{"location":"modules/JumpProcesses/jump_types/#Remaking-JumpProblems","page":"Jump types and JumpProblem","title":"Remaking JumpProblems","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"When running many simulations, it can often be convenient to update the initial condition or simulation parameters without having to create and initialize a new JumpProblem. In such situations remake can be used to change the initial condition, time span, and the parameter vector. Note, the new JumpProblem will alias internal data structures from the old problem, including core components of the SSA aggregators. As such, only the new problem generated by remake should be used for subsequent simulations.","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"As an example, consider the following SIR model:","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"rate1(u,p,t) = (0.1/1000.0)*u[1]*u[2]\nfunction affect1!(integrator)\n  integrator.u[1] -= 1\n  integrator.u[2] += 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = 0.01u[2]\nfunction affect2!(integrator)\n  integrator.u[2] -= 1\n  integrator.u[3] += 1\nend\njump2 = ConstantRateJump(rate2,affect2!)\nu0    = [999,1,0]\np     = (0.1/1000,0.01)\ntspan = (0.0,250.0)\ndprob = DiscreteProblem(u0, tspan, p)\njprob = JumpProblem(dprob, Direct(), jump, jump2)\nsol   = solve(jprob, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"We can change any of u0, p and tspan by either making a new DiscreteProblem","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"u02    = [10,1,0]\np2     = (.1/1000, 0.0)\ntspan2 = (0.0,2500.0)\ndprob2 = DiscreteProblem(u02, tspan2, p2)\njprob2 = remake(jprob, prob=dprob2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"or by directly remaking with the new parameters","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"jprob2 = remake(jprob, u0=u02, p=p2, tspan=tspan2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"To avoid ambiguities, the following will give an error","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"jprob2 = remake(jprob, prob=dprob2, u0=u02)","category":"page"},{"location":"modules/JumpProcesses/jump_types/","page":"Jump types and JumpProblem","title":"Jump types and JumpProblem","text":"as will trying to update either p or tspan while passing a new DiscreteProblem using the prob kwarg.","category":"page"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/#Solving-Nonlinear-Systems","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"","category":"section"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"A nonlinear system f(u) = 0 is specified by defining a function f(u,p), where p are the parameters of the system. For example, the following solves the vector equation f(u) = u^2 - p for a vector of equations:","category":"page"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"using NonlinearSolve, StaticArrays\n\nf(u,p) = u .* u .- p\nu0 = @SVector[1.0, 1.0]\np = 2.0\nprobN = NonlinearProblem{false}(f, u0, p)\nsolver = solve(probN, NewtonRaphson(), tol = 1e-9)","category":"page"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"where u0 is the initial condition for the rootfind. Native NonlinearSolve.jl solvers use the given type of u0 to determine the type used within the solver and the return. Note that the parameters p can be any type, but most are an AbstractArray for automatic differentiation.","category":"page"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/#Using-Bracketing-Methods","page":"Solving Nonlinear Systems","title":"Using Bracketing Methods","text":"","category":"section"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"For scalar rootfinding problems, bracketing methods exist. In this case, one passes a bracket instead of an initial condition, for example:","category":"page"},{"location":"modules/NonlinearSolve/tutorials/nonlinear/","page":"Solving Nonlinear Systems","title":"Solving Nonlinear Systems","text":"f(u, p) = u .* u .- 2.0\nu0 = (1.0, 2.0) # brackets\nprobB = NonlinearProblem(f, u0)\nsol = solve(probB, Falsi())","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"using PolyChaos, LinearAlgebra, Plots\nγ = 0.5;\nint_exact = 1+pi/2; # exact value of the integral\nfunction my_w(t, γ)\n    γ + (1 - γ) * 1 / sqrt(1 - t^2)\nend\nN = 1000;\nn,w = fejer(N);\nint_fejer = dot(w,my_w.(n,γ))\nprint(\"Fejer error:\\t$(abs(int_exact-int_fejer))\\twith $N nodes\")\nfunction quad_gaussleg(N,γ)\n    a, b = rm_legendre(N)\n    n, w = golubwelsch(a,b)\nend\nn, w = quad_gaussleg(N+1, γ)\nint_gaussleg = dot(w,γ .+ (1-γ)/sqrt.(1 .- n.^2))\nprint(\"Gauss-Legendre error:\\t$(abs(int_exact-int_gaussleg))\\twith $N nodes\")\nfunction quad_gausscheb(N,γ)\n    a, b = rm_chebyshev1(N)\n    n, w = golubwelsch(a, b)\nend\nn, w = quad_gausscheb(N+1,γ)\nint_gausscheb = dot(w,γ .+ (1-γ)*sqrt.(1 .- n.^2))\nprint(\"Gauss-Chebyshev error:\\t$(abs(int_exact-int_gausscheb))\\twith $(length(n)) nodes\")\nfunction quad_gaussleg_mod(N::Int,γ::Float64)\n    n,w = quad_gaussleg(N+1,γ)\n    return n,γ*w\nend\nfunction quad_gausscheb_mod(N::Int,γ::Float64)\n            n,w = quad_gausscheb(N+1,γ)\n    return n,(1-γ)*w\nend\nN = 8\na,b = mcdiscretization(N,[n->quad_gaussleg_mod(n,γ); n->quad_gausscheb_mod(n,γ)])\nn,w = golubwelsch(a,b)\nint_mc = sum(w)\nprint(\"Discretization error:\\t$(abs(int_exact-int_mc))\\twith $(length(n)) nodes\")\nΓ = 0:0.1:1;\nab = [ mcdiscretization(N,[n->quad_gaussleg_mod(n,gam); n->quad_gausscheb_mod(n,gam)]) for gam in Γ ];\nbb = hcat([ ab[i][2] for i=1:length(Γ)]...);\nb_leg = rm_legendre(N)[2];\nb_cheb = rm_chebyshev1(N)[2]\nbb[:,1]-b_cheb\nbb[:,end]-b_leg\nusing Plots\nplot(Γ,bb',yaxis=:log10, w=3, legend=false)\nzs, os = zeros(N), ones(N)\nscatter!(zs,b_cheb,marker=:x)\nscatter!(os,b_leg,marker=:circle)","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/#Multiple-Discretization","page":"Multiple Discretization","title":"Multiple Discretization","text":"","category":"section"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"This tutorial shows how to compute recurrence coefficients for non-trivial weight functions, and how they are being used for quadrature. The method we use is called multiple discretization, and follows W. Gautschi's book \"Orthogonal Polynomials: Computation and Approximation\", specifically Section 2.2.4, and Example 2.38.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Suppose we have the weight function","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"forall t in -11 gamma in 01 quad w(tgamma) = gamma + (1-gamma) frac1sqrt1-t^2","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"and we would like to solve","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"int_-1^1 f(t) w(tc) mathrmdt = sum_nu=1^N f(tau_nu) w_nu","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"by some quadrature rule. We will see that ad-hoc quadrature rules will fail to solve the integral even for the simplest choice f equiv 1. However, finding the recurrence coefficients of the underlying orthogonal polynomials, and then finding the quadrature rule will do just fine.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Let us first try to solve the integral for f equiv 1 by Féjer's rule.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"using PolyChaos, LinearAlgebra\nγ = 0.5;\nint_exact = 1 + pi / 2; # exact value of the integral\nfunction my_w(t, γ)\n    γ + (1 - γ) * 1 / sqrt(1 - t^2)\nend\n\nN = 1000;\nnodes, weights = fejer(N);\nint_fejer = dot(weights, my_w.(nodes, γ))\nprint(\"Fejer error:\\t$(abs(int_exact - int_fejer))\\twith $N nodes\")","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Clearly, that is not satisfying. Well, the term gamma of the weight w makes us think of Gauss-Legendre integration, so let's try it instead.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"function quad_gaussleg(N, γ)\n    a, b = rm_legendre(N)\n    nodes, weights = golubwelsch(a, b)\nend\nnodes, weights = quad_gaussleg(N+1, γ)\nint_gaussleg = dot(weights, γ .+ (1-γ)/sqrt.(1 .- nodes.^2))\nprint(\"Gauss-Legendre error:\\t$(abs(int_exact-int_gaussleg))\\twith $N nodes\")","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Even worse! Well, we can factor out frac1sqrt1-t^2, making the integral amenable to a Gauss-Chebyshev rule. So, let's give it anothery try.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"function quad_gausscheb(N, γ)\n    a, b = rm_chebyshev1(N)\n    nodes, weights = golubwelsch(a, b)\nend\nnodes, weights = quad_gausscheb(N+1, γ)\nint_gausscheb = dot(weights, γ .+ (1-γ)*sqrt.(1 .- nodes.^2))\n# int=sum(xw(:,2).*(1+sqrt(1-xw(:,1).^2)))\nprint(\"Gauss-Chebyshev error:\\t$(abs(int_exact - int_gausscheb))\\twith $(length(n)) nodes\")","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Okay, that's better, but it took us a lot of nodes to get this result. Is there a different way? Indeed, there is. As we have noticed, the weight w has a lot in common with Gauss-Legendre and Gauss-Chebyshev. We can decompose the integral as follows","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"int_-1^1 f(t) w(t) mathrmdt = sum_i=1^m int_-1^1 f(t) w_i(t) mathrmd t","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"with","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"beginalign*\nw_1(t) = gamma \nw_2(t) = (1-gamma) frac1sqrt1-t^2\nendalign*","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"To the weight w_1 we can apply Gauss-Legendre quadrature, to the weight w_2 we can apply Gauss-Chebyshev quadrature (with tiny modifications). This discretization of the measure can be used in our favor. The function mcdiscretization() takes the m discretization rules as an input","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"function quad_gaussleg_mod(N, γ)\n    nodes, weights = quad_gaussleg(N + 1, γ)\n    nodes, γ*weights\nend\nfunction quad_gausscheb_mod(N, γ)\n            nodes, weights = quad_gausscheb(N + 1,γ)\n    return nodes, (1-γ)*weights\nend\n\nN = 8\na, b = mcdiscretization(N, [n -> quad_gaussleg_mod(n, γ); n -> quad_gausscheb_mod(n, γ)])\nnodes, weights = golubwelsch(a, b)\nint_mc = sum(w)\nprint(\"Discretization error:\\t$(abs(int_exact-int_mc))\\twith $(length(n)) nodes\")","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Et voilà, no error with fewer nodes. (For this example, we'd need in fact just a single node.)","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"The function mcdiscretization() is able to construct the recurrence coefficients of the orthogonal polynomials relative to the weight w. Let's inspect the values of the recurrence coefficients a little more. For gamma = 0, we are in the world of Chebyshev polynomials, for gamma = 1, we enter the realm of Legendre polynomials. And in between? That's exactly where the weight w comes in: it can be thought of as an interpolatory weight, interpolating Legendre polynomials and Chebyshev polynomials. Let's verify this by plotting the recurrence coefficients for several values of gamma.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Γ = 0:0.1:1;\nab = [ mcdiscretization(N, [n -> quad_gaussleg_mod(n, gam); n -> quad_gausscheb_mod(n, gam)]) for gam in Γ ];\nbb = hcat([ab[i][2] for i in 1:length(Γ)]...);\nb_leg = rm_legendre(N)[2];\nb_cheb = rm_chebyshev1(N)[2]\nbb[:,1]-b_cheb","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"bb[:,end] - b_leg","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"Let's plot these values to get a better feeling.","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"using Plots\nplot(Γ, bb', yaxis=:log10, w=3, legend=false)\nzs, os = zeros(N), ones(N)\nscatter!(zs, b_cheb, marker=:x)\nscatter!(os, b_leg, marker=:circle)\n\nxlabel!(\"Gamma\")\nylabel!(\"Beta\")","category":"page"},{"location":"modules/PolyChaos/multiple_discretization/","page":"Multiple Discretization","title":"Multiple Discretization","text":"The crosses denote the values of the β recursion coefficients for Chebyshev polynomials; the circles the β recursion coefficients for Legendre polynomials. The interpolating line in between stands for the β recursion coefficients of w(t gamma).","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Problem-Interface","page":"Problem Interface","title":"Problem Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"This page defines the common problem interface. There are certain rules that can be applied to any function definition, and this page defines those behaviors.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#In-place-vs-Out-of-Place-Function-Definition-Forms","page":"Problem Interface","title":"In-place vs Out-of-Place Function Definition Forms","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Every problem definition has an in-place and out-of-place form, commonly referred throughout DiffEq as IIP (isinplace) and OOP (out of place). The in-place form is a mutating form. For example, on ODEs, we have that f!(du,u,p,t) is the in-place form which, as its output, mutates du. Whatever is returned is simply ignored. Similarly, for OOP we have the form du=f(u,p,t) which uses the return.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Each of the problem types have that the first argument is the option mutating argument. The SciMLBase system will automatically determine the functional form and place a specifier isinplace on the function to carry as type information whether the function defined for this DEProblem is in-place. However, every constructor allows for manually specifying the in-placeness of the function. For example, this can be done at the problem level like:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"ODEProblem{true}(f,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"which declares that isinplace=true. Similarly this can be done at the DEFunction level. For example:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"ODEFunction{true}(f,jac=myjac)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Type-Specifications","page":"Problem Interface","title":"Type Specifications","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Throughout DifferentialEquations.jl, the types that are given in a problem are the types used for the solution. If an initial value u0 is needed for a problem, then the state variable u will match the type of that u0. Similarly, if time exists in a problem the type for t will be derived from the types of the tspan. Parameters p can be any type and the type will be matching how it's defined in the problem.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"For internal matrices, such as Jacobians and Brownian caches, these also match the type specified by the user. jac_prototype and rand_prototype can thus be any Julia matrix type which is compatible with the operations that will be performed.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Functional-and-Condensed-Problem-Inputs","page":"Problem Interface","title":"Functional and Condensed Problem Inputs","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Note that the initial condition can be written as a function of parameters and initial time:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"u0(p,t0)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"and be resolved before going to the solver. Additionally, the initial condition can be a distribution from Distributions.jl, in which case a sample initial condition will be taken each time init or solve is called.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"In addition, tspan supports the following forms. The single value form t is equivalent to (zero(t),t). The functional form is allowed:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"tspan(p)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"which outputs a tuple.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Examples","page":"Problem Interface","title":"Examples","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"prob = ODEProblem((u,p,t)->u,(p,t0)->p[1],(p)->(0.0,p[2]),(2.0,1.0))\nusing Distributions\nprob = ODEProblem((u,p,t)->u,(p,t)->Normal(p,1),(0.0,1.0),1.0)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Lower-Level-__init-and-__solve","page":"Problem Interface","title":"Lower Level __init and __solve","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"At the high level, known problematic problems will emit warnings before entering the solver to better clarify the error to the user. The following cases are checked if the solver is adaptive:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Integer times warn\nDual numbers must be in the initial conditions and timespans\nMeasurements.jl values must be in the initial conditions and timespans","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"If there is an exception to these rules, please file an issue. If one wants to go around the high level solve interface and its warnings, one can call __init or __solve instead.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/#Modification-of-problem-types","page":"Problem Interface","title":"Modification of problem types","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Problem-related types in DifferentialEquations.jl are immutable.  This helps, e.g., parallel solvers to efficiently handle problem types.","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"However, you may want to modify the problem after it is created.  For example, to simulate it for longer timespan.  It can be done by the remake function:","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"prob1 = ODEProblem((u,p,t) -> u/2, 1.0, (0.0,1.0))\nprob2 = remake(prob1; tspan=(0.0,2.0))","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"A general syntax of remake is","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"modified_problem = remake(original_problem;\n  field_1 = value_1,\n  field_2 = value_2,\n  ...\n)","category":"page"},{"location":"modules/DiffEqDocs/basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"where field_N and value_N are renamed to appropriate field names and new desired values.","category":"page"},{"location":"modules/DiffEqDocs/solvers/benchmarks/#Solver-Benchmarks","page":"Solver Benchmarks","title":"Solver Benchmarks","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/benchmarks/","page":"Solver Benchmarks","title":"Solver Benchmarks","text":"Benchmarks for the solvers can be found at SciMLBenchmarks.jl. Many different problems are tested. However, if you would like additional problems to be benchmarked, please open an issue or PR at the SciMLBenchmarks.jl repository with the code that defines the DEProblem.","category":"page"},{"location":"modules/StructuralIdentifiability/utils/primality/#Primality-Checks","page":"Primality Chekcs","title":"Primality Checks","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/primality/","page":"Primality Chekcs","title":"Primality Chekcs","text":"Pages=[\"primality.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/primality/","page":"Primality Chekcs","title":"Primality Chekcs","text":"StructuralIdentifiability.check_primality","category":"page"},{"location":"modules/StructuralIdentifiability/utils/primality/#StructuralIdentifiability.check_primality","page":"Primality Chekcs","title":"StructuralIdentifiability.check_primality","text":"check_primality(polys::Dict{fmpq_mpoly, fmpq_mpoly}, extra_relations::Array{fmpq_mpoly, 1})\n\nThe function checks if the ideal generated by the polynomials and saturated at the leading coefficient with respect to the corresponding variables is prime over rationals.\n\nThe extra_relations allows to add more polynomials to the generators (not affecting the saturation).\n\n\n\n\n\ncheck_primality(polys::Dict{fmpq_mpoly, fmpq_mpoly})\n\nThe function checks if the ideal generated by the polynomials and saturated at the leading coefficient with respect to the corresponding variables is prime over rationals.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#scimlfunctions","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The SciML ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the SciMLFunction types which can be passed to the problems.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Definition-of-the-AbstractSciMLFunction-Interface","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Definition of the AbstractSciMLFunction Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The following standard principles should be adhered to across all AbstractSciMLFunction instantiations.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Common-Function-Choice-Definitions","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Common Function Choice Definitions","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The full interface available to the solvers is as follows:","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"jac: The Jacobian of the differential equation with respect to the state variable u at a time t with parameters p.\nparamjac: The Jacobian of the differential equation with respect to p at state u at time t.\nanalytic: Defines an analytical solution using u0 at time t with p which will cause the solvers to return errors. Used for testing.\nsyms: Allows you to name your variables for automatic names in plots and other output.\njac_prototype: Defines the type to be used for any internal Jacobians within the solvers.\nsparsity: Defines the sparsity pattern to be used for the sparse differentiation schemes. By default this is equal to jac_prototype. See the sparsity handling portion of this page for more information.\ncolorvec: The coloring pattern used by the sparse differentiator. See the sparsity handling portion of this page for more information.\nobserved: A function which allows for generating other observables from a solution.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"Each function type additionally has some specific arguments, refer to their documentation for details.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#In-place-Specification-and-No-Recompile-Mode","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"In-place Specification and No-Recompile Mode","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"Each SciMLFunction type can be called with an \"is inplace\" (iip) choice.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"ODEFunction(f)\nODEFunction{iip}(f)","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"which is a boolean for whether the function is in the inplace form (mutating to change the first value). This is automatically determined using the methods table but note that for full type-inferrability of the AbstractSciMLProblem this iip-ness should be specified.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"Additionally, the functions are fully specialized to reduce the runtimes. If one would instead like to not specialize on the functions to reduce compile time, then one can set recompile to false.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"ODEFunction{iip,false}(f)","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"This makes the ODE solver compilation independent of the function and so changing the function will not cause recompilation. One can change the default value by changing the const RECOMPILE_BY_DEFAULT = true to false in the SciMLBase.jl source code.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Specifying-Jacobian-Types","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Specifying Jacobian Types","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The jac field of an inplace style SciMLFunction has the signature jac(J,u,p,t), which updates the jacobian J in-place. The intended type for J can sometimes be inferred (e.g. when it is just a dense Matrix), but not in general. To supply the type information, you can provide a jac_prototype in the function's constructor.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The following example creates an inplace ODEFunction whose jacobian is a Diagonal:","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"using LinearAlgebra\nf = (du,u,p,t) -> du .= t .* u\njac = (J,u,p,t) -> (J[1,1] = t; J[2,2] = t; J)\njp = Diagonal(zeros(2))\nfun = ODEFunction(f; jac=jac, jac_prototype=jp)","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"Note that the integrators will always make a deep copy of fun.jac_prototype, so there's no worry of aliasing.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"In general the jacobian prototype can be anything that has mul! defined, in particular sparse matrices or custom lazy types that support mul!. A special case is when the jac_prototype is a AbstractDiffEqLinearOperator, in which case you do not need to supply jac as it is automatically set to update_coefficients!. Refer to the DiffEqOperators section for more information on setting up time/parameter dependent operators.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Sparsity-Handling","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Sparsity Handling","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The solver libraries internally use packages such as FiniteDiff.jl and SparseDiffTools.jl for high performance calculation of sparse Jacobians and Hessians, along with matrix-free calculations of Jacobian-Vector products (Jv), vector-Jacobian products (v'J), and Hessian-vector products (H*v). The SciML interface gives users the ability to control these connections in order to allow for top notch performance.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"The key arguments in the SciMLFunction is the prototype, which is an object that will be used as the underlying Jacobian/Hessian. Thus if one wants to use a sparse Jacobian, one should specify jac_prototype to be a sparse matrix. The sparsity pattern used in the differentiation scheme is defined by sparsity. By default, sparsity=jac_prototype, meaning that the sparse automatic differentiation scheme should specialize on the sparsity pattern given by the actual sparsity pattern. This can be overridden to say perform partial matrix coloring approximations. Additionally, the color vector for the sparse differentiation directions can be specified directly via colorvec. For more information on how these arguments control the differentiation process, see the aforementioned differentiation library documentations.","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Traits","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Traits","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"SciMLBase.isinplace(f::SciMLBase.AbstractSciMLFunction)","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#AbstractSciMLFunction-API","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"AbstractSciMLFunction API","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#Abstract-SciML-Functions","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"Abstract SciML Functions","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLFunctions (Jacobians, Sparsity, Etc.)","text":"SciMLBase.AbstractDiffEqFunction\nSciMLBase.AbstractODEFunction\nSciMLBase.AbstractSDEFunction\nSciMLBase.AbstractDDEFunction\nSciMLBase.AbstractDAEFunction\nSciMLBase.AbstractRODEFunction\nSciMLBase.AbstractDiscreteFunction\nSciMLBase.AbstractSDDEFunction\nSciMLBase.AbstractNonlinearFunction","category":"page"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractDiffEqFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractDiffEqFunction","text":"abstract type AbstractDiffEqFunction{iip} <: SciMLBase.AbstractSciMLFunction{iip}\n\nBase for types defining differential equation functions.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractODEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractODEFunction","text":"abstract type AbstractODEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractSDEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractSDEFunction","text":"abstract type AbstractSDEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractDDEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractDDEFunction","text":"abstract type AbstractDDEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractDAEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractDAEFunction","text":"abstract type AbstractDAEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractRODEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractRODEFunction","text":"abstract type AbstractRODEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractDiscreteFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractDiscreteFunction","text":"abstract type AbstractDiscreteFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractSDDEFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractSDDEFunction","text":"abstract type AbstractSDDEFunction{iip} <: SciMLBase.AbstractDiffEqFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/SciMLFunctions/#SciMLBase.AbstractNonlinearFunction","page":"SciMLFunctions (Jacobians, Sparsity, Etc.)","title":"SciMLBase.AbstractNonlinearFunction","text":"abstract type AbstractNonlinearFunction{iip} <: SciMLBase.AbstractSciMLFunction{iip}\n\n\n\n\n\n","category":"type"},{"location":"modules/Surrogates/tutorials/#Surrogates-101","page":"Basics","title":"Surrogates 101","text":"","category":"section"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Let's start with something easy to get our hands dirty. I want to build a surrogate for f(x) = log(x) cdot x^2+x^3. Let's choose the radial basis surrogate.","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> log(x)*x^2+x^3\nlb = 1.0\nub = 10.0\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at 5.4\napprox = my_radial_basis(5.4)","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Let's now see an example in 2D.","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing LinearAlgebra\nf = x -> x[1]*x[2]\nlb = [1.0,2.0]\nub = [10.0,8.5]\nx = sample(50,lb,ub,SobolSample())\ny = f.(x)\nmy_radial_basis = RadialBasis(x,y,lb,ub)\n\n#I want an approximation at (1.0,1.4)\napprox = my_radial_basis((1.0,1.4))","category":"page"},{"location":"modules/Surrogates/tutorials/#Kriging-standard-error","page":"Basics","title":"Kriging standard error","text":"","category":"section"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Let's now use the Kriging surrogate, which is a single-output Gaussian process. This surrogate has a nice feature: not only does it approximate the solution at a point, it also calculates the standard error at such point. Let's see an example:","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nf = x -> exp(x)*x^2+x^3\nlb = 0.0\nub = 10.0\nx = sample(100,lb,ub,UniformSample())\ny = f.(x)\np = 1.9\nmy_krig = Kriging(x,y,lb,ub,p=p)\n\n#I want an approximation at 5.4\napprox = my_krig(5.4)\n\n#I want to find the standard error at 5.4\nstd_err = std_error_at_point(my_krig,5.4)","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Let's now optimize the Kriging surrogate using Lower confidence bound method, this is just a one-liner:","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"surrogate_optimize(f,LCBS(),lb,ub,my_krig,UniformSample())","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Surrogate optimization methods have two purposes: they both sample the space in unknown regions and look for the minima at the same time.","category":"page"},{"location":"modules/Surrogates/tutorials/#Lobachevsky-integral","page":"Basics","title":"Lobachevsky integral","text":"","category":"section"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"The Lobachevsky surrogate has the nice feature of having a closed formula for its integral, which is something that other surrogates are missing. Let's compare it with QuadGK:","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing QuadGK\nobj = x -> 3*x + log(x)\na = 1.0\nb = 4.0\nx = sample(2000,a,b,SobolSample())\ny = obj.(x)\nalpha = 2.0\nn = 6\nmy_loba = LobachevskySurrogate(x,y,a,b,alpha=alpha,n=n)\n\n#1D integral\nint_1D = lobachevsky_integral(my_loba,a,b)\nint = quadgk(obj,a,b)\nint_val_true = int[1]-int[2]\n@assert int_1D ≈ int_val_true","category":"page"},{"location":"modules/Surrogates/tutorials/#Example-of-NeuralSurrogate","page":"Basics","title":"Example of NeuralSurrogate","text":"","category":"section"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"Basic example of fitting a neural network on a simple function of two variables.","category":"page"},{"location":"modules/Surrogates/tutorials/","page":"Basics","title":"Basics","text":"using Surrogates\nusing Flux\nusing Statistics\n\nf = x -> x[1]^2 + x[2]^2\nbounds = Float32[-1.0, -1.0], Float32[1.0, 1.0]\n# Flux models are in single precision by default.\n# Thus, single precision will also be used here for our training samples.\n\nx_train = sample(100, bounds..., SobolSample())\ny_train = f.(x_train)\n\n# Perceptron with one hidden layer of 20 neurons.\nmodel = Chain(Dense(2, 20, relu), Dense(20, 1))\nloss(x, y) = Flux.mse(model(x), y)\n\n# Training of the neural network\nlearning_rate = 0.1\noptimizer = Descent(learning_rate)  # Simple gradient descent. See Flux documentation for other options.\nn_epochs = 50\nsgt = NeuralSurrogate(x_train, y_train, bounds..., model=model, loss=loss, opt=optimizer, n_echos=n_epochs)\n\n# Testing the new model\nx_test = sample(30, bounds..., SobolSample())\ntest_error = mean(abs2, sgt(x)[1] - f(x) for x in x_test)","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Metaheuristics.jl","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Metaheuristics is a is a Julia package implementing metaheuristic algorithms for global optiimization that do not require for the optimized function to be differentiable.","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Installation:-OptimizationMetaheuristics.jl","page":"Metaheuristics.jl","title":"Installation: OptimizationMetaheuristics.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"To use this package, install the OptimizationMetaheuristics package:","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"import Pkg; Pkg.add(\"OptimizationMetaheuristics\")","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Global-Optimizer","page":"Metaheuristics.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Without-Constraint-Equations","page":"Metaheuristics.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"A Metaheuristics Single-Objective algorithm is called using one of the following:","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Evolutionary Centers Algorithm: ECA()\nDifferential Evolution: DE() with 5 different stratgies\nDE(strategy=:rand1) - default strategy\nDE(strategy=:rand2)\nDE(strategy=:best1)\nDE(strategy=:best2)\nDE(strategy=:randToBest1)\nParticle Swarm Optimization: PSO()\nArtificial Bee Colony: ABC()\nGravitational Search Algorithm: CGSA()\nSimulated Annealing: SA()\nWhale Optimization Algorithm: WOA()","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Metaheuristics also performs Multiobjective optimization but this is not yet supported by Optimization.","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Each optimizer sets default settings based on the optimization problem but specific parameters can be set as shown in the original Documentation ","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Additionally, Metaheuristics common settings which would be defined by Metaheuristics.Options can be simply passed as special keywoard arguments to solve without the need to use the Metaheuristics.Options struct.","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Lastly, information about the optimization problem such as the true optimum is set via Metaheuristics.Information and passed as part of the optimizer struct to solve e.g. solve(prob, ECA(information=Metaheuristics.Inoformation(f_optimum = 0.0)))","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"The currently available algorithms and their parameters are listed here.","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Notes","page":"Metaheuristics.jl","title":"Notes","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"The algorithms in Metaheuristics are performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#Examples","page":"Metaheuristics.jl","title":"Examples","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"The Rosenbrock function can optimized using the Evolutionary Centers Algorithm ECA() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, ECA(), maxiters=100000, maxtime=1000.0)","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"Per default Metaheuristics ignores the initial values x0 set in the OptimizationProblem. In order to for Optimization to use x0 we have to set use_initial=true:","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, ECA(), use_initial=true, maxiters=100000, maxtime=1000.0)","category":"page"},{"location":"modules/Optimization/optimization_packages/metaheuristics/#With-Constraint-Equations","page":"Metaheuristics.jl","title":"With Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/metaheuristics/","page":"Metaheuristics.jl","title":"Metaheuristics.jl","text":"While Metaheuristics.jl supports such constraints, Optimization.jl currently does not relay these constraints.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/#Specifying-and-Solving-PDESystems-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for PDEs","title":"Specifying and Solving PDESystems with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"In this example, we will solve a Poisson equation:","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"^2_x u(x y) + ^2_y u(x y) = - sin(pi x) sin(pi y)  ","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"with the boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"beginalign*\nu(0 y) = 0  \nu(1 y) = 0  \nu(x 0) = 0  \nu(x 1) = 0  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"on the space domain:","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"x in 0 1    y in 0 1  ","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"with grid discretization dx = 0.1 using physics-informed neural networks.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/#Copy-Pastable-Code","page":"Introduction to NeuralPDE for PDEs","title":"Copy-Pastable Code","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ 0.0,\n       u(x,0) ~ 0.0, u(x,1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]\n\n# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim,16,Lux.σ),Dense(16,16,Lux.σ),Dense(16,1))\n\n# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain,GridTraining(dx))\n\n@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = discretize(pde_system,discretization)\n\n#Optimizer\nopt = OptimizationOptimJL.BFGS()\n\n#Callback function\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, opt, callback = callback, maxiters=1000)\nphi = discretization.phi\n\nusing Plots\n\nxs,ys = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.u)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/#Detailed-Description","page":"Introduction to NeuralPDE for PDEs","title":"Detailed Description","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"The ModelingToolkit PDE interface for this example looks like this:","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables u(..)\n@derivatives Dxx''~x\n@derivatives Dyy''~y\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Boundary conditions\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ 0.0,\n       u(x,0) ~ 0.0, u(x,1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the neural network, where the input of NN equals the number of dimensions and output equals the number of equations in the system.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim,16,Lux.σ),Dense(16,16,Lux.σ),Dense(16,1))","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we build PhysicsInformedNN algorithm where dx is the step of discretization where strategy stores information for choosing a training strategy.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"As described in the API docs, we now need to define the PDESystem and create PINNs problem using the discretize method.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = discretize(pde_system,discretization)","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the callback function and the optimizer. And now we can solve the PDE using PINNs (with the number of epochs maxiters=1000).","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"#Optimizer\nopt = OptimizationOptimJL.BFGS()\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, opt, callback = callback, maxiters=1000)\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"xs,ys = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.u)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\nusing Plots\n\np1 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"(Image: poissonplot)","category":"page"},{"location":"modules/HighDimPDE/#HighDimPDE.jl","page":"Home","title":"HighDimPDE.jl","text":"","category":"section"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"HighDimPDE.jl is a Julia package to solve Highly Dimensional non-linear, non-local PDEs of the form","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"beginaligned\n    (partial_t u)(tx) = int_Omega fbig(txbf x u(tx)u(tbf x) ( nabla_x u )(tx )( nabla_x u )(tbf x ) big)  dbf x \n     quad + biglangle mu(tx) ( nabla_x u )( tx ) bigrangle + tfrac12 textTrace big(sigma(tx)  sigma(tx) ^* ( textHess_x u)(t x ) big)\nendaligned","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"where u colon 0T times Omega to R, Omega subseteq R^d is subject to initial and boundary conditions, and where d is large.","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"HighDimPDE.jl implements solver algorithms that break down the curse of dimensionality, including","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"the Deep Splitting scheme\nthe Multi-Level Picard iterations scheme.","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"To make the most out of HighDimPDE.jl, we advise to first have a look at the ","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"documentation on the Feynman Kac formula,","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"as all solver algorithms heavily rely on it.","category":"page"},{"location":"modules/HighDimPDE/#Algorithm-overview","page":"Home","title":"Algorithm overview","text":"","category":"section"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"Features DeepSplitting MLP\nTime discretization free ❌ ✅\nMesh-free ✅ ✅\nSingle point x in R^d approximation ✅ ✅\nd-dimensional cube ab^d approximation ✅ ❌\nGPU ✅ ❌\nGradient non-linearities ✔️ ❌","category":"page"},{"location":"modules/HighDimPDE/","page":"Home","title":"Home","text":"✔️ : will be supported in the future","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/#Neural-Ordinary-Differential-Equations-with-Flux","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"All of the tools of SciMLSensitivity.jl can be used with Flux.jl. A lot of the examples have been written to use FastChain and sciml_train, but in all cases this can be changed to the Chain and Flux.train! workflow.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/#Using-Flux-Chain-neural-networks-with-Flux.train!","page":"Neural Ordinary Differential Equations with Flux","title":"Using Flux Chain neural networks with Flux.train!","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"This should work almost automatically by using solve. Here is an example of optimizing u0 and p.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"using OrdinaryDiffEq, SciMLSensitivity, Flux, Plots\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0,1.5f0)\n\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nt = range(tspan[1],tspan[2],length=datasize)\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),saveat=t))\n\ndudt2 = Flux.Chain(x -> x.^3,\n             Flux.Dense(2,50,tanh),\n             Flux.Dense(50,2))\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\nprob = ODEProblem(dudt,u0,tspan)\n\nfunction predict_n_ode()\n  Array(solve(prob,Tsit5(),u0=u0,p=p,saveat=t))\nend\n\nfunction loss_n_ode()\n    pred = predict_n_ode()\n    loss = sum(abs2,ode_data .- pred)\n    loss\nend\n\nloss_n_ode() # n_ode.p stores the initial parameters of the neural ODE\n\ncallback = function (;doplot=false) #callback function to observe training\n  pred = predict_n_ode()\n  display(sum(abs2,ode_data .- pred))\n  # plot current prediction against data\n  pl = scatter(t,ode_data[1,:],label=\"data\")\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\n  display(plot(pl))\n  return false\nend\n\n# Display the ODE with the initial parameter values.\ncallback()\n\ndata = Iterators.repeated((), 1000)\nres1 = Flux.train!(loss_n_ode, Flux.params(u0,p), data, ADAM(0.05), cb = callback)\n\ncallback()","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/#Using-Flux-Chain-neural-networks-with-Optimization.jl","page":"Neural Ordinary Differential Equations with Flux","title":"Using Flux Chain neural networks with Optimization.jl","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"Flux neural networks can be used with Optimization.jl by using the Flux.destructure function. In this case, if dudt is a Flux chain, then:","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"p,re = Flux.destructure(chain)","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"returns p which is the vector of parameters for the chain and re which is a function re(p) that reconstructs the neural network with new parameters p. Using this function we can thus build our neural differential equations in an explicit parameter style.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"Let's use this to build and train a neural ODE from scratch. In this example we will optimize both the neural network parameters p and the input initial condition u0. Notice that Optimization.jl works on a vector input, so we have to concatenate u0 and p and then in the loss function split to the pieces.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"using Flux, OrdinaryDiffEq, SciMLSensitivity, Optimization, OptimizationOptimisers, OptimizationOptimJL, Plots\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0,1.5f0)\n\nfunction trueODEfunc(du,u,p,t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nt = range(tspan[1],tspan[2],length=datasize)\nprob = ODEProblem(trueODEfunc,u0,tspan)\node_data = Array(solve(prob,Tsit5(),saveat=t))\n\ndudt2 = Flux.Chain(x -> x.^3,\n             Flux.Dense(2,50,tanh),\n             Flux.Dense(50,2))\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\nprob = ODEProblem(dudt,u0,tspan)\n\nθ = [u0;p] # the parameter vector to optimize\n\nfunction predict_n_ode(θ)\n  Array(solve(prob,Tsit5(),u0=θ[1:2],p=θ[3:end],saveat=t))\nend\n\nfunction loss_n_ode(θ)\n    pred = predict_n_ode(θ)\n    loss = sum(abs2,ode_data .- pred)\n    loss,pred\nend\n\nloss_n_ode(θ)\n\ncallback = function (θ,l,pred;doplot=false) #callback function to observe training\n  display(l)\n  # plot current prediction against data\n  pl = scatter(t,ode_data[1,:],label=\"data\")\n  scatter!(pl,t,pred[1,:],label=\"prediction\")\n  display(plot(pl))\n  return false\nend\n\n# Display the ODE with the initial parameter values.\ncallback(θ,loss_n_ode(θ)...)\n\n# use Optimization.jl to solve the problem\nadtype = Optimization.AutoZygote()\n\noptf = Optimization.OptimizationFunction((p,_)->loss_n_ode(p), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\n\nresult_neuralode = Optimization.solve(optprob,\n                                       OptimizationOptimisers.Adam(0.05),\n                                       callback = callback,\n                                       maxiters = 300)\n\noptprob2 = remake(optprob,u0 = result_neuralode.u)\n\nresult_neuralode2 = Optimization.solve(optprob2,\n                                        LBFGS(),\n                                        callback = callback,\n                                        allow_f_increases = false)","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"Notice that the advantage of this format is that we can use Optim's optimizers, like LBFGS with a full Chain object for all of Flux's neural networks, like convolutional neural networks.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_ode_flux/","page":"Neural Ordinary Differential Equations with Flux","title":"Neural Ordinary Differential Equations with Flux","text":"(Image: )","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary:-Magnetic-Components","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary: Magnetic Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#Flux-Tubes","page":"Magnetic Components","title":"Flux Tubes","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/","page":"Magnetic Components","title":"Magnetic Components","text":"CurrentModule = ModelingToolkitStandardLibrary.Magnetic.FluxTubes","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#Flux-Tube-Utilities","page":"Magnetic Components","title":"Flux Tube Utilities","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/","page":"Magnetic Components","title":"Magnetic Components","text":"PositiveMagneticPort\nNegativeMagneticPort\nTwoPort","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.PositiveMagneticPort","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.PositiveMagneticPort","text":"Positive magnetic port\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.NegativeMagneticPort","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.NegativeMagneticPort","text":"Negative magnetic port\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.TwoPort","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.TwoPort","text":"TwoPort(;name, V_m_start=0.0, Phi_start=0.0)\n\nPartial component with magnetic potential difference between two magnetic ports p and n and magnetic flux Phi from p to n.\n\nParameters:\n\nV_m_start: Initial magnetic potential difference between both ports\nPhi_start: Initial magnetic flux from portp to portn\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#Basic-Flux-Tube-Blocks","page":"Magnetic Components","title":"Basic Flux Tube Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/","page":"Magnetic Components","title":"Magnetic Components","text":"Ground\nIdle\nShort\nCrossing\nConstantPermeance\nConstantReluctance\nEddyCurrent\nElectroMagneticConverter","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Ground","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Ground","text":"Ground(;name)\n\nZero magnetic potential.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Idle","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Idle","text":"Idle(;name)\n\nIdle running branch.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Short","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Short","text":"Short(;name)\n\nShort cut branch.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Crossing","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.Crossing","text":"Crossing(;name)\n\nCrossing of two branches.\n\nThis is a simple crossing of two branches. The ports portp1 and portp2 are connected, as well as portn1 and portn2.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantPermeance","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantPermeance","text":"ConstantPermeance(;name, G_m=1.0)\n\nConstant permeance.\n\nParameters:\n\nG_m: [H] Magnetic permeance\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantReluctance","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantReluctance","text":"ConstantReluctance(;name, R_m=1.0)\n\nConstant reluctance.\n\nParameters:\n\nR_m: [H^-1] Magnetic reluctance\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.EddyCurrent","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.EddyCurrent","text":"EddyCurrent(;name, rho=0.098e-6, l=1, A=1, Phi_start=0.0)\n\nFor modelling of eddy current in a conductive magnetic flux tube.\n\nParameters:\n\nrho: [ohm * m] Resistivity of flux tube material (default: Iron at 20degC)\nl: [m] Average length of eddy current path\nA: [m^2] Cross sectional area of eddy current path\nPhi_start: [Wb] Initial magnetic flux flowing into the port_p\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ElectroMagneticConverter","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ElectroMagneticConverter","text":"ElectroMagneticConverter(;name, N, Phi_start=0.0)\n\nIdeal electromagnetic energy conversion.\n\nThe electromagnetic energy conversion is given by Ampere's law and Faraday's law respectively V_m = N * i N * dΦ/dt = -v\n\nParameters:\n\nN: Number of turns\nPhi_start: [Wb] Initial magnetic flux flowing into the port_p\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#Flux-Tube-Sources","page":"Magnetic Components","title":"Flux Tube Sources","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/","page":"Magnetic Components","title":"Magnetic Components","text":"ConstantMagneticPotentialDifference\nConstantMagneticFlux","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantMagneticPotentialDifference","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantMagneticPotentialDifference","text":"Constant magnetomotive force.\n\nParameters:\n\nV_m: [A] Magnetic potential difference\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/magnetic/#ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantMagneticFlux","page":"Magnetic Components","title":"ModelingToolkitStandardLibrary.Magnetic.FluxTubes.ConstantMagneticFlux","text":"Source of constant magnetic flux.\n\nParameters:\n\nPhi: [Wb] Magnetic flux\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/tutorials/dae_example/#Differential-Algebraic-Equations","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"Differential Algebraic Equations (DAEs) are differential equations which have constraint equations on their evolution. This tutorial will introduce you to the functionality for solving differential algebraic equations (DAEs). Other introductions can be found by checking out SciMLTutorials.jl.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/#Mass-Matrix-Differential-Algebraic-Equations-(DAEs)","page":"Differential Algebraic Equations","title":"Mass-Matrix Differential-Algebraic Equations (DAEs)","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"Instead of just defining an ODE as u = f(upt), it can be common to express the differential equation in the form with a mass matrix:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"Mu = f(upt)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"where M is known as the mass matrix. Let's solve the Robertson equation. In previous tutorials we wrote this equation as:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\ndy_1 = -004 y_1 + 10^4 y_2 y_3 \ndy_2 =  004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \ndy_3 = 3*10^7 y_2^2 \nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"But we can instead write this with a conservation relation:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\nfracdy_1dt = -004 y_1 + 10^4 y_2 y_3 \nfracdy_2dt =  004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \n1 =  y_1 + y_2 + y_3 \nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"In this form, we can write this as a mass matrix ODE where M is singular (this is another form of a differential-algebraic equation (DAE)). Here, the last row of M is just zero. We can implement this form as:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using DifferentialEquations\nfunction rober(du,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  du[1] = -k₁*y₁ + k₃*y₂*y₃\n  du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n  du[3] =  y₁ + y₂ + y₃ - 1\n  nothing\nend\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\nf = ODEFunction(rober,mass_matrix=M)\nprob_mm = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\nsol = solve(prob_mm,Rodas5(),reltol=1e-8,abstol=1e-8)\n\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"note: Note\nIf your mass matrix is singular, i.e. your system is a DAE, then you need to make sure you choose a solver that is compatible with DAEs","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/#Implicitly-Defined-Differential-Algebraic-Equations-(DAEs)","page":"Differential Algebraic Equations","title":"Implicitly-Defined Differential-Algebraic Equations (DAEs)","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"In this example we will solve the Robertson equation in its implicit form:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"f(duupt) = 0","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"This equation is a DAE of the form:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\nfracdudt = f(upt) \n 0 = g(upt) \n endaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"which is also known as a constrained differential equation, where g is the constraint equation. The Robertson model can be written in the form:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \n1 =  y_1 + y_2 + y_3 \nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"with initial conditions y_1(0) = 1, y_2(0) = 0, y_3(0) = 0, dy_1 = - 004, dy_2 = 004, and dy_3 = 00.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"The workflow for DAEs is the same as for the other types of equations, where all you need to know is how to define the problem. A DAEProblem is specified by defining an in-place update f(out,du,u,p,t) which uses the values to mutate out as the output. To makes this into a DAE, we move all of the variables to one side. Thus, we can define the function:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"function f(out,du,u,p,t)\n  out[1] = - 0.04u[1]              + 1e4*u[2]*u[3] - du[1]\n  out[2] = + 0.04u[1] - 3e7*u[2]^2 - 1e4*u[2]*u[3] - du[2]\n  out[3] = u[1] + u[2] + u[3] - 1.0\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"with initial conditions","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"u₀ = [1.0, 0, 0]\ndu₀ = [-0.04, 0.04, 0.0]\ntspan = (0.0,100000.0)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"and make the DAEProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using DifferentialEquations\ndifferential_vars = [true,true,false]\nprob = DAEProblem(f,du₀,u₀,tspan,differential_vars=differential_vars)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"differential_vars is an option which states which of the variables are differential, i.e. not purely algebraic (which means that their derivative shows up in the residual equations). This is required for the algorithm to be able to find consistent initial conditions. Notice that the first two variables are determined by their changes, but the last is simply determined by the conservation equation. Thus, we use differential_vars = [true,true,false].","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"As with the other DifferentialEquations problems, the commands are then to solve and plot. Here we will use the IDA solver from Sundials:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using Sundials\nsol = solve(prob,IDA())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"In order to clearly see all the features of this solution, it should be plotted on a logarithmic scale. We'll also plot each on a different subplot, to allow scaling the y-axis appropriately.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using Plots\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"This gives the following plot:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/#Getting-the-index-for-a-symbol","page":"Frequently Asked Questions","title":"Getting the index for a symbol","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Since ordering of symbols is not guaranteed after symbolic transformations, one should normally refer to values by their name. For example, sol[lorenz.x] from the solution. But what if you need to get the index? The following helper function will do the trick:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"indexof(sym,syms) = findfirst(isequal(sym),syms)\nindexof(σ,parameters(sys))","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/#Transforming-value-maps-to-arrays","page":"Frequently Asked Questions","title":"Transforming value maps to arrays","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ModelingToolkit.jl allows (and recommends) input maps like [x => 2.0, y => 3.0] because symbol ordering is not guaranteed. However, what if you want to get the lowered array? You can use the internal function varmap_to_vars. For example:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"pnew = varmap_to_vars([β=>3.0, c=>10.0, γ=>2.0],parameters(sys))","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/#How-do-I-handle-if-statements-in-my-symbolic-forms?","page":"Frequently Asked Questions","title":"How do I handle if statements in my symbolic forms?","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For statements that are in the if then else form, use IfElse.ifelse from the IfElse.jl package to represent the code in a functional form. For handling direct if statements, you can use equivalent boolean mathematical expressions. For example if x > 0 ... can be implemented as just (x > 0) *, where if x <= 0 then the boolean will evaluate to 0 and thus the term will be excluded from the model.","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/#ERROR:-TypeError:-non-boolean-(Num)-used-in-boolean-context?","page":"Frequently Asked Questions","title":"ERROR: TypeError: non-boolean (Num) used in boolean context?","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you see the error:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ERROR: TypeError: non-boolean (Num) used in boolean context","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"then it's likely you are trying to trace through a function which cannot be directly represented in Julia symbols. The techniques to handle this problem, such as @register_symbolic, are described in detail in the Symbolics.jl documentation.","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/#Using-ModelingToolkit-with-Optimization-/-Automatic-Differentiation","page":"Frequently Asked Questions","title":"Using ModelingToolkit with Optimization / Automatic Differentiation","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you are using ModelingToolkit inside of a loss function and are having issues with mixing MTK with automatic differentiation, getting performance, etc... don't! Instead, use MTK outside of the loss function to generate the code, and then use the generated code inside of the loss function.","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For example, let's say you were building ODEProblems in the loss function like:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"function loss(p)\n    prob = ODEProblem(sys, [], [p1 => p[1], p2 => p[2]])\n    sol = solve(prob, Tsit5())\n    sum(abs2,sol)\nend","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Since ODEProblem on a MTK sys will have to generate code, this will be slower than caching the generated code, and will required automatic differentiation to go through the code generation process itself. All of this is unnecessary. Instead, generate the problem once outside of the loss function, and remake the prob inside of the loss function:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"prob = ODEProblem(sys, [], [p1 => p[1], p2 => p[2]])\nfunction loss(p)\n    remake(prob,p = ...)\n    sol = solve(prob, Tsit5())\n    sum(abs2,sol)\nend","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Now, one has to be careful with remake to ensure that the parameters are in the right order. One can use the previously mentioned indexing functionality to generate index maps for reordering p like:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"p = @parameters x y z\nidxs = ModelingToolkit.varmap_to_vars([p[1] => 1, p[2] => 2, p[3] => 3], p)\np[idxs]","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Using this, the fixed index map can be used in the loss function. This would look like:","category":"page"},{"location":"modules/ModelingToolkit/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"prob = ODEProblem(sys, [], [p1 => p[1], p2 => p[2]])\nidxs = Int.(ModelingToolkit.varmap_to_vars([p1 => 1, p2 => 2], p))\nfunction loss(p)\n    remake(prob,p = p[idxs])\n    sol = solve(prob, Tsit5())\n    sum(abs2,sol)\nend","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/#direct_sensitivity","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"While sensitivity analysis tooling can be used implicitly via integration with automatic differentiation libraries, one can often times obtain more speed and flexibility with the direct sensitivity analysis interfaces. This tutorial demonstrates some of those functions.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/#Example-using-an-ODEForwardSensitivityProblem","page":"Direct Sensitivity Analysis Functionality","title":"Example using an ODEForwardSensitivityProblem","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"Forward sensitivity analysis is performed by defining and solving an augmented ODE. To define this augmented ODE, use the ODEForwardSensitivityProblem type instead of an ODE type. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"using OrdinaryDiffEq, SciMLSensitivity\n\nfunction f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEForwardSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"This generates a problem which the ODE solvers can solve:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"sol = solve(prob,DP8())","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"Note that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"x,dp = extract_local_sensitivities(sol)\nx,dp = extract_local_sensitivities(sol,i)\nx,dp = extract_local_sensitivities(sol,t)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"In each case, x is the ODE values and dp is the matrix of sensitivities The first gives the full timeseries of values and dp[i] contains the time series of the sensitivities of all components of the ODE with respect to ith parameter. The second returns the ith time step, while the third interpolates to calculate the sensitivities at time t. For example, if we do:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"x,dp = extract_local_sensitivities(sol)\nda = dp[1]","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"then da is the timeseries for fracpartial u(t)partial p. We can plot this","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"using Plots\nplot(sol.t,da',lw=3)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"transposing so that the rows (the timeseries) is plotted.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"(Image: Local Sensitivity Solution)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"For more information on the internal representation of the ODEForwardSensitivityProblem solution, see the direct forward sensitivity analysis manual page.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/#Example-using-adjoint_sensitivities-for-discrete-adjoints","page":"Direct Sensitivity Analysis Functionality","title":"Example using adjoint_sensitivities for discrete adjoints","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let's solve the ODE and get a high quality continuous solution:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"function f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)\nsol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"Now let's calculate the sensitivity of the ell_2 error against 1 at evenly spaced points in time, that is:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"L(upt)=sum_i=1^nfracVert1-u(t_ip)Vert^22","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"for t_i = 05i. This is the assumption that the data is data[i]=1.0. For this function, notice we have that:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"beginaligned\ndg_1=1-u_1 \ndg_2=1-u_2 \n quad vdots\nendaligned","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"and thus:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"dg(out,u,p,t,i) = (out.=1.0.-u)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"Also, we can omit dgdp, because the cost function doesn't dependent on p. If we had data, we'd just replace 1.0 with data[i]. To get the adjoint sensitivities, call:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"ts = 0:0.5:10\nres = adjoint_sensitivities(sol,Vern9(),t=ts,dgdu_discrete=dg,abstol=1e-14,\n                            reltol=1e-14)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"This is super high accuracy. As always, there's a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"using ForwardDiff,Calculus,ReverseDiff,Tracker\nfunction G(p)\n  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=ts,\n              sensealg=SensitivityADPassThrough())\n  A = convert(Array,sol)\n  sum(((1 .- A).^2)./2)\nend\nG([1.5,1.0,3.0])\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])\nres4 = Tracker.gradient(G,[1.5,1.0,3.0])\nres5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/direct_sensitivity/","page":"Direct Sensitivity Analysis Functionality","title":"Direct Sensitivity Analysis Functionality","text":"and see this gives the same values.","category":"page"},{"location":"modules/QuasiMonteCarlo/#QuasiMonteCarlo.jl:-Quasi-Monte-Carlo-(QMC)-Samples-Made-Easy","page":"Home","title":"QuasiMonteCarlo.jl: Quasi-Monte Carlo (QMC) Samples Made Easy","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"QuasiMonteCarlo.jl is a lightweight package for generating Quasi-Monte Carlo (QMC) samples using various different methods.","category":"page"},{"location":"modules/QuasiMonteCarlo/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"To install QuasiMonteCarlo.jl, use the Julia package manager:","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"QuasiMonteCarlo\")","category":"page"},{"location":"modules/QuasiMonteCarlo/#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"using QuasiMonteCarlo, Distributions\nlb = [0.1,-0.5]\nub = [1.0,20.0]\nn = 5\nd = 2\n\ns = QuasiMonteCarlo.sample(n,lb,ub,GridSample([0.1,0.5]))\ns = QuasiMonteCarlo.sample(n,lb,ub,UniformSample())\ns = QuasiMonteCarlo.sample(n,lb,ub,SobolSample())\ns = QuasiMonteCarlo.sample(n,lb,ub,LatinHypercubeSample())\ns = QuasiMonteCarlo.sample(n,lb,ub,LatticeRuleSample())\ns = QuasiMonteCarlo.sample(n,lb,ub,LowDiscrepancySample([10,3]))","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"The output s is a matrix, so one can use things like @uview from UnsafeArrays.jl for a stack-allocated view of the ith point:","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"using UnsafeArrays\n@uview s[:,i]","category":"page"},{"location":"modules/QuasiMonteCarlo/#Adding-a-new-sampling-method","page":"Home","title":"Adding a new sampling method","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"Adding a new sampling method is a two-step process:","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"Add a new SamplingAlgorithm type.\nOverload the sample function with the new type.","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"All sampling methods are expected to return a matrix with dimension d by n, where d is the dimension of the sample space and n is the number of samples.","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"Example","category":"page"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb isa Number\n        ...\n        return x\n    else\n        ...\n        return reduce(hcat, x)\n    end\nend","category":"page"},{"location":"modules/QuasiMonteCarlo/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/#Optimization-Based-ODE-Parameter-Estimation","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We choose to optimize the parameters on the Lotka-Volterra equation. We do so by defining the function as a function with parameters:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - u[1]*u[2]\n  du[2] = dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We create data using the numerical result with a=1.5:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"sol = solve(prob,Tsit5())\nt = collect(range(0,stop=10,length=200))\nusing RecursiveArrayTools # for VectorOfArray\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we used VectorOfArray from RecursiveArrayTools.jl to turn the result of an ODE into a matrix.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"If we plot the solution with the parameter at a=1.42, we get the following:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Not Fit)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Notice that after one period this solution begins to drift very far off: this problem is sensitive to the choice of a.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"To build the objective function for Optim.jl, we simply call the build_loss_objective function:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),\n                                     maxiters=10000,verbose=false)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set maxiters in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set verbose=false because this divergence can get noisy.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Before optimizing, let's visualize our cost function by plotting it for a range of parameter values:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"vals = 0.0:0.1:10.0\nusing Plots; plotly()\nplot(vals,[cost_function(i) for i in vals],yscale=:log10,\n     xaxis = \"Parameter\", yaxis = \"Cost\", title = \"1-Parameter Cost Function\",\n     lw = 3)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Now this cost function can be used with Optim.jl in order to get the parameters. For example, we can use Brent's algorithm to search for the best solution on the interval [0,10] by:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using Optim\nresult = optimize(cost_function, 0.0, 10.0)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This returns result.minimizer[1]==1.5 as the best parameter to match the data. When we plot the fitted equation on the data, we receive the following:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"(Image: Parameter Estimation Fit)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Thus we see that after fitting, the lines match up with the generated data and receive the right parameter value.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use the multivariate optimization functions. For example, we can use the BFGS algorithm to optimize the parameter starting at a=1.42 using:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"result = optimize(cost_function, [1.42], BFGS())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Note that some of the algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the documentation for Optim.jl. We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus following the Optim.jl documentation we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"lower = [0.0]\nupper = [3.0]\nresult = optimize(cost_function, lower, upper, [1.42], Fminbox(BFGS()))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let's use the Lotka-Volterra equation with all parameters free:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function f2(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f2,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can build an objective function and solve the multiple parameter version just as before:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data),\n                                      maxiters=10000,verbose=false)\nresult_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use First-Differences in L2Loss by passing the kwarg differ_weight which decides the contribution of the differencing loss to the total loss.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob,Tsit5(),L2Loss(t,data,differ_weight=0.3,data_weight=0.7),\n                                      maxiters=10000,verbose=false)\nresult_bfgs = Optim.optimize(cost_function, [1.3,0.8,2.8,1.2], Optim.BFGS())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"To solve it using LeastSquaresOptim.jl, we use the build_lsoptim_objective function:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"cost_function = build_lsoptim_objective(prob1,t,data,Tsit5())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The result is a cost function which can be used with LeastSquaresOptim. For more details, consult the documentation for LeastSquaresOptim.jl:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"using LeastSquaresOptim # for LeastSquaresProblem\nx = [1.3,0.8,2.8,1.2]\nres = optimize!(LeastSquaresProblem(x = x, f! = cost_function,\n                output_length = length(t)*length(prob.u0)),\n                LeastSquaresOptim.Dogleg(LeastSquaresOptim.LSMR()))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can see the results are:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"println(res.minimizer)\n\nResults of Optimization Algorithm\n * Algorithm: Dogleg\n * Minimizer: [1.4995074428834114,0.9996531871795851,3.001556360700904,1.0006272074128821]\n * Sum of squares at Minimum: 0.035730\n * Iterations: 63\n * Convergence: true\n * |x - x'| < 1.0e-15: true\n * |f(x) - f(x')| / |f(x)| < 1.0e-14: false\n * |g(x)| < 1.0e-14: false\n * Function Calls: 64\n * Gradient Calls: 9\n * Multiplication Calls: 135","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"and thus this algorithm was able to correctly identify all four parameters.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"We can also use Multiple Shooting method by creating a multiple_shooting_objective","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"function ms_f(du,u,p,t)\n  dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\nms_u0 = [1.0;1.0]\ntspan = (0.0,10.0)\nms_p = [1.5,1.0]\nms_prob = ODEProblem(ms_f,ms_u0,tspan,ms_p)\nt = collect(range(0,stop=10,length=200))\ndata = Array(solve(ms_prob,Tsit5(),saveat=t,abstol=1e-12,reltol=1e-12))\nbound = Tuple{Float64, Float64}[(0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),\n                                (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10)]\n\n\nms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"This creates the objective function that can be passed to an optimizer from which we can then get the parameter values and the initial values of the short time periods keeping in mind the indexing.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"# ]add BlackBoxOptim\nusing BlackBoxOptim\n\nresult = bboptimize(ms_obj;SearchRange = bound, MaxSteps = 21e3)\nresult.archive_output.best_candidate[end-1:end]","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Giving us the results as","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Starting optimization with optimizer BlackBoxOptim.DiffEvoOpt{BlackBoxOptim.FitPopulation{Float64},BlackBoxOptim.RadiusLimitedSelector,BlackBoxOptim.AdaptiveDiffEvoRandBin{3},BlackBoxOptim.RandomBound{BlackBoxOptim.RangePerDimSearchSpace}}\n\nOptimization stopped after 21001 steps and 136.60030698776245 seconds\nTermination reason: Max number of steps (21000) reached\nSteps per second = 153.7405036862868\nFunction evals per second = 154.43596332393247\nImprovements/step = 0.17552380952380953\nTotal function evaluations = 21096\n\n\nBest candidate found: [0.997396, 1.04664, 3.77834, 0.275823, 2.14966, 4.33106, 1.43777, 0.468442, 6.22221, 0.673358, 0.970036, 2.05182, 2.4216, 0.274394, 5.64131, 3.38132, 1.52826, 1.01721]\n\nFitness: 0.126884213\n\nOut[4]:2-element Array{Float64,1}:\n        1.52826\n        1.01721","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"Here as our model had 2 parameters, we look at the last two indexes of result to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The objective function for Two Stage method can be created and passed to an optimizer as","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"two_stage_obj = two_stage_method(ms_prob,t,data)\nresult = Optim.optimize(two_stage_obj, [1.3,0.8,2.8,1.2], Optim.BFGS()\n)\nResults of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.3,0.8,2.8,1.2]\n * Minimizer: [1.5035938533664717,0.9925731153746833, ...]\n * Minimum: 1.513400e+00\n * Iterations: 9\n * Convergence: true\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 4.58e-10\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.87e-16 |f(x)|\n   * |g(x)| ≤ 1.0e-08: true\n     |g(x)| = 7.32e-11\n   * Stopped by an increasing objective: false\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 31\n * Gradient Calls: 31","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/ODE_inference/","page":"Optimization-Based ODE Parameter Estimation","title":"Optimization-Based ODE Parameter Estimation","text":"The default kernel used in the method is Epanechnikov others that are available are Uniform,  Triangular, Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic and Sigmoid, this can be passed by the kernel keyword argument. loss_func keyword argument can be used to pass the loss function (cost function) you want  to use and mpg_autodiff enables Auto Differentiation.","category":"page"},{"location":"modules/Catalyst/faqs/#FAQs","page":"FAQs","title":"FAQs","text":"","category":"section"},{"location":"modules/Catalyst/faqs/#How-to-index-solution-objects-using-symbolic-variables-and-observables?","page":"FAQs","title":"How to index solution objects using symbolic variables and observables?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"One can directly use symbolic variables to index into SciML solution objects. Moreover, observables can also be evaluated in this way. For example, consider the system","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"using Catalyst, DifferentialEquations, Plots\nrn = @reaction_network ABtoC begin\n  (k₊,k₋), A + B <--> C\nend k₊ k₋\n\n# initial condition and parameter values\nsetdefaults!(rn, [:A => 1.0, :B => 2.0, :C => 0.0, :k₊ => 1.0, :k₋ => 1.0])","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Let's convert it to a system of ODEs, using the conservation laws of the system to eliminate two of the species:","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"osys = convert(ODESystem, rn; remove_conserved=true)\nshow(osys) # hide","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Notice the resulting ODE system has just one ODE","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"equations(osys)\nshow(equations(osys)) # hide","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"while algebraic observables have been added for the two removed species (in terms of the conservation law constants, _ConLaw[1] and _ConLaw[2])","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"observed(osys)\nshow(observed(osys))  # hide","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Let's solve the system and see how to index the solution using our symbolic variables","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"oprob = ODEProblem(osys, [], (0.0, 10.0), [])\nsol = solve(oprob, Tsit5())","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Suppose we want to plot just species C, without having to know its integer index in the state vector. We can do this using the symbolic variable C, which we can get at in several ways","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"sol[osys.C]","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"or","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"@unpack C = osys\nsol[C]","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"To evaluate C at specific times and plot it we can just do","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"t = range(0.0, 10.0, length=101)\nplot(t, sol(t, idxs = C), label = \"C(t)\", xlabel = \"t\")","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"If we want to get multiple variables we can just do","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"@unpack A, B = osys\nsol(t, idxs = [A, B])","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Plotting multiple variables using the SciML plot recipe can be achieved like","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"plot(sol; vars = [A, B])","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-disable-rescaling-of-reaction-rates-in-rate-laws?","page":"FAQs","title":"How to disable rescaling of reaction rates in rate laws?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"As explained in the Reaction rate laws used in simulations section, for a reaction such as k, 2X --> 0, the generated rate law will rescale the rate constant, giving k*X^2/2 instead of k*X^2 for ODEs and k*X*(X-1)/2 instead of k*X*(X-1) for jumps. This can be disabled when directly converting a ReactionSystem. If rn is a generated ReactionSystem, we can do","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"osys = convert(ODESystem, rn; combinatoric_ratelaws=false)","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Disabling these rescalings should work for all conversions of ReactionSystems to other ModelingToolkit.AbstractSystems.","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-use-non-integer-stoichiometric-coefficients?","page":"FAQs","title":"How to use non-integer stoichiometric coefficients?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"rn = @reaction_network begin\n  k, 2.5*A --> 3*B\nend k","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"or directly via","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"@parameters k b\n@variables t A(t) B(t) C(t) D(t)\nrx1 = Reaction(k,[B,C],[B,D], [2.5,1],[3.5, 2.5])\nrx2 = Reaction(2*k, [B], [D], [1], [2.5])\nrx3 = Reaction(2*k, [B], [D], [2.5], [2])\n@named mixedsys = ReactionSystem([rx1,rx2,rx3],t,[A,B,C,D],[k,b])\nosys = convert(ODESystem, mixedsys; combinatoric_ratelaws=false)","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Note, when using convert(ODESystem, mixedsys; combinatoric_ratelaws=false) the combinatoric_ratelaws=false parameter must be passed. This is also true when calling ODEProblem(mixedsys,...; combinatoric_ratelaws=false). As described above, this disables Catalyst's standard rescaling of reaction rates when generating reaction rate laws, see also the Reaction rate laws used in simulations section. Leaving this keyword out for systems with floating point stoichiometry will give an error message.","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-set-default-values-for-initial-conditions-and-parameters?","page":"FAQs","title":"How to set default values for initial conditions and parameters?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"When directly constructing a ReactionSystem these can be passed to the constructor, and allow solving the system without needing initial condition or parameter vectors in the generated problem. For example","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"using Catalyst, Plots, OrdinaryDiffEq\n@parameters β ν\n@variables t S(t) I(t) R(t)\nrx1 = Reaction(β, [S,I], [I], [1,1], [2])\nrx2 = Reaction(ν, [I], [R])\ndefs = [β => 1e-4, ν => .01, S => 999.0, I => 1.0, R => 0.0]\n@named sir = ReactionSystem([rx1,rx2],t; defaults=defs)\noprob = ODEProblem(sir, [], (0.0,250.0))\nsol = solve(oprob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"alternatively we could also have said","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"@parameters β=1e-4 ν=.01\n@variables t S(t)=999.0 I(t)=1.0 R(t)=0.0\nrx1 = Reaction(β, [S,I], [I], [1,1], [2])\nrx2 = Reaction(ν, [I], [R])\n@named sir = ReactionSystem([rx1,rx2],t)","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"The @reaction_network macro does not currently provide a way to specify default values, however, they can be added after creating the system via the setdefaults! command, like","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"sir = @reaction_network sir begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nsetdefaults!(sir, [:β => 1e-4, :ν => .01, :S => 999.0, :I => 1.0, :R => 0.0])","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-specify-initial-conditions-and-parameters-values-for-ODEProblem-and-other-problem-types?","page":"FAQs","title":"How to specify initial conditions and parameters values for ODEProblem and other problem types?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"To explicitly pass initial conditions and parameters we can use mappings from Julia Symbols corresponding to each variable/parameter to values, or from ModelingToolkit symbolic variables to each variable/parameter. Using Symbols we have","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"rn = @reaction_network begin\n    α, S + I --> 2I\n    β, I --> R\nend α β\nu0 = [:S => 999.0, :I => 1.0, :R => 0.0]\np  = (:α => 1e-4, :β => .01)\nop  = ODEProblem(rn, u0, (0.0,250.0), p)\nsol = solve(op, Tsit5())","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"while using ModelingToolkit symbolic variables we have","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"@parameters α β\n@variables t S(t) I(t) R(t)\nu0 = [S => 999.0, I => 1.0, R => 0.0]\np  = (α => 1e-4, β => .01)\nop  = ODEProblem(rn, u0, (0.0,250.0), p)\nsol = solve(op, Tsit5())","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-modify-generated-ODEs?","page":"FAQs","title":"How to modify generated ODEs?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Conversion to other ModelingToolkit.AbstractSystems allows the possibility to modify the system with further terms that are difficult to encode as a chemical reaction. For example, suppose we wish to add a forcing term, 10sin(10t), to the ODE for dX/dt above. We can do so as:","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"dXdteq = equations(osys)[1]\nt      = get_iv(osys)\ndXdteq = Equation(dXdteq.lhs, dXdteq.rhs + 10*sin(10*t))\n@named osys2  = ODESystem([dXdteq], t, states(osys), parameters(osys))\noprob  = ODEProblem(osys2, u0map, tspan, pmap)\nosol   = solve(oprob, Tsit5())","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"We can add e^-X to dXdt as a forcing term by","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"dXdteq = equations(osys)[1]\n@variables X(t)\ndXdteq = Equation(dXdteq.lhs, dXdteq.rhs + exp(-X))\n@named osys2  = ODESystem([dXdteq], t, states(osys), parameters(osys))\noprob  = ODEProblem(osys2, u0map, tspan, pmap)\nosol   = solve(oprob, Tsit5())","category":"page"},{"location":"modules/Catalyst/faqs/#How-to-override-mass-action-kinetics-rate-laws?","page":"FAQs","title":"How to override mass action kinetics rate laws?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"While generally one wants the reaction rate law to use the law of mass action, so the reaction","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"rn = @reaction_network begin\n  k, X --> ∅\nend k","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"occurs at the (ODE) rate dXdt = -kX, it is possible to override this by using any of the following non-filled arrows when declaring the reaction: ⇐, ⟽, ⇒, ⟾, ⇔, ⟺. This means that the reaction","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"rn = @reaction_network begin\n  k, X ⇒ ∅\nend k","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"will occur at rate dXdt = -k (which might become a problem since X will be degraded at a constant rate even when very small or equal to 0).","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"Note, stoichiometric coefficients are still included, i.e. the reaction","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"rn = @reaction_network begin\n  k, 2*X ⇒ ∅\nend k","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"has rate dXdt = -2 k.","category":"page"},{"location":"modules/Catalyst/faqs/#user_functions","page":"FAQs","title":"How to specify user defined functions as reaction rates?","text":"","category":"section"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"The reaction network DSL can \"see\" user defined functions that work with ModelingToolkit. e.g., this is should work","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"myHill(x) = 2.0*x^3/(x^3+1.5^3)\nrn = @reaction_network begin\n  myHill(X), ∅ → X\nend","category":"page"},{"location":"modules/Catalyst/faqs/","page":"FAQs","title":"FAQs","text":"In some cases, it may be necessary or desirable to register functions with Symbolics.jl before their use in Catalyst, see the discussion here.","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#dde_prob","page":"DDE Problems","title":"DDE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"DDEProblem\nDDEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#SciMLBase.DDEProblem","page":"DDE Problems","title":"SciMLBase.DDEProblem","text":"Defines a delay differential equation (DDE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/dde_types/\n\nMathematical Specification of a DDE Problem\n\nTo define a DDE Problem, you simply need to give the function f, the initial condition u_0 at time point t_0, and the history function h which together define a DDE:\n\nfracdudt = f(uhpt) qquad (t geq t_0)\n\nu(t_0) = u_0\n\nu(t) = h(t) qquad (t  t_0)\n\nf should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)), u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.\n\nFunctional Forms of the History Function\n\nThe history function h can be called in the following ways:\n\nh(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs\n\nNote that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.\n\nDeclaring Lags\n\nLags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate and thus this is recommended.\n\nNeutral and Retarded Delay Differential Equations\n\nNote that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.\n\nNote that algebraic equations can be specified by using a singular mass matrix.\n\nProblem Type\n\nConstructors\n\nDDEProblem(f[, u0], h, tspan[, p]; <keyword arguments>)\nDDEProblem{isinplace}(f[, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nArguments\n\nf: The function in the DDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nDynamical Delay Differential Equations\n\nMuch like Dynamical ODEs, a Dynamical DDE is a Partitioned DDE of the form:\n\nfracdvdt = f_1(uth) \nfracdudt = f_2(vh) \n\nConstructors\n\nDynamicalDDEProblem(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)\nDynamicalDDEProblem{isinplace}(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nArguments\n\nf: The function in the DDE.\nv0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nThe for dynamical and second order DDEs, the history function will return an object with the indicies 1 and 2 defined, where h(p, t_prev)[1] is the value of f_2(v u h p t_mathrmprev) and h(p, t_prev)[2] is the value of f_1(v u h p t_mathrmprev) (this is for consistency with the ordering of the intitial conditions in the constructor). The supplied history function must also return such a 2-index object, which can be accomplished with a tuple (v,u) or vector [v,u].\n\n2nd Order Delay Differential Equations\n\nTo define a 2nd Order DDE Problem, you simply need to give the function f and the initial condition u_0 which define an DDE:\n\nu = f(uuhpt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFrom this form, a dynamical ODE:\n\nv = f(vuhpt) \nu = v \n\nConstructors\n\nSecondOrderDDEProblem(f, [, du0, u0], h, tspan[, p]; <keyword arguments>)\nSecondOrderDDEProblem{isinplace}(f, [, du0, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nArguments\n\nf: The function in the DDE.\ndu0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nAs above, the history function will return an object with indices 1 and 2, with the values of du and u respectively. The supplied history function must also match this return type, e.g. by returning a 2-element tuple or vector.\n\nExample Problems\n\nExample problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_ode_linear, you can do something like:\n\n#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dde_types/#SciMLBase.DDEFunction","page":"DDE Problems","title":"SciMLBase.DDEFunction","text":"DDEFunction{iip,F,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractDDEFunction{iip}\n\nA representation of a DDE function f, defined by:\n\nM fracdudt = f(uhpt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDDEFunction{iip,recompile}(f;\n                 mass_matrix=I,\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing,\n                 jvp=nothing,\n                 vjp=nothing,\n                 jac_prototype=nothing,\n                 sparsity=jac_prototype,\n                 paramjac = nothing,\n                 syms = nothing,\n                 indepsym = nothing,\n                 colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,h,p,t) or du = f(u,h,p,t). See the section on iip for more details on in-place vs out-of-place handling. The histroy function h acts as an interpolator over time, i.e. h(t) with options matching the solution interface, i.e. h(t; save_idxs = 2).\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,h,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,h,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,h,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,h,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,h,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DDEFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dde_types/#Solution-Type","page":"DDE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"DDEProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#Example-Problems","page":"DDE Problems","title":"Example Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"To use a sample problem, such as prob_ode_linear, you can do something like:","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DDEs-with-1-constant-delay","page":"DDE Problems","title":"DDEs with 1 constant delay","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"CurrentModule = DDEProblemLibrary","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_constant_1delay_ip\nprob_dde_constant_1delay_oop\nprob_dde_constant_1delay_scalar\nprob_dde_constant_1delay_long_ip\nprob_dde_constant_1delay_long_oop\nprob_dde_constant_1delay_long_scalar","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_ip","text":"prob_dde_constant_1delay_ip\n\nDelay differential equation\n\nu(t) = -u(t - 1)\n\nfor t in 0 1 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_oop","text":"prob_dde_constant_1delay_oop\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_scalar","text":"prob_dde_constant_1delay_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_ip","text":"prob_dde_constant_1delay_long_ip\n\nDelay differential equation\n\nu(t) = u(t) - u(t - 15)\n\nfor t in 0 100 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_oop","text":"prob_dde_constant_1delay_long_oop\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","text":"prob_dde_constant_1delay_long_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DDEs-with-2-constant-delays","page":"DDE Problems","title":"DDEs with 2 constant delays","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_constant_2delays_ip\nprob_dde_constant_2delays_oop\nprob_dde_constant_2delays_scalar\nprob_dde_constant_2delays_long_ip\nprob_dde_constant_2delays_long_oop\nprob_dde_constant_2delays_long_scalar","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_ip","text":"prob_dde_constant_2delays_ip\n\nDelay differential equation\n\nu(t) = -u(t - 13) - u(t - 15)\n\nfor t in 0 1 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_oop","text":"prob_dde_constant_2delays_oop\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_scalar","text":"prob_dde_constant_2delays_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_ip","text":"prob_dde_constant_2delays_long_ip\n\nDelay differential equation\n\nu(t) = - u(t - 13) - u(t - 15)\n\nfor t in 0 100 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_oop","text":"prob_dde_constant_2delays_long_oop\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","text":"prob_dde_constant_2delays_long_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DDETest-Problems","page":"DDE Problems","title":"DDETest Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Some details:","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"# DDEs with time dependent delays\nprob_dde_DDETST_A1, prob_dde_DDETST_A2,\n# DDEs with vanishing time dependent delays\nprob_dde_DDETST_B1, prob_dde_DDETST_B2,\n# DDEs with state dependent delays\nprob_dde_DDETST_C1, prob_dde_DDETST_C2, prob_dde_DDETST_C3, prob_dde_DDETST_C4,\n# DDEs with vanishing state dependent delays\nprob_dde_DDETST_D1, prob_dde_DDETST_D2,\n# neutral DDEs with time dependent delays\nprob_dde_DDETST_E1, prob_dde_DDETST_E2,\n# neutral DDEs with vanishing time dependent delays\nprob_dde_DDETST_F1, prob_dde_DDETST_F2, prob_dde_DDETST_F3, prob_dde_DDETST_F4, prob_dde_DDETST_F5,\n# neutral DDEs with state dependent delays\nprob_dde_DDETST_G1, prob_dde_DDETST_G2,\n# neutral DDEs with vanishing state dependent delays\nprob_dde_DDETST_H1, prob_dde_DDETST_H2, prob_dde_DDETST_H3, prob_dde_DDETST_H4","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_DDETST_A1\nprob_dde_DDETST_A2\nprob_dde_DDETST_B1\nprob_dde_DDETST_B2\nprob_dde_DDETST_C1\nprob_dde_DDETST_C2\nprob_dde_DDETST_C3\nprob_dde_DDETST_C4\nprob_dde_DDETST_D1\nprob_dde_DDETST_D2\nprob_dde_DDETST_E1\nprob_dde_DDETST_E2\nprob_dde_DDETST_F1\nprob_dde_DDETST_F2\nprob_dde_DDETST_F3\nprob_dde_DDETST_F4\nprob_dde_DDETST_F5\nprob_dde_DDETST_G1\nprob_dde_DDETST_G2\nprob_dde_DDETST_H1\nprob_dde_DDETST_H2\nprob_dde_DDETST_H3\nprob_dde_DDETST_H4","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A1","text":"prob_dde_DDETST_A1\n\nDelay differential equation model of blood production, given by\n\nu(t) = frac02 u(t - 14)1 + u(t - 14)^10 - 01 u(t)\n\nfor t in 0 500 and history function phi(t) = 05 for t leq 0.\n\nReferences\n\nMackey, M. C. and Glass, L. (1977). Oscillation and chaos in physiological control systems, Science (197), pp. 287-289.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A2","text":"prob_dde_DDETST_A2\n\nDelay differential equation model of chronic granulocytic leukemia, given by\n\nu_1(t) = frac111 + sqrt10 u_1(t - 20)^54 - frac10 u_1(t)1 + 40 u_2(t)\n\nu_2(t) = frac100 u_1(t)1 + 40 u_2(t) - 243 u_2(t)\n\nfor t in 0 100 and history function\n\nphi_1(t) = 1057670273\n\nphi_2(t) = 10307134913\n\nfor t leq 0.\n\nReferences\n\nWheldon, T., Kirk, J. and Finlay, H. (1974). Cyclical granulopoiesis in chronic granulocytic leukemia: A simulation study., Blood (43), pp. 379-387.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B1","text":"prob_dde_DDETST_B1\n\nDelay differential equation\n\nu(t) = 1 - u(exp(1 - 1t))\n\nfor t in 01 10 with history function phi(t) = log t for t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 10 is\n\nu(t) = log t\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B2","text":"prob_dde_DDETST_B2\n\nDelay differential equation\n\nu(t) = - 1 - u(t) + 2 u(t  2)  0\n\nfor t in 0 2 log 66 with history function phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 2 log 66 is\n\nu(t) = begincases\n  2 exp(-t) - 1  textif  t in 0 2 log 2 \n  1 - 6 exp(-t)  textif   t in (2 log 2 2 log 6 \n  66 exp(-t) - 1  textif  t in (2 log 6 2 log 66\nendcases\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C1","text":"prob_dde_DDETST_C1\n\nDelay differential equation\n\nu(t) = - 2 u(t - 1 - u(t)) (1 - u(t)^2)\n\nfor t in 0 30 with history function phi(t) = 05 for t leq 0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C2","text":"prob_dde_DDETST_C2\n\nDelay differential equation\n\nu_1(t) = - 2 u_1(t - u_2(t))\n\nu_₂(t) = fracu_1(t - u_2(t)) - u_1(t)1 + u_1(t - u_2(t))\n\nfor t in 0 40 with history function\n\nphi_1(t) = 1\n\nphi_2(t) = 05\n\nfor t leq 0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C3","text":"prob_dde_DDETST_C3\n\nDelay differential equation model of hematopoiesis, given by\n\nu_1(t) = hats_0 u_2(t - T_1) - gamma u_1(t) - Q\n\nu_2(t) = f(u_1(t)) - k u_2(t)\n\nu_3(t) = 1 - fracQ exp(gamma u_3(t))hats_0 u_2(t - T_1 - u_3(t))\n\nfor t in 0 300 with history function phi_1(0) = 3325, phi_3(0) = 120, and\n\nphi_2(t) = begincases\n  10  textif  t in - T_1 0\n  95  textif  t  - T_1\nendcases\n\nwhere f(y) = a  (1 + K y^r), hats_0 = 00031, T_1 = 6, gamma = 0001, Q = 00275, k = 28, a = 6570, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C4","text":"prob_dde_DDETST_C4\n\nDelay differential equation model of hematopoiesis, given by the same delay differential equation as prob_dde_DDETST_C3\n\nu_1(t) = hats_0 u_2(t - T_1) - gamma u_1(t) - Q\n\nu_2(t) = f(u_1(t)) - k u_2(t)\n\nu_3(t) = 1 - fracQ exp(gamma u_3(t))hats_0 u_2(t - T_1 - u_3(t))\n\nfor t in 0 100 with history function phi_1(0) = 35, phi_3(0) = 50, and phi_2(t) = 10 for t leq 0, where f(y) = a  (1 + K y^r), hats_0 = 000372, T_1 = 3, gamma = 01, Q = 000178, k = 665, a = 15600, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D1","text":"prob_dde_DDETST_D1\n\nDelay differential equation\n\nu_1(t) = u_2(t) \n\nu_2(t) = - u_2(exp(1 - u_2(t))) u_2(t)^2 exp(1 - u_2(t))\n\nfor t in 01 5 with history function\n\nphi_1(t) = log t \n\nphi_2(t) = 1  t\n\nfor t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 5 is\n\nu_1(t) = log t \n\nu_2(t) = 1  t\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D2","text":"prob_dde_DDETST_D2\n\nDelay differential equation model of antigen antibody dynamics with fading memory, given by\n\nu_1(t) = - r_1 u_1(t) u_2(t) + r_2 u_3(t) \n\nu_2(t) = - r_1 u_1(t) u_2(t) + alpha r_1 u_1(t - u_4(t)) u_2(t - u_4(t))\n\nu_3(t) = r_1 u_1(t) u_2(t) - r_2 u_3(t) \n\nu_4(t) = 1 + frac3 delta - u_1(t) u_2(t) - u_3(t)u_1(t - u_4(t)) u_2(t - u_4(t)) + u_3(t - u_4(t)) exp(delta u_4(t))\n\nfor t in 0 40 with history function\n\nphi_1(t) = 5 \n\nphi_2(t) = 01 \n\nphi_3(t) = 0 \n\nphi_4(t) = 0\n\nfor t leq 0, where r_1 = 002, r_2 = 0005, alpha = 3, and delta = 001.\n\nReferences\n\nGatica, J. and Waltman, P. (1982). A threshold model of antigen antibody dynamics with fading memory, in Lakshmikantham (ed.), Nonlinear phenomena in mathematical science, Academic Press, New York, pp. 425-439.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E1","text":"prob_dde_DDETST_E1\n\nDelay differential equation model of a food-limited population, given by\n\nu(t) = r u(t) (1 - u(t - 1) - c u(t - 1))\n\nfor t in 0 40 with history function phi(t) = 2 + t for t leq 0, where r = pi  sqrt3 + 120 and c = sqrt3  (2 pi) - 1  25.\n\nReferences\n\nKuang, Y. and Feldstein, A. (1991). Boundedness of solutions of a nonlinear nonautonomous neutral delay equation, J. Math. Anal. Appl. (156), pp. 293-304.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E2","text":"prob_dde_DDETST_E2\n\nDelay differential equation model of a logistic Gauss-type predator-prey system, given by\n\nu_1(t) = u_1(t) (1 - u_1(t - tau) - rho u_1(t - tau)) - fracu_2(t) u_1(t)^2u_1(t)^2 + 1 \n\nu_2(t) = u_2(t) left(fracu_1(t)^2u_1(t)^2 + 1 - alpharight)\n\nfor t in 0 2 with history function\n\nphi_1(t) = 033 - t  10 \n\nphi_2(t) = 222 + t  10\n\nfor t leq 0, where alpha = 01, rho = 29, and tau = 042.\n\nReferences\n\nKuang, Y. (1991). On neutral delay logistics Gauss-type predator-prey systems, Dyn. Stab. Systems (6), pp. 173-189.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F1","text":"prob_dde_DDETST_F1\n\nDelay differential equation\n\nu(t) = 2 cos(2t) u(t  2)^2 cos t + log(u(t  2)) - log(2 cos t) - sin t\n\nfor t in 0 1 with history function phi(0) = 1 and phi(0) = 2.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = exp(sin(2t))\n\nReferences\n\nJackiewicz, Z. (1981). One step methods for the numerical solution of Volterra functional differential equations of neutral type, Applicable Anal. (12), pp. 1-11.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F2","text":"prob_dde_DDETST_F2\n\nDelay differential equation\n\nu(t) = u(2t - 05)\n\nfor t in 025 0499 with history function phi(t) = exp(-t^2) and phi(t) = -2t exp(-t^2) for t leq 025.\n\nSolution\n\nThe analytical solution for t in 025 0499 is\n\nu(t) = u_i(t) = exp(-4^i t^2 + B_i t + C_i)  2^i + K_i\n\nif t in x_i x_i + 1, where\n\nx_i = (1 - 2^-i)  2 \n\nB_i = 2 (4^i-1 + B_i-1) \n\nC_i = - 4^i-2 - B_i-1  2 + C_i-1 \n\nK_i = - exp(-4^i x_i^2 + B_i x_i + C_i)  2^i + u_i-1(x_i)\n\nand B_0 = C_0 = K_0 = 0.\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F3","text":"prob_dde_DDETST_F3\n\nDelay differential equation\n\nu(t) = exp(-u(t)) + L_3 leftsin(u(alpha(t))) - sinleft(frac13 + alpha(t)right)right\n\nfor t in 0 10 with history function phi(0) = log 3 and phi(0) = 1  3, where alpha(t) = 05 t (1 - cos(2 pi t)) and L_3 = 02.\n\nSolution\n\nThe analytical solution for t in 0 10 is\n\nu(t) = log(t + 3)\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F4","text":"prob_dde_DDETST_F4\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 04.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F5","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F5","text":"prob_dde_DDETST_F5\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 06.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G1","text":"prob_dde_DDETST_G1\n\nDelay differential equation\n\nu(t) = - u(t - u(t)^2  4)\n\nfor t in 0 1 with history function phi(t) = 1 - t for t leq 0 and phi(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nReferences\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, p. 44.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G2","text":"prob_dde_DDETST_G2\n\nDelay differential equation\n\nu(t) = - u(u(t) - 2)\n\nfor t in 0 1 with history function phi(t) = 1 - t for t leq 0 and phi(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, pp. 44-45.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H1","text":"prob_dde_DDETST_H1\n\nDelay differential equation\n\nu(t) = - frac4 t u(t)^24 + log(cos(2t))^2 + tan(2t) + 05 arctanleft(uleft(fract u(t)^21 + u(t)^2right)right)\n\nfor t in 0 0225 pi with history function phi(0) = 0 and phi(0) = 0.\n\nSolution\n\nThe analytical solution for t in 0 0225 pi is\n\nu(t) = - log(cos(2t))  2\n\nReferences\n\nCastleton, R. N. and Grimm, L. J. (1973). A first order method for differential equations of neutral type, Math. Comput. (27), pp. 571-577.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H2","text":"prob_dde_DDETST_H2\n\nDelay differential equation\n\nu(t) = cos(t) (1 + u(t u(t)^2)) + L_3 u(t) u(t u(t)^2) + (1 - L_3) sin(t) cos(t sin(t)^2) - sin(t + t sin(t)^2)\n\nfor t in 0 pi with history function phi(0) = 0 and phi(0) = 1, where L_3 = 01.\n\nSolution\n\nThe analytical solution for t in 0 pi is\n\nu(t) = sin(t)\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H3","text":"prob_dde_DDETST_H3\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 03.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H4","text":"prob_dde_DDETST_H4\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 05.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#Radar5-Test-Problems","page":"DDE Problems","title":"Radar5 Test Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_RADAR5_oregonator\nprob_dde_RADAR5_robertson\nprob_dde_RADAR5_waltman","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_oregonator","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_oregonator","text":"prob_dde_RADAR5_oregonator\n\nDelay differential equation model from chemical kinetics, given by\n\n  u_1(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - tau) + k_3 B u_1(t) - 2 k_4 u_1(t)^2 \n\n  u_2(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - tau) + f k_3 B u_1(t)\n\nfor t in 0 1005 with history function\n\n  phi_1(t) = 1e-10 \n\n  phi_2(t) = 1e-5\n\nfor t leq 0, where k_1 = 134, k_2 = 16e9, k_3 = 8000, k_4 = 4e7, k_5 = 1, f = 1, A = 006, B = 006, and tau = 015.\n\nReferences\n\nEpstein, I. and Luo, Y. (1991). Differential delay equations in chemical kinetics. Nonlinear models, Journal of Chemical Physics (95), pp. 244-254.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_robertson","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_robertson","text":"prob_dde_RADAR5_robertson\n\nDelay differential equation model of a chemical reaction with steady state solution, given by\n\n  u_1(t) = - a u_1(t) + b u_2(t - tau) u_3(t) \n\n  u_2(t) = a u_1(t) - b u_2(t - tau) u_3(t) - c u_2(t)^2 \n\n  u_3(t) = c u_2(t)^2\n\nfor t in 0 10e10 with history function phi_1(0) = 1, phi_2(t) = 0 for t in -tau 0, and phi_3(0) = 0, where a = 004, b = 10_000, c = 3e7, and tau = 001.\n\nReferences\n\nGuglielmi, N. and Hairer, E. (2001). Implementing Radau IIA methods for stiff delay differential equations, Computing (67), pp. 1-12.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_waltman","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_waltman","text":"prob_dde_RADAR5_waltman\n\nDelay differential equation model of antibody production, given by\n\n  u_1(t) = - r u_1(t) u_2(t) - s u_1(t) u_4(t) \n\n  u_2(t) = - r u_1(t) u_2(t) + alpha r u_1(u_5(t)) u_2(u_5(t)) t geq t_0 \n\n  u_3(t) = r u_1(t) u_2(t) \n\n  u_4(t) = - s u_1(t) u_4(t) - gamma u_4(t) + beta r u_1(u_6(t)) u_2(u_6(t)) t  t_1 \n\n  u_5(t) = t geq t_0 fracu_1(t) u_2(t) + u_3(t)u_1(u_5(t)) u_2(u_5(t)) + u_3(u_5(t)) \n\n  u_6(t) = t geq t_1 frac1e-12 + u_2(t) + u_3(t)1e-12 + u_2(u_6(t)) + u_3(u_6(t))\n\nfor t in 0 300 with history function\n\n  phi_1(t) = phi_0 \n\n  phi_2(t) = 1e-15 \n\n  phi_3(t) = 0 \n\n  phi_4(t) = 0 \n\n  phi_5(t) = 0 \n\n  phi_6(t) = 0\n\nfor t leq 0, where alpha = 18, beta = 20, gamma = 0002, r = 5e4, s = 1e5, t_0 = 32, t_1 = 119, and phi_0 = 075e-4.\n\nReferences\n\nWaltman, P. (1978). A threshold model of antigen-stimulated antibody production, Theoretical Immunology (8), pp. 437-453.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/dde_types/#QS-Example","page":"DDE Problems","title":"QS Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_qs","category":"page"},{"location":"modules/DiffEqDocs/types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_qs","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_qs","text":"prob_dde_qs\n\nDelay differential equation model of Quorum Sensing (QS) of Pseudomonas putida IsoF in continuous cultures.\n\nReferences\n\nBuddrus-Schiemann et al. (2014). Analysis of N-Acylhomoserine Lactone Dynamics in Continuous Cultures of Pseudomonas Putida IsoF By Use of ELISA and UHPLC/qTOF-MS-derived Measurements and Mathematical Models, Analytical and Bioanalytical Chemistry.\n\n\n\n\n\n","category":"constant"},{"location":"modules/RecursiveArrayTools/#RecursiveArrayTools.jl:-Arrays-of-Arrays-and-Even-Deeper","page":"Home","title":"RecursiveArrayTools.jl: Arrays of Arrays and Even Deeper","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/","page":"Home","title":"Home","text":"RecursiveArrayTools.jl is a set of tools for dealing with recursive arrays like arrays of arrays. It contains type wrappers for making recursive arrays act more like normal arrays (for example, automating the recursion of broadcast, maps, iteration, and more), and utility functions which make it easier to work with recursive arrays.","category":"page"},{"location":"modules/RecursiveArrayTools/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/","page":"Home","title":"Home","text":"To install RecursiveArrayTools.jl, use the Julia package manager:","category":"page"},{"location":"modules/RecursiveArrayTools/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RecursiveArrayTools\")","category":"page"},{"location":"modules/RecursiveArrayTools/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Continuous-Normalizing-Flows","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now, we study a single layer neural network that can estimate the density p_x of a variable of interest x by re-parameterizing a base variable z with known density p_z through the Neural Network model passed to the layer.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Copy-Pasteable-Code","page":"Continuous Normalizing Flows","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Flux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationFlux, \n      OptimizationOptimJL, Distributions\n\nnn = Flux.Chain(\n    Flux.Dense(1, 3, tanh),\n    Flux.Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 1.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, Tsit5())\n\n# Training\ndata_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))\n\nfunction loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ffjord_mdl.p)\n\nres1 = Optimization.solve(optprob,\n                          ADAM(0.1),\n                          maxiters = 100)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2,\n                          Optim.LBFGS(),\n                          allow_f_increases=false)\n\n# Evaluation\nusing Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)\n\n# Data Generation\nffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Step-by-Step-Explanation","page":"Continuous Normalizing Flows","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We can use DiffEqFlux.jl to define, train and output the densities computed by CNF layers. In the same way as a neural ODE, the layer takes a neural network that defines its derivative function (see [1] for a reference). A possible way to define a CNF layer, would be:","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Flux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationFlux, \n      OptimizationOptimJL, Distributions\n\nnn = Flux.Chain(\n    Flux.Dense(1, 3, tanh),\n    Flux.Dense(3, 1, tanh),\n) |> f32\ntspan = (0.0f0, 10.0f0)\n\nffjord_mdl = FFJORD(nn, tspan, Tsit5())","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"where we also pass as an input the desired timespan for which the differential equation that defines log p_x and z(t) will be solved.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Training","page":"Continuous Normalizing Flows","title":"Training","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"First, let's get an array from a normal distribution as the training data. Note that we want the data in Float32 values to match how we have setup the neural network weights and the state space of the ODE.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"data_dist = Normal(6.0f0, 0.7f0)\ntrain_data = Float32.(rand(data_dist, 1, 100))","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Now we define a loss function that we wish to minimize","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"function loss(θ)\n    logpx, λ₁, λ₂ = ffjord_mdl(train_data, θ)\n    -mean(logpx)\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"In this example, we wish to choose the parameters of the network such that the likelihood of the re-parameterized variable is maximized. Other loss functions may be used depending on the application. Furthermore, the CNF layer gives the log of the density of the variable x, as one may guess from the code above.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then train the neural network to learn the distribution of x.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"Here we showcase starting the optimization with ADAM to more quickly find a minimum, and then honing in on the minimum by using LBFGS.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x, p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, ffjord_mdl.p)\n\nres1 = Optimization.solve(optprob,\n                          ADAM(0.1),\n                          maxiters = 100)","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"We then complete the training using a different optimizer starting from where ADAM stopped.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"optprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2,\n                          Optim.LBFGS(),\n                          allow_f_increases=false)","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Evaluation","page":"Continuous Normalizing Flows","title":"Evaluation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"For evaluating the result, we can use totalvariation function from Distances.jl. First, we compute densities using actual distribution and FFJORD model. then we use a distance function.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"using Distances\n\nactual_pdf = pdf.(data_dist, train_data)\nlearned_pdf = exp.(ffjord_mdl(train_data, res2.u)[1])\ntrain_dis = totalvariation(learned_pdf, actual_pdf) / size(train_data, 2)","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#Data-Generation","page":"Continuous Normalizing Flows","title":"Data Generation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"What's more, we can generate new data by using FFJORD as a distribution in rand.","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"ffjord_dist = FFJORDDistribution(FFJORD(nn, tspan, Tsit5(); p=res2.u))\nnew_data = rand(ffjord_dist, 100)","category":"page"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/#References","page":"Continuous Normalizing Flows","title":"References","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/normalizing_flows/","page":"Continuous Normalizing Flows","title":"Continuous Normalizing Flows","text":"[1] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).","category":"page"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#Power-Series-Utilities","page":"Power Series Tools","title":"Power Series Utilities","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/","page":"Power Series Tools","title":"Power Series Tools","text":"Pages =[\"power_series_utils.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/","page":"Power Series Tools","title":"Power Series Tools","text":"Modules = [StructuralIdentifiability]\nPages   = [\"power_series_utils.jl\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability._matrix_inv_newton_iteration-Union{Tuple{T}, Tuple{AbstractAlgebra.MatElem{T}, AbstractAlgebra.MatElem{T}}} where T<:(AbstractAlgebra.AbsSeriesElem{<:AbstractAlgebra.FieldElem})","page":"Power Series Tools","title":"StructuralIdentifiability._matrix_inv_newton_iteration","text":"_matrix_inv_newton_iteration(M, Minv)\n\nPerforms a single step of Newton iteration for inverting M with Minv being a partial result\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_diff-Tuple{AbstractAlgebra.AbsSeriesElem{<:AbstractAlgebra.RingElem}}","page":"Power Series Tools","title":"StructuralIdentifiability.ps_diff","text":"ps_diff(ps)\n\nInput:\n\nps - (absolute capped) unvariate power series\n\nOutput: \n\nthe derivative of ps\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_integrate-Tuple{AbstractAlgebra.AbsSeriesElem{<:AbstractAlgebra.FieldElem}}","page":"Power Series Tools","title":"StructuralIdentifiability.ps_integrate","text":"ps_integrate(ps)\n\nInput:\n\nps - (absolute capped) unvariate power series\n\nOutput:\n\nthe integral of ps without constant term\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_matrix_homlinear_de-Union{Tuple{T}, Tuple{AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:T}}, Tuple{AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:T}, Int64}} where T<:AbstractAlgebra.FieldElem","page":"Power Series Tools","title":"StructuralIdentifiability.ps_matrix_homlinear_de","text":"ps_matrix_homlinear_de(A, Y0, prec)\n\nInput:\n\nA - a square matrix with entries in a univariate power series ring\nY0 - a square invertible matrix over the base field\n\nOutput: \n\nmatrix Y such that Y' = AY up to precision of A - 1 and Y(0) = Y0\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_matrix_inv","page":"Power Series Tools","title":"StructuralIdentifiability.ps_matrix_inv","text":"ps_matrix_inv(M, prec)\n\nInput:\n\nM - a square matrix with entries in a univariate power series ring     it is assumed that M(0) is invertible and all entries having the same precision\nprec - an integer, precision, if -1 then defaults to precision of M\n\nOutput:\n\nthe inverse of M computed up to prec\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_matrix_linear_de-Union{Tuple{T}, Tuple{AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:T}}, Tuple{AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{T}}, AbstractAlgebra.MatElem{<:T}, Int64}} where T<:AbstractAlgebra.FieldElem","page":"Power Series Tools","title":"StructuralIdentifiability.ps_matrix_linear_de","text":"ps_matrix_linear_de(A, B, Y0, prec)\n\nInput:\n\nA, B - square matrices with entries in a univariate power series ring\nY0 - a matrix over the base field with the rows number the same as A\n\nOutput:\n\nmatrix Y such that Y' = AY + B up to precision of A - 1 and Y(0) = Y0\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_matrix_log-Tuple{AbstractAlgebra.MatElem{<:AbstractAlgebra.AbsSeriesElem{<:AbstractAlgebra.FieldElem}}}","page":"Power Series Tools","title":"StructuralIdentifiability.ps_matrix_log","text":"ps_matrix_log(M)\n\nInput:\n\nM - a square matrix with entries in a univariate power series ring     it is assumed that M(0) is the identity\n\nOutput: \n\nthe natural log of M\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/power_series_utils/#StructuralIdentifiability.ps_ode_solution-Union{Tuple{P}, Tuple{T}, Tuple{Vector{P}, Dict{P, T}, Dict{P, Vector{T}}, Int64}} where {T<:AbstractAlgebra.FieldElem, P<:AbstractAlgebra.MPolyElem{T}}","page":"Power Series Tools","title":"StructuralIdentifiability.ps_ode_solution","text":"ps_ode_solution(equations, ic, inputs, prec)\n\nInput:\n\nequations - a system of the form A(x u mu)x - B(x u mu) = 0,               where A is a generically nonsingular square matrix. Assumption: A is nonzero at zero\nic - initial conditions for x's (dictionary)\ninputs - power series for inputs represented as arrays (dictionary)\nprec - precision of the solution\n\nOutput: \n\npower series solution of the system\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary:-Electrical-Components","page":"Electrical Components","title":"ModelingToolkitStandardLibrary: Electrical Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"CurrentModule = ModelingToolkitStandardLibrary.Electrical","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Electrical-Utilities","page":"Electrical Components","title":"Electrical Utilities","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"Pin\nOnePort\nDigitalPin","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Pin","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Pin","text":"Pin(; name)\n\nA pin in an analog circuit.\n\nStates:\n\nv(t): [V] The voltage at this pin\ni(t): [A] The current passing through this pin\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.OnePort","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.OnePort","text":"OnePort(; name, v_start=0.0, i_start=0.0)\n\nComponent with two electrical pins p and n and current i flows from p to n.\n\nStates:\n\nv(t): [V] The voltage across component p.v - n.v\ni(t): [A] The current passing through positive pin\n\nParameters:\n\nv_start: [V] Initial voltage across the component\ni_start: [A] Initial current through the component\n\nConnectors:\n\np Positive pin\nn Negative pin\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.DigitalPin","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.DigitalPin","text":"DigitalPin(; name)\n\nA pin in a digital circuit.\n\nStates:\n\nv(t): [V] The voltage at this pin\ni(t): [A] The current passing through this pin\nval(t): The binary value of the pin at this point. A voltage from 0V to 0.8V is a binary value of 0.\n\nA voltage in the range 2.0V to 5.0V is 1. Any other value is X.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Analog-Components","page":"Electrical Components","title":"Analog Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"Ground\nResistor\nConductor\nCapacitor\nInductor\nIdealOpAmp","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Ground","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Ground","text":"Ground(; name)\n\nGround node with the potential of zero and connector g. Every circuit must have one ground node.\n\nConnectors:\n\ng\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Resistor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Resistor","text":"Resistor(; name, R)\n\nCreates an ideal Resistor following Ohm's Law.\n\nStates:\n\nSee OnePort\n\nConnectors:\n\np Positive pin\nn Negative pin\n\nParameters:\n\nR: [Ω] Resistance\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Conductor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Conductor","text":"Conductor(;name, G)\n\nCreates an ideal conductor.\n\nStates:\n\nSee OnePort\n\nConnectors:\n\np Positive pin\nn Negative pin\n\nParameters:\n\nG: [S] Conductance\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Capacitor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Capacitor","text":"Capacitor(; name, C)\n\nCreates an ideal capacitor.\n\nStates:\n\nv(t): [V] The voltage across the capacitor, given by D(v) ~ p.i / C\n\nConnectors:\n\np Positive pin\nn Negative pin\n\nParameters:\n\nC: [F] Capacitance\nv_start: [V] Initial voltage of capacitor\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.Inductor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.Inductor","text":"Inductor(; name, L)\n\nCreates an ideal Inductor.\n\nStates:\n\nSee OnePort\n\nConnectors:\n\np Positive pin\nn Negative pin\n\nParameters:\n\nL: [H] Inductance\ni_start: [A] Initial current through inductor\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.IdealOpAmp","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.IdealOpAmp","text":"IdealOpAmp(; name)\n\nIdeal operational amplifier (norator-nullator pair). The ideal OpAmp is a two-port. The left port is fixed to v1 = 0 and i1 = 0 (nullator). At the right port both any voltage v2 and any current i2 are possible (norator).\n\nStates:\n\nSee TwoPort\n\nConnectors:\n\np1 Positive pin (left port)\np2 Positive pin (right port)\nn1 Negative pin (left port)\nn2 Negative pin (right port)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Analog-Sensors","page":"Electrical Components","title":"Analog Sensors","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"CurrentSensor\nPotentialSensor\nVoltageSensor\nPowerSensor\nMultiSensor","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.CurrentSensor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.CurrentSensor","text":"CurrentSensor(; name)\n\nCreates a circuit component that measures the current flowing through it. Analogous to an ideal ammeter.\n\nStates:\n\ni(t): [A] Current through the sensor\n\nConnectors:\n\np Positive pin\nn Negative pin\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.PotentialSensor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.PotentialSensor","text":"PotentialSensor(; name)\n\nCreates a circuit component which measures the potential at a pin.\n\nStates:\n\nphi(t): [V] The measured potential at this point\n\nConnectors:\n\np Pin at which potential is to be measured\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.VoltageSensor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.VoltageSensor","text":"VoltageSensor(; name)\n\nCreates a circuit component that measures the voltage across it. Analogous to an ideal voltmeter.\n\nStates:\n\nv(t): [V] The voltage difference form positive to negative pin p.v - n.v\n\nConnectors:\n\np Positive pin\nn Negative pin\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.PowerSensor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.PowerSensor","text":"PowerSensor(; name)\n\nCombines a VoltageSensor and a CurrentSensor to measure the power being consumed by a circuit.\n\nStates:\n\npower(t): [W] The power being consumed, given by the product of voltage and current.\nSee VoltageSensor\nSee CurrentSensor\n\nConnectors:\n\npc Corresponds to the p pin of the CurrentSensor\nnc Corresponds to the n pin of the CurrentSensor\npv Corresponds to the p pin of the VoltageSensor\nnv Corresponds to the n pin of the VoltageSensor\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#ModelingToolkitStandardLibrary.Electrical.MultiSensor","page":"Electrical Components","title":"ModelingToolkitStandardLibrary.Electrical.MultiSensor","text":"MultiSensor(; name)\n\nCombines a VoltageSensor and a CurrentSensor.\n\nStates:\n\nv(t): [V] The voltage across the VoltageSensor\ni(t): [A] The current across the CurrentSensor\n\nConnectors:\n\npc Corresponds to the p pin of the CurrentSensor\nnc Corresponds to the n pin of the CurrentSensor\npv Corresponds to the p pin of the VoltageSensor\nnv Corresponds to the n pin of the VoltageSensor\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Analog-Sources","page":"Electrical Components","title":"Analog Sources","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"ConstantVoltage\nSineVoltage\nStepVoltage\nRampVoltage\nSquareVoltage\nTriangularVoltage\nCosineVoltage\nExpSineVoltage\nConstantCurrent\nSineCurrent\nStepCurrent\nRampCurrent\nSquareCurrent\nTriangularCurrent\nCosineCurrent\nExpSineCurrent","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Digital-Gates","page":"Electrical Components","title":"Digital Gates","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"Not\nAnd\nNand\nOr\nNor\nXor\nXnor","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Digital-Components","page":"Electrical Components","title":"Digital Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"HalfAdder\nFullAdder\nMUX\nDEMUX\nEncoder\nDecoder","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/#Digital-Sources","page":"Electrical Components","title":"Digital Sources","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/electrical/","page":"Electrical Components","title":"Electrical Components","text":"PulseDiff\nSet\nReset\nPulse","category":"page"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#Classical-Basis-Layers","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"The following basis are helper functions for easily building arrays of the form [f0(x), ..., f{n-1}(x)], where f is the corresponding function of the basis (e.g, Chebyshev Polynomials, Legendre Polynomials, etc.)","category":"page"},{"location":"modules/DiffEqFlux/layers/BasisLayers/","page":"Classical Basis Layers","title":"Classical Basis Layers","text":"ChebyshevBasis\nSinBasis\nCosBasis\nFourierBasis\nLegendreBasis\nPolynomialBasis","category":"page"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.ChebyshevBasis","page":"Classical Basis Layers","title":"DiffEqFlux.ChebyshevBasis","text":"Constructs a Chebyshev basis of the form [T{0}(x), T{1}(x), ..., T{n-1}(x)] where Tj(.) is the j-th Chebyshev polynomial of the first kind.\n\nChebyshevBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.SinBasis","page":"Classical Basis Layers","title":"DiffEqFlux.SinBasis","text":"Constructs a sine basis of the form [sin(x), sin(2x), ..., sin(nx)].\n\nSinBasis(n)\n\nArguments:\n\nn: number of terms in the sine expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.CosBasis","page":"Classical Basis Layers","title":"DiffEqFlux.CosBasis","text":"Constructs a cosine basis of the form [cos(x), cos(2x), ..., cos(nx)].\n\nCosBasis(n)\n\nArguments:\n\nn: number of terms in the cosine expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.FourierBasis","page":"Classical Basis Layers","title":"DiffEqFlux.FourierBasis","text":"Constructs a Fourier basis of the form Fj(x) = j is even ? cos((j÷2)x) : sin((j÷2)x) => [F0(x), F1(x), ..., Fn(x)].\n\nFourierBasis(n)\n\nArguments:\n\nn: number of terms in the Fourier expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.LegendreBasis","page":"Classical Basis Layers","title":"DiffEqFlux.LegendreBasis","text":"Constructs a Legendre basis of the form [P{0}(x), P{1}(x), ..., P{n-1}(x)] where Pj(.) is the j-th Legendre polynomial.\n\nLegendreBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/BasisLayers/#DiffEqFlux.PolynomialBasis","page":"Classical Basis Layers","title":"DiffEqFlux.PolynomialBasis","text":"Constructs a Polynomial basis of the form [1, x, ..., x^(n-1)].\n\nPolynomialBasis(n)\n\nArguments:\n\nn: number of terms in the polynomial expansion.\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/tutorials/low_level/#Investigating-symbolic_discretize-with-the-1-D-Burgers'-Equation","page":"The symbolic_discretize Interface","title":"Investigating symbolic_discretize with the 1-D Burgers' Equation","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"Let's consider the Burgers' equation:","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"begingather*\n_t u + u _x u - (001  pi) _x^2 u = 0   quad x in -1 1 t in 0 1   \nu(0 x) = - sin(pi x)   \nu(t -1) = u(t 1) = 0  \nendgather*","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"with Physics-Informed Neural Networks. Here is an example of using the low-level API:","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n#2D PDE\neq  = Dt(u(t,x)) + u(t,x)*Dx(u(t,x)) - (0.01/pi)*Dxx(u(t,x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [u(0,x) ~ -sin(pi*x),\n       u(t,-1) ~ 0.,\n       u(t,1) ~ 0.,\n       u(t,-1) ~ u(t,1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(-1.0,1.0)]\n\n# Discretization\ndx = 0.05\n\n# Neural network\nchain = Lux.Chain(Dense(2,16,Lux.σ),Dense(16,16,Lux.σ),Dense(16,1))\nstrategy = NeuralPDE.GridTraining(dx)\n\nindvars = [t,x]\ndepvars = [u(t,x)]\n@named pde_system = PDESystem(eq,bcs,domains,indvars,depvars)\n\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = symbolic_discretize(pde_system,discretization)\n\nphi = sym_prob.phi\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bc_loss_functions))\n    return false\nend\n\nloss_functions =  [pde_loss_functions;bc_loss_functions]\n\nfunction loss_function(θ,p)\n    sum(map(l->l(θ) ,loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(prob,OptimizationOptimJL.BFGS(); callback = callback, maxiters=2000)","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"And some analysis:","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using Plots\n\nts,xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_contourf = reshape([first(phi([t,x],res.u)) for t in ts for x in xs] ,length(xs),length(ts))\nplot(ts, xs, u_predict_contourf, linetype=:contourf,title = \"predict\")\n\nu_predict = [[first(phi([t,x],res.u)) for x in xs] for t in ts ]\np1= plot(xs, u_predict[3],title = \"t = 0.1\");\np2= plot(xs, u_predict[11],title = \"t = 0.5\");\np3= plot(xs, u_predict[end],title = \"t = 1\");\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"(Image: burgers)","category":"page"},{"location":"modules/NeuralPDE/tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"(Image: burgers2)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/#auto_diff","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"note: Note\nThis tutorial assumes familiarity with DifferentialEquations.jl   If you are not familiar with DifferentialEquations.jl, please consult   the DifferentialEquations.jl documentation","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"In this tutorial we will introduce how to use local sensitivity analysis via automatic differentiation. The automatic differentiation interfaces are the most common ways that local sensitivity analysis is done. It's fairly fast and flexible, but most notably, it's a very small natural extension to the  normal differential equation solving code and is thus the easiest way to do most things.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/#Setup","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Setup","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Let's first define a differential equation we wish to solve. We will choose the Lotka-Volterra equation. This is done via DifferentialEquations.jl using:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"using DifferentialEquations\n\nfunction lotka_volterra!(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(lotka_volterra!,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5(),reltol=1e-6,abstol=1e-6)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Now let's differentiate the solution to this ODE using a few different automatic differentiation methods.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/#Forward-Mode-Automatic-Differentiation-with-ForwardDiff.jl","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Forward-Mode Automatic Differentiation with ForwardDiff.jl","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Let's say we need the derivative of the solution with respect to the initial condition u0 and its parameters p. One of the simplest ways to do this is via ForwardDiff.jl. To do this, all that one needs to do is use  the ForwardDiff.jl library to differentiate some function f which uses a differential equation solve inside of it. For example, let's say we want the derivative of the first component of ODE solution with respect to  these quantities at evenly spaced time points of dt = 1. We can compute this via:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"using ForwardDiff\n\nfunction f(x)\n    _prob = remake(prob,u0=x[1:2],p=x[3:end])\n    solve(_prob,Tsit5(),reltol=1e-6,abstol=1e-6,saveat=1)[1,:]\nend\nx = [u0;p]\ndx = ForwardDiff.jacobian(f,x)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Let's dig into what this is saying a bit. x is a vector which concatenates the initial condition and parameters, meaning that the first 2 values are the initial conditions and the last 4 are the parameters. We use the remake function to build a function f(x) which uses these new initial conditions and parameters to solve the differential equation and return the time series of the first component. ","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Then ForwardDiff.jacobian(f,x) computes the Jacobian of f with respect to x. The output dx[i,j] corresponds to the derivative of the solution of the first component at time t=j-1 with respect to x[i]. For example, dx[3,2] is the derivative of the first component of the solution at time t=1 with respect to p[1].","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"note: Note\nSince the global error is 1-2 orders of magnitude higher than the local error, we use accuracies of 1e-6 (instead of the default 1e-3) to get reasonable sensitivities","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/#Reverse-Mode-Automatic-Differentiation","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Reverse-Mode Automatic Differentiation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"The solve function is automatically compatible with AD systems like Zygote.jl and thus there is no machinery that is necessary to use other than to put solve inside of a function that is differentiated by Zygote. For example, the following computes the solution  to an ODE and computes the gradient of a loss function (the sum of the ODE's output at each  timepoint with dt=0.1) via the adjoint method:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"using Zygote, SciMLSensitivity\n\nfunction sum_of_solution(u0,p)\n  _prob = remake(prob,u0=u0,p=p)\n  sum(solve(_prob,Tsit5(),reltol=1e-6,abstol=1e-6,saveat=0.1))\nend\ndu01,dp1 = Zygote.gradient(sum_of_solution,u0,p)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Zygote.jl's automatic differentiation system is overloaded to allow SciMLSensitivity.jl to redefine the way the derivatives are computed, allowing trade-offs between numerical stability, memory, and compute performance, similar to how ODE solver algorithms are chosen. The algorithms for differentiation calculation are called AbstractSensitivityAlgorithms, or sensealgs for short. These are choosen by passing the sensealg keyword argument into solve.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Let's demonstrate this by choosing the QuadratureAdjoint sensealg for the differentiation of this system:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"function sum_of_solution(u0,p)\n  _prob = remake(prob,u0=u0,p=p)\n  sum(solve(_prob,Tsit5(),reltol=1e-6,abstol=1e-6,saveat=0.1,sensealg=QuadratureAdjoint()))\nend\ndu01,dp1 = Zygote.gradient(sum_of_solution,u0,p)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Here this computes the derivative of the output with respect to the initial condition and the the derivative with respect to the parameters respectively using the QuadratureAdjoint(). For more information on the choices of sensitivity algorithms, see the reference documentation in choosing sensitivity algorithms","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/#When-Should-You-Use-Forward-or-Reverse-Mode?","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"When Should You Use Forward or Reverse Mode?","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/differentiating_ode/","page":"Differentiating an ODE Solution with Automatic Differentiation","title":"Differentiating an ODE Solution with Automatic Differentiation","text":"Good question! The simple answer is, if you are differentiating a system of 100 equations or less, use forward-mode, otherwise reverse-mode. But it can be a lot more complicated than that! For more information, see the  reference documentation in choosing sensitivity algorithms","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#split_ode_solve","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The solvers which are available for a SplitODEProblem depend on the input linearity and number of components. Each solver has functional form (or many) that it allows.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#Implicit-Explicit-(IMEX)-ODE","page":"Split ODE Solvers","title":"Implicit-Explicit (IMEX) ODE","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The Implicit-Explicit (IMEX) ODE is a SplitODEProblem with two functions:","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"fracdudt =  f_1(tu) + f_2(tu)","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"where the first function is the stiff part and the second function is the non-stiff part (implicit integration on f1, explicit integration on f2).","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#Recommended-Methods","page":"Split ODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The recommended method in most cases is KenCarp4. In cases of extreme stiffness or for high tolerances, KenCarp3 can be a good choice. The ARKODE methods are generally inefficient and diverge unless the options are tweaked to match the problem, though for large enough PDEs the ARKODE method with linear_solver=:GMRES is a good choice.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#OrdinaryDiffEq.jl","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"SplitEuler: 1st order fully explicit method. Used for testing accuracy of splits.\nIMEXEuler : 1st order explicit Euler mixed with implicit Euler. Fixed time step only.\nCNAB2: Crank-Nicolson Adams Bashforth Order 2. Fixed time step only.\nCNLF: Crank-Nicolson Leapfrog of Order 2. Fixed time step only.\nSBDF2 : 2nd order IMEX BDF method. Fixed time step only.\nSBDF3 : 3rd order IMEX BDF method. Fixed time step only. In development.\nSBDF4 : 4th order IMEX BDF method. Fixed time step only. In development.\nKenCarp3: An A-L stable stiffly-accurate 3rd order ESDIRK method.\nKenCarp4: An A-L stable stiffly-accurate 4rd order ESDIRK method.\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKenCarp5: An A-L stable stiffly-accurate 5rd order ESDIRK method.\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#Sundials.jl","page":"Split ODE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"ARKODE: An additive Runge-Kutta method. Order between 3rd and 5th. For a list of available options, please see its ODE solver page","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#Semilinear-ODE","page":"Split ODE Solvers","title":"Semilinear ODE","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The Semilinear ODE is a SplitODEProblem with one linear operator and one nonlinear function:","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"fracdudt =  Au + f(tu)","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"See the documentation page for DiffEqOperators for details about how to define linear operators from a matrix or finite difference discretization of derivative operators.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The appropriate algorithms for this form are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/#OrdinaryDiffEq.jl-2","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"LawsonEuler - First order exponential Euler scheme. Fixed timestepping only.\nNorsettEuler - First order exponential-RK scheme. Fixed timestepping only. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development). Fixed timestepping only. Doesn't support Krylov approximation.\nETDRK2 - 2nd order exponential-RK scheme. Fixed timestepping only.\nETDRK3 - 3rd order exponential-RK scheme. Fixed timestepping only.\nETDRK4 - 4th order exponential-RK scheme. Fixed timestepping only.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4. Fixed timestepping only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"Note that the generic algorithms GenericIIF1 and GenericIIF2 allow for a choice of nlsolve.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"By default, the exponential methods cache matrix functions such as exp(dt*A) to accelerate the time stepping for small systems. For large systems, using Krylov-based versions of the methods can allow for lazy calculation of exp(dt*A)*v and similar entities, and thus improve performance.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"To tell a solver to use Krylov methods, pass krylov=true to its constructor. You can also manually set the size of the Krylov subspace by setting the m parameter, which defaults to 30. For example","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"LawsonEuler(krylov=true, m=50)","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"constructs a Lawson-Euler method which uses a size-50 Krylov subspace. Note that m only sets an upper bound to the Krylov subspace size. If a convergence criterion is met (determined by the reltol of the integrator), \"happy breakdown\" will occur and the Krylov subspace will only be constructed partially.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"For more advanced control over the Krylov algorithms, you can change the length of the incomplete orthogonalization procedure (IOP) [1] by setting the iop parameter in the constructor. By default, IOP is turned off and full Arnoldi iteration is used. Note that if the linear operator is hermitian, then the Lanczos algorithm will always be used and IOP setting is ignored.","category":"page"},{"location":"modules/DiffEqDocs/solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"page"},{"location":"modules/StructuralIdentifiability/#StructuralIdentifiability.jl","page":"Home","title":"StructuralIdentifiability.jl","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"StructuralIdentifiability.jl is a comprehensive toolbox for assessing identifiability parameters.","category":"page"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"This documentation contains information about the functionality of the package as well as examples of use cases.","category":"page"},{"location":"modules/StructuralIdentifiability/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"To install StructuralIdentifiability.jl, use the Julia package manager:","category":"page"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"StructuralIdentifiability\")","category":"page"},{"location":"modules/StructuralIdentifiability/#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"@article{structidjl,\n  author  = {Dong, R. and Goodbrake, C. and Harrington, H. and Pogudin G.},\n  title   = {Differential elimination for dynamical models via projections with applications to structural identifiability},\n  journal = {preprint},\n  url     = {https://arxiv.org/abs/2111.00991},\n  year    = {2021}\n}","category":"page"},{"location":"modules/StructuralIdentifiability/#Feature-Summary","page":"Home","title":"Feature Summary","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"StructuralIdentifiability.jl can assess local and global identifiability of ODE models. In addition to these straightforward identifiability queries on individual parameters, the package is able to distinguish between single- and multi-experiment identifiability.","category":"page"},{"location":"modules/StructuralIdentifiability/#Feature-List","page":"Home","title":"Feature List","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"Local identifiability checks\nGlobal identifiability checks\nAssessment of identifiable functions of parameters","category":"page"},{"location":"modules/StructuralIdentifiability/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to StructuralIdentifiability.\nThere are a few community forums:\nThe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#ensemble","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Performing Monte Carlo simulations, solving with a predetermined set of initial conditions, and GPU-parallelizing a parameter search all fall under the ensemble simulation interface. This interface allows one to declare a template DEProblem to parallelize, tweak the template in trajectories for many trajectories, solve each in parallel batches, reduce the solutions down to specific answers, and compute summary statistics on the results.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Performing-an-Ensemble-Simulation","page":"Parallel Ensemble Simulations","title":"Performing an Ensemble Simulation","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/#Building-a-Problem","page":"Parallel Ensemble Simulations","title":"Building a Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To perform a simulation on an ensemble of trajectories, define a EnsembleProblem. The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleProblem(prob::DEProblem;\n                output_func = (sol,i) -> (sol,false),\n                prob_func= (prob,i,repeat)->(prob),\n                reduction = (u,data,I)->(append!(u,data),false),\n                u_init = [], safetycopy = prob_func !== DEFAULT_PROB_FUNC)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"output_func: The function determines what is saved from the solution to the output array. Defaults to saving the solution itself. The output is (out,rerun) where out is the output and rerun is a boolean which designates whether to rerun.\nprob_func: The function by which the problem is to be modified. prob is the problem, i is the unique id 1:trajectories for the problem, and repeat is the iteration of the repeat. At first it is 1, but if rerun was true this will be 2, 3, etc. counting the number of times problem i has been repeated.\nreduction: This function determines how to reduce the data in each batch. Defaults to appending the data from the batches. The second part of the output determines whether the simulation has converged. If true, the simulation will exit early. By default, this is always false.\nsafetycopy: Determines whether a safety deepcopy is called on the prob before the prob_func. By default this is true for any user-given prob_func, as without this, modifying the arguments of something in the prob_func, such as parameters or caches stored within the user function, are not necessarily thread-safe. If you know that your function is thread-safe, then setting this to false can improve performance when used with threads. For nested problems, e.g., SDE problems with custom noise processes, deepcopy might be insufficient. In such cases use a custom prob_func.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"One can specify a function prob_func which changes the problem. For example:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function prob_func(prob,i,repeat)\n  @. prob.u0 = randn()*prob.u0\n  prob\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"modifies the initial condition for all of the problems by a standard normal random number (a different random number per simulation). Notice that since problem types are immutable, it uses .=. Otherwise, one can just create a new problem type:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function prob_func(prob,i,repeat)\n  @. prob.u0 = u0_arr[i]\n  prob\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"If your function is a ParameterizedFunction, you can do similar modifications to prob.f to perform a parameter search. The output_func is a reduction function. It's arguments are the generated solution and the unique index for the run. For example, if we wish to only save the 2nd coordinate at the end of each solution, we can do:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"output_func(sol,i) = (sol[end,2],false)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Thus the ensemble simulation would return as its data an array which is the end value of the 2nd dependent variable for each of the runs.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Solving-the-Problem","page":"Parallel Ensemble Simulations","title":"Solving the Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"sim = solve(prob,alg,ensemblealg,kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The keyword arguments take in the arguments for the common solver interface and will pass them to the differential equation solver. The ensemblealg is optional, and will default to EnsembleThreads(). The special keyword arguments to note are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"trajectories: The number of simulations to run. This argument is required.\nbatch_size : The size of the batches on which the reductions are applies. Defaults to trajectories.\npmap_batch_size: The size of the pmap batches. Default is  batch_size÷100 > 0 ? batch_size÷100 : 1","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#EnsembleAlgorithms","page":"Parallel Ensemble Simulations","title":"EnsembleAlgorithms","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The choice of ensemble algorithm allows for control over how the multiple trajectories are handled. Currently, the ensemble algorithm types are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleSerial() - No parallelism\nEnsembleThreads() - The default. This uses multithreading. It's local (single computer, shared memory) parallelism only. Fastest when the trajectories are quick.\nEnsembleDistributed() - Uses pmap internally. It will use as many processors as you have Julia processes. To add more processes, use addprocs(n). See Julia's documentation for more details. Recommended for the case when each trajectory calculation isn't \"too quick\" (at least about a millisecond each?).\nEnsembleSplitThreads() - This uses threading on each process, splitting the problem into nprocs() even parts. This is for solving many quick trajectories on a multi-node machine. It's recommended you have one process on each node.\nEnsembleGPUArray() - Requires installing and using DiffEqGPU. This uses a GPU for computing the ensemble with hyperparallelism. It will automatically recompile your Julia functions to the GPU. A standard GPU sees a 5x performance increase over a 16 core Xeon CPU. However, there are limitations on what functions can auto-compile in this fashion, please see the DiffEqGPU README for more details","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For example, EnsembleThreads() is invoked by:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"solve(ensembleprob,alg,EnsembleThreads();trajectories=1000)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Solution-Type","page":"Parallel Ensemble Simulations","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The resulting type is a EnsembleSimulation, which includes the array of solutions.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Plot-Recipe","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"There is a plot recipe for a AbstractEnsembleSimulation which composes all of the plot recipes for the component solutions. The keyword arguments are passed along. A useful argument to use is linealpha which will change the transparency of the plots. An additional argument is idxs which allows you to choose which components of the solution to plot. For example, if the differential equation is a vector of 9 values, idxs=1:2:9 will plot only the solutions of the odd components. An other additional argument is zcolors (an alias of marker_z) which allows you to pass a zcolor for each series. For details about zcolor see the Series documentation for Plots.jl.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Analyzing-an-Ensemble-Experiment","page":"Parallel Ensemble Simulations","title":"Analyzing an Ensemble Experiment","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Analysis tools are included for generating summary statistics and summary plots for a EnsembleSimulation.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To use this functionality, import the analysis module via:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using DifferentialEquations.EnsembleAnalysis","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(or more directly SciMLBase.EnsembleAnalysis).","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Time-steps-vs-time-points","page":"Parallel Ensemble Simulations","title":"Time steps vs time points","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For the summary statistics, there are two types. You can either summarize by time steps or by time points. Summarizing by time steps assumes that the time steps are all the same time point, i.e. the integrator used a fixed dt or the values were saved using saveat. Summarizing by time points requires interpolating the solution.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Analysis-at-a-time-step-or-time-point","page":"Parallel Ensemble Simulations","title":"Analysis at a time step or time point","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"get_timestep(sim,i) # Returns an iterator of each simulation at time step i\nget_timepoint(sim,t) # Returns an iterator of each simulation at time point t\ncomponentwise_vectors_timestep(sim,i) # Returns a vector of each simulation at time step i\ncomponentwise_vectors_timepoint(sim,t) # Returns a vector of each simulation at time point t","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Summary-Statistics","page":"Parallel Ensemble Simulations","title":"Summary Statistics","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/#Single-Time-Statistics","page":"Parallel Ensemble Simulations","title":"Single Time Statistics","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for time steps are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timestep_mean(sim,i) # Computes the mean of each component at time step i\ntimestep_median(sim,i) # Computes the median of each component at time step i\ntimestep_quantile(sim,q,i) # Computes the quantile q of each component at time step i\ntimestep_meanvar(sim,i)  # Computes the mean and variance of each component at time step i\ntimestep_meancov(sim,i,j) # Computes the mean at i and j, and the covariance, for each component\ntimestep_meancor(sim,i,j) # Computes the mean at i and j, and the correlation, for each component\ntimestep_weighted_meancov(sim,W,i,j) # Computes the mean at i and j, and the weighted covariance W, for each component","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for time points are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timepoint_mean(sim,t) # Computes the mean of each component at time t\ntimepoint_median(sim,t) # Computes the median of each component at time t\ntimepoint_quantile(sim,q,t) # Computes the quantile q of each component at time t\ntimepoint_meanvar(sim,t) # Computes the mean and variance of each component at time t\ntimepoint_meancov(sim,t1,t2) # Computes the mean at t1 and t2, the covariance, for each component\ntimepoint_meancor(sim,t1,t2) # Computes the mean at t1 and t2, the correlation, for each component\ntimepoint_weighted_meancov(sim,W,t1,t2) # Computes the mean at t1 and t2, the weighted covariance W, for each component","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Full-Timeseries-Statistics","page":"Parallel Ensemble Simulations","title":"Full Timeseries Statistics","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Additionally, the following functions are provided for analyzing the full timeseries. The mean and meanvar versions return a DiffEqArray which can be directly plotted. The meancov and meancor return a matrix of tuples, where the tuples are the (mean_t1,mean_t2,cov or cor).","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for the time steps are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_steps_mean(sim) # Computes the mean at each time step\ntimeseries_steps_median(sim) # Computes the median at each time step\ntimeseries_steps_quantile(sim,q) # Computes the quantile q at each time step\ntimeseries_steps_meanvar(sim) # Computes the mean and variance at each time step\ntimeseries_steps_meancov(sim) # Computes the covariance matrix and means at each time step\ntimeseries_steps_meancor(sim) # Computes the correlation matrix and means at each time step\ntimeseries_steps_weighted_meancov(sim) # Computes the weighted covariance matrix and means at each time step","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for the time points are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_point_mean(sim,ts) # Computes the mean at each time point in ts\ntimeseries_point_median(sim,ts) # Computes the median at each time point in ts\ntimeseries_point_quantile(sim,q,ts) # Computes the quantile q at each time point in ts\ntimeseries_point_meanvar(sim,ts) # Computes the mean and variance at each time point in ts\ntimeseries_point_meancov(sim,ts) # Computes the covariance matrix and means at each time point in ts\ntimeseries_point_meancor(sim,ts) # Computes the correlation matrix and means at each time point in ts\ntimeseries_point_weighted_meancov(sim,ts) # Computes the weighted covariance matrix and means at each time point in ts","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#EnsembleSummary","page":"Parallel Ensemble Simulations","title":"EnsembleSummary","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The EnsembleSummary type is included to help with analyzing the general summary statistics. Two constructors are provided:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleSummary(sim;quantiles=[0.05,0.95])\nEnsembleSummary(sim,ts;quantiles=[0.05,0.95])","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The first produces a (mean,var) summary at each time step. As with the summary statistics, this assumes that the time steps are all the same. The second produces a (mean,var) summary at each time point t in ts. This requires the ability to interpolate the solution. Quantile is used to determine the qlow and qhigh quantiles at each timepoint. It defaults to the 5% and 95% quantiles.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Plot-Recipe-2","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The EnsembleSummary comes with a plot recipe for visualizing the summary statistics. The extra keyword arguments are:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"idxs: the solution components to plot. Defaults to plotting all components.\nerror_style: The style for plotting the error. Defaults to ribbon. Other choices are :bars for error bars and :none for no error bars.\nci_type : Defaults to :quantile which has (qlow,qhigh) quantiles whose limits were determined when constructing the EnsembleSummary. Gaussian CI 1.96*(standard error of the mean) can be set using ci_type=:SEM.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"One useful argument is fillalpha which controls the transparency of the ribbon around the mean.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Example-1:-Solving-an-ODE-With-Different-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Example 1: Solving an ODE With Different Initial Conditions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/#Random-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Random Initial Conditions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Let's test the sensitivity of the linear ODE to its initial condition. To do this, we would like to solve the linear ODE 100 times and plot what the trajectories look like. Let's start by opening up some extra processes so that way the computation will be parallelized. Here we will choose to use distributed parallelism which means that the required functions must be made available to all processes. This can be achieved with @everywhere macro:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using Distributed\nusing DifferentialEquations\nusing Plots\n\naddprocs()\n@everywhere using DifferentialEquations","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now let's define the linear ODE which is our base problem:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For our ensemble simulation, we would like to change the initial condition around. This is done through the prob_func. This function takes in the base problem and modifies it to create the new problem that the trajectory actually solves. Here we will take the base problem, multiply the initial condition by a rand(), and use that for calculating the trajectory:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"@everywhere function prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we build and solve the EnsembleProblem with this base problem and prob_func:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,Tsit5(),EnsembleDistributed(),trajectories=100)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can use the plot recipe to plot what the 100 ODEs look like:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plotly()\nplot(sim,linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_carlo_plot)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We note that if we wanted to find out what the initial condition was for a given trajectory, we can retrieve it from the solution. sim[i] returns the ith solution object. sim[i].prob is the problem that specific trajectory solved, and sim[i].prob.u0 would then be the initial condition used in the ith trajectory.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note: If the problem has callbacks, the functions for the condition and affect! must be named functions (not anonymous functions).","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Using-multithreading","page":"Parallel Ensemble Simulations","title":"Using multithreading","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The previous ensemble simulation can also be parallelized using a multithreading approach, which will make use of the different cores within a single computer. Because the memory is shared across the different threads, it is not necessary to use the @everywhere macro. Instead, the same problem can be implemented simply as:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using DifferentialEquations\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend\nensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=100)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The number of threads to be used has to be defined outside of Julia, in the environmental variable JULIA_NUM_THREADS (see Julia's documentation for details).","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Pre-Determined-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Pre-Determined Initial Conditions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In many cases, you may already know what initial conditions you want to use. This can be specified by the i argument of the prob_func. This i is the unique index of each trajectory. So, if we have trajectories=100, then we have i as some index in 1:100, and it's different for each trajectory.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"So, if we wanted to use a grid of evenly spaced initial conditions from 0 to 1, we could simply index the linspace type:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"initial_conditions = range(0, stop=1, length=100)\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=initial_conditions[i])\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"It's worth noting that if you run this code successfully, there will be no visible output.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Example-2:-Solving-an-SDE-with-Different-Parameters","page":"Parallel Ensemble Simulations","title":"Example 2: Solving an SDE with Different Parameters","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Let's solve the same SDE but with varying parameters. Let's create a Lotka-Volterra system with multiplicative noise. Our Lotka-Volterra system will have as its drift component:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function f(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For our noise function we will use multiplicative noise:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function g(du,u,p,t)\n  du[1] = p[3]*u[1]\n  du[2] = p[4]*u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we build the SDE with these functions:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"p = [1.5,1.0,0.1,0.1]\nprob = SDEProblem(f,g,[1.0,1.0],(0.0,10.0),p)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"This is the base problem for our study. What would like to do with this experiment is keep the same parameters in the deterministic component each time, but very the parameters for the amount of noise using 0.3rand(2) as our parameters. Once again, we do this with a prob_func, and here we modify the parameters in prob.p:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"# `p` is a global variable, referencing it would be type unstable. \n# Using a let block defines a small local scope in which we can\n# capture that local `p` which isn't redefined anywhere in that local scope.\n# This allows it to be type stable.\nprob_func = let p=p\n    (prob,i,repeat) -> begin\n        x = 0.3rand(2)\n        remake(prob, p=[p[1], p[2], x])\n    end\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we solve the problem 10 times and plot all of the trajectories in phase space:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,SRIW1(),trajectories=10)\nusing Plots; plotly()\nusing Plots; plot(sim,linealpha=0.6,color=:blue,vars=(0,1),title=\"Phase Space Plot\")\nplot!(sim,linealpha=0.6,color=:red,vars=(0,2),title=\"Phase Space Plot\")","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_lotka_blue)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can then summarize this information with the mean/variance bounds using a EnsembleSummary plot. We will take the mean/quantile at every 0.1 time units and directly plot the summary:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim,0:0.1:10)\npyplot() # Note that plotly does not support ribbon plots\nplot(summ,fillalpha=0.5)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_carlo_quantile)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that here we used the quantile bounds, which default to [0.05,0.95] in the EnsembleSummary constructor. We can change to standard error of the mean bounds using ci_type=:SEM in the plot recipe.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Example-3:-Using-the-Reduction-to-Halt-When-Estimator-is-Within-Tolerance","page":"Parallel Ensemble Simulations","title":"Example 3: Using the Reduction to Halt When Estimator is Within Tolerance","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In this problem we will solve the equation just as many times as needed to get the standard error of the mean for the final time point below our tolerance 0.5. Since we only care about the endpoint, we can tell the output_func to discard the rest of the data.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function output_func(sol,i)\n  last(sol)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Our prob_func will simply randomize the initial condition:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\n\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Our reduction function will append the data from the current batch to the previous batch, and declare convergence if the standard error of the mean is calculated as sufficiently small:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function reduction(u,batch,I)\n  u = append!(u,batch)\n  finished = (var(u) / sqrt(last(I))) / mean(u) < 0.5\n  u, finished\nend","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Then we can define and solve the problem:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"prob2 = EnsembleProblem(prob,prob_func=prob_func,output_func=output_func,reduction=reduction,u_init=Vector{Float64}())\nsim = solve(prob2,Tsit5(),trajectories=10000,batch_size=20)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Since batch_size=20, this means that every 20 simulations, it will take this batch, append the results to the previous batch, calculate (var(u)/sqrt(last(I)))/mean(u), and if that's small enough, exit the simulation. In this case, the simulation exits only after 20 simulations (i.e. after calculating the first batch). This can save a lot of time!","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In addition to saving time by checking convergence, we can save memory by reducing between batches. For example, say we only care about the mean at the end once again. Instead of saving the solution at the end for each trajectory, we can instead save the running summation of the endpoints:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function reduction(u,batch,I)\n  u+sum(batch),false\nend\nprob2 = EnsembleProblem(prob,prob_func=prob_func,output_func=output_func,reduction=reduction,u_init=0.0)\nsim2 = solve(prob2,Tsit5(),trajectories=100,batch_size=20)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"this will sum up the endpoints after every 20 solutions, and save the running sum. The final result will have sim2.u as simply a number, and thus sim2.u/100 would be the mean.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/#Example-4:-Using-the-Analysis-Tools","page":"Parallel Ensemble Simulations","title":"Example 4: Using the Analysis Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In this example we will show how to analyze a EnsembleSolution. First, let's generate a 10 solution Monte Carlo experiment. For our problem we will use a 4x2 system of linear stochastic differential equations:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function f(du,u,p,t)\n  for i = 1:length(u)\n    du[i] = 1.01*u[i]\n  end\nend\nfunction σ(du,u,p,t)\n  for i in 1:length(u)\n    du[i] = .87*u[i]\n  end\nend\nprob = SDEProblem(f,σ,ones(4,2)/2,(0.0,1.0)) #prob_sde_2Dlinear","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To solve this 10 times, we use the EnsembleProblem constructor and solve with trajectories=10. Since we wish to compare values at the timesteps, we need to make sure the steps all hit the same times. Thus we set adaptive=false and explicitly give a dt.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"prob2 = EnsembleProblem(prob)\nsim = solve(prob2,SRIW1(),dt=1//2^(3),trajectories=10,adaptive=false)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that if you don't do the timeseries_steps calculations, this code is compatible with adaptive timestepping. Using adaptivity is usually more efficient!","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can compute the mean and the variance at the 3rd timestep using:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m,v = timestep_meanvar(sim,3)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"or we can compute the mean and the variance at the t=0.5 using:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m,v = timepoint_meanvar(sim,0.5)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can get a series for the mean and the variance at each time step using:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m_series,v_series = timeseries_steps_meanvar(sim)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"or at chosen values of t:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ts = 0:0.1:1\nm_series = timeseries_point_mean(sim,ts)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that these mean and variance series can be directly plotted. We can compute covariance matrices similarly:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_steps_meancov(sim) # Use the time steps, assume fixed dt\ntimeseries_point_meancov(sim,0:1//2^(3):1,0:1//2^(3):1) # Use time points, interpolate","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For general analysis, we can build a EnsembleSummary type.","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"will summarize at each time step, while","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim,0.0:0.1:1.0)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"will summarize at the 0.1 time points using the interpolations. To visualize the results we can plot it. Since there are 8 components to the differential equation, this can get messy, so let's only plot the 3rd component:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;idxs=3)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_ribbon)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can change to errorbars instead of ribbons and plot two different indices:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;idxs=(3,5),error_style=:bars)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_bars)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Or we can simply plot the mean of every component over time:","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;error_style=:none)","category":"page"},{"location":"modules/DiffEqDocs/features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_means)","category":"page"},{"location":"modules/NBodySimulator/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"modules/NBodySimulator/examples/","page":"Examples","title":"Examples","text":"Further examples can be found here.","category":"page"},{"location":"modules/DiffEqDocs/basics/common_solver_opts/#solver_options","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/common_solver_opts/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"solve(prob::SciMLBase.DEProblem,args...;kwargs...)","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#deepsplitting","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Modules = [HighDimPDE]\nPages   = [\"DeepSplitting.jl\"]","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#HighDimPDE.DeepSplitting","page":"The DeepSplitting algorithm","title":"HighDimPDE.DeepSplitting","text":"DeepSplitting(nn, K=1, opt = ADAM(0.01), λs = nothing, mc_sample =  NoSampling())\n\nDeep splitting algorithm.\n\nArguments\n\nnn: a Flux.Chain, or more generally a functor.\nK: the number of Monte Carlo integrations.\nopt: optimiser to be use. By default, Flux.ADAM(0.01).\nλs: the learning rates, used sequentially. Defaults to a single value taken from opt.\nmc_sample::MCSampling : sampling method for Monte Carlo integrations of the non local term. Can be UniformSampling(a,b), NormalSampling(σ_sampling, shifted), or NoSampling (by default).\n\nExample\n\nhls = d + 50 # hidden layer size\nd = 10 # size of the sample\n\n# Neural network used by the scheme\nnn = Flux.Chain(Dense(d, hls, tanh),\n                Dense(hls,hls,tanh),\n                Dense(hls, 1, x->x^2))\n\nalg = DeepSplitting(nn, K=10, opt = ADAM(), λs = [5e-3,1e-3],\n                    mc_sample = UniformSampling(zeros(d), ones(d)) )\n\n\n\n\n\n","category":"type"},{"location":"modules/HighDimPDE/DeepSplitting/#HighDimPDE.solve-Tuple{PIDEProblem, DeepSplitting, Any}","page":"The DeepSplitting algorithm","title":"HighDimPDE.solve","text":"solve(prob::PIDEProblem,\n    alg::DeepSplitting,\n    dt;\n    batch_size = 1,\n    abstol = 1f-6,\n    verbose = false,\n    maxiters = 300,\n    use_cuda = false,\n    cuda_device = nothing,\n    verbose_rate = 100)\n\nReturns a PIDESolution object.\n\nArguments\n\nmaxiters: number of iterations per time step. Can be a tuple, where maxiters[1] is used for the training of the neural network used in the first time step (which can be long) and maxiters[2] is used for the rest of the time steps.\nbatch_size : the batch size.\nabstol : threshold for the objective function under which the training is stopped.\nverbose : print training information.\nverbose_rate : rate for printing training information (every verbose_rate iterations).\nuse_cuda : set to true to use CUDA.\ncuda_device : integer, to set the CUDA device used in the training, if use_cuda == true.\n\n\n\n\n\n","category":"method"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm reformulates the PDE as a stochastic learning problem.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The algorithm relies on two main ideas:","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"the approximation of the solution u by a parametric function bf u^theta. This function is generally chosen as a (Feedforward) Neural Network, as it is a universal approximator.\nthe training of bf u^theta by simulated stochastic trajectories of particles, through the link between linear PDEs and the expected trajectory of associated Stochastic Differential Equations (SDEs), explicitly stated by the Feynman Kac formula.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#The-general-idea","page":"The DeepSplitting algorithm","title":"The general idea 💡","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Consider the PDE","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx)) tag1","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"with initial conditions u(0 x) = g(x), where u colon R^d to R. ","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#Local-Feynman-Kac-formula","page":"The DeepSplitting algorithm","title":"Local Feynman Kac formula","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting solves the PDE iteratively over small time intervals by using an approximate Feynman-Kac representation locally.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"More specifically, considering a small time step dt = t_n+1 - t_n one has that","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx mathbbE left f(t X_T - t_n u(t_nX_T - t_n))(t_n+1 - t_n) + u(t_n X_T - t_n)  X_T - t_n+1right tag3","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"One can therefore use Monte Carlo integrations to approximate the expectations","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx frac1textbatch_sizesum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + (t_n+1 - t_n)sum_k=1^K big f(t_n X_T - t_n^(j) u(t_nX_T - t_n^(j))) big right","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#Reformulation-as-a-learning-problem","page":"The DeepSplitting algorithm","title":"Reformulation as a learning problem","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The DeepSplitting algorithm approximates u(t_n+1 x) by a parametric function bf u^theta_n(x). It is advised to let this function be a neural network bf u_theta equiv NN_theta as they are universal approximators.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"For each time step t_n, the DeepSplitting algorithm ","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Generates the particle trajectories X^x (j) satisfying Eq. (2) over the whole interval 0T.\nSeeks bf u_n+1^theta  by minimising the loss function","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"L(theta) = bf u^theta_n+1(X_T - t_n) - left f(t X_T - t_n-1 bf u_n-1(X_T - t_n-1))(t_n - t_n-1) + bf u_n-1(X_T - t_n-1) right ","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"This way the PDE approximation problem is decomposed into a sequence of separate learning problems. In HighDimPDE.jl the right parameter combination theta is found by iteratively minimizing L using stochastic gradient descent.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\nTo solve with DeepSplitting, one needs to provide to solvedt\nbatch_size\nmaxiters: the number of iterations for minimising the loss function\nabstol: the absolute tolerance for the loss function\nuse_cuda: if you have a Nvidia GPU, recommended.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#Solving-point-wise-or-on-a-hypercube","page":"The DeepSplitting algorithm","title":"Solving point-wise or on a hypercube","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/#Pointwise","page":"The DeepSplitting algorithm","title":"Pointwise","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting allows to obtain u(tx) on a single point  x in Omega with the keyword x.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(g, f, μ, σ, x, tspan)","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#Hypercube","page":"The DeepSplitting algorithm","title":"Hypercube","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Yet more generally, one wants to solve Eq. (1) on a d-dimensional cube ab^d. This is offered by HighDimPDE.jl with the keyworkd x0_sample.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"prob = PIDEProblem(g, f, μ, σ, x, tspan, x0_sample = x0_sample)","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Internally, this is handled by assigning a random variable as the initial point of the particles, i.e.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"X_t^xi = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + xi","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"where xi a random variable uniformly distributed over ab^d. This way, the neural network is trained on the whole interval ab^d instead of a single point.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#Nonlocal-PDEs","page":"The DeepSplitting algorithm","title":"Nonlocal PDEs","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"DeepSplitting can solve for non-local reaction diffusion equations of the type","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"partial_t u = mu(x) nabla_x u + frac12 sigma^2(x) Delta u + int_Omegaf(xy u(tx) u(ty))dy","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"The non-localness is handled by a Monte Carlo integration.","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"u(t_n+1 X_T - t_n+1) approx sum_j=1^textbatch_size left u(t_n X_T - t_n^(j)) + frac(t_n+1 - t_n)Ksum_k=1^K big f(t X_T - t_n^(j) Y_X_T - t_n^(j)^(k) u(t_nX_T - t_n^(j)) u(t_nY_X_T - t_n^(j)^(k))) big right","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"tip: Tip\nIn practice, if you have a non-local model you need to provide the sampling method and the number K of MC integration through the keywords mc_sample and K. alg = DeepSplitting(nn, opt = opt, mc_sample = mc_sample, K = 1)mc_sample can be whether UniformSampling(a, b) or NormalSampling(σ_sampling, shifted).","category":"page"},{"location":"modules/HighDimPDE/DeepSplitting/#References","page":"The DeepSplitting algorithm","title":"References","text":"","category":"section"},{"location":"modules/HighDimPDE/DeepSplitting/","page":"The DeepSplitting algorithm","title":"The DeepSplitting algorithm","text":"Boussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\nBeck, C., Becker, S., Cheridito, P., Jentzen, A., Neufeld, A., Deep splitting method for parabolic PDEs. arXiv (2019)\nHan, J., Jentzen, A., E, W., Solving high-dimensional partial differential equations using deep learning. arXiv (2018)","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary:-Blocks","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary: Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"CurrentModule = ModelingToolkitStandardLibrary.Blocks","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#Utility-Blocks","page":"Basic Blocks","title":"Utility Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"RealInput\nRealOutput\nSISO","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.RealInput","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.RealInput","text":"RealInput(;name, nin, u_start)\n\nConnector with one input signal of type Real.\n\nParameters:\n\nnin=1: Number of inputs\nu_start=0: Initial value for u  \n\nStates:\n\nu: Value of of the connector; if nin=1 this is a scalar\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.RealOutput","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.RealOutput","text":"RealOutput(;name, nout, u_start)\n\nConnector with one output signal of type Real.\n\nParameters:\n\nnout=1: Number of inputs\nu_start=0: Initial value for u  \n\nStates:\n\nu: Value of of the connector; if nout=1 this is a scalar\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.SISO","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.SISO","text":"SISO(;name, u_start=0.0, y_start=0.0)\n\nSingle Input Single Output continuous control block.\n\nParameters:\n\nu_start: Initial value for the input\ny_start: Initial value for the output\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#Math-Blocks","page":"Basic Blocks","title":"Math Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"Gain\nMatrixGain\nSum\nFeedback\nAdd\nAdd3\nProduct\nDivision\nStaticNonLinearity\nAbs\nSign\nSqrt\nSin\nCos\nTan\nAsin\nAcos\nAtan\nAtan2\nSinh\nCosh\nTanh\nExp\nLog\nLog10","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Gain","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Gain","text":"Gain(k; name)\n\nOutput the product of a gain value with the input signal.\n\nParameters:\n\nk: Scalar gain\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.MatrixGain","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.MatrixGain","text":"MatrixGain(K::AbstractArray; name)\n\nOutput the product of a gain matrix with the input signal vector.\n\nParameters:\n\nK: Matrix gain\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sum","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sum","text":"Sum(n::Int; name)\n\nOutput the sum of the elements of the input port vector.\n\nParameters:\n\nn: Input port dimension\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Feedback","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Feedback","text":"Feedback(;name)\n\nOutput difference between reference input (input1) and feedback input (input2).\n\nConnectors:\n\ninput1\ninput2\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Add","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Add","text":"Add(;name, k1=1, k2=1)\n\nOutput the sum of the two scalar inputs.\n\nParameters:\n\nk1: Gain for first input\nk2: Gain for second input\n\nConnectors:\n\ninput1\ninput2\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Add3","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Add3","text":"Add(;name, k1=1, k2=1,k3=1)\n\nOutput the sum of the three scalar inputs.\n\nParameters:\n\nk1: Gain for first input\nk2: Gain for second input\nk3: Gain for third input\n\nConnectors:\n\ninput1\ninput2\ninput3\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Product","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Product","text":"Product(;name)\n\nOutput product of the two inputs.\n\nConnectors:\n\ninput1\ninput2\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Division","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Division","text":"Division(;name)\n\nOutput first input divided by second input.\n\nConnectors:\n\ninput1\ninput2\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.StaticNonLinearity","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.StaticNonLinearity","text":"StaticNonLinearity(func ;name)\n\nApplies the given function to the input. \n\nIf the given function is not composed of simple core methods (e.g. sin, abs, ...), it has to be registered via @register_symbolic func(u)\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Abs","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Abs","text":"Abs(;name)\n\nOutput the absolute value of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sign","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sign","text":"Sign(;name)\n\nOutput the sign of the input\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sqrt","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sqrt","text":"Sqrt(;name)\n\nOutput the square root of the input (input >= 0 required).\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sin","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sin","text":"Sin(;name)\n\nOutput the sine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Cos","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Cos","text":"Cos(;name)\n\nOutput the cosine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Tan","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Tan","text":"Tan(;name)\n\nOutput the tangent of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Asin","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Asin","text":"Asin(;name)\n\nOutput the arc sine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Acos","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Acos","text":"Acos(;name)\n\nOutput the arc cosine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Atan","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Atan","text":"Atan(;name)\n\nOutput the arc tangent of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Atan2","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Atan2","text":"Atan2(;name)\n\nOutput the arc tangent of the input.\n\nConnectors:\n\ninput1\ninput2\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sinh","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sinh","text":"Sinh(;name)\n\nOutput the hyperbolic sine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Cosh","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Cosh","text":"Cosh(;name)\n\nOutput the hyperbolic cosine of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Tanh","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Tanh","text":"Tanh(;name)\n\nOutput the hyperbolic tangent of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Exp","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Exp","text":"Exp(;name)\n\nOutput the exponential (base e) of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Log","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Log","text":"Log(;name)\n\nOutput the natural (base e) logarithm of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Log10","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Log10","text":"Log10(;name)\n\nOutput the base 10 logarithm of the input.\n\nConnectors:\n\nSee StaticNonLinearity\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#Source-Blocks","page":"Basic Blocks","title":"Source Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"Constant\nSine\nCosine\nContinuousClock\nRamp\nStep\nExpSine","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Constant","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Constant","text":"Generate constant signal.\n\nParameters:\n\nk: Constant output value\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Sine","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Sine","text":"Generate sine signal.\n\nParameters:\n\nfrequency: [Hz] Frequency of sine wave\namplitude: Amplitude of sine wave\nphase: [rad] Phase of sine wave\noffset: Offset of output signal\nstart_time: [s] Output y = offset for t < start_time\nsmooth:  If true, returns a smooth wave. Defaults to false            It uses a smoothing factor of δ=1e-5\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.ContinuousClock","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.ContinuousClock","text":"Generate current time signal.\n\nParameters:\n\noffset: Offset of output signal\nstart_time: [s] Output y = offset for t < start_time\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Ramp","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Ramp","text":"Generate ramp signal.\n\nParameters:\n\nheight: Height of ramp\nduration: [s] Duration of ramp (= 0.0 gives a Step)\noffset: Offset of output signal\nstart_time: [s] Output y = offset for t < start_time\nsmooth:  If true, returns a smooth wave. Defaults to false            It uses a smoothing factor of δ=1e-5\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Step","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Step","text":"Step(;name, height=1, offset=0, start_time=0, duration=Inf, smooth=true)\n\nGenerate step signal.\n\nParameters:\n\nheight: Height of step\noffset: Offset of output signal\nstart_time: [s] Output y = offset for t < start_time and thereafter offset+height.\nduration: [s] If duration < Inf is supplied, the output will revert to offset after duration seconds.\nsmooth:  If true, returns a smooth wave. Defaults to false            It uses a smoothing factor of δ=1e-5\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.ExpSine","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.ExpSine","text":"Generate exponentially damped sine signal.\n\nParameters:\n\nfrequency: [Hz] Frequency of sine wave\namplitude: Amplitude of sine wave\ndamping: [1/s] Damping coefficient of sine wave\nphase: [rad] Phase of sine wave\noffset: Offset of output signal\nstart_time: [s] Output y = offset for t < start_time\nsmooth:  If true, returns a smooth wave. Defaults to false            It uses a smoothing factor of δ=1e-5\n\nConnectors:\n\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#Nonlinear-Blocks","page":"Basic Blocks","title":"Nonlinear Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"Limiter\nDeadZone\nSlewRateLimiter","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Limiter","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Limiter","text":"Limiter(;name, y_max, y_min=y_max > 0 ? -y_max : -Inf)\n\nLimit the range of a signal.\n\nParameters:\n\ny_max: Maximum of output signal\ny_min: Minimum of output signal\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.DeadZone","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.DeadZone","text":"DeadZone(; name, u_max, u_min=-u_max)\n\nThe DeadZone block defines a region of zero output. If the input is within u_min ... u_max, the output is zero. Outside of this zone, the output is a linear function of the input with a slope of 1.\n\n       y▲\n        │     /\n        │    /\n  u_min │   /\n─────|──┼──|───────► u\n    /   │   u_max\n   /    │\n  /     │\n\nParameters:\n\nu_max: Upper limit of dead zone\nu_min: Lower limit of dead zone\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.SlewRateLimiter","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.SlewRateLimiter","text":"SlewRateLimiter(;name, rising=1, falling=-rising, Td=0.001, y_start=0.0)\n\nLimits the slew rate of a signal.\n\nParameters:\n\nrising: Maximum rising slew rate\nfalling: Maximum falling slew rate\nTd: [s] Derivative time constant\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#Continuous-Blocks","page":"Basic Blocks","title":"Continuous Blocks","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/","page":"Basic Blocks","title":"Basic Blocks","text":"Integrator\nDerivative\nFirstOrder\nSecondOrder\nStateSpace\nPI\nLimPI\nPID\nLimPID","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Integrator","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Integrator","text":"Integrator(;name, k=1, x_start=0.0)\n\nOutputs y = ∫k*u dt, corresponding to the transfer function 1/s.\n\nConnectors:\n\ninput\noutput\n\nParameters:\n\nk: Gain of integrator\nx_start: Initial value of integrator\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.Derivative","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.Derivative","text":"Derivative(; name, k=1, T, x_start=0.0)\n\nOutputs an approximate derivative of the input. The transfer function of this block is\n\nk       k     \n─ - ──────────\nT    2 ⎛    1⎞\n    T ⋅⎜s + ─⎟\n       ⎝    T⎠\n\nand a state-space realization is given by ss(-1/T, 1/T, -k/T, k/T) where T is the time constant of the filter. A smaller T leads to a more ideal approximation of the derivative.\n\nParameters:\n\nk: Gain\nT: [s] Time constants (T>0 required; T=0 is ideal derivative block)\nx_start: Initial value of state\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.FirstOrder","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.FirstOrder","text":"FirstOrder(; name, k=1, T, x_start=0.0)\n\nA first-order filter with a single real pole in s = -T and gain k. The transfer function is given by Y(s)/U(s) =\n\n   k   \n───────\nsT + 1\n\nParameters:\n\nk: Gain\nT: [s] Time constants (T>0 required)\nx_start: Initial value of state\n\nConnectors:\n\ninput\noutput\n\nSee also SecondOrder\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.SecondOrder","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.SecondOrder","text":"SecondOrder(; name, k=1, w, d, x_start=0.0, xd_start=0.0)\n\nA second-order filter with gain k, a bandwidth of w rad/s and relative damping d. The transfer function is given by Y(s)/U(s) =\n\n      k*w^2   \n─────────────────\ns² + 2d*w*s + w^2\n\nCritical damping corresponds to d=1, which yields the fastest step response without overshoot, d < 1results in an under-damped filter whiled > 1results in an over-damped filter.d = 1/√2` corresponds to a Butterworth filter of order 2 (maximally flat frequency response).\n\nParameters:\n\nk: Gain\nw: Angular frequency\nd: Damping\nx_start: Initial value of state (output)\nxd_start: Initial value of derivative of state (output)\n\nConnectors:\n\ninput\noutput\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.StateSpace","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.StateSpace","text":"StateSpace(A, B, C, D=0; x_start=zeros(size(A,1)), name)\n\nA linear, time-invariant state-space system on the form.\n\nẋ = Ax + Bu\ny = Cx + Du\n\nTransfer functions can also be simulated by converting them to a StateSpace form.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.PI","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.PI","text":"PI(;name, k=1, T, x_start=0.0)\n\nTextbook version of a PI-controller without actuator saturation and anti-windup measure.\n\nParameters:\n\nk: Gain\nT: [s] Integrator time constant (T>0 required)\nx_start: Initial value for the integrator\n\nConnectors:\n\nerr_input\nctr_output\n\nSee also LimPI\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.LimPI","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.LimPI","text":"LimPI(;name, k=1, T, u_max=1, u_min=-u_max, Ta)\n\nText-book version of a PI-controller with actuator saturation and anti-windup measure.\n\nParameters:\n\nk: Gain\nT: [s] Integrator time constant (T>0 required)\nTa: [s] Tracking time constant (Ta>0 required)\nx_start: Initial value for the integrator\n\nConnectors:\n\nerr_input\nctr_output\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.PID","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.PID","text":"PID(;name, k=1, Ti=false, Td=false, Nd=10, xi_start=0, xd_start=0)\n\nText-book version of a PID-controller without actuator saturation and anti-windup measure.\n\nParameters:\n\nk: Gain\nTi: [s] Integrator time constant (Ti>0 required). If set to false no integral action is used.\nTd: [s] Derivative time constant (Td>0 required). If set to false no derivative action is used.\nNd: [s] Time constant for the derivative approximation (Nd>0 required; Nd=0 is ideal derivative).\nx_start: Initial value for the integrator.\nxd_start: Initial value for the derivative state.\n\nConnectors:\n\nerr_input\nctr_output\n\nSee also LimPID\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/blocks/#ModelingToolkitStandardLibrary.Blocks.LimPID","page":"Basic Blocks","title":"ModelingToolkitStandardLibrary.Blocks.LimPID","text":"LimPID(; k, Ti=false, Td=false, wp=1, wd=1, Ni, Nd=12, u_max=Inf, u_min=-u_max, gains = false, name)\n\nProportional-Integral-Derivative (PID) controller with output saturation, set-point weighting and integrator anti-windup.\n\nThe equation for the control signal is roughly\n\nk(ep + 1/Ti * ∫e + 1/Td * d/dt(ed))\ne = u_r - u_y\nep = wp*u_r - u_y\ned = wd*u_r - u_y\n\nwhere the transfer function for the derivative includes additional filtering, see ? Derivative for more details.\n\nParameters:\n\nk: Proportional gain\nTi: [s] Integrator time constant. Set to false to turn off integral action.\nTd: [s] Derivative time constant. Set to false to turn off derivative action.\nwp: [0,1] Set-point weighting in the proportional part.\nwd: [0,1] Set-point weighting in the derivative part.\nNd: [1/s] Derivative limit, limits the derivative gain to Nd/Td. Reasonable values are ∈ [8, 20]. A higher value gives a better approximation of an ideal derivative at the expense of higher noise amplification.\nNi: Ni*Ti controls the time constant Ta of anti-windup tracking. A common (default) choice is Ta = √(Ti*Td) which is realized by Ni = √(Td / Ti). Anti-windup can be effectively turned off by setting Ni = Inf.\n\ngains: Ifgains = true,TiandTdwill be interpreted as gains with a fundamental PID transfer function on parallel formki=Ti, kd=Td, k + ki/s + kd*s`\n\nConnectors:\n\nreference\nmeasurement\nctr_output\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/features/low_dep/#Low-Dependency-Usage","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"DifferentialEquations.jl is a large library containing the functionality of many different solver and addon packages. However in many cases you may want to cut down on the size of the dependency and only use the parts of the the library which are essential to your application. This is possible due to SciML's modular package structure.","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/#Common-Example:-Using-only-OrdinaryDiffEq.jl","page":"Low Dependency Usage","title":"Common Example: Using only OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"One common example is using only the ODE solvers OrdinaryDiffEq.jl. The solvers all reexport SciMLBase.jl (which holds the problem and solution types) and so OrdinaryDiffEq.jl is all that's needed. Thus replacing","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using DifferentialEquations","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"with","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"#Add the OrdinaryDiffEq Package first!\n#using Pkg; Pkg.add(\"OrdinaryDiffEq\")\nusing OrdinaryDiffEq","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"will work if these are the only features you are using.","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/#Generalizing-the-Idea","page":"Low Dependency Usage","title":"Generalizing the Idea","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"In general, you will always need SciMLBase.jl, since it defines all of the fundamental types, but the solvers will automatically reexport it. For solvers, you typically only need that solver package. So SciMLBase+Sundials, SciMLBase+LSODA, etc. will get you the common interface with that specific solver setup. SciMLBase.jl is a very lightweight dependency, so there is no issue here! For PDEs, you normally need SciMLBase+DiffEqPDEBase in addition to the solver package.","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"For the addon packages, you will normally need SciMLBase, the solver package you choose, and the addon package. So for example, for parameter estimation you would likely want SciMLBase+OrdinaryDiffEq+DiffEqParamEstim. If you aren't sure which package a specific command is from, then use @which. For example, from the parameter estimation docs we have:","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using DifferentialEquations\nfunction f(du,u,p,t)\n  dx = p[1]*u[1] - u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())\nt = collect(range(0, stop=10, length=200))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\nusing RecursiveArrayTools\ndata = convert(Array,randomized)\ncost_function = build_loss_objective(prob,t,data,Tsit5(),maxiters=10000)","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"If we wanted to know where build_loss_objective came from, we can do:","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"@which build_loss_objective(prob,t,data,Tsit5(),maxiters=10000)\n\n(::DiffEqParamEstim.#kw##build_loss_objective)(::Array{Any,1}, ::DiffEqParamEstim.#build_loss_objective, prob::SciMLBase.DEProblem, t, data, alg)","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"This says it's in the DiffEqParamEstim.jl package. Thus in this case, we could have done","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using OrdinaryDiffEq, DiffEqParamEstim","category":"page"},{"location":"modules/DiffEqDocs/features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"instead of the full using DifferentialEquations. Note that due to the way Julia dependencies work, any internal function in the package will work. The only dependencies you need to explicitly using are the functions you are specifically calling. Thus this method can be used to determine all of the DiffEq packages you are using.","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/#Relation-to-NamedTuples","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"","category":"section"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"Julia's Base has NamedTuples in v0.7+. They are constructed as:","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"p = (σ = 10.0,ρ = 28.0,β = 8/3)","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"and they support p[1] and p.σ as well. The LVector, SLVector, LArray and SLArray constructors also support named tuples as their arguments:","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"julia> LVector((a=1, b=2))\n2-element LArray{Int64,1,(:a, :b)}:\n 1\n 2\n\njulia> SLVector((a=1, b=2))\n2-element SLArray{Tuple{2},1,(:a, :b),Int64}:\n 1\n 2\n\njulia> LArray((2,2), (a=1, b=2, c=3, d=4))\n2×2 LArray{Int64,2,(:a, :b, :c, :d)}:\n 1  3\n 2  4\n\njulia> SLArray{Tuple{2,2}}((a=1, b=2, c=3, d=4))\n2×2 SLArray{Tuple{2,2},2,(:a, :b, :c, :d),Int64}:\n 1  3\n 2  4","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"Converting to a named tuple from a labelled array x is available using convert(NamedTuple, x). Furthermore, pairs(x) creates an iterator that is functionally the same as pairs(convert(NamedTuple, x)), yielding :label => x.label for each label of the array.","category":"page"},{"location":"modules/LabelledArrays/NamedTuples_relation/","page":"Relation to NamedTuples","title":"Relation to NamedTuples","text":"There are some crucial differences between a labelled array and a named tuple. Labelled arrays can have any dimensions while  named tuples are always 1D. A named tuple can have different types on each element, while an SLArray can only have one element type and furthermore it has the actions of a static vector. As a result SLArray has less element type information, which  improves compilation speed while giving more vector functionality than a NamedTuple. LArray also only has a single element type and, unlike a named tuple, is mutable.","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#NonlinearSystem","page":"NonlinearSystem","title":"NonlinearSystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#System-Constructors","page":"NonlinearSystem","title":"System Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"NonlinearSystem","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#ModelingToolkit.NonlinearSystem","page":"NonlinearSystem","title":"ModelingToolkit.NonlinearSystem","text":"struct NonlinearSystem <: AbstractTimeIndependentSystem\n\nA nonlinear system of equations.\n\nFields\n\neqs\nVector of equations defining the system.\nstates\nUnknown variables.\nps\nParameters.\nvar_to_name\nArray variables.\nobserved\njac\nJacobian matrix. Note: this field will not be defined until calculate_jacobian is called on the system.\n\nname\nName: the name of the system. These are required to have unique names.\n\nsystems\nsystems: The internal systems\n\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nconnector_type\ntype: type of the system\n\nconnections\nconnections: connections in a system\n\ntearing_state\ntearing_state: cache for intermediate tearing state\n\nsubstitutions\nsubstitutions: substitutions generated by tearing.\n\nExamples\n\n@variables x y z\n@parameters σ ρ β\n\neqs = [0 ~ σ*(y-x),\n       0 ~ x*(ρ-z)-y,\n       0 ~ x*y - β*z]\n@named ns = NonlinearSystem(eqs, [x,y,z],[σ,ρ,β])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Composition-and-Accessor-Functions","page":"NonlinearSystem","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"get_eqs(sys) or equations(sys): The equations that define the nonlinear system.\nget_states(sys) or states(sys): The set of states in the nonlinear system.\nget_ps(sys) or parameters(sys): The parameters of the nonlinear system.","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Transformations","page":"NonlinearSystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"structural_simplify\r\nalias_elimination\r\ntearing","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#ModelingToolkit.StructuralTransformations.tearing","page":"NonlinearSystem","title":"ModelingToolkit.StructuralTransformations.tearing","text":"tearing(sys; simplify=false)\n\nTear the nonlinear equations in system. When simplify=true, we simplify the new residual residual equations after tearing. End users are encouraged to call structural_simplify instead, which calls this function internally.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Analyses","page":"NonlinearSystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"ModelingToolkit.isaffine\r\nModelingToolkit.islinear","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Applicable-Calculation-and-Generation-Functions","page":"NonlinearSystem","title":"Applicable Calculation and Generation Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"calculate_jacobian\r\ngenerate_jacobian\r\njacobian_sparsity","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Problem-Constructors","page":"NonlinearSystem","title":"Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"NonlinearProblem","category":"page"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#SciMLBase.NonlinearProblem","page":"NonlinearSystem","title":"SciMLBase.NonlinearProblem","text":"Defines a nonlinear system problem. Documentation Page: https://nonlinearsolve.sciml.ai/dev/basics/NonlinearProblem/\n\nMathematical Specification of a Nonlinear Problem\n\nTo define a Nonlinear Problem, you simply need to give the function f which defines the nonlinear system:\n\nf(up) = 0\n\nand an initial guess u₀ of where f(u,p)=0. f should be specified as f(u,p) (or in-place as f(du,u,p)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher-dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nNonlinearProblem(f::NonlinearFunction,u0,p=NullParameters();kwargs...)\nNonlinearProblem{isinplace}(f,u0,p=NullParameters();kwargs...)\n\nisinplace optionally sets whether the function is in-place or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the NonlinearFunctions page.\n\nFields\n\nf: The function in the problem.\nu0: The initial guess for the steady state.\np: The parameters for the problem. Defaults to NullParameters.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/#Torn-Problem-Constructors","page":"NonlinearSystem","title":"Torn Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/NonlinearSystem/","page":"NonlinearSystem","title":"NonlinearSystem","text":"BlockNonlinearProblem","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkit-Standard-Library:-Mechanical-Components","page":"Mechanical Components","title":"ModelingToolkit Standard Library: Mechanical Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#Rotational-Components","page":"Mechanical Components","title":"Rotational Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/","page":"Mechanical Components","title":"Mechanical Components","text":"CurrentModule = ModelingToolkitStandardLibrary.Mechanical.Rotational","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#Rotational-Utils","page":"Mechanical Components","title":"Rotational Utils","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/","page":"Mechanical Components","title":"Mechanical Components","text":"Flange\nSupport\nPartialCompliantWithRelativeStates\nPartialElementaryOneFlangeAndSupport2\nPartialElementaryTwoFlangesAndSupport2\nPartialCompliant","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Flange","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Flange","text":"Flange(;name)\n\n1-dim. rotational flange of a shaft.\n\nStates:\n\nphi: [rad] Absolute rotation angle of flange\ntau: [N.m] Cut torque in the flange\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Support","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Support","text":"Support(;name)\n\nSupport/housing of a 1-dim. rotational shaft\n\nStates:\n\nphi: [rad] Absolute rotation angle of the support/housing\ntau: [N.m] Cut torque in the support/housing\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialCompliantWithRelativeStates","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialCompliantWithRelativeStates","text":"PartialCompliantWithRelativeStates(;name, phi_rel_start=0.0, tau_start=0.0)\n\nPartial model for the compliant connection of two rotational 1-dim. shaft flanges where the relative angle and speed are used as preferred states\n\nParameters:\n\nphi_rel_start: [rad] Initial relative rotation angle\nw_rel_start: [rad/s] Initial relative angular velocity (= der(phi_rel))\na_rel_start: [rad/s²] Initial relative angular acceleration (= der(w_rel))\ntau_start: [N.m] Initial torque between flanges\n\nStates:\n\nphi_rel: [rad] Relative rotation angle (= flangeb.phi - flangea.phi)\nw_rel: [rad/s] Relative angular velocity (= der(phi_rel))\na_rel: [rad/s²] Relative angular acceleration (= der(w_rel))\ntau: [N.m] Torque between flanges (= flange_b.tau)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialElementaryOneFlangeAndSupport2","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialElementaryOneFlangeAndSupport2","text":"PartialElementaryOneFlangeAndSupport2(;name, use_support=false)\n\nPartial model for a component with one rotational 1-dim. shaft flange and a support used for textual modeling, i.e., for elementary models\n\nParameters:\n\nuse_support: If support flange enabled, otherwise implicitly grounded\n\nStates:\n\nphi_support: [rad] Absolute angle of support flange\"\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialElementaryTwoFlangesAndSupport2","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialElementaryTwoFlangesAndSupport2","text":"PartialElementaryTwoFlangesAndSupport2(;name, use_support=false)\n\nPartial model for a component with two rotational 1-dim. shaft flanges and a support used for textual modeling, i.e., for elementary models\n\nParameters:\n\nuse_support: If support flange enabled, otherwise implicitly grounded\n\nStates:\n\nphi_support: [rad] Absolute angle of support flange\"\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialCompliant","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.PartialCompliant","text":"PartialCompliant(;name, phi_rel_start=0.0, tau_start=0.0)\n\nPartial model for the compliant connection of two rotational 1-dim. shaft flanges.\n\nParameters:\n\nphi_rel_start: [rad] Initial relative rotation angle\ntau_start: [N.m] Initial torque between flanges\n\nStates:\n\nphi_rel: [rad] Relative rotation angle (= flangeb.phi - flangea.phi)\ntau: [N.m] Torque between flanges (= flange_b.tau)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#Rotational-Core-Components","page":"Mechanical Components","title":"Rotational Core Components","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/","page":"Mechanical Components","title":"Mechanical Components","text":"Fixed\nInertia\nSpring\nDamper\nIdealGear","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Fixed","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Fixed","text":"Fixed(;name, phi0=0.0)\n\nFlange fixed in housing at a given angle.\n\nParameters:\n\nphi0: [rad] Fixed offset angle of housing\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Inertia","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Inertia","text":"Inertia(;name, J, phi_start=0.0, w_start=0.0, a_start=0.0)\n\n1D-rotational component with inertia.\n\nParameters:\n\nJ: [kg·m²] Moment of inertia \nphi_start: [rad] Initial value of absolute rotation angle of component \nw_start: [rad/s] Initial value of absolute angular velocity of component\na_start: [rad/s²] Initial value of absolute angular acceleration of component\n\nStates:\n\nphi: [rad] Absolute rotation angle of component \nw: [rad/s] Absolute angular velocity of component (= der(phi)) \na: [rad/s²] Absolute angular acceleration of component (= der(w)) \n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Spring","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Spring","text":"Spring(;name, c, phi_rel0=0.0)\n\nLinear 1D rotational spring\n\nParameters:\n\nc: [N.m/rad] Spring constant\nphi_rel0: Unstretched spring angle\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Damper","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Damper","text":"Damper(;name, d)\n\nLinear 1D rotational damper\n\nParameters:\n\nd: [N.m.s/rad] Damping constant\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.IdealGear","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.IdealGear","text":"IdealGear(;name, ratio, use_support=false)\n\nIdeal gear without inertia.\n\nThis element characterizes any type of gear box which is fixed in the ground and which has one driving shaft and one driven shaft.\n\nParameters:\n\nratio: Transmission ratio (flangea.phi/flangeb.phi)\nuse_support: If support flange enabled, otherwise implicitly grounded\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#Rotational-Sources","page":"Mechanical Components","title":"Rotational Sources","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/","page":"Mechanical Components","title":"Mechanical Components","text":"Torque","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/API/mechanical/#ModelingToolkitStandardLibrary.Mechanical.Rotational.Torque","page":"Mechanical Components","title":"ModelingToolkitStandardLibrary.Mechanical.Rotational.Torque","text":"Torque(;name)\n\nInput signal acting as external torque on a flange\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/models/financial/#financial_models","page":"Financial Models","title":"Financial Models","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"The financial models functionality is provided by DiffEqFinancial.jl and helps the user build and solve the differential equation based financial models.","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#SDE-Model-Library","page":"Financial Models","title":"SDE Model Library","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"The following constructors create SDEProblem types which can be solved using the stochastic differential equation solvers.","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#HestonProblem","page":"Financial Models","title":"HestonProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"dS = μSdt + sqrtvSdW_1 \ndv = κ(Θ-v)dt + σsqrtvdW_2 \ndW_1 dW_2 = ρ dt","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"HestonProblem(μ,κ,Θ,σ,ρ,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#GeneralizedBlackScholesProblem","page":"Financial Models","title":"GeneralizedBlackScholesProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"d ln S(t) = left(r(t) - q(t) - fracΘ(tS)^22right)dt + σ dW_t","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"Solves for log S(t). Constructor:","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"GeneralizedBlackScholesProblem(r,q,Θ,σ,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#BlackScholesProblem","page":"Financial Models","title":"BlackScholesProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"d ln S(t) = left(r(t) - fracΘ(tS)^22right)dt + σ dW_t","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"Solves for log S(t). Constructor:","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"BlackScholesProblem(r,Θ,σ,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#ExtendedOrnsteinUhlenbeckProblem","page":"Financial Models","title":"ExtendedOrnsteinUhlenbeckProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"dx = a(b(t)-x)dt + σ dW_t","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"ExtendedOrnsteinUhlenbeckProblem(a,b,σ,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/models/financial/#OrnsteinUhlenbeckProblem","page":"Financial Models","title":"OrnsteinUhlenbeckProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"dx = a(r-x)dt + σ dW_t","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"modules/DiffEqDocs/models/financial/","page":"Financial Models","title":"Financial Models","text":"OrnsteinUhlenbeckProblem(a,r,σ,u0,tspan)","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#The-PDE-Definition-Interface","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"While ODEs u = f(upt) can be defined by a user-function f, for PDEs the function form can be different for every PDE. How many functions, and how many inputs? This can always change. The SciML ecosystem solves this problem by using ModelingToolkit.jl to define PDESystem, a high-level symbolic description of the PDE to be consumed by other packages.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"The vision for the common PDE interface is that a user should only have to specify their PDE once, mathematically, and have instant access to everything as simple as a finite difference method with constant grid spacing, to something as complex as a distributed multi-GPU discontinuous Galerkin method.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"The key to the common PDE interface is a separation of the symbolic handling from the numerical world. All of the discretizers should not \"solve\" the PDE, but instead be a conversion of the mathematical specification to a numerical problem. Preferably, the transformation should be to another ModelingToolkit.jl AbstractSystem via a symbolic_discretize dispatch, but in some cases this cannot be done or will not be performant. Thus in some cases, only a discretize definition is given to a AbstractSciMLProblem, with symbolic_discretize simply providing diagnostic or lower level information about the construction process.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"These elementary problems, such as solving linear systems Ax=b, solving nonlinear systems f(x)=0, ODEs, etc. are all defined by SciMLBase.jl, which then numerical solvers can all target these common forms. Thus someone who works on linear solvers doesn't necessarily need to be working on a Discontinuous Galerkin or finite element library, but instead \"linear solvers that are good for matrices A with properties ...\" which are then accessible by every other discretization method in the common PDE interface.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"Similar to the rest of the AbstractSystem types, transformation and analyses functions will allow for simplifying the PDE before solving it, and constructing block symbolic functions like Jacobians.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#Constructors","page":"The PDE Definition Interface","title":"Constructors","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"ModelingToolkit.PDESystem","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#Domains-(WIP)","page":"The PDE Definition Interface","title":"Domains (WIP)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"Domains are specifying by saying indepvar in domain, where indepvar is a single or a collection of independent variables, and domain is the chosen domain type. A 2-tuple can be used to indicate an Interval. Thus forms for the indepvar can be like:","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"t ∈ (0.0,1.0)\n(t,x) ∈ UnitDisk()\n[v,w,x,y,z] ∈ VectorUnitBall(5)","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#Domain-Types-(WIP)","page":"The PDE Definition Interface","title":"Domain Types (WIP)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"Interval(a,b): Defines the domain of an interval from a to b (requires explicit","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"import from DomainSets.jl, but a 2-tuple can be used instead)","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#discretize-and-symbolic_discretize","page":"The PDE Definition Interface","title":"discretize and symbolic_discretize","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"The only functions which act on a PDESystem are the following:","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"discretize(sys,discretizer): produces the outputted AbstractSystem or AbstractSciMLProblem.\nsymbolic_discretize(sys,discretizer): produces a debugging symbolic description of the discretized problem.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#Boundary-Conditions-(WIP)","page":"The PDE Definition Interface","title":"Boundary Conditions (WIP)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/#Transformations","page":"The PDE Definition Interface","title":"Transformations","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/#Analyses","page":"The PDE Definition Interface","title":"Analyses","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/#Discretizer-Ecosystem","page":"The PDE Definition Interface","title":"Discretizer Ecosystem","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/#NeuralPDE.jl:-PhysicsInformedNN","page":"The PDE Definition Interface","title":"NeuralPDE.jl: PhysicsInformedNN","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"NeuralPDE.jl defines the PhysicsInformedNN discretizer which uses a DiffEqFlux.jl neural network to solve the differential equation.","category":"page"},{"location":"modules/SciMLBase/interfaces/PDE/#MethodOfLines.jl:-MOLFiniteDifference-(WIP)","page":"The PDE Definition Interface","title":"MethodOfLines.jl: MOLFiniteDifference (WIP)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/PDE/","page":"The PDE Definition Interface","title":"The PDE Definition Interface","text":"MethodOfLines.jl defines the MOLFiniteDifference discretizer which performs a finite difference discretization using the DiffEqOperators.jl stencils. These stencils make use of NNLib.jl for fast operations on semi-linear domains.","category":"page"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#JumpSystem","page":"JumpSystem","title":"JumpSystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#System-Constructors","page":"JumpSystem","title":"System Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/","page":"JumpSystem","title":"JumpSystem","text":"JumpSystem","category":"page"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#ModelingToolkit.JumpSystem","page":"JumpSystem","title":"ModelingToolkit.JumpSystem","text":"struct JumpSystem{U<:ArrayPartition} <: AbstractTimeDependentSystem\n\nA system of jump processes.\n\nFields\n\neqs\nThe jumps of the system. Allowable types are ConstantRateJump, VariableRateJump, MassActionJump.\n\niv\nThe independent variable, usually time.\nstates\nThe dependent variables, representing the state of the system.  Must not contain the independent variable.\nps\nThe parameters of the system. Must not contain the independent variable.\nvar_to_name\nArray variables.\nobserved\nname\nThe name of the system. . These are required to have unique names.\nsystems\nThe internal systems.\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nconnector_type\ntype: type of the system\n\nExample\n\nusing ModelingToolkit\n\n@parameters β γ\n@variables t S(t) I(t) R(t)\nrate₁   = β*S*I\naffect₁ = [S ~ S - 1, I ~ I + 1]\nrate₂   = γ*I\naffect₂ = [I ~ I - 1, R ~ R + 1]\nj₁      = ConstantRateJump(rate₁,affect₁)\nj₂      = ConstantRateJump(rate₂,affect₂)\nj₃      = MassActionJump(2*β+γ, [R => 1], [S => 1, R => -1])\n@named js      = JumpSystem([j₁,j₂,j₃], t, [S,I,R], [β,γ])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#Composition-and-Accessor-Functions","page":"JumpSystem","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/","page":"JumpSystem","title":"JumpSystem","text":"get_eqs(sys) or equations(sys): The equations that define the jump system.\nget_states(sys) or states(sys): The set of states in the jump system.\nget_ps(sys) or parameters(sys): The parameters of the jump system.\nget_iv(sys): The independent variable of the jump system.","category":"page"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#Transformations","page":"JumpSystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/","page":"JumpSystem","title":"JumpSystem","text":"structural_simplify","category":"page"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#Analyses","page":"JumpSystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/#Problem-Constructors","page":"JumpSystem","title":"Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/JumpSystem/","page":"JumpSystem","title":"JumpSystem","text":"DiscreteProblem\nJumpProblem","category":"page"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#adjoint_sense","page":"Direct Adjoint Sensitivities of Differential Equations","title":"Direct Adjoint Sensitivities of Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#First-Order-Adjoint-Sensitivities","page":"Direct Adjoint Sensitivities of Differential Equations","title":"First Order Adjoint Sensitivities","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/","page":"Direct Adjoint Sensitivities of Differential Equations","title":"Direct Adjoint Sensitivities of Differential Equations","text":"adjoint_sensitivities","category":"page"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#SciMLSensitivity.adjoint_sensitivities","page":"Direct Adjoint Sensitivities of Differential Equations","title":"SciMLSensitivity.adjoint_sensitivities","text":"adjoint_sensitivities(sol,alg;t=nothing,\n                            dgdu_discrete = nothing, dgdp_discrete = nothing,\n                            dgdu_continuous = nothing, dgdp_continuous = nothing,\n                            g=nothing,\n                            abstol=1e-6,reltol=1e-3,\n                            checkpoints=sol.t,\n                            corfunc_analytical=nothing,\n                            callback = nothing,\n                            sensealg=InterpolatingAdjoint(),\n                            kwargs...)\n\nAdjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function. It is equivalent to \"backpropagation\" or reverse-mode automatic differentiation of a differential equation.\n\nUsing adjoint_sensitivities directly let's you do three things. One it can allow you to be more efficient, since the sensitivity calculation can be done directly on a cost function, avoiding the overhead of building the derivative of the full concretized solution. It can also allow you to be more efficient by directly controlling the forward solve that is then reversed over. Lastly, it allows one to define a continuous cost function on the continuous solution, instead of just at discrete data points.\n\nwarning: Warning\nAdjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct p. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that p could be a valid   type for being the initial condition u0 of an array. This means that   many simple types, such as Tuples and NamedTuples, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining p using AbstractArray libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that p is an AbstractArray with a concrete element type.\n\nwarning: Warning\nNon-checkpointed InterpolatingAdjoint and QuadratureAdjoint sensealgs   require that the forward solution sol(t) has an accurate dense   solution unless checkpointing is used. This means that you should   not use solve(prob,alg,saveat=ts) unless checkpointing. If specific   saving is required, one should solve dense solve(prob,alg), use the   solution in the adjoint, and then sol(ts) interpolate.\n\nMathematical Definition\n\nAdjoint sensitivity analysis finds the gradient of a cost function G defined by the infinitesimal cost over the whole time period (t_0 T), given by the equation:\n\nG(up)=G(u(cdotp))=int_t_0^Tg(u(tp)pt)dt\n\nIt does so by solving the adjoint problem:\n\nfracdlambda^stardt=g_u(u(tp)p)-lambda^star(t)f_u(tu(tp)p)thinspacethinspacethinspacelambda^star(T)=0\n\nand obtaining the sensitivities through the integral:\n\nfracdGdp=int_t_0^Tlambda^star(t)f_p(t)+g_p(t)dt+lambda^star(t_0)u_p(t_0)\n\nAs defined, that cost function only has non-zero values over nontrivial intervals. However, in many cases one may want to include in the cost function loss values at discrete points, for example, matching the data at time points t. In this case, terms of g can be represented by Dirac delta functions which are then applied in the corresponding lambda^star and fracdGdp equations.\n\nFor more information, see Sensitivity Math Details.\n\nPositional Arguments\n\nsol: the solution from the forward pass of the ODE. Note that if not using a checkpointing sensitivity algorithm, then it's assumed that the (dense) interpolation of the forward solution is of sufficient accuracy for recreating the solution at any time point.\nalg: the algorithm (i.e., DiffEq solver) to use for the solution of the adjoint problem.\n\nKeyword Arguments\n\nt: the time points at which the discrete cost function is to be evaluated. This argument is only required if discrete cost functions are declared.\ng: the continuous instantaneous cost g(upt) at a given time point represented by a Julia function g(u,p,t). This argument is only required if there is a continuous instantaneous cost contribution.\ndgdu_discrete: the partial derivative g_u evaluated at the discrete (Dirac delta) times. If discrete cost values are given, then dgdu_discrete is required.\ndgdp_discrete: the partial derivative g_p evaluated at the discrete (Dirac delta) times. If discrete cost values are given, then dgdp_discrete is not required and is assumed to be zero.\ndgdu_continuous: the partial derivative g_u evaluated at times not corresponding to terms with an associated Dirac delta. If g is given, then this term is not required and will be approximated by numerical or (forward-mode) automatic differentiation (via the autodiff keyword argument in the sensealg) if this term is not given by the user.\ndgdp_continuous: the partial derivative g_p evaluated at times not corresponding to terms with an associated Dirac delta. If g is given, then this term is not required and will be approximated by numerical or (forward-mode) automatic differentiation (via the autojacvec keyword argument in the sensealg) if this term is not given by the user.\nabstol: the absolute tolerance of the adjoint solve. Defaults to 1e-3\nreltol: the relative tolerance of the adjoint solve. Defaults to 1e-3\ncheckpoints: the values to use for the checkpoints of the reverse solve, if the adjoint sensealg has checkpointing = true. Defaults to sol.t, i.e. the saved points in the sol.\ncorfunc_analytical: the function corresponding to the conversion from an Ito to a Stratanovich definition of an SDE, i.e.\n\nFor sensitivity analysis of an SDE in the Ito sense dX = a(Xt)dt + b(Xt)dW_t with conversion term - 12 b_X b, corfunc_analytical denotes b_X b. Only used if thesol.prob isa SDEProblem`. If not given, this is   computed using automatic differentiation. Note that this inside of the reverse solve SDE then implies automatic   differentiation of a function being automatic differentiated, and nested higher order automatic differentiation   has more restrictions on the function plus some performance disadvantages.\n\ncallback: callback functions to be used in the adjoint solve. Defaults to nothing.\nsensealg: the choice for what adjoint method to use for the reverse solve. Defaults to InterpolatingAdjoint(). See the sensitivity algorithms page for more details.\nkwargs: any extra keyword arguments passed to the adjoint solve.\n\nDetailed Description\n\nFor discrete adjoints where the cost functions only depend on parameters through the ODE solve itself (for example, parameter estimation with L2 loss), use:\n\ndu0,dp = adjoint_sensitivities(sol,alg;t=ts,dgdu_discrete=dg,\n                               sensealg=InterpolatingAdjoint(),\n                               checkpoints=sol.t,kwargs...)\n\nwhere alg is the ODE algorithm to solve the adjoint problem, dgdu_discrete is the jump function, sensealg is the sensitivity algorithm, and ts are the time points for data. dg is given by:\n\ndg(out,u,p,t,i)\n\nwhich is the in-place gradient of the cost functional g at time point ts[i] with u=u(t).\n\nFor continuous functionals, the form is:\n\ndu0,dp = adjoint_sensitivities(sol,alg;dgdu_continuous=dgdu,g=g,\n                               dgdp_continuous = dgdp,\n                               sensealg=InterpolatingAdjoint(),\n                               checkpoints=sol.t,kwargs...)\n\nfor the cost functional\n\ng(u,p,t)\n\nwith in-place gradient\n\ndgdu(out,u,p,t)\ndgdp(out,u,p,t)\n\nIf the gradient is omitted, i.e.\n\ndu0,dp = adjoint_sensitivities(sol,alg;g=g,kwargs...)\n\nthen we assume dgdp is zero and dgdu will be computed automatically using ForwardDiff or finite differencing, depending on the autodiff setting in the AbstractSensitivityAlgorithm. Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem.\n\nnote: Note\nMixing discrete and continuous terms in the cost function is allowed\n\nExamples\n\nExample discrete adjoints on a cost function\n\nIn this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let's solve the ODE and get a high quality continuous solution:\n\nfunction f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)\nsol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)\n\nNow let's calculate the sensitivity of the ell_2 error against 1 at evenly spaced points in time, that is:\n\nL(upt)=sum_i=1^nfracVert1-u(t_ip)Vert^22\n\nfor t_i = 05i. This is the assumption that the data is data[i]=1.0. For this function, notice we have that:\n\nbeginaligned\ndg_1=-1+u_1 \ndg_2=-1+u_2 \n quad vdots\nendaligned\n\nand thus:\n\ndg(out,u,p,t,i) = (out.=-1.0.+u)\n\nAlso, we can omit dgdp, because the cost function doesn't dependent on p. If we had data, we'd just replace 1.0 with data[i]. To get the adjoint sensitivities, call:\n\nts = 0:0.5:10\nres = adjoint_sensitivities(sol,Vern9();t=ts,dg_discrete=dg,abstol=1e-14,\n                            reltol=1e-14)\n\nThis is super high accuracy. As always, there's a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:\n\nusing ForwardDiff,Calculus,Tracker\nfunction G(p)\n  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=ts,\n              sensealg=SensitivityADPassThrough())\n  A = convert(Array,sol)\n  sum(((1 .- A).^2)./2)\nend\nG([1.5,1.0,3.0])\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])\nres4 = Tracker.gradient(G,[1.5,1.0,3.0])\nres5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])\n\nand see this gives the same values.\n\nExample controlling adjoint method choices and checkpointing\n\nIn the previous examples, all calculations were done using the interpolating method. This maximizes speed but at a cost of requiring a dense sol. If it is not possible to hold a dense forward solution in memory, then one can use checkpointing. For example:\n\nts = [0.0,0.2,0.5,0.7]\nsol = solve(prob,Vern9(),saveat=ts)\n\nCreates a non-dense solution with checkpoints at [0.0,0.2,0.5,0.7]. Now we can do:\n\nres = adjoint_sensitivities(sol,Vern9();t=ts,dg_discrete=dg,\n                            sensealg=InterpolatingAdjoint(checkpointing=true))\n\nWhen grabbing a Jacobian value during the backwards solution, it will no longer interpolate to get the value. Instead, it will start a forward solution at the nearest checkpoint to build local interpolants in a way that conserves memory. By default the checkpoints are at sol.t, but we can override this:\n\nres = adjoint_sensitivities(sol,Vern9();t=ts,dg_discrte=dg,\n                            sensealg=InterpolatingAdjoint(checkpointing=true),\n                            checkpoints = [0.0,0.5])\n\nExample continuous adjoints on an energy functional\n\nIn this case we'd like to calculate the adjoint sensitivity of the scalar energy functional:\n\nG(up)=int_0^Tfracsum_i=1^nu_i^2(t)2dt\n\nwhich is:\n\ng(u,p,t) = (sum(u).^2) ./ 2\n\nNotice that the gradient of this function with respect to the state u is:\n\nfunction dg(out,u,p,t)\n  out[1]= u[1] + u[2]\n  out[2]= u[1] + u[2]\nend\n\nTo get the adjoint sensitivities, we call:\n\nres = adjoint_sensitivities(sol,Vern9();dg_continuous=dg,g=g,abstol=1e-8,\n                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)\n\nNotice that we can check this against autodifferentiation and numerical differentiation as follows:\n\nusing QuadGK\nfunction G(p)\n  tmp_prob = remake(prob,p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)\n  res,err = quadgk((t)-> (sum(sol(t)).^2)./2,0.0,10.0,atol=1e-14,rtol=1e-10)\n  res\nend\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])\n\n\n\n","category":"function"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#Second-Order-Adjoint-Sensitivities","page":"Direct Adjoint Sensitivities of Differential Equations","title":"Second Order Adjoint Sensitivities","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/","page":"Direct Adjoint Sensitivities of Differential Equations","title":"Direct Adjoint Sensitivities of Differential Equations","text":"second_order_sensitivities\nsecond_order_sensitivity_product","category":"page"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#SciMLSensitivity.second_order_sensitivities","page":"Direct Adjoint Sensitivities of Differential Equations","title":"SciMLSensitivity.second_order_sensitivities","text":"H = secondordersensitivities(loss,prob,alg,args...;                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP())),                                kwargs...)\n\nSecond order sensitivity analysis is used for the fast calculation of Hessian matrices.\n\nwarning: Warning\nAdjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct p. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that p could be a valid   type for being the initial condition u0 of an array. This means that   many simple types, such as Tuples and NamedTuples, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining p using AbstractArray libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that p is an AbstractArray with a concrete element type.\n\nExample second order sensitivity analysis calculation\n\nusing SciMLSensitivity, OrdinaryDiffEq, ForwardDiff\nusing Test\n\nfunction lotka!(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(lotka!,u0,(0.0,10.0),p)\nloss(sol) = sum(sol)\nv = ones(4)\n\nH  = second_order_sensitivities(loss,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)\n\nArguments\n\nThe arguments for this function match adjoint_sensitivities. The only notable difference is sensealg which requires a second order sensitivity algorithm, of which currently the only choice is ForwardDiffOverAdjoint which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse. ForwardDiffOverAdjoint's positional argument just accepts a first order sensitivity algorithm.\n\n\n\n","category":"function"},{"location":"modules/SciMLSensitivity/manual/direct_adjoint_sensitivities/#SciMLSensitivity.second_order_sensitivity_product","page":"Direct Adjoint Sensitivities of Differential Equations","title":"SciMLSensitivity.second_order_sensitivity_product","text":"Hv = secondordersensitivity_product(loss,v,prob,alg,args...;                                sensealg=ForwardDiffOverAdjoint(InterpolatingAdjoint(autojacvec=ReverseDiffVJP())),                                kwargs...)\n\nSecond order sensitivity analysis product is used for the fast calculation of Hessian-vector products Hv without requiring the construction of the Hessian matrix.\n\nwarning: Warning\nAdjoint sensitivity analysis functionality requires being able to solve   a differential equation defined by the parameter struct p. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with adjoint sensitivity analysis requires that p could be a valid   type for being the initial condition u0 of an array. This means that   many simple types, such as Tuples and NamedTuples, will work as   parameters in normal contexts but will fail during adjoint differentiation.   To work around this issue for complicated cases like nested structs, look   into defining p using AbstractArray libraries such as RecursiveArrayTools.jl   or ComponentArrays.jl so that p is an AbstractArray with a concrete element type.\n\nExample second order sensitivity analysis calculation\n\nusing SciMLSensitivity, OrdinaryDiffEq, ForwardDiff\nusing Test\n\nfunction lotka!(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(lotka!,u0,(0.0,10.0),p)\nloss(sol) = sum(sol)\nv = ones(4)\n\nHv = second_order_sensitivity_product(loss,v,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)\n\nArguments\n\nThe arguments for this function match adjoint_sensitivities. The only notable difference is sensealg which requires a second order sensitivity algorithm, of which currently the only choice is ForwardDiffOverAdjoint which uses forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a faster computation than either double forward or double reverse. ForwardDiffOverAdjoint's positional argument just accepts a first order sensitivity algorithm.\n\n\n\n","category":"function"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/#Universal-Differential-Equations-for-Neural-Feedback-Control","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"You can also mix a known differential equation and a neural differential equation, so that the parameters and the neural network are estimated simultaneously!","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"We will assume that we know the dynamics of the second equation (linear dynamics), and our goal is to find a neural network that is dependent on the current state of the dynamical system that will control the second equation to stay close to 1.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"using Flux, Optimization, OptimizationPolyalgorithms, OptimizationOptimJL, \n      SciMLSensitivity, Zygote, DifferentialEquations, Plots, Random\n\nrng = Random.default_rng()\nu0 = 1.1f0\ntspan = (0.0f0, 25.0f0)\ntsteps = 0.0f0:1.0:25.0f0\n\nmodel_univ = Flux.Chain(Flux.Dense(2, 16, tanh),\n                       Flux.Dense(16, 16, tanh),\n                       Flux.Dense(16, 1))\n\n# The model weights are destructured into a vector of parameters\np_model,re = Flux.destructure(model_univ)\nn_weights = length(p_model)\n\n# Parameters of the second equation (linear dynamics)\np_system = Float32[0.5, -0.5]\n\np_all = [p_model; p_system]\nθ = Float32[u0; p_all]\n\nfunction dudt_univ!(du, u, p, t)\n    # Destructure the parameters\n    model_weights = p[1:n_weights]\n    α = p[end - 1]\n    β = p[end]\n\n    # The neural network outputs a control taken by the system\n    # The system then produces an output\n    model_control, system_output = u\n\n    # Dynamics of the control and system\n    dmodel_control = re(model_weights)(u)[1]\n    dsystem_output = α*system_output + β*model_control\n\n    # Update in place\n    du[1] = dmodel_control\n    du[2] = dsystem_output\nend\n\nprob_univ = ODEProblem(dudt_univ!, [0f0, u0], tspan, p_all)\nsol_univ = solve(prob_univ, Tsit5(),abstol = 1e-8, reltol = 1e-6)\n\nfunction predict_univ(θ)\n  return Array(solve(prob_univ, Tsit5(), u0=[0f0, θ[1]], p=θ[2:end],\n                              sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)),\n                              saveat = tsteps))\nend\n\nloss_univ(θ) = sum(abs2, predict_univ(θ)[2,:] .- 1)\nl = loss_univ(θ)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"list_plots = []\niter = 0\ncallback = function (θ, l)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  println(l)\n\n  plt = plot(predict_univ(θ)', ylim = (0, 6))\n  push!(list_plots, plt)\n  display(plt)\n  return false\nend","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/feedback_control/","page":"Universal Differential Equations for Neural Feedback Control","title":"Universal Differential Equations for Neural Feedback Control","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_univ(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\nresult_univ = Optimization.solve(optprob, PolyOpt(),\n                                     callback = callback)","category":"page"},{"location":"modules/Optimization/#Optimization.jl","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl","text":"","category":"section"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"Optimization.jl is a package with a scope that is beyond your normal global optimization package. Optimization.jl seeks to bring together all of the optimization packages it can find, local and global, into one unified Julia interface. This means, you learn one package and you learn them all! Optimization.jl adds a few high-level features, such as integrating with automatic differentiation, to make its usage fairly simple for most cases, while allowing all of the options in a single unified interface.","category":"page"},{"location":"modules/Optimization/#Installation","page":"Optimization.jl: A Unified Optimization Package","title":"Installation","text":"","category":"section"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"Assuming that you already have Julia correctly installed, it suffices to import Optimization.jl in the standard way:","category":"page"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"import Pkg; Pkg.add(\"Optimization\")","category":"page"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"The packages relevant to the core functionality of Optimization.jl will be imported accordingly and, in most cases, you do not have to worry about the manual installation of dependencies. However, you will need to add the specific optimizer packages.","category":"page"},{"location":"modules/Optimization/#Overview-of-the-Optimizers","page":"Optimization.jl: A Unified Optimization Package","title":"Overview of the Optimizers","text":"","category":"section"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"Package Local Gradient-Based Local Hessian-Based Local Derivative-Free Local Constrained Global Unconstrained Global Constrained\nBlackBoxOptim ❌ ❌ ❌ ❌ ✅ ❌\nCMAEvolutionaryStrategy ❌ ❌ ❌ ❌ ✅ ❌\nEvolutionary ❌ ❌ ❌ ❌ ✅ 🟡\nFlux ✅ ❌ ❌ ❌ ❌ ❌\nGCMAES ❌ ❌ ❌ ❌ ✅ ❌\nMathOptInterface ✅ ✅ ✅ ✅ ✅ 🟡\nMultistartOptimization ❌ ❌ ❌ ❌ ✅ ❌\nMetaheuristics ❌ ❌ ❌ ❌ ✅ 🟡\nNOMAD ❌ ❌ ❌ ❌ ✅ 🟡\nNLopt ✅ ❌ ✅ 🟡 ✅ 🟡\nNonconvex ✅ ✅ ✅ 🟡 ✅ 🟡\nOptim ✅ ✅ ✅ ✅ ✅ ✅\nQuadDIRECT ❌ ❌ ❌ ❌ ✅ ❌","category":"page"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"✅ = supported","category":"page"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"🟡 = supported in downstream library but not yet implemented in Optimization; PR to add this functionality are welcome","category":"page"},{"location":"modules/Optimization/","page":"Optimization.jl: A Unified Optimization Package","title":"Optimization.jl: A Unified Optimization Package","text":"❌ = not supported","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/#Neural-Second-Order-Ordinary-Differential-Equation","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"The neural ODE focuses and finding a neural network such that:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^prime = NN(u)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"However, in many cases in physics-based modeling, the key object is not the velocity but the acceleration: knowing the acceleration tells you the force field and thus the generating process for the dynamical system. Thus what we want to do is find the force, i.e.:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"u^primeprime = NN(u)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"(Note that in order to be the acceleration, we should divide the output of the neural network by the mass!)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"An example of training a neural network on a second order ODE is as follows:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_neural/","page":"Neural Second Order Ordinary Differential Equation","title":"Neural Second Order Ordinary Differential Equation","text":"using DifferentialEquations, Flux, Optimization, OptimizationFlux, RecursiveArrayTools, Random\n\nu0 = Float32[0.; 2.]\ndu0 = Float32[0.; 0.]\ntspan = (0.0f0, 1.0f0)\nt = range(tspan[1], tspan[2], length=20)\n\nmodel = Flux.Chain(Flux.Dense(2, 50, tanh), Flux.Dense(50, 2))\np,re = Flux.destructure(model)\n\nff(du,u,p,t) = re(p)(u)\nprob = SecondOrderODEProblem{false}(ff, du0, u0, tspan, p)\n\nfunction predict(p)\n    Array(solve(prob, Tsit5(), p=p, saveat=t))\nend\n\ncorrect_pos = Float32.(transpose(hcat(collect(0:0.05:1)[2:end], collect(2:-0.05:1)[2:end])))\n\nfunction loss_n_ode(p)\n    pred = predict(p)\n    sum(abs2, correct_pos .- pred[1:2, :]), pred\nend\n\ndata = Iterators.repeated((), 1000)\nopt = ADAM(0.01)\n\nl1 = loss_n_ode(p)\n\ncallback = function (p,l,pred)\n    println(l)\n    l < 0.01\nend\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_n_ode(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\n\nres = Optimization.solve(optprob, opt; callback = callback, maxiters=1000)","category":"page"},{"location":"modules/Integrals/solvers/IntegralSolvers/#Integral-Solver-Algorithms","page":"Integral Solver Algorithms","title":"Integral Solver Algorithms","text":"","category":"section"},{"location":"modules/Integrals/solvers/IntegralSolvers/","page":"Integral Solver Algorithms","title":"Integral Solver Algorithms","text":"The following algorithms are available:","category":"page"},{"location":"modules/Integrals/solvers/IntegralSolvers/","page":"Integral Solver Algorithms","title":"Integral Solver Algorithms","text":"QuadGKJL: Uses QuadGK.jl. Requires nout=1 and batch=0.\nHCubatureJL: Uses HCubature.jl. Requires batch=0.\nVEGAS: Uses MonteCarloIntegration.jl. Requires nout=1.\nCubatureJLh: h-Cubature from Cubature.jl. Requires using IntegralsCubature.\nCubatureJLp: p-Cubature from Cubature.jl. Requires using IntegralsCubature.\nCubaVegas: Vegas from Cuba.jl. Requires using IntegralsCuba.\nCubaSUAVE: SUAVE from Cuba.jl. Requires using IntegralsCuba.\nCubaDivonne: Divonne from Cuba.jl. Requires using IntegralsCuba.\nCubaCuhre: Cuhre from Cuba.jl. Requires using IntegralsCuba.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Optimization-Based-Methods","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#The-Objective-Function-Builders","page":"Optimization-Based Methods","title":"The Objective Function Builders","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Standard-Nonlinear-Regression","page":"Optimization-Based Methods","title":"Standard Nonlinear Regression","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"build_loss_objective builds an objective function to be used with Optim.jl and MathProgBase-associated solvers like NLopt.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function build_loss_objective(prob::DEProblem,alg,loss_func\n                              regularization=nothing;\n                              mpg_autodiff = false,\n                              verbose_opt = false,\n                              verbose_steps = 100,\n                              prob_generator = (prob,p)->remake(prob,p=p),\n                              kwargs...)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The first argument is the DEProblem to solve, and next is the alg to use. The alg must match the problem type, which can be any DEProblem (ODEs, SDEs, DAEs, DDEs, etc.). regularization defaults to nothing which has no regularization function. One can also choose verbose_opt and verbose_steps, which, in the optimization routines, will print the steps and the values at the steps every verbose_steps steps. mpg_autodiff uses autodifferentiation to define the derivative for the MathProgBase solver. The extra keyword arguments are passed to the differential equation solver.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Multiple-Shooting","page":"Optimization-Based Methods","title":"Multiple Shooting","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Multiple Shooting is generally used in Boundary Value Problems (BVP) and is more robust than the regular objective function used in these problems. It proceeds as follows:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Divide up the time span into short time periods and solve the equation with the current parameters which here consist of both, the parameters of the differential equations and also the initial values for the short time periods.\nThis objective additionally involves a discontinuity error term that imposes higher cost if the end of the solution of one time period doesn't match the beginning of the next one.\nMerge the solutions from the shorter intervals and then calculate the loss.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function multiple_shooting_objective(prob::DiffEqBase.DEProblem,alg,loss,\n                              regularization=nothing;prior=nothing,\n                              mpg_autodiff = false,discontinuity_weight=1.0,\n                              verbose_opt = false,\n                              prob_generator = STANDARD_PROB_GENERATOR,\n                              autodiff_prototype = mpg_autodiff ? zeros(init_N_params) : nothing,\n                              autodiff_chunk = mpg_autodiff ? ForwardDiff.Chunk(autodiff_prototype) : nothing,\n                              kwargs...)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For consistency multiple_shooting_objective takes exactly the same arguments as build_loss_objective. It also has the option for discontinuity_error as a keyword argument which assigns weight to the error occurring due to the discontinuity that arises from the breaking up of the time span.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Detailed-Explanations-of-Arguments","page":"Optimization-Based Methods","title":"Detailed Explanations of Arguments","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#The-Loss-Function","page":"Optimization-Based Methods","title":"The Loss Function","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"loss_func(sol)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"is a function which reduces the problem's solution to a scalar which the optimizer will try to minimize. While this is very flexible, two convenience routines are included for fitting to data with standard cost functions:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t,data;differ_weight=nothing,data_weight=nothing,\n              colloc_grad=nothing,dudt=nothing)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"where t is the set of timepoints which the data is found at, and data are the values that are known where each column corresponds to measures of the values of the system. L2Loss is an optimized version of the L2-distance. The data_weight is a scalar or vector of weights for the loss function which must match the size of the data. Note that minimization of a weighted L2Loss is equivalent to maximum likelihood estimation of a heteroskedastic Normally distributed likelihood. differ_weight allows one to add a weight on the first differencing terms sol[i+1]-sol[i] against the data first differences. This smooths out the loss term and can make it easier to fit strong solutions of stochastic models, but is zero (nothing) by default. Additionally, colloc_grad allows one to give a matrix of the collocation gradients for the data. This is used to add an interpolation derivative term, like the two-stage method. A convenience function colloc_grad(t,data) returns a collocation gradient from a 3rd order spline calculated by Dierckx.jl, which can be used as the colloc_grad. Note that, with a collocation gradient and regularization, this loss is equivalent to a 4DVAR.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Additionally, we include a more flexible log-likelihood approach:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"LogLikeLoss(t,distributions,diff_distributions=nothing)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"In this case, there are two forms. The simple case is where distributions[i,j] is the likelihood distributions from a UnivariateDistribution from Distributions.jl, where it corresponds to the likelihood at t[i] for component j. The second case is where distributions[i] is a MultivariateDistribution which corresponds to the likelihood at t[i] over the vector of components. This likelihood function then calculates the negative of the total loglikelihood over time as its objective value (negative since optimizers generally find minimums, and thus this corresponds to maximum likelihood estimation). The third term, diff_distributions, acts similarly but allows putting a distribution on the first difference terms sol[i+1]-sol[i].","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Note that these distributions can be generated via fit_mle on some dataset against some chosen distribution type.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Note-About-Loss-Functions","page":"Optimization-Based Methods","title":"Note About Loss Functions","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For parameter estimation problems, it's not uncommon for the optimizers to hit unstable regions of parameter space. This causes warnings that the solver exited early, and the built-in loss functions like L2Loss automatically handle this. However, if using a user-supplied loss function, you should make sure it's robust to these issues. One common pattern is to apply infinite loss when the integration is not successful. Using the retcodes, this can be done via:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function my_loss_function(sol)\n   tot_loss = 0.0\n   if any((s.retcode != :Success for s in sol))\n     tot_loss = Inf\n   else\n     # calculation for the loss here\n   end\n   tot_loss\nend","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Note-on-First-Differencing","page":"Optimization-Based Methods","title":"Note on First Differencing","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t,data,differ_weight=0.3,data_weight=0.7)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"First differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the L2Loss alone leads to non-identifiable parameters but adding a first differencing term makes it more identifiable. This can be noted on stochastic differential equation models, where this aims to capture the autocorrelation and therefore helps us avoid getting the same stationary distribution despite different trajectories and thus wrong parameter estimates.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#The-Regularization-Function","page":"Optimization-Based Methods","title":"The Regularization Function","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization can be any function of p, the parameter vector:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"regularization(p)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The Regularization helper function builds a regularization using a penalty function penalty from PenaltyFunctions.jl:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Regularization(λ,penalty=L2Penalty())","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization defaults to L2 if no penalty function is specified. λ is the weight parameter for the addition of the regularization term.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#The-Problem-Generator-Function","page":"Optimization-Based Methods","title":"The Problem Generator Function","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The argument prob_generator allows one to specify a function for generating new problems from a given parameter set. By default, this just builds a new problem which fixes the element types in a way that's autodifferentiation compatible and adds the new parameter vector p. For example, the code for this is:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Then the new problem with these new values is returned.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"One can use this to change the meaning of the parameters using this function. For example, if one instead wanted to optimize the initial conditions for a function without parameters, you could change this to:","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob,p) -> remake(prob.f,u0=p)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"which simply uses p as the initial condition in the initial value problem.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/#Using-the-Objectives-for-MAP-estimates","page":"Optimization-Based Methods","title":"Using the Objectives for MAP estimates","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"You can also add a prior option to build_loss_objective and multiple_shooting_objective that essentially turns it into MAP by multiplying the loglikelihood (the cost) by the prior. The option was added as a keyword argument priors, it can take in either an array of univariate distributions for each of the parameters or a multivariate distribution.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"ms_obj = multiple_shooting_objective(ms_prob,Tsit5(),L2Loss(t,data);priors=priors,discontinuity_weight=1.0,abstol=1e-12,reltol=1e-12)","category":"page"},{"location":"modules/NonlinearSolve/basics/NonlinearFunctions/#nonlinearfunctions","page":"NonlinearFunctions and Jacobian Types","title":"NonlinearFunctions and Jacobian Types","text":"","category":"section"},{"location":"modules/NonlinearSolve/basics/NonlinearFunctions/","page":"NonlinearFunctions and Jacobian Types","title":"NonlinearFunctions and Jacobian Types","text":"The SciML ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the NonlinearFunction types, which can be passed to the problems.","category":"page"},{"location":"modules/NonlinearSolve/basics/NonlinearFunctions/#Function-Type-Definitions","page":"NonlinearFunctions and Jacobian Types","title":"Function Type Definitions","text":"","category":"section"},{"location":"modules/NonlinearSolve/basics/NonlinearFunctions/","page":"NonlinearFunctions and Jacobian Types","title":"NonlinearFunctions and Jacobian Types","text":"SciMLBase.NonlinearFunction","category":"page"},{"location":"modules/NonlinearSolve/basics/NonlinearFunctions/#SciMLBase.NonlinearFunction","page":"NonlinearFunctions and Jacobian Types","title":"SciMLBase.NonlinearFunction","text":"NonlinearFunction{iip,F,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractNonlinearFunction{iip}\n\nA representation of an nonlinear system of equations f, defined by:\n\n0 = f(up)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nNonlinearFunction{iip,recompile}(f;\n                           analytic=nothing,\n                           jac=nothing,\n                           jvp=nothing,\n                           vjp=nothing,\n                           jac_prototype=nothing,\n                           sparsity=jac_prototype,\n                           paramjac = nothing,\n                           syms = nothing,\n                           indepsym = nothing,\n                           colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p) or du = f(u,p). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\njac(J,u,p) or J=jac(u,p): returns fracdfdu\njvp(Jv,v,u,p) or Jv=jvp(v,u,p): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p) or Jv=vjp(v,u,p): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the NonlinearFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLSolutions","page":"SciMLSolutions","title":"SciMLSolutions","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/#Definition-of-the-AbstractSciMLSolution-Interface","page":"SciMLSolutions","title":"Definition of the AbstractSciMLSolution Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"All AbstractSciMLSolution types are a subset of some AbstractArray. Types with time series (like ODESolution) are subtypes of RecursiveArrayTools.AbstractVectorOfArray and RecursiveArrayTools.AbstractDiffEqArray where appropriate. Types without a time series (like OptimizationSolution) are directly subsets of AbstractArray.","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/#Array-Interface","page":"SciMLSolutions","title":"Array Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"Instead of working on the Vector{uType} directly, we can use the provided array interface.","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"sol[j]","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"to access the value at timestep j (if the timeseries was saved), and","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"sol.t[j]","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"to access the value of t at timestep j. For multi-dimensional systems, this will address first by component and lastly by time, and thus","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"sol[i,j]","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"will be the ith component at timestep j. Hence, sol[j][i] == sol[i, j]. This is done because Julia is column-major, so the leading dimension should be contiguous in memory. If the independent variables had shape (for example, was a matrix), then i is the linear index. We can also access solutions with shape:","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"sol[i,k,j]","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"gives the [i,k] component of the system at timestep j. The colon operator is supported, meaning that","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"sol[i,:]","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"gives the timeseries for the ith component.","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/#Common-Field-Names","page":"SciMLSolutions","title":"Common Field Names","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"u: the solution values\nt: the independent variable values, matching the length of the solution, if applicable\nresid: the residual of the solution, if applicable\noriginal: the solution object from the original solver, if it's a wrapper algorithm\nretcode: see the documentation section on return codes\nprob: the problem that was solved\nalg: the algorithm used to solve the problem","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/#retcodes","page":"SciMLSolutions","title":"Return Codes (RetCodes)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"The solution types have a retcode field which returns a symbol signifying the error state of the solution. The retcodes are as follows:","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":":Default: The solver did not set retcodes.\n:Success: The integration completed without erroring or the steady state solver from SteadyStateDiffEq found the steady state.\n:Terminated: The integration is terminated with terminate!(integrator). Note that this may occur by using TerminateSteadyState from the callback library DiffEqCallbacks.\n:MaxIters: The integration exited early because it reached its maximum number of iterations.\n:DtLessThanMin: The timestep method chose a stepsize which is smaller than the allowed minimum timestep, and exited early.\n:Unstable: The solver detected that the solution was unstable and exited early.\n:InitialFailure: The DAE solver could not find consistent initial conditions.\n:ConvergenceFailure: The internal implicit solvers failed to converge.\n:Failure: General uncategorized failures or errors.","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/#Traits","page":"SciMLSolutions","title":"Traits","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/#AbstractSciMLSolution-API","page":"SciMLSolutions","title":"AbstractSciMLSolution API","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/#Abstract-SciML-Solutions","page":"SciMLSolutions","title":"Abstract SciML Solutions","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Solutions/","page":"SciMLSolutions","title":"SciMLSolutions","text":"SciMLBase.AbstractSciMLSolution\nSciMLBase.AbstractNoTimeSolution\nSciMLBase.AbstractTimeseriesSolution\nSciMLBase.AbstractNoiseProcess\nSciMLBase.AbstractEnsembleSolution\nSciMLBase.AbstractLinearSolution\nSciMLBase.AbstractNonlinearSolution\nSciMLBase.AbstractQuadratureSolution\nSciMLBase.AbstractSteadyStateSolution\nSciMLBase.AbstractAnalyticalSolution\nSciMLBase.AbstractODESolution\nSciMLBase.AbstractDDESolution\nSciMLBase.AbstractRODESolution\nSciMLBase.AbstractDAESolution","category":"page"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractSciMLSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractSciMLSolution","text":"Union of all base solution types.\n\nUses a Union so that solution types can be <: AbstractArray\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractNoTimeSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractNoTimeSolution","text":"abstract type AbstractNoTimeSolution{T, N} <: AbstractArray{T, N}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractTimeseriesSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractTimeseriesSolution","text":"abstract type AbstractTimeseriesSolution{T, N, A} <: AbstractDiffEqArray{T, N, A}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractNoiseProcess","page":"SciMLSolutions","title":"SciMLBase.AbstractNoiseProcess","text":"abstract type AbstractNoiseProcess{T, N, A, isinplace} <: AbstractDiffEqArray{T, N, A}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractEnsembleSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractEnsembleSolution","text":"abstract type AbstractEnsembleSolution{T, N, A} <: AbstractVectorOfArray{T, N, A}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractLinearSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractLinearSolution","text":"abstract type AbstractLinearSolution{T, N} <: SciMLBase.AbstractNoTimeSolution{T, N}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractNonlinearSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractNonlinearSolution","text":"abstract type AbstractNonlinearSolution{T, N} <: SciMLBase.AbstractNoTimeSolution{T, N}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractSteadyStateSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractSteadyStateSolution","text":"abstract type AbstractNonlinearSolution{T, N} <: SciMLBase.AbstractNoTimeSolution{T, N}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractAnalyticalSolution","page":"SciMLSolutions","title":"SciMLBase.AbstractAnalyticalSolution","text":"abstract type AbstractAnalyticalSolution{T, N, S} <: SciMLBase.AbstractTimeseriesSolution{T, N, S}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractODESolution","page":"SciMLSolutions","title":"SciMLBase.AbstractODESolution","text":"abstract type AbstractODESolution{T, N, S} <: SciMLBase.AbstractTimeseriesSolution{T, N, S}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractDDESolution","page":"SciMLSolutions","title":"SciMLBase.AbstractDDESolution","text":"abstract type AbstractDDESolution{T, N, S} <: SciMLBase.AbstractODESolution{T, N, S}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractRODESolution","page":"SciMLSolutions","title":"SciMLBase.AbstractRODESolution","text":"abstract type AbstractRODESolution{T, N, S} <: SciMLBase.AbstractODESolution{T, N, S}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Solutions/#SciMLBase.AbstractDAESolution","page":"SciMLSolutions","title":"SciMLBase.AbstractDAESolution","text":"abstract type AbstractDAESolution{T, N, S} <: SciMLBase.AbstractODESolution{T, N, S}\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Network-Analysis-in-Catalyst","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"In this tutorial we introduce several of the Catalyst API functions for network analysis. A complete summary of the exported functions is given in the API section Network-Analysis-and-Representations.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Note, currently API functions for network analysis and conservation law analysis do not work with constant species (currently only generated by SBMLToolkit).","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Network-representation-of-the-Repressilator-ReactionSystem","page":"Network Analysis in Catalyst","title":"Network representation of the Repressilator ReactionSystem","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We first load Catalyst and construct our model of the repressilator","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"using Catalyst\nrepressilator = @reaction_network Repressilator begin\n       hillr(P₃,α,K,n), ∅ --> m₁\n       hillr(P₁,α,K,n), ∅ --> m₂\n       hillr(P₂,α,K,n), ∅ --> m₃\n       (δ,γ), m₁ <--> ∅\n       (δ,γ), m₂ <--> ∅\n       (δ,γ), m₃ <--> ∅\n       β, m₁ --> m₁ + P₁\n       β, m₂ --> m₂ + P₂\n       β, m₃ --> m₃ + P₃\n       μ, P₁ --> ∅\n       μ, P₂ --> ∅\n       μ, P₃ --> ∅\nend α K n δ γ β μ","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"In the Using Catalyst tutorial we showed how the above network could be visualized as a species-reaction graph. There species are represented by the nodes of the graph and edges show the reactions in which a given species is a substrate or product.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"g = Graph(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: Repressilator solution)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We also showed in the Using Catalyst tutorial that the reaction rate equation ODE model for the repressilator is","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"beginaligned\nfracdm_1(t)dt = fracalpha K^nK^n + left( P_3left( t right) right)^n - delta m_1left( t right) + gamma \nfracdm_2(t)dt = fracalpha K^nK^n + left( P_1left( t right) right)^n - delta m_2left( t right) + gamma \nfracdm_3(t)dt = fracalpha K^nK^n + left( P_2left( t right) right)^n - delta m_3left( t right) + gamma \nfracdP_1(t)dt = beta m_1left( t right) - mu P_1left( t right) \nfracdP_2(t)dt = beta m_2left( t right) - mu P_2left( t right) \nfracdP_3(t)dt = beta m_3left( t right) - mu P_3left( t right)\nendaligned","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Matrix-Vector-Reaction-Rate-Equation-Representation","page":"Network Analysis in Catalyst","title":"Matrix-Vector Reaction Rate Equation Representation","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"In general, reaction rate equation (RRE) ODE models for chemical reaction networks can be represented as a first order system of ODEs in a compact matrix-vector notation. Suppose we have a reaction network with K reactions and M species, labelled by the state vector","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"mathbfx(t) = beginpmatrix x_1(t)  vdots  x_M(t)) endpmatrix","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"For the repressilator, mathbfx(t) is just","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"x = species(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The RRE ODEs satisfied by mathbfx(t) are then","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"fracdmathbfxdt = N mathbfv(mathbfx(t)t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"where N is a constant M by K matrix with N_m k the net stoichiometric coefficient of species m in reaction k. mathbfv(mathbfx(t)t) is the rate law vector, with v_k(mathbfx(t)t) the rate law for the kth reaction. For example, for the first reaction of the repressilator above, the rate law is","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"v_1(mathbfx(t)t) = fracalpha K^nK^n + left( P_3(t) right)^n","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can calculate each of these in Catalyst via","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"N = netstoichmat(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"and by using the oderatelaw function","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rxs = reactions(repressilator)\nν = oderatelaw.(rxs)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Note, as oderatelaw takes just one reaction as input we use broadcasting to apply it to each element of rxs.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Let's check this really gives the same ODEs as Catalyst. Here is what Catalyst generates by converting to an ODESystem","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"osys = convert(ODESystem, repressilator)\n\n# for display purposes we just pull out the right side of the equations\nodes = [eq.rhs for eq in equations(osys)]","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"whereas our matrix-vector representation gives","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"odes2 = N * ν","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Let's check these are equal symbolically","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"isequal(odes, odes2)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Reaction-Complex-Representation","page":"Network Analysis in Catalyst","title":"Reaction Complex Representation","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We now introduce a further decomposition of the RRE ODEs, which has been used to facilitate analysis of a variety of reaction network properties. Consider a simple reaction system like","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rn = @reaction_network begin\n k*A, 2*A + 3*B --> A + 2*C + D\n b, C + D --> 2*A + 3*B\nend k b","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can think of the first reaction as converting the reaction complex, 2A+3B to the complex A+2C+D with rate kA. Suppose we order our species the same way as Catalyst does, i.e.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"beginpmatrix\nx_1(t)\nx_2(t)\nx_3(t)\nx_4(t)\nendpmatrix =\nbeginpmatrix\nA(t)\nB(t)\nC(t)\nD(t)\nendpmatrix","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"which should be the same as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"species(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can describe a given reaction complex by the stoichiometric coefficients of each species within the complex. For the reactions in rn these vectors would be","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"beginalign*\n2A+3B = beginpmatrix\n2\n3\n0\n0\nendpmatrix \nA+2C+D = beginpmatrix\n1\n0\n2\n1\nendpmatrix\n \nC+D = beginpmatrix\n0\n0\n1\n1\nendpmatrix\nendalign*","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Catalyst can calculate these representations as the columns of the complex stoichiometry matrix,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Z = complexstoichmat(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"If we have C complexes, Z is a M by C matrix with Z_m c giving the stoichiometric coefficient of species m within complex c.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can use this representation to provide another representation of the RRE ODEs. The net stoichiometry matrix can be factored as N = Z B, where B is called the incidence matrix of the reaction network,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"B = incidencemat(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Here B is a C by K matrix with B_c k = 1 if complex c appears as a product of reaction k, and B_c k = -1 if complex c is a substrate of reaction k.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Using our decomposition of N, the RRE ODEs become","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"fracdxdt = Z B mathbfv(mathbfx(t)t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Let's verify that N = Z B,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"N = netstoichmat(rn)\nN == Z*B","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Reaction complexes give an alternative way to visualize a reaction network graph. Catalyst's complexgraph command will calculate the complexes of a network and then show how they are related. For example,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"complexgraph(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"gives","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: Simple example complex graph)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"while for the repressilator we find","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"complexgraph(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: Repressilator complex)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Here ∅ represents the empty complex, black arrows show reactions converting substrate complexes into product complexes where the rate is just a number or parameter, and red arrows indicate conversion of substrate complexes into product complexes where the rate is an expression involving chemical species.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Aspects-of-Reaction-Network-Structure","page":"Network Analysis in Catalyst","title":"Aspects of Reaction Network Structure","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The reaction complex representation can be exploited via Chemical Reaction Network Theory to provide insight into possible steady-state and time-dependent properties of RRE ODE models and  stochastic chemical kinetics models. We'll now illustrate some of the types of network properties that Catalyst can determine, using the reaction complex representation in these calculations.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Consider the following reaction network.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rn = @reaction_network begin\n     (k1,k2), A + B <--> C\n     k3, C --> D+E\n     (k4,k5), D+E <--> F\n     (k6,k7), 2A <--> B+G\n     k8, B+G --> H\n     k9, H --> 2A\nend k1 k2 k3 k4 k5 k6 k7 k8 k9","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"with graph","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"complexgraph(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: network_1)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Linkage-classes-and-sub-networks-of-the-reaction-network","page":"Network Analysis in Catalyst","title":"Linkage classes and sub-networks of the reaction network","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The preceding reaction complex graph shows that rn is composed of two disconnected sub-graphs, one containing the complexes A+B, C, D+E, and F, the other containing the complexes 2A, B + G, and H. These sets, A+B C D+E F and 2A B + GH are called the \"linkage classes\" of the reaction network. The function linkageclasses will calculate these for a given network, returning a vector of the integer indices of reaction complexes participating in each set of linkage-classes. Note, indices of reaction complexes can be determined from the ordering returned by reactioncomplexes.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"# we must first calculate the reaction complexes -- they are cached in rn\nreactioncomplexes(rn)\n\n# now we can calculate the linkage classes\nlcs = linkageclasses(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"It can often be convenient to obtain the disconnected sub-networks as distinct ReactionSystems, which are returned by the subnetworks function:","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"subnets = subnetworks(rn)\n\n# check the reactions in each subnetwork\nreactions.(subnets)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The graphs of the reaction complexes in the two sub-networks are then","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"  complexgraph(subnets[1])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: subnetwork_1)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"and,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":" complexgraph(subnets[2])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: subnetwork_2)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Deficiency-of-the-network","page":"Network Analysis in Catalyst","title":"Deficiency of the network","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"A famous theorem in Chemical Reaction Network Theory, the Deficiency Zero Theorem [1], allows us to use knowledge of the net stoichiometry matrix and the linkage classes of a mass action RRE ODE system to draw conclusions about the system's possible steady-states. In this section we'll see how Catalyst can calculate a network's deficiency.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The rank, r, of a reaction network is defined as the dimension of the subspace spanned by the net stoichiometry vectors of the reaction-network [1], i.e. the span of the columns of the net stoichiometry matrix N. It corresponds to the number of independent species in a chemical reaction network. That is, if we calculate the linear conservation laws of a network, and use them to eliminate the dependent species of the network, we will have r independent species remaining. For our current example the conservation laws are given by","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"# first we calculate the conservation laws -- they are cached in rn\nconservationlaws(rn)\n\n# then we display them as equations for the dependent variables\nconservedequations(rn)\nshow(stdout, MIME\"text/plain\"(), ans) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Here the parameters _ConLaw[i] represent the constants of the three conservation laws, and we see that there are three dependent species that could be eliminated. As","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"numspecies(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"we find that there are five independent species. Let's check this is correct:","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"using LinearAlgebra\nrank(netstoichmat(rn)) == 5","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"So we know that the rank of our reaction network is five.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"The deficiency, delta, of a reaction network is defined as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"delta = textrm(number of complexes) - textrm(number of linkage classes) - textrm(rank)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"For our network this is 7 - 2 - 5 = 0, which we can calculate in Catalyst as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"# first we calculate the reaction complexes of rn and cache them in rn\nreactioncomplexes(rn)\n\n# then we can calculate the deficiency\nδ = deficiency(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Quoting Feinberg [1]","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Deficiency zero networks are ones for which the reaction vectors [i.e. net stoichiometry vectors] are as independent as the partition of complexes into linkage classes will allow.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Reversibility-of-the-network","page":"Network Analysis in Catalyst","title":"Reversibility of the network","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"A reaction network is reversible if the \"arrows\" of the reactions are symmetric so that every reaction is accompanied by its reverse reaction. Catalyst's API provides the isreversible function to determine whether a reaction network is reversible. As an example, consider","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rn = @reaction_network begin\n  (k1,k2),A <--> B\n  (k3,k4),A + C <--> D\n  (k5,k6),D <--> B+E\n  (k7,k8),B+E <--> A+C\nend k1 k2 k3 k4 k5 k6 k7 k8\n\n# calculate the set of reaction complexes\nreactioncomplexes(rn)\n\n# test if the system is reversible\nisreversible(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Consider another example,","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rn = @reaction_network begin\n  (k1,k2),A <--> B\n  k3, A + C --> D\n  k4, D --> B+E\n  k5, B+E --> A+C\nend k1 k2 k3 k4 k5\nreactioncomplexes(rn)\nisreversible(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"complexgraph(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(Image: reversibility)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"It is evident from the preceding graph that the network is not reversible. However, it satisfies a weaker property in that there is a path from each reaction complex back to itself within its associated subgraph. This is known as weak reversiblity. One can test a network for weak reversibility by using the isweaklyreversible function:","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"# need subnetworks from the reaction network first\nsubnets = subnetworks(rn)\nisweaklyreversible(rn, subnets)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Every reversible network is also weakly reversible, but not every weakly reversible network is reversible.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Deficiency-Zero-Theorem","page":"Network Analysis in Catalyst","title":"Deficiency Zero Theorem","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Knowing the deficiency and weak reversibility of a mass action chemical reaction network ODE model allows us to make inferences about the corresponding steady-state behavior. Before illustrating how this works for one example, we need one last definition.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Recall that in the matrix-vector representation for the RRE ODEs, the entries, N_m k, of the stoichiometry matrix, N, give the net change in species m due to reaction k. If we let mathbfN_k denote the kth column of this matrix, this vector corresponds to the change in the species state vector, mathbfx(t), due to reaction k, i.e. when reaction k occurs mathbfx(t) to mathbfx(t) + mathbfN_k. Moreover, by integrating the ODE","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"fracdmathbfxdt = N mathbfv(mathbfx(t)) = sum_k=1^K v_k(mathbfx(t))  mathbfN_k","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"we find","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"mathbfx(t) = mathbfx(0) + sum_k=1^K left(int_0^t v_k(mathbfx)(s)  dsright) mathbfN_k","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"which demonstrates that mathbfx(t) - mathbfx(0) is always given by a linear combination of the stochiometry vectors, i.e.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"DeclareMathOperatorspanspan\nmathbfx(t) - mathbfx(0) in spanmathbfN_k ","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"In particular, this says that mathbfx(t) lives in the translation of the spanmathbfN_k  by mathbfx(0) which we write as (mathbfx(0) + spanmathbfN_k). In fact, since the solution should stay non-negative, if we let barmathbbR_+^M denote the subset of vectors in mathbbR^M with non-negative components, the possible physical values for the solution, mathbfx(t), must be in the set","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"(mathbfx(0) + spanmathbfN_k) cap barmathbbR_+^M","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"This set is called the stoichiometric compatibility class of mathbfx(t). The key property of stoichiometric compatibility classes is that they are invariant under the RRE ODE's dynamics, i.e. a solution will always remain within the subspace given by the stoichiometric compatibility class. Finally, we note that the positive stoichiometric compatibility class generated by mathbfx(0) is just (mathbfx(0) + spanmathbfN_k) cap mathbbR_+^M, where mathbbR_+^M denotes the vectors in mathbbR^M with strictly positive components.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"With these definitions we can now see how knowing the deficiency and weak reversibility of the network can tell us about its steady-state behavior. Consider the previous example, which we know is weakly reversible. Its deficiency is","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"deficiency(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We also verify that the system is purely mass action (though it is apparent from the network's definition):","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"all(rx -> ismassaction(rx, rn), reactions(rn))","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can therefore apply the Deficiency Zero Theorem to draw conclusions about the system's steady-state behavior. The Deficiency Zero Theorem (roughly) says that a mass action network with deficiency zero satisfies","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"If the network is weakly reversible, then independent of the reaction rate constants the RRE ODEs have exactly one equilibrium solution within each positive stoichiometric compatibility class. That equilibrium is locally asymptotically stable.\nIf the network is not weakly reversible, then the RRE ODEs cannot admit a positive equilibrium solution.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"See [1] for a more precise statement, proof, and additional examples.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"We can therefore conclude that for any initial condition that is positive, and hence in some positive stoichiometric compatibility class, rn will have exactly one equilibrium solution which will be positive and locally asymptotically stable.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"As a final example, consider the following network from [1]","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rn = @reaction_network begin\n  (k1,k2),A <--> 2B\n  (k3,k4), A + C <--> D\n  k5, B+E --> C + D\nend k1 k2 k3 k4 k5\nreactioncomplexes(rn)\nsubnets = subnetworks(rn)\nisma = all(rx -> ismassaction(rx,rn), reactions(rn))\ndef = deficiency(rn)\niswr = isweaklyreversible(rn, subnets)\nisma,def,iswr","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"which we see is mass action and has deficiency zero, but is not weakly reversible. As such, we can conclude that for any choice of rate constants the RRE ODEs cannot have a positive equilibrium solution.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#Caching-of-Network-Properties-in-ReactionSystems","page":"Network Analysis in Catalyst","title":"Caching of Network Properties in ReactionSystems","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"When calling many of the network API functions, Catalyst calculates and caches in rn a variety of information. For example the first call to","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rcs,B = reactioncomplexes(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"calculates, caches, and returns the reaction complexes, rcs, and the incidence matrix, B, of rn. Subsequent calls simply return rcs and B from the cache.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Similarly, the first call to","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"N = netstoichmat(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"calculates, caches and returns the net stoichiometry matrix. Subsequent calls then simply return the cached value of N. Caching such information means users do not need to manually know which subsets of network properties are needed for a given calculation (like the deficiency). Generally only","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"rcs,B = reactioncomplexes(rn)    # must be called once to cache rcs and B\nany_other_network_property(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"should work to calculate a desired network property, with the API doc strings indicating when reactioncomplexes(rn) must be called at least once before a given function is used.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Because of the caching of network properties, subsequent calls to most API functions will be fast, simply returning the previously calculated and cached values. In some cases it may be desirable to reset the cache and recalculate these properties, for example after modifying a network (see addspecies!, addparam!, or addreaction!). This can be done by calling","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Catalyst.reset_networkproperties!(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"Network property functions will then recalculate their associated properties and cache the new values the next time they are called.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/#References","page":"Network Analysis in Catalyst","title":"References","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_network_representation/","page":"Network Analysis in Catalyst","title":"Network Analysis in Catalyst","text":"[1]: Feinberg, M. Foundations of Chemical Reaction Network Theory, Applied Mathematical Sciences 202, Springer (2019).","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#SDESystem","page":"SDESystem","title":"SDESystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/#System-Constructors","page":"SDESystem","title":"System Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"SDESystem","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#ModelingToolkit.SDESystem","page":"SDESystem","title":"ModelingToolkit.SDESystem","text":"struct SDESystem <: ModelingToolkit.AbstractODESystem\n\nA system of stochastic differential equations.\n\nFields\n\neqs\nThe expressions defining the drift term.\nnoiseeqs\nThe expressions defining the diffusion term.\niv\nIndependent variable.\nstates\nDependent (state) variables. Must not contain the independent variable.\nps\nParameter variables. Must not contain the independent variable.\nvar_to_name\nArray variables.\nctrls\nControl parameters (some subset of ps).\nobserved\nObserved states.\ntgrad\nTime-derivative matrix. Note: this field will not be defined until calculate_tgrad is called on the system.\n\njac\nJacobian matrix. Note: this field will not be defined until calculate_jacobian is called on the system.\n\nctrl_jac\nControl Jacobian matrix. Note: this field will not be defined until calculate_control_jacobian is called on the system.\n\nWfact\nWfact matrix. Note: this field will not be defined until generate_factorized_W is called on the system.\n\nWfact_t\nWfact_t matrix. Note: this field will not be defined until generate_factorized_W is called on the system.\n\nname\nName: the name of the system\n\nsystems\nSystems: the internal systems. These are required to have unique names.\n\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nconnector_type\ntype: type of the system\n\nExample\n\nusing ModelingToolkit\n\n@parameters σ ρ β\n@variables t x(t) y(t) z(t)\nD = Differential(t)\n\neqs = [D(x) ~ σ*(y-x),\n       D(y) ~ x*(ρ-z)-y,\n       D(z) ~ x*y - β*z]\n\nnoiseeqs = [0.1*x,\n            0.1*y,\n            0.1*z]\n\n@named de = SDESystem(eqs,noiseeqs,t,[x,y,z],[σ,ρ,β])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"To convert an ODESystem to an SDESystem directly:","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"ode = ODESystem(eqs,t,[x,y,z],[σ,ρ,β])\r\nsde = SDESystem(ode, noiseeqs)","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#Composition-and-Accessor-Functions","page":"SDESystem","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"get_eqs(sys) or equations(sys): The equations that define the SDE.\nget_states(sys) or states(sys): The set of states in the SDE.\nget_ps(sys) or parameters(sys): The parameters of the SDE.\nget_iv(sys): The independent variable of the SDE.","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#Transformations","page":"SDESystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"structural_simplify\r\nalias_elimination\r\nGirsanov_transform","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#Analyses","page":"SDESystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/#Applicable-Calculation-and-Generation-Functions","page":"SDESystem","title":"Applicable Calculation and Generation Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"calculate_jacobian\r\ncalculate_tgrad\r\ncalculate_factorized_W\r\ngenerate_jacobian\r\ngenerate_tgrad\r\ngenerate_factorized_W\r\njacobian_sparsity","category":"page"},{"location":"modules/ModelingToolkit/systems/SDESystem/#Problem-Constructors","page":"SDESystem","title":"Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/SDESystem/","page":"SDESystem","title":"SDESystem","text":"SDEFunction\r\nSDEProblem","category":"page"},{"location":"modules/DiffEqDocs/features/progress_bar/#Progress-Bar-Integration","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"DifferentialEquations.jl integrates with the Juno progress bar in order to make long calculations more manageable. By default this feature is off for ODE and SDE solvers, but can be turned on via the keyword argument progress=true. The progress bar updates every progress_steps timesteps, which has a default value of 1000. Note that making this value really low could cause a performance hit, though from some basic testing it seems that with updates of at least 1000 steps on number (the fastest problems) there's no discernable performance degradation, giving a high upper bound.","category":"page"},{"location":"modules/DiffEqDocs/features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"Note that the progress bar also includes a time estimate. This time-estimate is provided by linear extrapolation for how long it has taken to get to what percentage. For adaptive timestepping methods this should only be used as a rough estimate since the timesteps may (and will) change. By scrolling over the progress bar one will also see the current timestep. This can be used to track the solution's progress and find tough locations for the solvers.","category":"page"},{"location":"modules/DiffEqDocs/features/progress_bar/#Using-Progress-Bars-Outside-Juno","page":"Progress Bar Integration","title":"Using Progress Bars Outside Juno","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"To use the progress bars outside of Juno, use TerminalLoggers.jl. Follow these directions to add TerminalLogging to your startup.jl, if you want it enabled by default.","category":"page"},{"location":"modules/DiffEqDocs/features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"Otherwise, follow the example down below. Note that global_logger is initialized  before any other julia call. This step is crucial, otherwise no logging will  appear in the terminal.","category":"page"},{"location":"modules/DiffEqDocs/features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"using Logging: global_logger\nusing TerminalLoggers: TerminalLogger\nglobal_logger(TerminalLogger())\n\nusing OrdinaryDiffEq\n\nsolve(\n    ODEProblem((u, p, t) -> (sleep(0.01); -u), 1.0, nothing),\n    Euler();\n    dt = 0.5,\n    tspan = (0.0, 1000.0),\n    progress = true,\n    progress_steps = 1,\n)","category":"page"},{"location":"modules/PolyChaos/numerical_integration/#NumericalIntegration","page":"Numerical Integration","title":"Numerical Integration","text":"","category":"section"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"The goal of this tutorial is to solve an integral using Gauss quadrature,","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"I = int_0^1 f(t) mathrmd t approx sum_k=1^n w_k f(t_k)","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"where we choose f(t) = sin(t), and n = 5.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Make sure to check out this tutorial too.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/#Variant-0","page":"Numerical Integration","title":"Variant 0","text":"","category":"section"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> using PolyChaos\n\njulia> n = 5;\n\njulia> f(t) = sin(t);\n\njulia> op = Uniform01OrthoPoly(n, addQuadrature=true);\n\njulia> variant0 = integrate(f, op)\n0.4596976941320484\n\njulia> print(\"Numerical error: $(abs(1 - cos(1) - variant0))\")\nNumerical error: 1.8868240303504535e-13","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"with negligible numerical errors.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/#Variant-1","page":"Numerical Integration","title":"Variant 1","text":"","category":"section"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Let us  now solve the same problem, while elaborating what is going on under the hood. At first, we load the package by calling","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"using PolyChaos","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Now we define a measure, specifically the uniform measure mathrmdlambda(t) = w(t) mathrmd t with the weight w defined as","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"  w mathcalW = 01 rightarrow mathbbR quad w(t) = 1","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"This measure can be defined using the composite type Uniform01Measure:","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> measure = Uniform01Measure()\nUniform01Measure(PolyChaos.w_uniform01, (0.0, 1.0), true)","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Next, we need to compute the quadrature rule relative to the uniform measure. To do this we use the composite type Quad.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> quadRule1 = Quad(n-1, measure)\n┌ Warning: For measures of type Uniform01Measure the quadrature rule should be based on the recurrence coefficients.\n└ @ PolyChaos ~/Documents/Code/JuliaDev/PolyChaos/src/typesQuad.jl:58\nQuad{Float64,Array{Float64,1}}(\"quadgp\", 4, [1.0, 0.8535533905932737, 0.5, 0.14644660940672627, 0.0], [0.033333333333333354, 0.26666666666666666, 0.4, 0.26666666666666666, 0.033333333333333354])\n\njulia> nw(quadRule1)\n5×2 Array{Float64,2}:\n 1.0       0.0333333\n 0.853553  0.266667 \n 0.5       0.4      \n 0.146447  0.266667 \n 0.0       0.0333333","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"This creates a quadrature rule quadRule_1 relative to the measure measure. The function nw() prints the nodes and weights. To solve the integral we call integrate()","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> variant1 = integrate(f, quadRule1)\n0.4596977927043755\n\njulia> print(\"Numerical error: $(abs(1 - cos(1) - variant1))\")\nNumerical error: 9.857251526135258e-8","category":"page"},{"location":"modules/PolyChaos/numerical_integration/#Revisiting-Variant-0","page":"Numerical Integration","title":"Revisiting Variant 0","text":"","category":"section"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Why is the error from variant 0 so much smaller? It's because the quadrature rule for variant 0 is based on the recurrence coefficients of the polynomials that are orthogonal relative to the measure measure. Let's take a closer look First, we compute the orthogonal polynomials using the composite type OrthoPoly, and we set the keyword addQuadrature to false.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> op = Uniform01OrthoPoly(n, addQuadrature=false)\nUniform01OrthoPoly{Array{Float64,1},Uniform01Measure,EmptyQuad{Float64}}(5, [0.5, 0.5, 0.5, 0.5, 0.5, 0.5], [1.0, 0.08333333333333333, 0.06666666666666667, 0.06428571428571428, 0.06349206349206349, 0.06313131313131314], Uniform01Measure(PolyChaos.w_uniform01, (0.0, 1.0), true), EmptyQuad{Float64}())","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Note how op has a field EmptyQuad, i.e. we computed no quadrature rule. The resulting system of orthogonal polynomials is characterized by its recursion coefficients (alpha beta), which can be extracted with the function coeffs().","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> coeffs(op)\n6×2 Array{Float64,2}:\n 0.5  1.0      \n 0.5  0.0833333\n 0.5  0.0666667\n 0.5  0.0642857\n 0.5  0.0634921\n 0.5  0.0631313","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"Now, the quadrature rule can be constructed based on op, and the integral be solved.","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> quadRule2 = Quad(n, op)\nQuad{Float64,Array{Float64,1}}(\"golubwelsch\", 5, [0.046910077030667935, 0.23076534494715842, 0.49999999999999994, 0.7692346550528418, 0.9530899229693321], [0.11846344252809445, 0.23931433524968332, 0.28444444444444444, 0.23931433524968337, 0.1184634425280949])\n\njulia> nw(quadRule2)\n5×2 Array{Float64,2}:\n 0.0469101  0.118463\n 0.230765   0.239314\n 0.5        0.284444\n 0.769235   0.239314\n 0.95309    0.118463\n\njulia> variant0_revisited = integrate(f, quadRule2)\n0.4596976941320484\n\njulia> print(\"Numerical error: $(abs(1 - cos(1) - variant0_revisited))\")\nNumerical error: 1.8818280267396403e-13","category":"page"},{"location":"modules/PolyChaos/numerical_integration/#Comparison","page":"Numerical Integration","title":"Comparison","text":"","category":"section"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"We see that the different variants provide slightly different results:","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"julia> 1-cos(1) .- [variant0 variant1 variant0_revisited]\n1×3 Array{Float64,2}:\n -1.88183e-13  -9.85725e-8  -1.88183e-13","category":"page"},{"location":"modules/PolyChaos/numerical_integration/","page":"Numerical Integration","title":"Numerical Integration","text":"with variant0 and variant0_revisited being the same and more accurate than variant1. The increased accuracy is based on the fact that for variant0 and variant0_revisited the quadrature rules are based on the recursion coefficients of the underlying orthogonal polynomials. The quadrature for variant1 is based on an general-purpose method that can be significantly less accurate, see also the next tutorial.","category":"page"},{"location":"modules/NeuralPDE/manual/neural_adapters/#Transfer-Learning-with-neural_adapter","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/neural_adapters/","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"NeuralPDE.neural_adapter","category":"page"},{"location":"modules/NeuralPDE/manual/neural_adapters/#NeuralPDE.neural_adapter","page":"Transfer Learning with neural_adapter","title":"NeuralPDE.neural_adapter","text":"neural_adapter(loss, init_params, pde_system, strategy) \n\nTrains a neural network using the results from one already obtained prediction.\n\nPositional Arguments\n\nloss: the body of loss function,\ninit_params: the initial parameter of the neural network,,\npde_system: PDEs are defined using the ModelingToolkit.jl,\nstrategy: determines which training strategy will be used.\n\n\n\n\n\n","category":"function"},{"location":"modules/Surrogates/randomforest/#Random-forests-surrogate-tutorial","page":"RandomForest","title":"Random forests surrogate tutorial","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"note: Note\nThis surrogate requires the 'SurrogatesRandomForest' module which can be added by inputting \"]add SurrogatesRandomForest\" from the Julia command line. ","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"Random forests is a supervised learning algorithm that randomly creates and merges multiple decision trees into one forest.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"We are going to use a Random forests surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"using Surrogates\nusing SurrogatesRandomForest\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/randomforest/#Sampling","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 2.7\nupper_bound = 7.5\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/randomforest/#Building-a-surrogate","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"With our sampled points we can build the Random forests surrogate using the RandomForestSurrogate function.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"randomforest_surrogate behaves like an ordinary function which we can simply plot. Addtionally you can specify the number of trees created using the parameter num_round","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"num_round = 2\nrandomforest_surrogate = RandomForestSurrogate(x ,y ,lower_bound, upper_bound, num_round = 2)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/randomforest/#Optimizing","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, randomforest_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(randomforest_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/randomforest/#Random-Forest-ND","page":"RandomForest","title":"Random Forest ND","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"First of all we will define the Bukin Function N. 6 function we are going to build surrogate for.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction bukin6(x)\n    x1=x[1]\n    x2=x[2]\n    term1 = 100 * sqrt(abs(x2 - 0.01*x1^2));\n    term2 = 0.01 * abs(x1+10);\n    y = term1 + term2;\nend","category":"page"},{"location":"modules/Surrogates/randomforest/#Sampling-2","page":"RandomForest","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = bukin6.(xys);","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> bukin6((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/randomforest/#Building-a-surrogate-2","page":"RandomForest","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"using SurrogatesRandomForest\nRandomForest = RandomForestSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/randomforest/#Optimizing-2","page":"RandomForest","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"surrogate_optimize(bukin6, SRBF(), lower_bound, upper_bound, RandomForest, SobolSample(), maxiters=20)","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/randomforest/","page":"RandomForest","title":"RandomForest","text":"p1 = surface(x, y, (x, y) -> RandomForest([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = bukin6.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> RandomForest([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/#Bifurcation-Diagrams","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Bifurcation diagrams can be produced from Catalyst generated models through the use of the BifurcationKit.jl package. This tutorial gives a simple example of how to create such a bifurcation diagram. This tutorial is written for BifurcationKit version 0.1.11.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"First, we declare our model. For our example we will use a bistable switch, but which also contains a Hopf bifurcation.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"using Catalyst\nrn = @reaction_network begin\n    (v0+v*(S*X)^n/((S*X)^n+(D*A)^n+K^n),d), ∅ ↔ X\n    (τ*X,τ), ∅ ↔ A\nend S D τ v0 v K n d","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Next, BifurcationKit requires another form for the system function and Jacobian than what is used by the SciML ecosystem, so we need to declare these:","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"odefun = ODEFunction(convert(ODESystem,rn),jac=true)\nF = (u,p) -> odefun(u,p,0)      \nJ = (u,p) -> odefun.jac(u,p,0)","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"We also need to specify the system parameters for which we wish to plot the bifurcation diagram:","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"params = [1.,9.,0.001,0.01,2.,20.,3,0.05]","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Finally, we also need to set the input required to make the diagram. This is the index (in the parameter array) of the bifurcation parameter, the range over which we wish to plot the bifurcation diagram, as well as for which variable we wish to plot the steady state values in the diagram.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"p_idx = 1            # The index of the bifurcation parameter.\np_span = (0.1,20.)   # The parameter range for the bifurcation diagram.\nplot_var_idx = 1     # The index of the variable we plot in the bifurcation diagram.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Now we need to fetch the required packages to create the bifurcation diagram.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"using BifurcationKit, Plots, LinearAlgebra, Setfield","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Next, we need to specify the input options for the pseudo-arclength continuation method which produces the diagram. We will use a deflated continuation.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"opts = ContinuationPar( dsmax = 0.05,        # Maximum arclength value of the pseudo-arc length continuation method.\n                        dsmin = 1e-4,        # Minimum arclength value of the pseudo-arc length continuation method.\n                        ds=0.001,            # Initial arclength value of the pseudo-arc length continuation method (should be positive).\n                        maxSteps = 100000,   # The maximum number of steps.\n                        pMin = p_span[1],    # Minimum p-vale (if hit, the method stops).\n                        pMax = p_span[2],    # Maximum p-vale (if hit, the method stops).\n                        detectBifurcation=3, # Value in {0,1,2,3} determening to what extent bofurcation points are detected (0 means nothing is done, 3 both them and there localisation are detected).\n                        newtonOptions = NewtonPar(tol = 1e-9, verbose = false, maxIter = 15)) #Parameters to the newton solver (when finding fixed points) see BifurcationKit documentation.\n                        \nDO = DeflationOperator( 2,      # Algorithm parameter required when using deflated continuation, see BifurcationKit documentation.\n                        dot,    # Algorithm parameter required when using deflated continuation, see BifurcationKit documentation.\n                        1.,     # Algorithm parameter required when using deflated continuation, see BifurcationKit documentation.\n                        [fill(0.,length(rn.states))]); # Guess(es) of the fixed point for the initial parameter set. Do not need to be exact.\n","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"With all this done, we can compute the bifurcations:","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"params_input = setindex!(copy(params),p_span[1],p_idx)                                # The input parameter values have to start at the first index of our parameter span.\nbranches, = continuation(F, J, params_input, (@lens _[p_idx]), opts , DO,             # Gives our input.\n    verbosity = 0, plot=false,                                                    # We do not want to display, or plot, intermediary results.\n    recordFromSolution = (x, p) -> x[plot_var_idx],                                   # How we wish to print the output in the diagram. Here we simply want the value of the target varriable.\n    perturbSolution = (x,p,id) -> (x  .+ 0.8 .* rand(length(x))),                     # Parameter for the continuation method, see BifurcationKit documentation.\n    callbackN = BifurcationKit.cbMaxNorm(1e7))                      # Parameter for the continuation method, see BifurcationKit documentation.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"which can then be plotted using","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"plot(branches...,xlabel=rn.ps[1],ylabel=Symbol(rn.states[1].val.f),markersize=4,\n     ylim=(0.,Inf),                                  # This ensures we do not display negative solutions.\n     color=:blue,                                    # Otherwise each individual branch will have their separate colors.\n     plotbifpoints = false, putbifptlegend = false,  # Plots the bifurcation point(s).\n     linewidthstable=4, linewidthunstable=1)         # Stable/unstable values are distinguised by line thickness.","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"(Image: bifurcation_diagram1)","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"Here the Hopf bifurcation is amrked with a blue square. The region with a thiner linewidth corresponds to unstable steady states. If one wishes to mark these differently it is possible to plot the individual brances separatly:","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"plot(branches[1],lw=4,color=map(i->(i==0) ? :blue : :red, getproperty.(branches[1].branch,:n_unstable)))\nplot!(branches[3],lw=4,color=map(i->(i==0) ? :blue : :red, getproperty.(branches[3].branch,:n_unstable)))\nplot!(branches[4],lw=4,color=map(i->(i==0) ? :blue : :red, getproperty.(branches[4].branch,:n_unstable)),plotbifpoints = false,xlabel=rn.ps[1],ylabel=Symbol(rn.states[1].val.f))","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"(Image: bifurcation_diagram2)","category":"page"},{"location":"modules/Catalyst/tutorials/bifurcation_diagram/","page":"Bifurcation Diagrams","title":"Bifurcation Diagrams","text":"(Note that the second branch corresponds to a negative steady state, which is biological irrelevant, and we hence do not plot)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Symbolic-metadata","page":"Symbolic metadata","title":"Symbolic metadata","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"It is possible to add metadata to symbolic variables, the metadata will be displayed when calling help on a variable.","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"The following information can be added (note, it's possible to extend this to user-defined metadata as well)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Variable-descriptions","page":"Symbolic metadata","title":"Variable descriptions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Descriptive strings can be attached to variables using the [description = \"descriptive string\"] syntax:","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"using ModelingToolkit\n@variables u [description = \"This is my input\"]\ngetdescription(u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"When variables with descriptions are present in systems, they will be printed when the system is shown in the terminal:","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@parameters t\n@variables u(t) [description = \"A short description of u\"]\n@parameters p   [description = \"A description of p\"]\n@named sys = ODESystem([u ~ p], t)\nshow(stdout, \"text/plain\", sys) # hide","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Calling help on the variable u displays the description, alongside other metadata:","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"help?> u\n\n  A variable of type Symbolics.Num (Num wraps anything in a type that is a subtype of Real)\n\n  Metadata\n  ≡≡≡≡≡≡≡≡≡≡\n\n  ModelingToolkit.VariableDescription: This is my input\n\n  Symbolics.VariableSource: (:variables, :u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Input-or-output","page":"Symbolic metadata","title":"Input or output","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Designate a variable as either an input or an output using the following","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"using ModelingToolkit\n@variables u [input=true]\nisinput(u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@variables y [output=true]\nisoutput(y)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Bounds","page":"Symbolic metadata","title":"Bounds","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Bounds are useful when parameters are to be optimized, or to express intervals of uncertainty.","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@variables u [bounds=(-1,1)]\nhasbounds(u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"getbounds(u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Mark-input-as-a-disturbance","page":"Symbolic metadata","title":"Mark input as a disturbance","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Indicate that an input is not available for control, i.e., it's a disturbance input.","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@variables u [input=true, disturbance=true]\nisdisturbance(u)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Mark-parameter-as-tunable","page":"Symbolic metadata","title":"Mark parameter as tunable","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Indicate that a parameter can be automatically tuned by parameter optimization or automatic control tuning apps.","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@parameters Kp [tunable=true]\nistunable(Kp)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Probability-distributions","page":"Symbolic metadata","title":"Probability distributions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"A probability distribution may be associated with a parameter to indicate either uncertainty about it's value, or as a prior distribution for Bayesian optimization.","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"using Distributions\nd = Normal(10, 1)\n@parameters m [dist=d]\nhasdist(m)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"getdist(m)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Additional-functions","page":"Symbolic metadata","title":"Additional functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"For systems that contain parameters with metadata like described above have some additional functions defined for convenience. In the example below, we define a system with tunable parameters and extract bounds vectors","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"@parameters t\nDₜ = Differential(t)\n@variables x(t)=0 u(t)=0 [input=true] y(t)=0 [output=true]\n@parameters T [tunable = true, bounds = (0, Inf)]\n@parameters k [tunable = true, bounds = (0, Inf)]\neqs = [\n    Dₜ(x) ~ (-x + k*u) / T # A first-order system with time constant T and gain k\n    y ~ x\n]\nsys = ODESystem(eqs, t, name=:tunable_first_order)","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"p = tunable_parameters(sys) # extract all parameters marked as tunable","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"lb, ub = getbounds(p) # operating on a vector, we get lower and upper bound vectors","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"b = getbounds(sys) # Operating on the system, we get a dict","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Index","page":"Symbolic metadata","title":"Index","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Pages = [\"Variable_metadata.md\"]","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#Docstrings","page":"Symbolic metadata","title":"Docstrings","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/","page":"Symbolic metadata","title":"Symbolic metadata","text":"Modules = [ModelingToolkit]\nPages = [\"variables.jl\"]\nPrivate = false","category":"page"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.getbounds","page":"Symbolic metadata","title":"ModelingToolkit.getbounds","text":"getbounds(sys::ModelingToolkit.AbstractSystem, p = parameters(sys))\n\nReturns a dict with pairs p => (lb, ub) mapping parameters of sys to lower and upper bounds. Create parameters with bounds like this\n\n@parameters p [bounds=(-1, 1)]\n\nTo obtain state bounds, call getbounds(sys, states(sys))\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.getbounds-Tuple{AbstractVector}","page":"Symbolic metadata","title":"ModelingToolkit.getbounds","text":"lb, ub = getbounds(p::AbstractVector)\n\nReturn vectors of lower and upper bounds of parameter vector p. Create parameters with bounds like this\n\n@parameters p [bounds=(-1, 1)]\n\nSee also tunable_parameters, hasbounds\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.getbounds-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.getbounds","text":"getbounds(x)\n\nGet the bounds associated with symbolc variable x. Create parameters with bounds like this\n\n@parameters p [bounds=(-1, 1)]\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.getdescription-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.getdescription","text":"getdescription(x)\n\nReturn any description attached to variables x. If no description is attached, an empty string is returned.\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.getdist-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.getdist","text":"getdist(x)\n\nGet the probability distribution associated with symbolc variable x. If no distribution is associated with x, nothing is returned. Create parameters with associated distributions like this\n\nusing Distributions\nd = Normal(0, 1)\n@parameters u [dist=d]\nhasdist(u) # true\ngetdist(u) # retrieve distribution\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.hasbounds-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.hasbounds","text":"hasbounds(x)\n\nDetermine whether or not symbolic variable x has bounds associated with it. See also getbounds.\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.hasdist-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.hasdist","text":"hasdist(x)\n\nDetermine whether or not symbolic variable x has a probability distribution associated with it.\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.isdisturbance-Tuple{Any}","page":"Symbolic metadata","title":"ModelingToolkit.isdisturbance","text":"isdisturbance(x)\n\nDetermine whether or not symbolic variable x is marked as a disturbance input.\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.istunable","page":"Symbolic metadata","title":"ModelingToolkit.istunable","text":"istunable(x, default = false)\n\nDetermine whether or not symbolic variable x is marked as a tunable for an automatic tuning algorithm.\n\ndefault indicates whether variables without tunable metadata are to be considered tunable or not.\n\nCreate a tunable parameter by\n\n@parameters u [tunable=true]\n\nSee also tunable_parameters, getbounds\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/Variable_metadata/#ModelingToolkit.tunable_parameters","page":"Symbolic metadata","title":"ModelingToolkit.tunable_parameters","text":"tunable_parameters(sys, p = parameters(sys); default=false)\n\nGet all parameters of sys that are marked as tunable.\n\nKeyword argument default indicates whether variables without tunable metadata are to be considered tunable or not.\n\nCreate a tunable parameter by\n\n@parameters u [tunable=true]\n\nSee also getbounds, istunable\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/types/sde_types/#SDE-Problems","page":"SDE Problems","title":"SDE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"SDEProblem\nSDEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/#SciMLBase.SDEProblem","page":"SDE Problems","title":"SciMLBase.SDEProblem","text":"Defines an stochastic differential equation (SDE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/sde_types/\n\nMathematical Specification of a SDE Problem\n\nTo define an SDE Problem, you simply need to give the forcing function f, the noise function g, and the initial condition u₀ which define an SDE:\n\ndu = f(upt)dt + Σgᵢ(upt)dWⁱ\n\nf and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.\n\nProblem Type\n\nWraps the data which defines an SDE problem\n\nu = f(upt)dt + Σgᵢ(upt)dWⁱ\n\nwith initial condition u0.\n\nConstructors\n\nSDEProblem(f::SDEFunction,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing)\nSDEProblem{isinplace}(f,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing) : Defines the SDE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The drift function in the SDE.\ng: The noise function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page\nnoise_rate_prototype: A prototype type instance for the noise rates, that is the output g. It can be any type which overloads A_mul_B! with itself being the middle argument. Commonly, this is a matrix or sparse matrix. If this is not given, it defaults to nothing, which means the problem should be interpreted as having diagonal noise.  \nkwargs: The keyword arguments passed onto the solves.\n\nExample Problems\n\nExamples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_sde_linear, you can do something like:\n\n#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.SDEProblemLibrary\n# load problems\nSDEProblemLibrary.importsdeproblems()\nprob = SDEProblemLibrary.prob_sde_linear\nsol = solve(prob)\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/sde_types/#SciMLBase.SDEFunction","page":"SDE Problems","title":"SciMLBase.SDEFunction","text":"SDEFunction{iip,F,G,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,GG,S,O,TCV} <: AbstractSDEFunction{iip}\n\nA representation of an SDE function f, defined by:\n\nM du = f(upt)dt + g(upt) dW\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nSDEFunction{iip,recompile}(f,g;\n                           mass_matrix=I,\n                           analytic=nothing,\n                           tgrad=nothing,\n                           jac=nothing,\n                           jvp=nothing,\n                           vjp=nothing,\n                           ggprime = nothing,\n                           jac_prototype=nothing,\n                           sparsity=jac_prototype,\n                           paramjac = nothing,\n                           syms = nothing,\n                           indepsym = nothing,\n                           colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\nggprime(J,u,p,t) or J = ggprime(u,p,t): returns the Milstein derivative fracdg(upt)du g(upt)\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the ODEFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/sde_types/#Solution-Type","page":"SDE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"SDEProblem solutions return an RODESolution. For more information, see the RODE problem definition page for the RODESolution docstring.","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/#Example-Problems","page":"SDE Problems","title":"Example Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"To use a sample problem, such as prob_sde_linear, you can do something like:","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.SDEProblemLibrary\n# load problems\nSDEProblemLibrary.importsdeproblems()\nprob = SDEProblemLibrary.prob_sde_linear\nsol = solve(prob)","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"CurrentModule = SDEProblemLibrary","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"prob_sde_linear\nprob_sde_2Dlinear\nprob_sde_wave\nprob_sde_lorenz\nprob_sde_cubic\nprob_sde_additive\nprob_sde_additivesystem\nprob_sde_nltest\noval2ModelExample\nprob_sde_stiffquadstrat\nprob_sde_stiffquadito\ngenerate_stiff_stoch_heat\nprob_sde_bistable\nprob_sde_bruss\nprob_sde_oscilreact","category":"page"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_linear","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_linear","text":"du_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condtion u_0=12, with solution\n\nu(u_0ptW_t)=u_0exp((α-fracβ^22)t+βW_t)\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_2Dlinear","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_2Dlinear","text":"8 linear SDEs (as a 4x2 matrix):\n\ndu_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condtion u_0=frac12 with solution\n\nu(u_0ptW_t)=u_0exp((α-fracβ^22)t+βW_t)\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_wave","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_wave","text":"du_t = -frac1100sin(u)cos^3(u)dt + frac110cos^2(u_t) dW_t\n\nand initial condition u_0=1 with solution\n\nu(u_0ptW_t)=arctan(fracW_t10 + tan(u_0))\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_lorenz","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_lorenz","text":"Lorenz Attractor with additive noise\n\ndx = σ(y-x)dt + αdW_t\n\ndy = (x(ρ-z) - y)dt + αdW_t\n\ndz = (xy - βz)dt + αdW_t\n\nwith σ=10, ρ=28, β=83, α=30 and inital condition u_0=111.\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_cubic","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_cubic","text":"du_t = frac14u(1-u^2)dt + frac12(1-u^2)dW_t\n\nand initial condtion u_0=frac12, with solution\n\nu(u0ptW_t)=frac(1+u_0)exp(W_t)+u)0-1(1+u_0)exp(W_t)+1-u_0\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additive","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additive","text":"Additive noise problem\n\nu_t = (fracβsqrt1+t-frac12(1+t)u_t)dt + fracαβsqrt1+tdW_t\n\nand initial condition u_0=1 with α=01 and β=005, with solution\n\nu(u_0ptW_t)=fracu_0sqrt1+t + fracβ(t+αW_t)sqrt1+t\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additivesystem","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additivesystem","text":"A multiple dimension extension of additiveSDEExample\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_nltest","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_nltest","text":"Runge–Kutta methods for numerical solution of stochastic differential equations Tocino and Ardanuy\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.oval2ModelExample","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.oval2ModelExample","text":"oval2ModelExample(;largeFluctuations=false,useBigs=false,noiseLevel=1)\n\nA function which generates the Oval2 Epithelial-Mesenchymal Transition model from:\n\nRackauckas, C., & Nie, Q. (2017). Adaptive methods for stochastic differential equations  via natural embeddings and rejection sampling with memory. Discrete and continuous  dynamical systems. Series B, 22(7), 2731.\n\n19 SDEs which are only stiff during transitions between biological states.\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadstrat","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadstrat","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadito","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadito","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.generate_stiff_stoch_heat","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.generate_stiff_stoch_heat","text":"Stochastic Heat Equation with scalar multiplicative noise\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nRaising D or k increases stiffness\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bistable","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bistable","text":"Bistable chemical reaction network with a semi-stable lower state.\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bruss","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bruss","text":"Stochastic Brusselator\n\n\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_oscilreact","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_oscilreact","text":"An oscillatory chemical reaction system\n\n\n\n\n\n","category":"constant"},{"location":"modules/NonlinearSolve/tutorials/iterator_interface/#Nonlinear-Solver-Iterator-Interface","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"","category":"section"},{"location":"modules/NonlinearSolve/tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"There is an iterator form of the nonlinear solver which mirrors the DiffEq integrator interface:","category":"page"},{"location":"modules/NonlinearSolve/tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"f(u, p) = u .* u .- 2.0\nu0 = (1.0, 2.0) # brackets\nprobB = NonlinearProblem(f, u0)\nsolver = init(probB, Falsi()) # Can iterate the solver object\nsolver = solve!(solver)","category":"page"},{"location":"modules/NonlinearSolve/tutorials/iterator_interface/","page":"Nonlinear Solver Iterator Interface","title":"Nonlinear Solver Iterator Interface","text":"Note that the solver object is actually immutable since we want to make it live on the stack for the sake of performance.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"The Inverse Distance Surrogate is an interpolating method and in this method the unknown points are calculated with a weighted average of the sampling points. This model uses the inverse distance between the unknown and training points to predict the unknown point. We do not need to fit this model because the response of an unknown point x is computed with respect to the distance between x and the training points.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Let's optimize the following function to use Inverse Distance Surrogate:","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":".","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"First of all, we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Sampling","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing LowDiscrepancySample() to the sample function.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"f(x) = sin(x) + sin(x)^2 + sin(x)^3\r\n\r\nn_samples = 25\r\nlower_bound = 0.0\r\nupper_bound = 10.0\r\nx = sample(n_samples, lower_bound, upper_bound, LowDiscrepancySample(2))\r\ny = f.(x)\r\n\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Building-a-Surrogate","page":"InverseDistance","title":"Building a Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(InverseDistance, 5.0, f(5.0))\r\nadd_point!(InverseDistance, [5.1,5.2], [f(5.1),f(5.2)])\r\nprediction = InverseDistance(5.0)","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Now, we will simply plot InverseDistance:","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Optimizing","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample())\r\nscatter(x, y, label=\"Sampled points\", legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(InverseDistance, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Inverse-Distance-Surrogate-Tutorial-(ND):","page":"InverseDistance","title":"Inverse Distance Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction schaffer(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    fact1 = (sin(x1^2-x2^2))^2 - 0.5;\r\n    fact2 = (1 + 0.001*(x1^2+x2^2))^2;\r\n    y = 0.5 + fact1/fact2;\r\nend","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Sampling-2","page":"InverseDistance","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"n_samples = 60\r\nlower_bound = [-5.0, 0.0]\r\nupper_bound = [10.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = schaffer.(xys);","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"x, y = -5:10, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Building-a-surrogate","page":"InverseDistance","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/InverseDistance/#Optimizing-2","page":"InverseDistance","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, InverseDistance, SobolSample(), maxiters=10)","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/InverseDistance/","page":"InverseDistance","title":"InverseDistance","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = schaffer.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"modules/DiffEqDocs/analysis/neural_networks/#Neural-Networks","page":"Neural Networks","title":"Neural Networks","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/neural_networks/","page":"Neural Networks","title":"Neural Networks","text":"To use DifferentialEquations.jl with the Flux.jl neural network package, consult the documentation at DiffEqFlux.jl.","category":"page"},{"location":"modules/Optimization/optimization_packages/optimisers/#optimisers","page":"Optimisers.jl","title":"Optimisers.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optimisers/#Installation:-OptimizationFlux.jl","page":"Optimisers.jl","title":"Installation: OptimizationFlux.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optimisers/","page":"Optimisers.jl","title":"Optimisers.jl","text":"To use this package, install the OptimizationOptimisers package:","category":"page"},{"location":"modules/Optimization/optimization_packages/optimisers/","page":"Optimisers.jl","title":"Optimisers.jl","text":"import Pkg; Pkg.add(\"OptimizationOptimisers\")","category":"page"},{"location":"modules/Optimization/optimization_packages/optimisers/#Local-Unconstrained-Optimizers","page":"Optimisers.jl","title":"Local Unconstrained Optimizers","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optimisers/","page":"Optimisers.jl","title":"Optimisers.jl","text":"Optimisers.Descent: Classic gradient descent optimizer with learning rate\nsolve(problem, Descent(η))\nη is the learning rate\nDefaults:\nη = 0.1\nOptimisers.Momentum: Classic gradient descent optimizer with learning rate and momentum\nsolve(problem, Momentum(η, ρ))\nη is the learning rate\nρ is the momentum\nDefaults:\nη = 0.01\nρ = 0.9\nOptimisers.Nesterov: Gradient descent optimizer with learning rate and Nesterov momentum\nsolve(problem, Nesterov(η, ρ))\nη is the learning rate\nρ is the Nesterov momentum\nDefaults:\nη = 0.01\nρ = 0.9\nOptimisers.RMSProp: RMSProp optimizer\nsolve(problem, RMSProp(η, ρ))\nη is the learning rate\nρ is the momentum\nDefaults:\nη = 0.001\nρ = 0.9\nOptimisers.Adam: Adam optimizer\nsolve(problem, Adam(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nOptimisers.RAdam: Rectified Adam optimizer\nsolve(problem, RAdam(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nOptimisers.RAdam: Optimistic Adam optimizer\nsolve(problem, OAdam(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.5, 0.999)\nOptimisers.AdaMax: AdaMax optimizer\nsolve(problem, AdaMax(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nOptimisers.ADAGrad: ADAGrad optimizer\nsolve(problem, ADAGrad(η))\nη is the learning rate\nDefaults:\nη = 0.1\nOptimisers.ADADelta: ADADelta optimizer\nsolve(problem, ADADelta(ρ))\nρ is the gradient decay factor\nDefaults:\nρ = 0.9\nOptimisers.AMSGrad: AMSGrad optimizer\nsolve(problem, AMSGrad(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nOptimisers.NAdam: Nesterov variant of the Adam optimizer\nsolve(problem, NAdam(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nOptimisers.AdamW: AdamW optimizer\nsolve(problem, AdamW(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\ndecay is the decay to weights\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\ndecay = 0\nOptimisers.ADABelief: ADABelief variant of Adam\nsolve(problem, ADABelief(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)","category":"page"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#Recursive-Array-Functions","page":"Recursive Array Functions","title":"Recursive Array Functions","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/","page":"Recursive Array Functions","title":"Recursive Array Functions","text":"These are functions designed for recursive arrays, like arrays of arrays, and do not require that the RecursiveArrayTools types are used.","category":"page"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#Function-List","page":"Recursive Array Functions","title":"Function List","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/","page":"Recursive Array Functions","title":"Recursive Array Functions","text":"recursivecopy\nrecursivecopy!\nvecvecapply\ncopyat_or_push!","category":"page"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#RecursiveArrayTools.recursivecopy","page":"Recursive Array Functions","title":"RecursiveArrayTools.recursivecopy","text":"recursivecopy(b::AbstractArray{T,N},a::AbstractArray{T,N})\n\nA recursive copy function. Acts like a deepcopy on arrays of arrays, but like copy on arrays of scalars.\n\n\n\n\n\n","category":"function"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#RecursiveArrayTools.recursivecopy!","page":"Recursive Array Functions","title":"RecursiveArrayTools.recursivecopy!","text":"recursivecopy!(b::AbstractArray{T,N},a::AbstractArray{T,N})\n\nA recursive copy! function. Acts like a deepcopy! on arrays of arrays, but like copy! on arrays of scalars.\n\n\n\n\n\n","category":"function"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#RecursiveArrayTools.vecvecapply","page":"Recursive Array Functions","title":"RecursiveArrayTools.vecvecapply","text":"vecvecapply(f::Base.Callable,v)\n\nCalls f on each element of a vecvec v.\n\n\n\n\n\n","category":"function"},{"location":"modules/RecursiveArrayTools/recursive_array_functions/#RecursiveArrayTools.copyat_or_push!","page":"Recursive Array Functions","title":"RecursiveArrayTools.copyat_or_push!","text":"copyat_or_push!{T}(a::AbstractVector{T},i::Int,x)\n\nIf i<length(x), it's simply a recursivecopy! to the ith element. Otherwise, it will push! a deepcopy.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLAlgorithms","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Definition-of-the-AbstractSciMLAlgorithm-Interface","page":"SciMLAlgorithms","title":"Definition of the AbstractSciMLAlgorithm Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"SciMLAlgorithms are defined as types which have dispatches to the function signature:","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"CommonSolve.solve(prob::AbstractSciMLProblem,alg::AbstractSciMLAlgorithm;kwargs...)","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Algorithm-Specific-Arguments","page":"SciMLAlgorithms","title":"Algorithm-Specific Arguments","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"Note that because the keyword arguments of solve are designed to be common across the whole problem type, algorithms should have the algorithm-specific keyword arguments defined as part of the algorithm constructor. For example, Rodas5 has a choice of autodiff::Bool which is not common across all ODE solvers, and thus autodiff is a algorithm-specific keyword argument handled via Rodas5(autodiff=true).","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Remake","page":"SciMLAlgorithms","title":"Remake","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"Note that remake is applicable to AbstractSciMLAlgorithm types, but this is not used in the public API. It's used for solvers to swap out components like ForwardDiff chunk sizes.","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Common-Algorithm-Keyword-Arguments","page":"SciMLAlgorithms","title":"Common Algorithm Keyword Arguments","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"Commonly used algorithm keyword arguments are:","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Traits","page":"SciMLAlgorithms","title":"Traits","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"SciMLBase.isautodifferentiable\nSciMLBase.allows_arbitrary_number_types\nSciMLBase.allowscomplex\nSciMLBase.isadaptive\nSciMLBase.isdiscrete","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.isautodifferentiable","page":"SciMLAlgorithms","title":"SciMLBase.isautodifferentiable","text":"isautodifferentiable(alg::AbstractDEAlgorithm)\n\nTrait declaration for whether an algorithm is compatible with direct automatic differentiation, i.e. can have algorithms like ForwardDiff or ReverseDiff attempt to differentiate directly through the solver.\n\nDefaults to false as only pure-Julia algorithms can have this be true.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.allows_arbitrary_number_types","page":"SciMLAlgorithms","title":"SciMLBase.allows_arbitrary_number_types","text":"allowsarbitrarynumber_types(alg::AbstractDEAlgorithm)\n\nTrait declaration for whether an algorithm is compatible with direct automatic differentiation, i.e. can have algorithms like ForwardDiff or ReverseDiff attempt to differentiate directly through the solver.\n\nDefaults to false as only pure-Julia algorithms can have this be true.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.allowscomplex","page":"SciMLAlgorithms","title":"SciMLBase.allowscomplex","text":"allowscomplex(alg::AbstractDEAlgorithm)\n\nTrait declaration for whether an algorithm is compatible with having complex numbers as the state variables.\n\nDefaults to false.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.isadaptive","page":"SciMLAlgorithms","title":"SciMLBase.isadaptive","text":"isadaptive(alg::AbstractDEAlgorithm)\n\nTrait declaration for whether an algorithm uses adaptivity, i.e. has a non-quasi-static compute graph.\n\nDefaults to true.\n\n\n\n\n\nis_integrator_adaptive(i::DEIntegrator)\n\nChecks if the integrator is adaptive\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.isdiscrete","page":"SciMLAlgorithms","title":"SciMLBase.isdiscrete","text":"isdiscrete(alg::AbstractDEAlgorithm)\n\nTrait declaration for whether an algorithm allows for discrete state values, such as integers.\n\nDefaults to false.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Algorithms/#Abstract-SciML-Algorithms","page":"SciMLAlgorithms","title":"Abstract SciML Algorithms","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Algorithms/","page":"SciMLAlgorithms","title":"SciMLAlgorithms","text":"SciMLBase.AbstractSciMLAlgorithm\nSciMLBase.AbstractDEAlgorithm\nSciMLBase.AbstractLinearAlgorithm\nSciMLBase.AbstractNonlinearAlgorithm\nSciMLBase.AbstractQuadratureAlgorithm\nSciMLBase.AbstractOptimizationAlgorithm\nSciMLBase.AbstractSteadyStateAlgorithm\nSciMLBase.AbstractODEAlgorithm\nSciMLBase.AbstractSecondOrderODEAlgorithm\nSciMLBase.AbstractRODEAlgorithm\nSciMLBase.AbstractSDEAlgorithm\nSciMLBase.AbstractDAEAlgorithm\nSciMLBase.AbstractDDEAlgorithm\nSciMLBase.AbstractSDDEAlgorithm","category":"page"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractSciMLAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractSciMLAlgorithm","text":"abstract type AbstractSciMLAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractDEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractDEAlgorithm","text":"abstract type AbstractDEAlgorithm <: SciMLBase.AbstractSciMLAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractLinearAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractLinearAlgorithm","text":"abstract type AbstractLinearAlgorithm <: SciMLBase.AbstractSciMLAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractNonlinearAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractNonlinearAlgorithm","text":"abstract type AbstractNonlinearAlgorithm <: SciMLBase.AbstractSciMLAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractQuadratureAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractQuadratureAlgorithm","text":"abstract type AbstractIntegralAlgorithm <: SciMLBase.AbstractSciMLAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractOptimizationAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractOptimizationAlgorithm","text":"abstract type AbstractOptimizationAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractSteadyStateAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractSteadyStateAlgorithm","text":"abstract type AbstractSteadyStateAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractODEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractODEAlgorithm","text":"abstract type AbstractODEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractSecondOrderODEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractSecondOrderODEAlgorithm","text":"abstract type AbstractSecondOrderODEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractRODEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractRODEAlgorithm","text":"abstract type AbstractRODEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractSDEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractSDEAlgorithm","text":"abstract type AbstractSDEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractDAEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractDAEAlgorithm","text":"abstract type AbstractDAEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractDDEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractDDEAlgorithm","text":"abstract type AbstractDDEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Algorithms/#SciMLBase.AbstractSDDEAlgorithm","page":"SciMLAlgorithms","title":"SciMLBase.AbstractSDDEAlgorithm","text":"abstract type AbstractSDDEAlgorithm <: SciMLBase.AbstractDEAlgorithm\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Non-autonomous-Linear-ODE-/-Lie-Group-ODE-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Non-autonomous linear ODE solvers focus on equations in the general form of","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(upt)u","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"and utilize the Lie group structure of the solution to accelerate the numerical methods and capture certain properties in the solution process. One common simplification is for solvers to require state-independent operators, which implies the form:","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(t)u","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Another type of solvers are needed when the operators are state-dependent, i.e.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(u)u","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Others specifically require linearity, i.e.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = Au","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"where A is a constant operator.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Recommendations","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Recommendations","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"It is recommended to always specialize on the properties of the operator as much as possible.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Standard-ODE-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Standard ODE Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"The standard ODE integrators will work on Non-autonomous linear ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Note that all of these methods are fixed timestep unless otherwise specified.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Exponential-Methods-for-Linear-and-Affine-Problems","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Exponential Methods for Linear and Affine Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods require that A is constant.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"LinearExponential - Exact solution formula for linear, time-independent problems.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Options:","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"krylov - symbol. One of\n:off (default) - cache the operator beforehand. Requires Matrix(A) method defined for the operator A.\n:simple - uses simple Krylov approximations with fixed subspace size m.\n:adaptive - uses adaptive Krylov approximations with internal timestepping.\nm - integer, default: 30. Controls the size of Krylov subsapce if krylov=:simple, and the initial subspace size if krylov=:adaptive.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"_A = [2 -1;-3 -5]/5\nA = DiffEqArrayOperator(_A)\nprob = ODEProblem(A, [1.0,-1.0], (1.0, 6.0))\nsol = solve(prob, LinearExponential())","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"note: Note\nLinearExponential is exact, and thus it uses dt=tspan[2]-tspan[1] by default. The interpolation used is inexact (3rd order Hermite). Thus values generated by the interpolation (via sol(t) or saveat) will be inexact with increasing error as the size of the time span grows. To counteract this, directly set dt or use tstops instead of of saveat. For more information, see this issue","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#State-Independent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Independent Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods require A is only dependent on the independent variable, i.e. A(t).","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"MagnusMidpoint - Second order Magnus Midpoint method.\nMagnusLeapfrog- Second order Magnus Leapfrog method.\nMagnusGauss4 - Fourth order Magnus method approximated using a two stage Gauss quadrature.\nMagnusGL4- Fourth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC6- Sixth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL6- Sixth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC8- Eighth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL8- Eighth order Magnus method approximated using Gauss-Legendre quadrature.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = cos(t)\n    A[2,1] = sin(t)\n    A[1,2] = -sin(t)\n    A[2,2] = cos(t)\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (1.0, 6.0))\nsol = solve(prob,MagnusGL6(),dt=1/10)","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#State-Dependent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Dependent Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods can be used when A is dependent on the state variables, i.e. A(u).","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"CayleyEuler - First order method using Cayley transformations.\nLieEuler - First order Lie Euler method.\nRKMK2 - Second order Runge–Kutta–Munthe-Kaas method.\nRKMK4 - Fourth order Runge–Kutta–Munthe-Kaas method.\nLieRK4 - Fourth order Lie Runge-Kutta method.\nCG2 - Second order Crouch–Grossman method.\nMagnusAdapt4 - Fourth Order Adaptive Magnus method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = sin(u[1])\n    A[1,2] = -1\n    A[2,2] = 0\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (0, 30.))\nsol = solve(prob,LieRK4(),dt=1/4)","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"The above example solves a non-stiff Non-Autonomous Linear ODE with a state dependent operator, using the LieRK4 method. Similarly, a stiff Non-Autonomous Linear ODE with state dependent operators can be solved using specialized adaptive algorithms, like MagnusAdapt4. ","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = 1\n    A[1,2] = -2*(1 - cos(u[2]) - u[2]*sin(u[2]))\n    A[2,2] = 0\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (30, 150.))\nsol = solve(prob,MagnusAdapt4())","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/#Time-and-State-Dependent-Operators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Time and State-Dependent Operators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods can be used when A is dependent on both time and state variables, i.e. A(ut)","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"CG3 - Third order Crouch-Grossman method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"[1]: A description of IOP can be found in this paper.","category":"page"},{"location":"modules/Surrogates/kriging/#Kriging-surrogate-tutorial-(1D)","page":"Kriging","title":"Kriging surrogate tutorial (1D)","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"Kriging or Gaussian process regression is a method of interpolation for which the interpolated values are modeled by a Gaussian process.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"We are going to use a Kriging surrogate to optimize f(x)=(6x-2)^2sin(12x-4). (function from Forrester et al. (2008)).","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/kriging/#Sampling","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"We choose to sample f in 4 points between 0 and 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"# https://www.sfu.ca/~ssurjano/forretal08.html\n# Forrester et al. (2008) Function\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\n\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\n\nxs = lower_bound:0.001:upper_bound\n\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17))\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/kriging/#Building-a-surrogate","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"With our sampled points we can build the Kriging surrogate using the Kriging function.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate behaves like an ordinary function which we can simply plot. A nice statistical property of this surrogate is being able to calculate the error of the function at each point, we plot this as a confidence interval using the ribbon argument.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(x, y, lower_bound, upper_bound);\n\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"modules/Surrogates/kriging/#Optimizing","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample())\n\nscatter(x, y, label=\"Sampled points\", ylims=(-7, 7), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(xs, kriging_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(kriging_surrogate, p), legend=:top)","category":"page"},{"location":"modules/Surrogates/kriging/#Kriging-surrogate-tutorial-(ND)","page":"Kriging","title":"Kriging surrogate tutorial (ND)","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"First of all let's define the function we are going to build a surrogate for. Notice how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction branin(x)\n    x1=x[1]\n    x2=x[2]\n    a=1;\n    b=5.1/(4*π^2);\n    c=5/π;\n    r=6;\n    s=10;\n    t=1/(8π);\n    a*(x2-b*x1+c*x1-r)^2+s*(1-t)*cos(x1)+s\nend","category":"page"},{"location":"modules/Surrogates/kriging/#Sampling-2","page":"Kriging","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -5, 10, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"n_samples = 50\nlower_bound = [-5.0, 0.0]\nupper_bound = [10.0, 15.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = branin.(xys);","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"x, y = -5:10, 0:15 # hide\np1 = surface(x, y, (x1,x2) -> branin((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> branin((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/kriging/#Building-a-surrogate-2","page":"Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound, p=[2.0, 2.0], theta=[0.03, 0.003])","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/kriging/#Optimizing-2","page":"Kriging","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"With our surrogate we can now search for the minima of the branin function.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"surrogate_optimize(branin, SRBF(), lower_bound, upper_bound, kriging_surrogate, SobolSample(), maxiters=10)","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/kriging/","page":"Kriging","title":"Kriging","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nzs = branin.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"modules/LinearSolve/basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/LinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Ask more questions.","category":"page"},{"location":"modules/LinearSolve/basics/FAQ/#How-do-I-use-IterativeSolvers-solvers-with-a-weighted-tolerance-vector?","page":"Frequently Asked Questions","title":"How do I use IterativeSolvers solvers with a weighted tolerance vector?","text":"","category":"section"},{"location":"modules/LinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"IterativeSolvers.jl computes the norm after the application of the left precondtioner Pl. Thus in order to use a vector tolerance weights, one can mathematically hack the system via the following formulation:","category":"page"},{"location":"modules/LinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\nPl = LinearSolve.InvPreconditioner(Diagonal(weights))\nPr = Diagonal(weights)\n\nA = rand(n,n)\nb = rand(n)\n\nprob = LinearProblem(A,b)\nsol = solve(prob,IterativeSolvers_GMRES(),Pl=Pl,Pr=Pr)","category":"page"},{"location":"modules/LinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you want to use a \"real\" preconditioner under the norm weights, then one can use ComposePreconditioner to apply the preconditioner after the application of the weights like as follows:","category":"page"},{"location":"modules/LinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearSolve, LinearAlgebra\nPl = ComposePreconitioner(LinearSolve.InvPreconditioner(Diagonal(weights),realprec))\nPr = Diagonal(weights)\n\nA = rand(n,n)\nb = rand(n)\n\nprob = LinearProblem(A,b)\nsol = solve(prob,IterativeSolvers_GMRES(),Pl=Pl,Pr=Pr)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#callbacks","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"DifferentialEquations.jl allows for using callback functions to inject user code into the solver algorithms. It allows for safely and accurately applying events and discontinuities. Multiple callbacks can be chained together, and these callback types can be used to build libraries of extension behavior.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#The-Callback-Types","page":"Event Handling and Callback Functions","title":"The Callback Types","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback types are defined as follows. There are three primitive callback types: the ContinuousCallback, DiscreteCallback and the VectorContinuousCallback: ","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The ContinuousCallback is applied when a given continuous condition function hits zero. This hitting can happen even within an integration step and the solver must be able to detect it and adjust the integration step accordingly. This type of callback implements what is known in other problem solving environments as an Event. \nThe DiscreteCallback is applied when its condition function is true, but the condition is only evaluated at the end of every integration step. \nThe VectorContinuousCallback works like a vector of ContinuousCallbacks and lets the user specify which callback is called when.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#ContinuousCallback","page":"Event Handling and Callback Functions","title":"ContinuousCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"ContinuousCallback","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#SciMLBase.ContinuousCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.ContinuousCallback","text":"ContinuousCallback(condition,affect!,affect_neg!;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nfunction ContinuousCallback(condition,affect!;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   affect_neg! = affect!,\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nContains a single callback whose condition is a continuous function. The callback is triggered when this function evaluates to 0.\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition hits 0 within the time interval. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to modify the current state of the integrator. If you do not pass an affect_neg! function, it is called when condition is found to be 0 (at a root) and the cross is either an upcrossing (from negative to positive) or a downcrossing (from positive to negative). You need to explicitly pass nothing as the affect_neg! argument if it should only be called at upcrossings, e.g. ContinuousCallback(condition, affect!, nothing). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\naffect_neg!=affect!: This is the function affect_neg!(integrator) where one is allowed to modify the current state of the integrator. This is called when condition is found to be 0 (at a root) and the cross is an downcrossing (from positive to negative). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\nrootfind=LeftRootFind: This is a flag to specify the type of rootfinding to do for finding event location. If this is set to LeftRootfind, the solution will be backtracked to the point where condition==0 and if the solution isn't exact, the left limit of root is used. If set to RightRootFind, the solution would be set to the right limit of the root. Otherwise the systems and the affect! will occur at t+dt. Note that these enums are not exported and thus one needs to reference them as SciMLBase.LeftRootFind, SciMLBase.RightRootFind, or SciMLBase.NoRootFind.\ninterp_points=10: The number of interpolated points to check the condition. The condition is found by checking whether any interpolation point / endpoint has a different sign. If interp_points=0, then conditions will only be noticed if the sign of condition is different at t than at t+dt. This behavior is not robust when the solution is oscillatory, and thus it's recommended that one use some interpolation points (they're cheap to compute!). 0 within the time interval.\nsave_positions=(true,true): Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\nidxs=nothing: The components which will be interpolated into the condition. Defaults to nothing which means u will be all components.\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It can modify the argument c and the return is ignored.\nabstol=1e-14 & reltol=0: These are used to specify a tolerance from zero for the rootfinder: if the starting condition is less than the tolerance from zero, then no root will be detected. This is to stop repeat events happening just after a previously rootfound event.\nrepeat_nudge = 1//100: This is used to set the next testing point after a previously found zero. Defaults to 1//100, which means after a callback the next sign check will take place at t + dt*1//100 instead of at t to avoid repeats.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/features/callback_functions/#discrete_callback","page":"Event Handling and Callback Functions","title":"DiscreteCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"DiscreteCallback","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#SciMLBase.DiscreteCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.DiscreteCallback","text":"DiscreteCallback(condition,affect!;\n                 initialize = INITIALIZE_DEFAULT,\n                 finalize = FINALIZE_DEFAULT,\n                 save_positions=(true,true))\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition evaluates to true. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to\nmodify the current state of the integrator. For more information on what can be done, see the Integrator Interface manual page.\nsave_positions: Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It should can the argument c and the return is ignored.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/features/callback_functions/#CallbackSet","page":"Event Handling and Callback Functions","title":"CallbackSet","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"CallbackSet","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#SciMLBase.CallbackSet","page":"Event Handling and Callback Functions","title":"SciMLBase.CallbackSet","text":"struct CallbackSet{T1<:Tuple, T2<:Tuple} <: SciMLBase.DECallback\n\nMultiple callbacks can be chained together to form a CallbackSet. A CallbackSet is constructed by passing the constructor ContinuousCallback, DiscreteCallback, VectorContinuousCallback or other CallbackSet instances:\n\nCallbackSet(cb1,cb2,cb3)\n\nYou can pass as many callbacks as you like. When the solvers encounter multiple callbacks, the following rules apply:\n\nContinuousCallbacks and VectorContinuousCallbacks are applied before DiscreteCallbacks. (This is because they often implement event-finding that will backtrack the timestep to smaller than dt).\nFor ContinuousCallbacks and VectorContinuousCallbacks, the event times are found by rootfinding and only the first ContinuousCallback or VectorContinuousCallback affect is applied.\nThe DiscreteCallbacks are then applied in order. Note that the ordering only matters for the conditions: if a previous callback modifies u in such a way that the next callback no longer evaluates condition to true, its affect will not be applied.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/features/callback_functions/#VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"VectorContinuousCallback","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#SciMLBase.VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.VectorContinuousCallback","text":"VectorContinuousCallback(condition,affect!,affect_neg!,len;\n                         initialize = INITIALIZE_DEFAULT,\n                         finalize = FINALIZE_DEFAULT,\n                         idxs = nothing,\n                         rootfind=LeftRootFind,\n                         save_positions=(true,true),\n                         interp_points=10,\n                         abstol=10eps(),reltol=0,repeat_nudge = 1//100)\n\nVectorContinuousCallback(condition,affect!,len;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   affect_neg! = affect!,\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nThis is also a subtype of AbstractContinuousCallback. CallbackSet is not feasible when you have a large number of callbacks, as it doesn't scale well. For this reason, we have VectorContinuousCallback - it allows you to have a single callback for multiple events.\n\nArguments\n\ncondition: This is a function condition(out, u, t, integrator) which should save the condition value in the array out  at the right index. Maximum index of out should be specified in the len property of callback. So this way you can have  a chain of len events, which would cause the ith event to trigger when out[i] = 0.\naffect!: This is a function affect!(integrator, event_index) which lets you modify integrator and it tells you about  which event occured using event_idx i.e. gives you index i for which out[i] came out to be zero.\nlen: Number of callbacks chained. This is compulsory to be specified.\n\nRest of the arguments have the same meaning as in ContinuousCallback.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/features/callback_functions/#Using-Callbacks","page":"Event Handling and Callback Functions","title":"Using Callbacks","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback type is then sent to the solver (or the integrator) via the callback keyword argument:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,alg,callback=cb)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"You can supply nothing, a single DiscreteCallback or ContinuousCallback or VectorContinuousCallback, or a CallbackSet.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Note-About-Saving","page":"Event Handling and Callback Functions","title":"Note About Saving","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"When a callback is supplied, the default saving behavior is turned off. This is because otherwise events would \"double save\" one of the values. To re-enable the standard saving behavior, one must have the first save_positions value be true for at least one callback.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Modifying-the-Stepping-Within-A-Callback","page":"Event Handling and Callback Functions","title":"Modifying the Stepping Within A Callback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"A common issue with callbacks is that they cause a large discontinuous change, and so it may be wise to pull down dt after such a change. To control the timestepping from a callback, please see the timestepping controls in the integrator interface. Specifically, set_proposed_dt! is used to set the next stepsize, and terminate! can be used to cause the simulation to stop.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#DiscreteCallback-Examples","page":"Event Handling and Callback Functions","title":"DiscreteCallback Examples","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-1:-Interventions-at-Preset-Times","page":"Event Handling and Callback Functions","title":"Example 1: Interventions at Preset Times","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Assume we have a patient whose internal drug concentration follows exponential decay, i.e. the linear ODE with a negative coefficient:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"using DifferentialEquations\nfunction f(du,u,p,t)\n    du[1] = -u[1]\nend\nu0 = [10.0]\nconst V = 1\nprob = ODEProblem(f,u0,(0.0,10.0))\nsol = solve(prob,Tsit5())\nusing Plots; plot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now assume we wish to give the patient a dose of 10 at time t==4. For this, we can use a DiscreteCallback which will only be true at t==4:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = t==4\naffect!(integrator) = integrator.u[1] += 10\ncb = DiscreteCallback(condition,affect!)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"If we then solve with this callback enabled, we see no change:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The reason there is no change is because the DiscreteCallback only applies at a specific time, and the integrator never hit that time. Thus we would like to force the ODE solver to step exactly at t=4 so that the condition can be applied. We can do that with the tstops argument:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Tsit5(),callback=cb,tstops=[4.0])\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"and thus we achieve the desired result.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Performing multiple doses then just requires that we have multiple points which are hit. For example, to dose at time t=4 and t=8, we can do the following:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,8.0]\ncondition(u,t,integrator) = t ∈ dosetimes\naffect!(integrator) = integrator.u[1] += 10\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb,tstops=dosetimes)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"We can then use this mechanism to make the model arbitrarily complex. For example, let's say there's now 3 dose times, but the dose only triggers if the current concentration is below 1.0. Additionally, the dose is now 10t instead of just 10. This model is implemented as simply:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,6.0,8.0]\ncondition(u,t,integrator) = t ∈ dosetimes && (u[1] < 1.0)\naffect!(integrator) = integrator.u[1] += 10integrator.t\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb,tstops=dosetimes)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#PresetTimeCallback","page":"Event Handling and Callback Functions","title":"PresetTimeCallback","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Because events at preset times is a very common occurrence, DifferentialEquations.jl provides a pre-built callback in the Callback Library. The PresetTimeCallback(tstops,affect!) takes an array of times and an affect! function to apply. Thus to do the simple 2 dose example with this callback, we could do the following:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,8.0]\naffect!(integrator) = integrator.u[1] += 10\ncb = PresetTimeCallback(dosetimes,affect!)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that this version will automatically set the tstops for you.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-2:-A-Control-Problem","page":"Event Handling and Callback Functions","title":"Example 2: A Control Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Another example of a DiscreteCallback is a control problem switching parameters. Our parameterized ODE system is as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Our ODE function will use this field as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n    du[1] = -0.5*u[1] + p\n    du[2] = -0.5*u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we will setup our control mechanism. It will be a simple setup which uses set timepoints at which we will change p. At t=5.0 we will want to increase the value of p, and at t=8.0 we will want to decrease the value of p. Using the DiscreteCallback interface, we code these conditions as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const tstop1 = [5.]\nconst tstop2 = [8.]\n\n\nfunction condition(u,t,integrator)\n  t in tstop1\nend\n\nfunction condition2(u,t,integrator)\n  t in tstop2\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we have to apply an effect when these conditions are reached. When condition is hit (at t=5.0), we will increase p to 1.5. When condition2 is reached, we will decrease p to -1.5. This is done via the functions:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  integrator.p = 1.5\nend\n\nfunction affect2!(integrator)\n  integrator.p = -1.5\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With these functions we can build our callbacks:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"save_positions = (true,true)\n\ncb = DiscreteCallback(condition, affect!, save_positions=save_positions)\n\nsave_positions = (false,true)\n\ncb2 = DiscreteCallback(condition2, affect2!, save_positions=save_positions)\n\ncbs = CallbackSet(cb,cb2)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we define our initial condition. We will start at [10.0;10.0] with p=0.0.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [10.0;10.0]\np = 0.0\nprob = ODEProblem(f,u0,(0.0,10.0),p)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Lastly we solve the problem. Note that we must pass tstop values of 5.0 and 8.0 to ensure the solver hits those timepoints exactly:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const tstop = [5.;8.]\nsol = solve(prob,Tsit5(),callback = cbs, tstops=tstop)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: data_array_plot)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"It's clear from the plot how the controls affected the outcome.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-3:-AutoAbstol","page":"Event Handling and Callback Functions","title":"Example 3: AutoAbstol","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"MATLAB's Simulink has the option for an automatic absolute tolerance. In this example we will implement a callback which will add this behavior to any JuliaDiffEq solver which implements the integrator and callback interface.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The algorithm is as follows. The default value is set to start at 1e-6, though we will give the user an option for this choice. Then as the simulation progresses, at each step the absolute tolerance is set to the maximum value that has been reached so far times the relative tolerance. This is the behavior that we will implement in affect!.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Since the effect is supposed to occur every timestep, we use the trivial condition:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition = function (u,t,integrator)\n    true\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"which always returns true. For our effect we will overload the call on a type. This type will have a value for the current maximum. By doing it this way, we can store the current state for the running maximum. The code is as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"mutable struct AutoAbstolAffect{T}\n  curmax::T\nend\n# Now make `affect!` for this:\nfunction (p::AutoAbstolAffect)(integrator)\n  p.curmax = max(p.curmax,integrator.u)\n  integrator.opts.abstol = p.curmax * integrator.opts.reltol\n  u_modified!(integrator,false)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This makes affect!(integrator) use an internal mutating value curmax to update the absolute tolerance of the integrator as the algorithm states.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Lastly, we can wrap it in a nice little constructor:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function AutoAbstol(save=true;init_curmax=1e-6)\n  affect! = AutoAbstolAffect(init_curmax)\n  condtion = (u,t,integrator) -> true\n  save_positions = (save,false)\n  DiscreteCallback(condtion,affect!,save_positions=save_positions)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This creates the DiscreteCallback from the affect! and condition functions that we implemented. Now","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"cb = AutoAbstol(save=true;init_curmax=1e-6)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"returns the callback that we created. We can then solve an equation using this by simply passing it with the callback keyword argument. Using the integrator interface rather than the solve interface, we can step through one by one to watch the absolute tolerance increase:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"integrator = init(prob,BS3(),callback=cb)\nat1 = integrator.opts.abstol\nstep!(integrator)\nat2 = integrator.opts.abstol\n@test at1 < at2\nstep!(integrator)\nat3 = integrator.opts.abstol\n@test at2 < at3","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that this example is contained in the Callback Library, a library of useful callbacks for JuliaDiffEq solvers.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#ContinuousCallback-Examples","page":"Event Handling and Callback Functions","title":"ContinuousCallback Examples","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-1:-Bouncing-Ball","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Let's look at the bouncing ball. Let the first variable y is the height which changes by v the velocity, where the velocity is always changing at -g which is the gravitational constant. This is the equation:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"All we have to do in order to specify the event is to have a function which should always be positive with an event occurring at 0. For now at least that's how it's specified. If a generalization is needed we can talk about this (but it needs to be \"root-findable\"). For here it's clear that we just want to check if the ball's height ever hits zero:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(u,t,integrator) # Event when event_f(u,t) == 0\n  u[1]\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that here we used the values u instead of the value from the integrator. This is because the values u,t will be appropriately modified at the interpolation points, allowing for the rootfinding behavior to occur.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we have to say what to do when the event occurs. In this case we just flip the velocity (the second variable)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  integrator.u[2] = -integrator.u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback is thus specified by:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"cb = ContinuousCallback(condition,affect!)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Then you can solve and plot:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0]\ntspan = (0.0,15.0)\np = 9.8\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: BallBounce)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"As you can see from the resulting image, DifferentialEquations.jl is smart enough to use the interpolation to hone in on the time of the event and apply the event back at the correct time. Thus one does not have to worry about the adaptive timestepping \"overshooting\" the event as this is handled for you. Notice that the event macro will save the value(s) at the discontinuity.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback is robust to having multiple discontinuities occur. For example, we can integrate for long time periods and get the desired behavior:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol,plotdensity=10000)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce_long)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Handling-Changing-Dynamics-and-Exactness","page":"Event Handling and Callback Functions","title":"Handling Changing Dynamics and Exactness","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Let's make a version of the bouncing ball where the ball sticks to the ground. We can do this by introducing a parameter p to send the velocity to zero on the bounce. This looks as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = p[1] * -9.8\nend\nfunction floor_aff!(int)\n    int.p[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show sol[end] # [4.329177480185359e-16, 0.0]","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: sticky bounce)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that at the end, the ball is not at 0.0 like the condition would let you believe, but instead it's at 4.329177480185359e-16. From the printing inside of the affect function, we can see that this is the value it had at the event time t=0.4517539514526232. Why did the event handling not make it exactly zero? If you instead would have run the simulation to nextfloat(0.4517539514526232) = 0.45175395145262326, we would see that the value of u[1] = -1.2647055847076505e-15. You can see this by changing the rootfind argument of the callback:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"floor_event = ContinuousCallback(floor_cond, floor_aff!,rootfind=DiffEqBase.RightRootFind)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\n@show sol[end] # [-1.2647055847076505e-15, 0.0]","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"What this means is that there is not 64-bit floating point number t such that the condition is zero! By default, if there is no t such that condition=0, then rootfinder defaults to choosing the floating point number exactly before the exactly before the event (LeftRootFind). This way manifold constraints are preserved by default (i.e. the ball never goes below the floor). However, if you require that the condition is exactly satisfied after the event, you will want to add such a change to the affect! function. For example, the error correction in this case is to add int.u[1] = 0 to the affect!, i.e.:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function floor_aff!(int)\n    int.p[1] = 0\n    int.u[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\n@show sol[end] # [0.0,0.0]","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"and now the sticky behavior is perfect to the floating point.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Handling-Accumulation-Points","page":"Event Handling and Callback Functions","title":"Handling Accumulation Points","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now let's take a look at the bouncing ball with friction. After the bounce, we will send the velocity to -v/2. Since the velocity is halving each time, we should have Zeno-like behavior and see an accumulation point of bounces. We will use some extra parameters to count the number of bounces (to infinity) and find the accumulation point. Let's watch!","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = -9.8\nend\nfloor_cond(u, t, int) = u[1]\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    int.p[1] += 1\n    int.p[2] = int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [0.0,0.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 2.), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show p # [8.0, 1.3482031881786312]","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: ball floor)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"From the readout we can see the ball only bounced 8 times before it went below the floor, what happened? What happened is floating point error. Because one cannot guarantee that floating point numbers exist to make the condition=0, a heuristic is used to ensure that a zero is not accidentally detected at nextfloat(t) after the simulation restarts (otherwise it would repeatly find the same event!). However, sooner or later the ability to detect minute floating point differences will crash, and what should be infinitely many bounces finally misses a bounce.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This leads to two questions:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"How can you improve the accuracy of an accumulation calculation?\nHow can you make it gracefully continue?","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"For (1), note that floating point accuracy is dependent on the current dt. If you know that an accumulation point is coming, one can use set_proposed_dt! to shrink the dt value and help find the next bounce point. You can use t - tprev to know the length of the previous interval for this calculation. For this example, we can set the proposed dt to (t - tprev)/10 to ensure an ever increasing accuracy of the check.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"However, at some point we will hit machine epsilon, the value where t + eps(t) == t, so we cannot measure infinitely many bounces and instead will be limited by the floating point accuracy of our number representation. Using alternative number types like ArbFloats.jl can allow for this to be done at very high accuracy, but still not infinite. Thus what we need to do is determine a tolerance after which we assume the accumulation has been reached and define the exit behavior. In this case we will say when the dt<1e-12, we are almost at the edge of Float64 accuracy (eps(1.0) = 2.220446049250313e-16), so we will change the position and velocity to exactly zero.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With these floating point corrections in mind, the accumulation calculations looks as follows:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = p[1] * -9.8\nend\nfloor_cond(u, t, int) = u[1]\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    if int.dt > 1e-12\n        set_proposed_dt!(int,(int.t-int.tprev)/100)\n    else\n        int.u[1] = 0\n        int.u[2] = 0\n        int.p[1] = 0\n    end\n    int.p[2] += 1\n    int.p[3] = int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0,0.0,0.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 2.), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show p # [0.0, 41.0, 1.355261854357056]","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce accumulation)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With this corrected version, we see that after 41 bounces the accumulation point is reached at t = 1.355261854357056. To really see the accumulation, let's zoom in:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"p1 = plot(sol,vars=1,tspan=(1.25,1.40))\np2 = plot(sol,vars=1,tspan=(1.35,1.36))\np3 = plot(sol,vars=1,tspan=(1.354,1.35526))\np4 = plot(sol,vars=1,tspan=(1.35526,1.35526185))\nplot(p1,p2,p3,p4)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce accumulation zoom)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"I think Zeno would be proud of our solution.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-2:-Terminating-an-Integration","page":"Event Handling and Callback Functions","title":"Example 2: Terminating an Integration","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"In many cases you might want to terminate an integration when some condition is satisfied. To terminate an integration, use terminate!(integrator) as the affect! in a callback.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"In this example we will solve the differential equation:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [1.,0.]\nfunction fun2(du,u,p,t)\n   du[2] = -u[1]\n   du[1] = u[2]\nend\ntspan = (0.0,10.0)\nprob = ODEProblem(fun2,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"which has cosine and -sine as the solutions respectively. We wish to solve until the sine part, u[2] becomes positive. There are two things we may be looking for.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"A DiscreteCallback will cause this to halt at the first step such that the condition is satisfied. For example, we could use:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]>0\naffect!(integrator) = terminate!(integrator)\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: discrete_terminate)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"However, in many cases we wish to halt exactly at the point of time that the condition is satisfied. To do that, we use a continuous callback. The condition must thus be a function which is zero at the point we want to halt. Thus we use the following:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]\naffect!(integrator) = terminate!(integrator)\ncb = ContinuousCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: simple_terminate)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that this uses rootfinding to approximate the \"exact\" moment of the crossing. Analytically we know the value is pi, and here the integration terminates at","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol.t[end] # 3.1415902502224307","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Using a more accurate integration increases the accuracy of this prediction:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Vern8(),callback=cb,reltol=1e-12,abstol=1e-12)\nsol.t[end] # 3.1415926535896035\n#π = 3.141592653589703...","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now say we wish to find the when the first period is over, i.e. we want to ignore the upcrossing and only stop on the downcrossing. We do this by ignoring the affect! and only passing an affect! for the second:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]\naffect!(integrator) = terminate!(integrator)\ncb = ContinuousCallback(condition,nothing,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: downcrossing_terminate)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that passing only one affect! is the same as ContinuousCallback(condition,affect!,affect!), i.e. both upcrossings and downcrossings will activate the event. Using ContinuousCallback(condition,affect!,nothing)will thus be the same as above because the first event is an upcrossing.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-3:-Growing-Cell-Population","page":"Event Handling and Callback Functions","title":"Example 3: Growing Cell Population","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Another interesting issue is with models of changing sizes. The ability to handle such events is a unique feature of DifferentialEquations.jl! The problem we would like to tackle here is a cell population. We start with 1 cell with a protein X which increases linearly with time with rate parameter α. Since we are going to be changing the size of the population, we write the model in the general form:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const α = 0.3\nfunction f(du,u,p,t)\n  for i in 1:length(u)\n    du[i] = α*u[i]\n  end\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Our model is that, whenever the protein X gets to a concentration of 1, it triggers a cell division. So we check to see if any concentrations hit 1:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(u,t,integrator) # Event when event_f(u,t) == 0\n  1-maximum(u)\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Again, recall that this function finds events as when condition==0, so 1-maximum(u) is positive until a cell has a concentration of X which is 1, which then triggers the event. At the event, we have that the cell splits into two cells, giving a random amount of protein to each one. We can do this by resizing the cache (adding 1 to the length of all of the caches) and setting the values of these two cells at the time of the event:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  u = integrator.u\n  resize!(integrator,length(u)+1)\n  maxidx = findmax(u)[2]\n  Θ = rand()\n  u[maxidx] = Θ\n  u[end] = 1-Θ\n  nothing\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"As noted in the Integrator Interface, resize!(integrator,length(integrator.u)+1) is used to change the length of all of the internal caches (which includes u) to be their current length + 1, growing the ODE system. Then the following code sets the new protein concentrations. Now we can solve:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"callback = ContinuousCallback(condition,affect!)\nu0 = [0.2]\ntspan = (0.0,10.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob,callback=callback)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The plot recipes do not have a way of handling the changing size, but we can plot from the solution object directly. For example, let's make a plot of how many cells there are at each time. Since these are discrete values, we calculate and plot them directly:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"plot(sol.t,map((x)->length(x),sol[:]),lw=3,\n     ylabel=\"Number of Cells\",xlabel=\"Time\")","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: NumberOfCells)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now let's check-in on a cell. We can still use the interpolation to get a nice plot of the concentration of cell 1 over time. This is done with the command:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"ts = range(0, stop=10, length=100)\nplot(ts,map((x)->x[1],sol.(ts)),lw=3,\n     ylabel=\"Amount of X in Cell 1\",xlabel=\"Time\")","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Cell1)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that every time it hits 1 the cell divides, giving cell 1 a random amount of X which then grows until the next division.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that one macro which was not shown in this example is deleteat! on the caches. For example, to delete the second cell, we could use:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"deleteat!(integrator,2)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This allows you to build sophisticated models of populations with births and deaths.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/#VectorContinuousCallback-Example","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/#Example-1:-Bouncing-Ball-with-multiple-walls","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball with multiple walls","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This is similar to the above Bouncing Ball example, but now we have two more vertical walls, at x = 0 and x = 10.0. We have our ODEFunction as -","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p\n  du[3] = u[4]\n  du[4] = 0.0\nend","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"where u[1] denotes y-coordinate, u[2] denotes velocity in y-direction, u[3] denotes x-coordinate and u[4] denotes velocity in x-direction. We will make a VectorContinuousCallback of length 2 - one for x axis collision, one for walls parallel to y axis.","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(out,u,t,integrator) # Event when event_f(u,t) == 0\n  out[1] = u[1]\n  out[2] = (u[3] - 10.0)u[3]\nend\n\nfunction affect!(integrator, idx)\n  if idx == 1\n    integrator.u[2] = -0.9integrator.u[2]\n  elseif idx == 2\n    integrator.u[4] = -0.9integrator.u[4]\n  end\nend\n\ncb = VectorContinuousCallback(condition,affect!,2)","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"It is evident that out[2] will be zero when u[3] (x-coordinate) is either 0.0 or 10.0. And when that happens, we flip the velocity with some coefficient of restitution (0.9).","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Completeting rest of the code-","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0,0.0,2.0]\ntspan = (0.0,15.0)\np = 9.8\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb,dt=1e-3,adaptive=false)\nplot(sol,vars=(1,3))","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"And you get the following output:","category":"page"},{"location":"modules/DiffEqDocs/features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Cell1)","category":"page"},{"location":"modules/DiffEqDocs/#DifferentialEquations.jl:-Scientific-Machine-Learning-(SciML)-Enabled-Simulation-and-Estimation","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"This is a suite for numerically solving differential equations written in Julia and available for use in Julia, Python, and R. The purpose of this package is to supply efficient Julia implementations of solvers for various differential equations. Equations within the realm of this package include:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations)\nOrdinary differential equations (ODEs)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods)\nStochastic ordinary differential equations (SODEs or SDEs)\nStochastic differential-algebraic equations (SDAEs)\nRandom differential equations (RODEs or RDEs)\nDifferential algebraic equations (DAEs)\nDelay differential equations (DDEs)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions)\n(Stochastic) partial differential equations ((S)PDEs) (with both finite difference and finite element methods)","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The well-optimized DifferentialEquations solvers benchmark as some of the fastest implementations, using classic algorithms and ones from recent research which routinely outperform the \"standard\" C/Fortran methods, and include algorithms optimized for high-precision and HPC applications. At the same time, it wraps the classic C/Fortran methods, making it easy to switch over to them whenever necessary. Solving differential equations with different methods from different languages and packages can be done by changing one line of code, allowing for easy benchmarking to ensure you are using the fastest method possible.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"DifferentialEquations.jl integrates with the Julia package sphere with:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"GPU acceleration through CUDA.jl and DiffEqGPU.jl\nAutomated sparsity detection with SparsityDetection.jl\nAutomatic Jacobian coloring with SparseDiffTools.jl, allowing for fast solutions to problems with sparse or structured (Tridiagonal, Banded, BlockBanded, etc.) Jacobians\nAllowing the specification of linear solvers for maximal efficiency\nProgress meter integration with the Juno IDE for estimated time to solution\nAutomatic plotting of time series and phase plots\nBuilt-in interpolations\nWraps for common C/Fortran methods like Sundials and Hairer's radau\nArbitrary precision with BigFloats and Arbfloats\nArbitrary array types, allowing the definition of differential equations on matrices and distributed arrays\nUnit checked arithmetic with Unitful","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Additionally, DifferentialEquations.jl comes with built-in analysis features, including:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Forward and Adjoint Sensitivity Analysis (Automatic Differentiation) for fast gradient computations\nParameter Estimation and Bayesian Analysis\nNeural differential equations with DiffEqFlux.jl for efficient scientific machine learning (scientific ML) and scientific AI.\nAutomatic distributed, multithreaded, and GPU Parallel Ensemble Simulations\nGlobal Sensitivity Analysis\nUncertainty Quantification","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"If you have any questions, or just want to chat about solvers/using the package, please feel free to use the Gitter channel. For bug reports, feature requests, etc., please submit an issue. If you're interested in contributing, please see the Developer Documentation.","category":"page"},{"location":"modules/DiffEqDocs/#Supporting-and-Citing","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Supporting and Citing","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The software in this ecosystem was developed as part of academic research. If you would like to help support it, please star the repository as such metrics may help us secure funding in the future. If you use SciML software as part of your research, teaching, or other activities, we would be grateful if you could cite our work.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"@article{rackauckas2017differentialequations,\n  title={Differentialequations.jl--a performant and feature-rich ecosystem for solving differential equations in julia},\n  author={Rackauckas, Christopher and Nie, Qing},\n  journal={Journal of Open Research Software},\n  volume={5},\n  number={1},\n  year={2017},\n  publisher={Ubiquity Press}\n}","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"is necessary for any use of DifferentialEquations.jl or the packages that are maintained as part of its suite (OrdinaryDiffEq.jl, Sundials.jl, DiffEqDevTools.jl, etc.). Additionally, many of the solvers utilize novel algorithms, and if these algorithms  are used we asked that you cite the methods. Please see our citation page for guidelines.","category":"page"},{"location":"modules/DiffEqDocs/#Getting-Started:-Installation-And-First-Steps","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Getting Started: Installation And First Steps","text":"","category":"section"},{"location":"modules/DiffEqDocs/#Installing-from-Julia","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from Julia","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To install the package, use the following command inside the Julia REPL:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using Pkg\nPkg.add(\"DifferentialEquations\")","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To load the package, use the command:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using DifferentialEquations","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"This will add solvers and dependencies for all kinds of Differential Equations (e.g. ODEs or SDEs etc., see the Supported Equations section below). If you are interested in only one type of equation solvers of DifferentialEquations.jl or simply want a more lightweight version, see the Low Dependency Usage page.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To understand the package in more detail, check out the following tutorials in this manual. It is highly recommended that new users start with the ODE tutorial. Example IJulia notebooks can also be found in DiffEqTutorials.jl. If you find any example where there seems to be an error, please open an issue.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"For the most up to date information on using the package, please join the Gitter channel.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Using the bleeding edge for the latest features and development is only recommended for power users. Information on how to get to the bleeding edge is found in the developer documentation.","category":"page"},{"location":"modules/DiffEqDocs/#Installing-from-Python","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from Python","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Use of DifferentialEquations.jl from the Python programming language is available through the diffeqpy module. To install diffeqpy, use pip:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"pip install diffeqpy","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Using diffeqpy requires that Julia is installed and in the path, along with DifferentialEquations.jl and PyCall.jl. To install Julia, download a generic binary from the JuliaLang site and add it to your path. To install Julia packages required for diffeqpy, open up Python interpreter then run:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":">>> import diffeqpy\n>>> diffeqpy.install()","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"and you're good! In addition, to improve the performance of your code it is recommended that you use Numba to JIT compile your derivative functions. To install Numba, use:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"pip install numba","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"diffeqpy supports the majority of DifferentialEquations.jl with very similar syntax, see the diffeqpy README for more details. One important point to note is that Numba is generally an order of magnitude slower than Julia in terms of  the generated differential equation solver code, and thus it is recommended to use julia.Main.eval for Julia-side derivative function implementations for maximal efficiency. See this blog post for more information.","category":"page"},{"location":"modules/DiffEqDocs/#Installing-from-R","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from R","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Use of DifferentialEquations.jl from the R programming language is available through the diffeqr module. diffeqr is registered into CRAN. Thus to add the package, use:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"install.packages(\"diffeqr\")","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To install the master branch of the package (for developers), use:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"devtools::install_github('SciML/diffeqr', build_vignettes=T)","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"You will need a working installation of Julia in your path. To install Julia, download a generic binary from the JuliaLang site and add it to your path. The download and installation of DifferentialEquations.jl will happen on the first invocation of diffeqr::diffeq_setup().","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Currently, use from R supported a subset of DifferentialEquations.jl which is documented through CRAN.","category":"page"},{"location":"modules/DiffEqDocs/#IJulia-Notebook-Tutorials","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"IJulia Notebook Tutorials","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"You can access extra tutorials supplied in the DiffEqTutorials.jl repository via the commands:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using Pkg\npkg\"add https://github.com/SciML/SciMLTutorials.jl\"\nusing SciMLTutorials\nSciMLTutorials.open_notebooks()","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Or you can view the webpages for the rendered tutorials at the links found in the repository.","category":"page"},{"location":"modules/DiffEqDocs/#Video-Tutorial","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Video Tutorial","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"(Image: Video Tutorial)","category":"page"},{"location":"modules/DiffEqDocs/#Tutorials","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Tutorials","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The following tutorials will introduce you to the functionality of DifferentialEquations.jl. More examples can be found by checking out the IJulia notebooks in the examples folder.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"tutorials/ode_example.md\",\n    \"tutorials/sde_example.md\",\n    \"tutorials/dde_example.md\",\n    \"tutorials/dae_example.md\",\n    \"tutorials/discrete_stochastic_example.md\",\n    \"tutorials/jump_diffusion.md\",\n    \"tutorials/bvp_example.md\",\n    \"tutorials/additional.md\"\n    ]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Removing-and-Reducing-Compile-Times","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Removing and Reducing Compile Times","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"In some situations one may wish to decrease the compile time associated with DifferentialEquations.jl usage. If that's the case, there's two strategies to employ. One strategy is to use the low dependency usage. DifferentialEquations.jl is a metapackage composed of many smaller packages, and thus one could directly use a single component, such as OrdinaryDiffEq.jl for the pure Julia ODE solvers, and decrease the compile times by ignoring the rest (note: the interface is exactly the same, except using a solver other than those in OrdinaryDiffEq.jl will error). We recommend that downstream packages only rely on exactly the packages they need.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The other strategy is to use PackageCompiler.jl to create  a system image that precompiles the whole package. To do this, one simply does:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using PackageCompiler\nPackageCompiler.create_sysimage([:DifferentialEquations,:Plots];replace_default=true)","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Note that there are some drawbacks to adding a package in your system image, for example the package will never update until you manually rebuild the system image again. For more information on the consequences,  see this portion of the PackageCompiler manual","category":"page"},{"location":"modules/DiffEqDocs/#Basics","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Basics","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages introduce you to the core of DifferentialEquations.jl and the common interface. It explains the general workflow, options which are generally available, and the general tools for analysis.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"basics/overview.md\",\n    \"basics/common_solver_opts.md\",\n    \"basics/solution.md\",\n    \"basics/plot.md\",\n    \"basics/integrator.md\",\n    \"basics/problem.md\",\n    \"basics/faq.md\",\n    \"basics/compatibility_chart.md\"\n    ]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Problem-Types","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Problem Types","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages describe building the problem types to define differential equations for the solvers, and the special features of the different solution types.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n  \"types/discrete_types.md\",\n  \"types/ode_types.md\",\n  \"types/dynamical_types.md\",\n  \"types/split_ode_types.md\",\n  \"types/steady_state_types.md\",\n  \"types/bvp_types.md\",\n  \"types/sde_types.md\",\n  \"types/rode_types.md\",\n  \"types/dde_types.md\",\n  \"types/dae_types.md\",\n  \"types/jump_types.md\",\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Solver-Algorithms","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Solver Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages describe the solvers and available algorithms in detail.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n  \"solvers/discrete_solve.md\",\n  \"solvers/ode_solve.md\",\n  \"solvers/dynamical_solve.md\",\n  \"solvers/split_ode_solve.md\",\n  \"solvers/steady_state_solve.md\",\n  \"solvers/bvp_solve.md\",\n  \"solvers/jump_solve.md\",\n  \"solvers/sde_solve.md\",\n  \"solvers/rode_solve.md\",\n  \"solvers/dde_solve.md\",\n  \"solvers/dae_solve.md\",\n  \"solvers/benchmarks.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Additional-Features","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Additional Features","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These sections discuss extra performance enhancements, event handling, and other in-depth features.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"features/performance_overloads.md\",\n    \"features/diffeq_arrays.md\",\n    \"features/diffeq_operator.md\",\n    \"features/noise_process.md\",\n    \"features/linear_nonlinear.md\",\n    \"features/callback_functions.md\",\n    \"features/callback_library.md\",\n    \"features/ensemble.md\",\n    \"features/io.md\",\n    \"features/low_dep.md\",\n    \"features/progress_bar.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Analysis-Tools","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Analysis Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Because DifferentialEquations.jl has a common interface on the solutions, it is easy to add functionality to the entire DiffEq ecosystem by developing it to the solution interface. These pages describe the add-on analysis tools which are available.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"analysis/parameterized_functions.md\",\n    \"analysis/parameter_estimation.md\",\n    \"analysis/bifurcation.md\",\n    \"analysis/sensitivity.md\",\n    \"analysis/global_sensitivity.md\",\n    \"analysis/uncertainty_quantification.md\",\n    \"analysis/neural_networks.md\",\n    \"analysis/dev_and_test.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Modeling-Tools","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Modeling Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"While DifferentialEquations.jl can be used to directly build any differential or difference equation (/ discrete stochastic) model, in many cases it can be helpful to have a tailored-built API for making certain types of common models easier. This is provided by the modeling functionality.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"models/multiscale.md\",\n    \"models/physical.md\",\n    \"models/financial.md\",\n    \"models/chemical_reactions.md\",\n    \"models/external_modeling.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Extra-Details","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Extra Details","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These are just assorted extra explanations for the curious.","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"extras/timestepping.md\"\n    \"extras/sensitivity_math.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDocs/#Acknowledgements","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Acknowledgements","text":"","category":"section"},{"location":"modules/DiffEqDocs/#Core-Contributors","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Core Contributors","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"JuliaDiffEq and DifferentialEquations.jl has been a collaborative effort by many individuals. Significant contributions have been made by the following individuals:","category":"page"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Chris Rackauckas (@ChrisRackauckas) (lead developer)\nYingbo Ma (@YingboMa)\nDavid Widmann (@devmotion)\nHendrik Ranocha (@ranocha)\nEthan Levien (@elevien)\nTom Short (@tshort)\n@dextorious\nSamuel Isaacson (@isaacsas)","category":"page"},{"location":"modules/DiffEqDocs/#Google-Summer-of-Code-Alumni","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Google Summer of Code Alumni","text":"","category":"section"},{"location":"modules/DiffEqDocs/","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Yingbo Ma (@YingboMa)\nShivin Srivastava (@shivin9)\nAyush Pandey (@Ayush-iitkgp)\nXingjian Guo (@MSeeker1340)\nShubham Maddhashiya (@sipah00)\nVaibhav Kumar Dixit (@Vaibhavdixit02)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/#Automatically-Accelerating-ODEProblem-Code","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"","category":"section"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"For some DEProblem types, automatic tracing functionality is already included via the modelingtoolkitize function. Take, for example, the Robertson ODE defined as an ODEProblem for DifferentialEquations.jl:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"using DifferentialEquations, ModelingToolkit\nfunction rober(du,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  du[1] = -k₁*y₁+k₃*y₂*y₃\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  du[3] =  k₂*y₂^2\n  nothing\nend\nprob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"If we want to get a symbolic representation, we can simply call modelingtoolkitize on the prob, which will return an ODESystem:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"sys = modelingtoolkitize(prob)","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"Using this, we can symbolically build the Jacobian and then rebuild the ODEProblem:","category":"page"},{"location":"modules/ModelingToolkit/mtkitize_tutorials/modelingtoolkitize/","page":"Automatically Accelerating ODEProblem Code","title":"Automatically Accelerating ODEProblem Code","text":"prob_jac = ODEProblem(sys,[],(0.0,1e5),jac=true)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/#continuous_loss","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"The automatic differentiation tutorial demonstrated how to use AD packages like ForwardDiff.jl and Zygote.jl to compute derivatives of differential equation solutions with respect to initial conditions and parameters. The subsequent direct sensitivity analysis tutorial showed how to directly use the SciMLSensitivity.jl internals to define and solve the augmented differential equation systems which are used in the automatic differentiation process.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"While these internal functions give more flexibility, the previous demonstration focused on a case which was possible via automatic differentiation: discrete cost functionals. What is meant by discrete cost functionals is differentiation of a cost which uses a finite number of time points. In the automatic differentiation case, these finite time points are the points returned by solve, i.e. those chosen by the saveat option in the solve call. In the direct adjoint sensitivity tooling, these were the time points chosen by the ts vector.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"However, there is an expanded set of cost functionals supported by SciMLSensitivity.jl, continuous cost functionals, which are not possible through automatic differentiation interfaces. In an abstract sense, a continuous cost functional is a total cost G defined as the integral of the instantanious cost g at all time points. In other words, the total cost is defined as:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"G(up)=G(u(cdotp))=int_t_0^Tg(u(tp)p)dt","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"Notice that this cost function cannot accurately be computed using only estimates of u at discrete time points. The purpose of this tutorial is to demonstrate how such cost functionals can be easily evaluated using the direct sensitivity analysis interfaces.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/#Example:-Continuous-Functionals-with-Forward-Sensitivity-Analysis-via-Interpolation","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Example: Continuous Functionals with Forward Sensitivity Analysis via Interpolation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"Evaluating continuous cost functionals with forward sensitivity analysis is rather straightforward since one can simply use the fact that the solution from ODEForwardSensitivityProblem is continuous when dense=true. For example,","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"using OrdinaryDiffEq, SciMLSensitivity\n\nfunction f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEForwardSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)\nsol = solve(prob,DP8())","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"gives a continuous solution sol(t) with the derivative at each time point. This can then be used to define a continuous cost function via Integrals.jl, though the derivative would need to be defined by hand using the extra sensitivity terms.","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/#Example:-Continuous-Adjoints-on-an-Energy-Functional","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Example: Continuous Adjoints on an Energy Functional","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"Continuous adjoints on a continuous functional are more automatic than forward mode. In this case we'd like to calculate the adjoint sensitivity of the scalar energy functional:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"G(up)=int_0^Tfracsum_i=1^nu_i^2(t)2dt","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"which is:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"g(u,p,t) = (sum(u).^2) ./ 2","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"Notice that the gradient of this function with respect to the state u is:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"function dg(out,u,p,t)\n  out[1]= u[1] + u[2]\n  out[2]= u[1] + u[2]\nend","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"To get the adjoint sensitivities, we call:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"prob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)\nsol = solve(prob,DP8())\nres = adjoint_sensitivities(sol,Vern9(),dgdu_continuous=dg,g=g,abstol=1e-8,reltol=1e-8)","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"Notice that we can check this against autodifferentiation and numerical differentiation as follows:","category":"page"},{"location":"modules/SciMLSensitivity/ad_examples/adjoint_continuous_functional/","page":"Adjoint Sensitivity Analysis of Continuous Functionals","title":"Adjoint Sensitivity Analysis of Continuous Functionals","text":"using QuadGK, ForwardDiff, Calculus\nfunction G(p)\n  tmp_prob = remake(prob,p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)\n  res,err = quadgk((t)-> (sum(sol(t)).^2)./2,0.0,10.0,atol=1e-14,rtol=1e-10)\n  res\nend\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])","category":"page"},{"location":"modules/ExponentialUtilities/#ExponentialUtilities.jl:-High-Performance-Matrix-Exponentiation-and-Products","page":"Home","title":"ExponentialUtilities.jl: High-Performance Matrix Exponentiation and Products","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"ExponentialUtilities is a package of utility functions for matrix functions of exponential type,  including functionality for the matrix exponential and phi-functions. The tools are used by the  exponential integrators in OrdinaryDiffEq.","category":"page"},{"location":"modules/ExponentialUtilities/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"To install ExponentialUtilities.jl, use the Julia package manager:","category":"page"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"ExponentialUtilities\")","category":"page"},{"location":"modules/ExponentialUtilities/#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"using ExponentialUtilities\n\nA = rand(2,2)\nexponential!(A)\n\nv = rand(2); t = rand()\nexpv(t,A,v)","category":"page"},{"location":"modules/ExponentialUtilities/#Matrix-phi-vector-product","page":"Home","title":"Matrix-phi-vector product","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"The main functionality of ExponentialUtilities is the computation of matrix-phi-vector products. The phi functions are defined as","category":"page"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"ϕ_0(z) = exp(z)\nϕ_(k+1)(z) = (ϕ_k(z) - 1) / z","category":"page"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"In exponential algorithms, products in the form of ϕ_m(tA)b is frequently encountered. Instead of computing the matrix function first and then computing the matrix-vector product, the common alternative is to construct a Krylov subspace K_m(A,b) and then approximate the matrix-phi-vector product.","category":"page"},{"location":"modules/ExponentialUtilities/#expv-and-phiv","page":"Home","title":"expv and phiv","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"expv(t,A,b;kwargs) -> exp(tA)b\nphiv(t,A,b,k;kwargs) -> [ϕ_0(tA)b ϕ_1(tA)b ... ϕ_k(tA)b][, errest]","category":"page"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"For phiv, all ϕ_m(tA)b products up to order k is returned as a matrix. This is because it's more economical to produce all the results at once than individually. A second output is returned if errest=true in kwargs. The error estimate is given for the second-to-last product, using the last product as an estimator. If correct=true, then ϕ_0 through ϕ_(k-1) are updated using the last Arnoldi vector. The correction algorithm is described in [1].","category":"page"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"You can adjust how the Krylov subspace is constructed by setting various keyword arguments. See the Arnoldi iteration section for more details.","category":"page"},{"location":"modules/ExponentialUtilities/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/ExponentialUtilities/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"highlevels/uncertainty_quantification/#Uncertainty-Quantification-Overview","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"There's always uncertainty in our models. Whether it's in the form of the model's equations or in the model's parameters, the uncertainty in our simulation's output often needs to be quantified. The following tools automate this process.","category":"page"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"For Measurements.jl vs MonteCarloMeasurements.jl vs Intervals.jl, and the relation to other methods, see the Uncertainty Programming chapter of the SciML Book.","category":"page"},{"location":"highlevels/uncertainty_quantification/#PolyChaos.jl:-Intrusive-Polynomial-Chaos-Expansions-Made-Unintrusive","page":"Uncertainty Quantification Overview","title":"PolyChaos.jl: Intrusive Polynomial Chaos Expansions Made Unintrusive","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"PolyChaos.jl is a library for calculating intrusive polynomial chaos expansions (PCE) on arbitrary Julia functions. This allows for inputing representations of probability distributions into functions to compute the output distribution in an expansion representation. While normally this would require deriving the PCE-expanded equations by hand, PolyChaos.jl does this at the compiler level using Julia's multiple dispatch, giving a high-performance implementation to a normally complex and tedious mathematical transformation.","category":"page"},{"location":"highlevels/uncertainty_quantification/#DiffEqUncertainty.jl:-Fast-Calculations-of-Expectations-of-Equation-Solutions","page":"Uncertainty Quantification Overview","title":"DiffEqUncertainty.jl: Fast Calculations of Expectations of Equation Solutions","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"DiffEqUncertainty.jl is a library for accelerating the calculation of expectations of equation solutions with respect to input probability distributions, allowing for applications like robust optimization with respect to uncertainty. It uses Koopman operator techniques to calculate these expectations without requiring the propagation of uncertainties through a solver, effectively performing the adjoint of uncertainty quantification and being much more efficient in the process.","category":"page"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"Additionally, DiffEqUncertainty.jl has the ProbInts method for generating stochastic equations to mimic the error distribution of an ODE solver in order to quantify with  respect to numerical error.","category":"page"},{"location":"highlevels/uncertainty_quantification/#Third-Party-Libraries-to-Note","page":"Uncertainty Quantification Overview","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/#Measurements.jl:-Automated-Linear-Error-Propagation","page":"Uncertainty Quantification Overview","title":"Measurements.jl: Automated Linear Error Propagation","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"Measurements.jl is a library for automating linear error propagation. Uncertain numbers are defined as x = 3.8 ± 0.4 and are pushed through calculations using a normal distribution approximation in order to compute an approximate uncertain output. Measurements.jl uses a dictionary-based approach to keep track of correlations to improve the accuracy over naive implementations, though note that linear error propagation theory still has some major issues handling some types of equations  as described in detail in the MonteCarloMeasurements.jl documentation.","category":"page"},{"location":"highlevels/uncertainty_quantification/#MonteCarloMeasurements.jl:-Automated-Monte-Carlo-Error-Propogation","page":"Uncertainty Quantification Overview","title":"MonteCarloMeasurements.jl: Automated Monte Carlo Error Propogation","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"MonteCarloMeasurements.jl is a library for automating the uncertainty quantification of equation solution using Monte Carlo methods. It defines number types which sample from an input distribution to receive a representative set of parameters that propagate through the solver to calculate a representative set of possible solutions. Note that Monte Carlo techniques can be expensive but are exact, in the sense that as the number of sample points increases to infinity it will compute a correct approximation of the output uncertainty.","category":"page"},{"location":"highlevels/uncertainty_quantification/#ProbNumDiffEq.jl:-Probabilstic-Numerics-Based-Differential-Equation-Solvers","page":"Uncertainty Quantification Overview","title":"ProbNumDiffEq.jl: Probabilstic Numerics Based Differential Equation Solvers","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"ProbNumDiffEq.jl is a a set of probabilistic numerical ODE solvers which compute the solution of a differential equation along with a posterior distribution to estimate its numerical approximation error. Thus these specialized integrators compute an uncertainty output similar to the ProbInts technique of DiffEqUncertainty, but use specialized integration techniques in order to do it much faster for specific kinds of equations.","category":"page"},{"location":"highlevels/uncertainty_quantification/#TaylorIntegration.jl:-Taylor-Series-Integration-for-Rigorous-Numerical-Bounds","page":"Uncertainty Quantification Overview","title":"TaylorIntegration.jl: Taylor Series Integration for Rigorous Numerical Bounds","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"TaylorIntegration.jl is a library for Taylor series integrators which has special functionality for computing the interval bound of possible solutions with respect to numerical approximation error.","category":"page"},{"location":"highlevels/uncertainty_quantification/#IntervalArithmetic.jl:-Rigorous-Numerical-Intervals","page":"Uncertainty Quantification Overview","title":"IntervalArithmetic.jl: Rigorous Numerical Intervals","text":"","category":"section"},{"location":"highlevels/uncertainty_quantification/","page":"Uncertainty Quantification Overview","title":"Uncertainty Quantification Overview","text":"IntervalArithmetic.jl is a library for performing interval arithmetic calculations on arbitrary Julia code. Interval arithmetic computes rigorous computations with respect to finite-precision floating point arithmetic, i.e. its intervals are guarenteed to include the true solution. However, interval arithmetic intervals can grow at exponential rates in many problems, thus being unsuitable for analyses in many equation solver contexts.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/#Adding-Algorithms","page":"Adding Algorithms","title":"Adding Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"New algorithms can either be added by extending one of the current solver (or add-on packages), or by contributing a new package to the organization. If it's a new problem (a new PDE, a new type of differential equation, a new subclass of problems for which special methods exist, etc.) then the problem and solution types should be added to DiffEqBase first.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"After the problem and solutions are defined, the __solve method should be implemented. It should take in keyword arguments which match the common interface (implement \"as many as possible\"). One should note and document the amount of compatibility with the common interface and Julia-defined types. After that, testing should be done using DiffEqDevTools. Convergence tests and benchmarks should be included to show the effectiveness of the algorithm and the correctness. Do not worry if the algorithm is not \"effective\": the implementation can improve over time and some algorithms useful just for the comparison they give!","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"After some development, one may want to document the algorithm in DiffEqBenchmarks and DiffEqTutorials.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/#Adding-new-algorithms-to-OrdinaryDiffEq","page":"Adding Algorithms","title":"Adding new algorithms to OrdinaryDiffEq","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"This recipe has been used to add the strong stability preserving Runge-Kutta methods SSPRK22, SSPRK33, and SSPRK104 to OrdinaryDiffEq. SSPRK22 will be used as an example.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"To create a new solver, two (three) types have to be created. The first is the algorithm SSPRK22 used for dispatch, the other ones are the corresponding caches SSPRK22Cache (for inplace updates) and SSPRK22ConstantCache.\nThe algorithm is defined in algorithms.jl as struct SSPRK22 <: OrdinaryDiffEqAlgorithm end. Although it does not have the FSAL property, this is set to true since the derivative at the start and the end of the interval are used for the Hermite interpolation, and so this is FSAL'd so that way only a single extra function evaluation occurs over the whole integration. This is done in alg_utils.jl via isfsal(alg::SSPRK22) = true. Additionally, the order is set in the same file via alg_order(alg::SSPRK22) = 2.\nThe algorithm SSPRK22 is exported in OrdinaryDiffEq.jl.\nIn caches.jl, the two cache types SSPRK22Cache (for inplace updates) and SSPRK22ConstantCache are defined, similarly to the other ones. Note: u_cache(c::SSPRK22Cache) = () and du_cache(c::SSPRK22Cache) = (c.k,c.du,c.fsalfirst) return the parts of the modifiable cache that are changed if the size of the ODE changes.\nA new file perform_step/ssprk_perform_step.jl has been used for the new implementations. For both types of caches, the functions initialize! and perform_step! are defined there.\nFinally, tests are added. A new file test/ode/ode_ssprk_tests.jl is created and included in tests/runtests.jl via @time @testset \"SSPRK Tests\" begin include(\"ode/ode_ssprk_tests.jl\") end.\nAdditionally, regression tests for the dense output are added in test/ode/ode_dense_tests.jl.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"For more details, refer to https://github.com/JuliaDiffEq/OrdinaryDiffEq.jl/pull/40","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/#Self-Contained-Example","page":"Adding Algorithms","title":"Self-Contained Example","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"using OrdinaryDiffEq\nimport OrdinaryDiffEq: \n      OrdinaryDiffEqAlgorithm, OrdinaryDiffEqMutableCache, OrdinaryDiffEqConstantCache,\n      alg_order, alg_cache, initialize!, perform_step!, trivial_limiter!, constvalue,\n      @muladd, @unpack, @cache, @..\n\nstruct RK_ALG{StageLimiter,StepLimiter} <: OrdinaryDiffEq.OrdinaryDiffEqAlgorithm \n  stage_limiter!::StageLimiter\n  step_limiter!::StepLimiter\nend\nRK_ALG(stage_limiter! = trivial_limiter!) = RK_ALG(stage_limiter!, trivial_limiter!)\nexport RK_ALG\nalg_order(alg::RK_ALG) = 3\n\n@cache struct RK_ALGCache{uType,rateType,StageLimiter,StepLimiter,TabType} <: OrdinaryDiffEqMutableCache\n  u::uType\n  uprev::uType\n  k::rateType\n  tmp::uType\n  u₂::uType\n  fsalfirst::rateType\n  stage_limiter!::StageLimiter\n  step_limiter!::StepLimiter\n  tab::TabType\nend\n\nstruct RK_ALGConstantCache{T,T2} <: OrdinaryDiffEqConstantCache\n  α40::T\n  α41::T\n  α43::T\n  α62::T\n  α65::T\n  β10::T\n  β21::T\n  β32::T\n  β43::T\n  β54::T\n  β65::T\n  c1::T2\n  c2::T2\n  c3::T2\n  c4::T2\n  c5::T2\nend\n\nfunction RK_ALGConstantCache(T, T2)\n  α40 = T(0.476769811285196)\n  α41 = T(0.098511733286064)\n  α43 = T(0.424718455428740)\n  α62 = T(0.155221702560091)\n  α65 = T(0.844778297439909)\n  β10 = T(0.284220721334261)\n  β21 = T(0.284220721334261)\n  β32 = T(0.284220721334261)\n  β43 = T(0.120713785765930)\n  β54 = T(0.284220721334261)\n  β65 = T(0.240103497065900)\n  c1 = T2(0.284220721334261)\n  c2 = T2(0.568441442668522)\n  c3 = T2(0.852662164002783)\n  c4 = T2(0.510854218958172)\n  c5 = T2(0.795074940292433)\n\n  RK_ALGConstantCache(α40, α41, α43, α62, α65, β10, β21, β32, β43, β54, β65, c1, c2, c3, c4, c5)\nend\n\nfunction alg_cache(alg::RK_ALG,u,rate_prototype,uEltypeNoUnits,uBottomEltypeNoUnits,tTypeNoUnits,uprev,uprev2,f,t,dt,reltol,p,calck,::Val{true})\n  tmp = similar(u)\n  u₂ = similar(u)\n  k = zero(rate_prototype)\n  fsalfirst = zero(rate_prototype)\n  tab = RK_ALGConstantCache(real(uBottomEltypeNoUnits), real(tTypeNoUnits))\n  RK_ALGCache(u,uprev,k,tmp,u₂,fsalfirst,alg.stage_limiter!,alg.step_limiter!,tab)\nend\n\nfunction alg_cache(alg::RK_ALG,u,rate_prototype,uEltypeNoUnits,uBottomEltypeNoUnits,tTypeNoUnits,uprev,uprev2,f,t,dt,reltol,p,calck,::Val{false})\n  RK_ALGConstantCache(real(uBottomEltypeNoUnits), real(tTypeNoUnits))\nend\n\nfunction initialize!(integrator,cache::RK_ALGConstantCache)\n  integrator.fsalfirst = integrator.f(integrator.uprev,integrator.p,integrator.t) # Pre-start fsal\n  integrator.destats.nf += 1\n  integrator.kshortsize = 1\n  integrator.k = typeof(integrator.k)(undef, integrator.kshortsize)\n\n  # Avoid undefined entries if k is an array of arrays\n  integrator.fsallast = zero(integrator.fsalfirst)\n  integrator.k[1] = integrator.fsalfirst\nend\n\n@muladd function perform_step!(integrator,cache::RK_ALGConstantCache,repeat_step=false)\n  @unpack t,dt,uprev,u,f,p = integrator\n  @unpack α40,α41,α43,α62,α65,β10,β21,β32,β43,β54,β65,c1,c2,c3,c4,c5 = cache\n\n  # u1 -> stored as u\n  u = uprev + β10 * dt * integrator.fsalfirst\n  k = f(u, p, t+c1*dt)\n  # u2\n  u₂ = u + β21 * dt * k\n  k = f(u₂,p,t+c2*dt)\n  # u3\n  tmp = u₂ + β32 * dt * k\n  k = f(tmp, p, t+c3*dt)\n  # u4\n  tmp = α40 * uprev + α41 * u + α43 * tmp + β43 * dt * k\n  k = f(tmp, p, t+c4*dt)\n  # u5\n  tmp = tmp + β54 * dt * k\n  k = f(tmp, p, t+c5*dt)\n  # u\n  u = α62 * u₂ + α65 * tmp + β65 * dt * k\n\n  integrator.fsallast = f(u, p, t+dt) # For interpolation, then FSAL'd\n  integrator.destats.nf += 6\n  integrator.k[1] = integrator.fsalfirst\n  integrator.u = u\nend\n\nfunction initialize!(integrator,cache::RK_ALGCache)\n  @unpack k,fsalfirst = cache\n  integrator.fsalfirst = fsalfirst\n  integrator.fsallast = k\n  integrator.kshortsize = 1\n  resize!(integrator.k, integrator.kshortsize)\n  integrator.k[1] = integrator.fsalfirst\n  integrator.f(integrator.fsalfirst,integrator.uprev,integrator.p,integrator.t) # FSAL for interpolation\n  integrator.destats.nf += 1\nend\n\n@muladd function perform_step!(integrator,cache::RK_ALGCache,repeat_step=false)\n  @unpack t,dt,uprev,u,f,p = integrator\n  @unpack k,tmp,u₂,fsalfirst,stage_limiter!,step_limiter! = cache\n  @unpack α40,α41,α43,α62,α65,β10,β21,β32,β43,β54,β65,c1,c2,c3,c4,c5 = cache.tab\n\n  # u1 -> stored as u\n  @.. u = uprev + β10 * dt * integrator.fsalfirst\n  stage_limiter!(u, f, p, t+c1*dt)\n  f( k,  u, p, t+c1*dt)\n  # u2\n  @.. u₂ = u + β21 * dt * k\n  stage_limiter!(u₂, f, p, t+c2*dt)\n  f(k,u₂,p,t+c2*dt)\n  # u3\n  @.. tmp = u₂ + β32 * dt * k\n  stage_limiter!(tmp, f, p, t+c3*dt)\n  f( k,  tmp, p, t+c3*dt)\n  # u4\n  @.. tmp = α40 * uprev + α41 * u + α43 * tmp + β43 * dt * k\n  stage_limiter!(tmp, f, p, t+c4*dt)\n  f( k,  tmp, p, t+c4*dt)\n  # u5\n  @.. tmp = tmp + β54 * dt * k\n  stage_limiter!(tmp, f, p, t+c5*dt)\n  f( k,  tmp, p, t+c5*dt)\n  # u\n  @.. u = α62 * u₂ + α65 * tmp + β65 * dt * k\n  stage_limiter!(u, f, p, t+dt)\n  step_limiter!(u, f, p, t+dt)\n  integrator.destats.nf += 6\n  f( k,  u, p, t+dt)\nend\n\n#oop test\nf = ODEFunction((u,p,t)->1.01u,\n            analytic = (u0,p,t) -> u0*exp(1.01t))\nprob = ODEProblem(f,1.01,(0.0,1.0))\nsol = solve(prob,RK_ALG(),dt=0.1)\n\nusing Plots\nplot(sol)\nplot(sol,denseplot=false,plot_analytic=true)\n\nusing DiffEqDevTools\ndts = (1/2) .^ (8:-1:1)\nsim = test_convergence(dts,prob,RK_ALG())\nsim.𝒪est[:final]\nplot(sim)\n\n# Example of a good one!\nsim = test_convergence(dts,prob,BS3())\nsim.𝒪est[:final]\nplot(sim)\n\n#iip test\nf = ODEFunction((du,u,p,t)->(du .= 1.01.*u),\n            analytic = (u0,p,t) -> u0*exp(1.01t))\nprob = ODEProblem(f,[1.01],(0.0,1.0))\nsol = solve(prob,RK_ALG(),dt=0.1)\n\nplot(sol)\nplot(sol,denseplot=false,plot_analytic=true)\n\ndts = (1/2) .^ (8:-1:1)\nsim = test_convergence(dts,prob,RK_ALG())\nsim.𝒪est[:final]\nplot(sim)\n\n# Example of a good one!\nsim = test_convergence(dts,prob,BS3())\nsim.𝒪est[:final]\nplot(sim)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/#Adding-new-exponential-algorithms","page":"Adding Algorithms","title":"Adding new exponential algorithms","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"The exponential algorithms follow the same recipe as the general algorithms, but there are automation utilities that make this easier. It is recommended that you refer to one of the model algorithms for reference:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"For traditional exponential Runge-Kutta type methods (that come with a corresponding Butcher table), refer to ETDRK2.\nFor adaptive exponential Rosenbrock type methods, refer to Exprb32.\nFor exponential propagation iterative Runge-Kutta methods (EPIRK), refer to EPIRK5P1.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"The first two classes support two modes of operation: operator caching and Krylov approximation. The perform_step! method in perform_step/exponential_rk_perform_step.jl, as a result, is split into two branches depending on whether alg.krylov is true. The caching branch utilizes precomputed operators, which are calculated by the expRK_operators method in caches/linear_nonlinear_caches.jl. Both expRK_operators and the arnoldi/phiv methods in perform_step! comes from the ExponentialUtilities package.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"The EPIRK methods can only use Krylov approximation, and unlike the previous two they use the timestepping variant phiv_timestep. The timestepping method follows the convention of Neisen & Wright, and can be toggled to use adaptation by alg.adaptive_krylov.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"Although the exponential integrators (especially the in-place version) can seem complex, they share similar structures. The infrastructure for the existing exponential methods utilize the fact to reduce boilerplate code. In particular, the cache construction code in caches/linear_nonlinear_caches.jl and the initialize! method in perform_step/exponential_rk_perform_step.jl can be mostly automated and only perform_step! needs implementing.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_algorithms/","page":"Adding Algorithms","title":"Adding Algorithms","text":"Finally, to construct tests for the new exponential algorithm, append the new algorithm to the corresponding algorithm class in test/linear_nonlinear_convergence_tests.jl and test/linear_nonlinear_krylov_tests.jl.","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/#Symbolic-Problem-Building-with-ModelingToolkit","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"","category":"section"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"note: Note\nThis example uses the OptimizationOptimJL.jl package. See the Optim.jl page for details on the installation and usage.","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"ModelingToolkit.jl is a comprehensive system for symbolic modeling in Julia. Allows for doing many manipulations before the solver phase, such as detecting sparsity patterns, analytically solving parts of the model to reduce the solving complexity, and more. One of the types of system types that it supports is OptimizationSystem, i.e. the symbolic counterpart to OptimizationProblem. Let's demonstrate how to use the OptimizationSystem to construct optimized OptimizationProblems.","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"First we need to start by defining our symbolic variables, this is done as follows:","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"using ModelingToolkit, Optimization, OptimizationOptimJL\r\n\r\n@variables x y\r\n@parameters a b","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"We can now construct the OptimizationSystem by building a symbolic expression  for the loss function:","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"loss = (a - x)^2 + b * (y - x^2)^2\r\nsys = OptimizationSystem(loss,[x,y],[a,b])","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"In order to turn it into a problem for numerical solutions, we need to specify what our parameter values are and the initial conditions. This looks like:","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"u0 = [\r\n    x=>1.0\r\n    y=>2.0\r\n]\r\np = [\r\n    a => 6.0\r\n    b => 7.0\r\n]","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"And now we solve.","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"prob = OptimizationProblem(sys,u0,p,grad=true,hess=true)\r\nsolve(prob,Newton())","category":"page"},{"location":"modules/Optimization/tutorials/symbolic/","page":"Symbolic Problem Building with ModelingToolkit","title":"Symbolic Problem Building with ModelingToolkit","text":"It has a lot of other features like auto-parallelism and sparsification too. Plus you can hierarchically nest systems to have it generate huge optimization problems. Check out the ModelingToolkit.jl OptimizationSystem documentation for more information.","category":"page"},{"location":"modules/DiffEqFlux/utilities/Collocation/#Smoothed-Collocation","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"","category":"section"},{"location":"modules/DiffEqFlux/utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Smoothed collocation, also referred to as the two-stage method, allows for fitting differential equations to time series data without relying on a numerical differential equation solver by building a smoothed collocating polynomial and using this to estimate the true (u',u) pairs, at which point u'-f(u,p,t) can be directly estimated as a loss to determine the correct parameters p. This method can be extremely fast and robust to noise, though, because it does not accumulate through time, is not as exact as other methods.","category":"page"},{"location":"modules/DiffEqFlux/utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"collocate_data","category":"page"},{"location":"modules/DiffEqFlux/utilities/Collocation/#DiffEqFlux.collocate_data","page":"Smoothed Collocation","title":"DiffEqFlux.collocate_data","text":"u′,u = collocate_data(data,tpoints,kernel=SigmoidKernel())\nu′,u = collocate_data(data,tpoints,tpoints_sample,interp,args...)\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nFor kernels, the following exist:\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC2631937/\n\nAdditionally, we can use interpolation methods from DataInterpolations.jl to generate data from intermediate timesteps. In this case, pass any of the methods like QuadraticInterpolation as interp, and the timestamps to sample from as tpoints_sample.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqFlux/utilities/Collocation/#Kernel-Choice","page":"Smoothed Collocation","title":"Kernel Choice","text":"","category":"section"},{"location":"modules/DiffEqFlux/utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"Note that the kernel choices of DataInterpolations.jl, such as CubicSpline(), are exact, i.e. go through the data points, while the smoothed kernels are regression splines. Thus CubicSpline() is preferred if the data is not too noisy or is relatively sparse. If data is sparse and very noisy, a BSpline()  can be the best regression spline, otherwise one of the other kernels such as as EpanechnikovKernel.","category":"page"},{"location":"modules/DiffEqFlux/utilities/Collocation/#Non-Allocating-Forward-Mode-L2-Collocation-Loss","page":"Smoothed Collocation","title":"Non-Allocating Forward-Mode L2 Collocation Loss","text":"","category":"section"},{"location":"modules/DiffEqFlux/utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"The following is an example of a loss function over the collocation that is non-allocating and compatible with forward-mode automatic differentiation:","category":"page"},{"location":"modules/DiffEqFlux/utilities/Collocation/","page":"Smoothed Collocation","title":"Smoothed Collocation","text":"using PreallocationTools\ndu = PreallocationTools.dualcache(similar(prob.u0))\npreview_est_sol = [@view estimated_solution[:,i] for i in 1:size(estimated_solution,2)]\npreview_est_deriv = [@view estimated_derivative[:,i] for i in 1:size(estimated_solution,2)]\n\nfunction construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)\n  function (p)\n      _du = PreallocationTools.get_tmp(du,p)\n      vecdu = vec(_du)\n      cost = zero(first(p))\n      for i in 1:length(preview_est_sol)\n        est_sol = preview_est_sol[i]\n        f(_du,est_sol,p,tpoints[i])\n        vecdu .= vec(preview_est_deriv[i]) .- vec(_du)\n        cost += sum(abs2,vecdu)\n      end\n      sqrt(cost)\n  end\nend\ncost_function = construct_iip_cost_function(f,du,preview_est_sol,preview_est_deriv,tpoints)","category":"page"},{"location":"highlevels/numerical_utilities/#SciML-Numerical-Utility-Libraries","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#Surrogates.jl:-Easy-Generation-of-Differentiable-Surrogate-Models","page":"SciML Numerical Utility Libraries","title":"Surrogates.jl: Easy Generation of Differentiable Surrogate Models","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Surrogates.jl is a library for generating surrogate approximations to computationally expensive simulations. It has the following high-dimensional function approximators:","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Kriging\nKriging using Stheno\nRadial Basis\nWendland\nLinear\nSecond Order Polynomial\nSupport Vector Machines (Wait for LIBSVM resolution)\nNeural Networks\nRandom Forests\nLobachevsky splines\nInverse-distance\nPolynomial expansions\nVariable fidelity\nMixture of experts (Waiting GaussianMixtures package to work on v1.5)\nEarth\nGradient Enhanced Kriging","category":"page"},{"location":"highlevels/numerical_utilities/#ExponentialUtilities.jl:-Faster-Matrix-Exponentials","page":"SciML Numerical Utility Libraries","title":"ExponentialUtilities.jl: Faster Matrix Exponentials","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"ExponentialUtilities.jl is a library for efficient computation of matrix exponentials. While Julia has a built-in exp(A) method, ExponentialUtilities.jl offers many features around this to improve performance in scientific contexts, including:","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Faster methods for (non-allocating) matrix exponentials via exponential!\nMethods for computing matrix exponential that are generic to number types and arrays (i.e. GPUs)\nMethods for computing arnoldi iterations on Krylov subspaces\nDirect computation of exp(t*A)*v, i.e. exponentiation of a matrix times a vector, without computing the matrix exponential\nDirect computation of ϕ_m(t*A)*v operations, where ϕ_0(z) = exp(z) and ϕ_(k+1)(z) = (ϕ_k(z) - 1) / z","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"ExponentialUtilities.jl includes complex adaptive time stepping techniques such as KIOPS in order to perform these calculations in a fast and numerically-stable way.","category":"page"},{"location":"highlevels/numerical_utilities/#QuasiMonteCarlo.jl:-Fast-Quasi-Random-Number-Generation","page":"SciML Numerical Utility Libraries","title":"QuasiMonteCarlo.jl: Fast Quasi-Random Number Generation","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"QuasiMonteCarlo.jl is a library for fast generation of  ow discrepency Quasi-Monte Carlo samples, using methods like:","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"GridSample(dx) where the grid is given by lb:dx[i]:ub in the ith direction.\nUniformSample for uniformly distributed random numbers.\nSobolSample for the Sobol sequence.\nLatinHypercubeSample for a Latin Hypercube.\nLatticeRuleSample for a randomly-shifted rank-1 lattice rule.\nLowDiscrepancySample(base) where base[i] is the base in the ith direction.\nGoldenSample for a Golden Ratio sequence.\nKroneckerSample(alpha, s0) for a Kronecker sequence, where alpha is an length-d vector of irrational numbers (often sqrt(d)) and s0 is a length-d seed vector (often 0).\nSectionSample(x0, sampler) where sampler is any sampler above and x0 is a vector of either NaN for a free dimension or some scalar for a constrained dimension.","category":"page"},{"location":"highlevels/numerical_utilities/#PoissonRandom.jl:-Fast-Poisson-Random-Number-Generation","page":"SciML Numerical Utility Libraries","title":"PoissonRandom.jl: Fast Poisson Random Number Generation","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"PoissonRandom.jl is just fast Poisson random number generation for Poisson processes, like chemical master equations.","category":"page"},{"location":"highlevels/numerical_utilities/#PreallocationTools.jl:-Write-Non-Allocating-Code-Easier","page":"SciML Numerical Utility Libraries","title":"PreallocationTools.jl: Write Non-Allocating Code Easier","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"PreallocationTools.jl is a library of tools for writing non-allocating code that interacts well with advanced features like automatic differentiation and symbolics.","category":"page"},{"location":"highlevels/numerical_utilities/#RuntimeGeneratedFunctions.jl:-Efficient-Staged-Programming-in-Julia","page":"SciML Numerical Utility Libraries","title":"RuntimeGeneratedFunctions.jl: Efficient Staged Programming in Julia","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"RuntimeGeneratedFunctions.jl allows for staged programming in Julia, compiling functions at runtime with full optimizations. This is used by many libraries such as ModelingToolkit.jl to allow for runtime code generation for improved performance.","category":"page"},{"location":"highlevels/numerical_utilities/#Third-Party-Libraries-to-Note","page":"SciML Numerical Utility Libraries","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#Distributions.jl:-Representations-of-Probability-Distributions","page":"SciML Numerical Utility Libraries","title":"Distributions.jl: Representations of Probability Distributions","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Distributions.jl is a library for defining distributions in Julia. It's used all throughout the SciML libraries for specifications of probability distributions.","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"note: Note\nFor full compatibility with automatic differentiation, see  DistributionsAD.jl","category":"page"},{"location":"highlevels/numerical_utilities/#FFTW.jl:-Fastest-Fourier-Transformation-in-the-West","page":"SciML Numerical Utility Libraries","title":"FFTW.jl: Fastest Fourier Transformation in the West","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"FFTW.jl is the preferred library for fast Fourier Transformations on the CPU.","category":"page"},{"location":"highlevels/numerical_utilities/#SpecialFunctions.jl:-Implementations-of-Mathematical-Special-Functions","page":"SciML Numerical Utility Libraries","title":"SpecialFunctions.jl: Implementations of Mathematical Special Functions","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"SpecialFunctions.jl is a library of implementations of special functions, like Bessel functions and error functions (erf). This library is compatible with automatic differentiation.","category":"page"},{"location":"highlevels/numerical_utilities/#LoopVectorization.jl:-Automated-Loop-Acceleator","page":"SciML Numerical Utility Libraries","title":"LoopVectorization.jl: Automated Loop Acceleator","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"LoopVectorization.jl is a library which provides the @turbo and @tturbo macros for accelerating the computation of loops. This can be used to accelerating the model functions sent to the equation solvers, for example, accelerating handwritten PDE discretizations.","category":"page"},{"location":"highlevels/numerical_utilities/#Polyester.jl:-Cheap-Threads","page":"SciML Numerical Utility Libraries","title":"Polyester.jl: Cheap Threads","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Polyester.jl is a cheaper version of threads for Julia which use a set pool of threads for lower overhead. Note that Polyester does not compose with the standard Julia composable theading infrastructure, and thus one must take care to not compose two levels of Polyester as this will oversubscribe the computation and lead to performance degredation. Many SciML solvers have options to use Polyseter for threading to achieve the top performance.","category":"page"},{"location":"highlevels/numerical_utilities/#Tullio.jl:-Fast-Tensor-Calculations-and-Einstein-Notation","page":"SciML Numerical Utility Libraries","title":"Tullio.jl: Fast Tensor Calculations and Einstein Notation","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"Tullio.jl is a library for fast tensor calculations with Einstein notation. It allows for defining operations which are compatible with automatic differentiation, GPUs, and more.","category":"page"},{"location":"highlevels/numerical_utilities/#ParallelStencil.jl:-High-Level-Code-for-Parallelized-Stencil-Computations","page":"SciML Numerical Utility Libraries","title":"ParallelStencil.jl: High-Level Code for Parallelized Stencil Computations","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"ParallelStencil.jl is a library for writing high level code for parallelized stencil computations. It is compatible with SciML equation solvers and is thus a good way to generate GPU and distributed parallel model code.","category":"page"},{"location":"highlevels/numerical_utilities/#DataInterpolations.jl:-One-Dimensional-Interpolations","page":"SciML Numerical Utility Libraries","title":"DataInterpolations.jl: One-Dimensional Interpolations","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"DataInterpolations.jl is a library of one-dimensional interpolation schemes which are composable with automatic differentiation and the SciML ecosystem. It includes direct interpolation methods and regression techniques for handling noisy data. Its methods include:","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"ConstantInterpolation(u,t) - A piecewise constant interpolation.\nLinearInterpolation(u,t) - A linear interpolation.\nQuadraticInterpolation(u,t) - A quadratic interpolation.\nLagrangeInterpolation(u,t,n) - A Lagrange interpolation of order n.\nQuadraticSpline(u,t) - A quadratic spline interpolation.\nCubicSpline(u,t) - A cubic spline interpolation.\nBSplineInterpolation(u,t,d,pVec,knotVec) - An interpolation B-spline.  This is a B-spline which hits each of the data points. The argument choices are: \t- d - degree of B-spline   \t- pVec - Symbol to Parameters Vector, pVec = :Uniform for uniform spaced parameters and      pVec = :ArcLen for parameters generated by chord length method.   \t- knotVec - Symbol to Knot Vector, knotVec = :Uniform for uniform knot vector,      knotVec = :Average for average spaced knot vector.\nBSplineApprox(u,t,d,h,pVec,knotVec) - A regression B-spline which smooths the fitting curve.  The argument choices are the same as the BSplineInterpolation, with the additional parameter  h<length(t) which is the number of control points to use, with smaller h indicating more smoothing.\nCurvefit(u,t,m,p,alg) - An interpolation which is done by fitting a user-given functional form  m(t,p) where p is the vector of parameters. The user's input p is a an initial value for a  least-square fitting, alg is the algorithm choice to use for optimize the cost function (sum of  squared deviations) via Optim.jl and optimal ps are used in the interpolation.","category":"page"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"These interpolations match the SciML interfaces and have direct support for packages like ModelingToolkit.jl.","category":"page"},{"location":"highlevels/numerical_utilities/#Julia-Utilities","page":"SciML Numerical Utility Libraries","title":"Julia Utilities","text":"","category":"section"},{"location":"highlevels/numerical_utilities/#StaticCompiler.jl","page":"SciML Numerical Utility Libraries","title":"StaticCompiler.jl","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"StaticCompiler.jl is a package for generating static binaries from Julia code. It only supports a subset of Julia, so not all equation solver algorithms are compatible with StaticCompiler.jl.","category":"page"},{"location":"highlevels/numerical_utilities/#PackageCompiler.jl","page":"SciML Numerical Utility Libraries","title":"PackageCompiler.jl","text":"","category":"section"},{"location":"highlevels/numerical_utilities/","page":"SciML Numerical Utility Libraries","title":"SciML Numerical Utility Libraries","text":"PackageCompiler.jl is a package for generating shared libraries from Julia code. It the entirety of Julia by bundling a system image with the Julia runtime,  thus it builds complete binaries that can hold all of the functionality of SciML. It can also be used to generate new system images to decrease startup times and remove JIT-compilation from SciML usage.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/#Delay-Differential-Equations","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"This tutorial will introduce you to the functionality for solving delay differential equations.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Delay differential equations are equations which have a delayed argument. To allow for specifying the delayed argument, the function definition for a delay differential equation is expanded to include a history function h(p, t) which uses interpolations throughout the solution's history to form a continuous extension of the solver's past and depends on parameters p and time t. The function signature for a delay differential equation is f(u, h, p, t) for not in-place computations, and f(du, u, h, p, t) for in-place computations.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"In this example we will solve a model of breast cancer growth kinetics:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"beginaligned\ndx_0 = fracv_01+beta_0left(x_2(t-tau)right)^2left(p_0-q_0right)x_0(t)-d_0x_0(t)\ndx_1 = fracv_01+beta_0left(x_2(t-tau)right)^2left(1-p_0+q_0right)x_0(t)\n       + fracv_11+beta_1left(x_2(t-tau)right)^2left(p_1-q_1right)x_1(t)-d_1x_1(t)\ndx_2 = fracv_11+beta_1left(x_2(t-tau)right)^2left(1-p_1+q_1right)x_1(t)-d_2x_2(t)\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"For this problem we note that tau is constant, and thus we can use a method which exploits this behavior. We first write out the equation using the appropriate function signature. Most of the equation writing is the same, though we use the history function by first interpolating and then choosing the components. Thus the ith component at time t-tau is given by h(p, t-tau)[i]. Components with no delays are written as in the ODE.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Thus, the function for this model is given by:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"function bc_model(du,u,h,p,t)\n  p0,q0,v0,d0,p1,q1,v1,d1,d2,beta0,beta1,tau = p\n  hist3 = h(p, t-tau)[3]\n  du[1] = (v0/(1+beta0*(hist3^2))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(hist3^2))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(hist3^2))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(hist3^2))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Now we build a DDEProblem. The signature","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(f, u0, h, tspan, p=SciMLBase.NullParameters();\n                  constant_lags=[], dependent_lags=[], kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"is very similar to ODEs, where we now have to give the lags and a function h. h is the history function that declares what the values were before the time the model starts. Here we will assume that for all time before t0 the values were 1 and define h as an out-of-place function:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t) = ones(3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"To use the constant lag model, we have to declare the lags. Here we will use tau=1.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"tau = 1\nlags = [tau]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Next, we choose to solve on the timespan (0.0,10.0) and create the problem type:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"p0 = 0.2; q0 = 0.3; v0 = 1; d0 = 5\np1 = 0.2; q1 = 0.3; v1 = 1; d1 = 1\nd2 = 1; beta0 = 1; beta1 = 1\np = (p0,q0,v0,d0,p1,q1,v1,d1,d2,beta0,beta1,tau)\ntspan = (0.0,10.0)\nu0 = [1.0,1.0,1.0]\n\nprob = DDEProblem(bc_model,u0,h,tspan,p; constant_lags=lags)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"An efficient way to solve this problem (given the constant lags) is with the MethodOfSteps solver. Through the magic that is Julia, it translates an OrdinaryDiffEq.jl ODE solver method into a method for delay differential equations which is highly efficient due to sweet compiler magic. A good choice is the order 5 method Tsit5():","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"alg = MethodOfSteps(Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"For lower tolerance solving, one can use the BS3() algorithm to good effect (this combination is similar to the MATLAB dde23, but more efficient tableau), and for high tolerances the Vern6() algorithm will give an 6th order solution.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"To solve the problem with this algorithm, we do the same thing we'd do with other methods on the common interface:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"sol = solve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that everything available to OrdinaryDiffEq.jl can be used here, including event handling and other callbacks. The solution object has the same interface as for ODEs. For example, we can use the same plot recipes to view the results:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using Plots; plot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"(Image: DDE Example Plot)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/#Speeding-Up-Interpolations-with-Idxs","page":"Delay Differential Equations","title":"Speeding Up Interpolations with Idxs","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We can speed up the previous problem in two different ways. First of all, if we need to interpolate multiple values from a previous time, we can use the in-place form for the history function h(out, p, t) which writes the output to out. In this case, we must supply the history initial conditions as in-place as well. For the previous example, that's simply","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(out, p, t) = (out.=1.0)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"and then our DDE is:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"const out = zeros(3) # Define a cache variable\nfunction bc_model(du,u,h,p,t)\n  h(out, p, t-tau) # updates out to be the correct history function\n  du[1] = (v0/(1+beta0*(out[3]^2))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(out[3]^2))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(out[3]^2))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(out[3]^2))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"However, we can do something even slicker in most cases. We only ever needed to interpolate past values at index 3. Instead of generating a bunch of arrays, we can instead ask specifically for that value by passing the keyword idxs = 3. The DDE function bc_model is now:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"function bc_model(du,u,h,p,t)\n  u3_past_sq = h(p, t-tau; idxs=3)^2\n  du[1] = (v0/(1+beta0*(u3_past_sq))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(u3_past_sq))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(u3_past_sq))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(u3_past_sq))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that this requires that we define the historical values","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t; idxs=nothing) = typeof(idxs) <: Number ? 1.0 : ones(3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"where idxs can be an integer for which variable in the history to compute, and here for any number idxs we give back 1.0. Note that if we wanted to use past values of the ith derivative then we would call the history function h(p, t, Val{i}) in our DDE function and would have to define a dispatch like","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t, ::Type{Val{1}}) = zeros(3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"to say that derivatives before t0 are zero for any index. Again, we could use an in-place function instead or only compute specific indices by passing an idxs keyword.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"The functional forms for the history function are discussed also on the DDEProblem page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/#Undeclared-Delays-and-State-Dependent-Delays-via-Residual-Control","page":"Delay Differential Equations","title":"Undeclared Delays and State-Dependent Delays via Residual Control","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"You might have noticed DifferentialEquations.jl allows you to solve problems with undeclared delays since you can interpolate h at any value. This is a feature, but use it with caution. Undeclared delays can increase the error in the solution. It's recommended that you use a method with a residual control, such as MethodOfSteps(RK4()) whenever there are undeclared delays. With this you can use interpolated derivatives, solve functional differential equations by using quadrature on the interpolant, etc. However, note that residual control solves with a low level of accuracy, so the tolerances should be made very small and the solution should not be trusted for more than 2-3 decimal places.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note: MethodOfSteps(RK4()) with undeclared delays is similar to MATLAB's ddesd. Thus, for example, the following is similar to solving the example from above with residual control:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(bc_model,u0,h,tspan)\nalg = MethodOfSteps(RK4())\nsol = solve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that this method can solve problems with state-dependent delays.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/#State-Dependent-Delay-Discontinuity-Tracking","page":"Delay Differential Equations","title":"State-Dependent Delay Discontinuity Tracking","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"State-dependent delays are problems where the delay is allowed to be a function of the current state. They can be more efficiently solved with discontinuity tracking. To do this in DifferentialEquations.jl, requires to pass lag functions g(u,p,t) as keyword dependent_lags to the DDEProblem definition. Other than that, everything else is the same, and one solves that problem using the common interface.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We can solve the above problem with dependent delay tracking by declaring the dependent lags and solving with a MethodOfSteps algorithm:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(bc_model,u0,h,tspan; dependent_lags = ((u,p,t) -> tau,))\nalg = MethodOfSteps(Tsit5())\nsol = solve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Here we treated the single lag t-tau as a state-dependent delay. Of course, you can then replace that tuple of functions with whatever functions match your lags.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/#Parameter-Estimation-for-Stochastic-Differential-Equations-and-Ensembles","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We can use any DEProblem, which not only includes DAEProblem and DDEProblems, but also stochastic problems. In this case, let's use the generalized maximum likelihood to fit the parameters of an SDE's ensemble evaluation.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Let's use the same Lotka-Volterra equation as before, but this time add noise:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"pf_func = function (du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0]\npg_func = function (du,u,p,t)\n  du[1] = 1e-6u[1]\n  du[2] = 1e-6u[2]\nend\nprob = SDEProblem(pf_func,pg_func,u0,tspan,p)\nsol = solve(prob,SRIW1())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now lets generate a dataset from 10,000 solutions of the SDE","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0, stop=10, length=200))\nfunction generate_data(t)\n  sol = solve(prob,SRIW1())\n  randomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\n  data = convert(Array,randomized)\nend\naggregate_data = convert(Array,VectorOfArray([generate_data(t) for i in 1:10000]))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now let's estimate the parameters. Instead of using single runs from the SDE, we will use a EnsembleProblem. This means that it will solve the SDE N times to come up with an approximate probability distribution at each time point and use that in the likelihood estimate.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"monte_prob = EnsembleProblem(prob)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We use Optim.jl for optimization below","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"obj = build_loss_objective(monte_prob,SOSRI(),L2Loss(t,aggregate_data),\n                                     maxiters=10000,verbose=false,num_monte = 1000,\n                                     parallel_type = :threads)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Parameter Estimation in case of SDE's with a regular L2Loss can have poor accuracy due to only fitting against the mean properties as mentioned in First Differencing.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Results of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [6.070728870478734,5.113357737345448]\n * Minimum: 1.700440e+03\n * Iterations: 14\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 1.00e-03\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 1.81e-07 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 2.34e+00\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 61\n * Gradient Calls: 61","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Instead when we use L2Loss with first differencing enabled we get much more accurate estimates.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":" obj = build_loss_objective(monte_prob,SRIW1(),L2Loss(t,data,differ_weight=1.0,data_weight=0.5),maxiters=1000,\n                                  verbose=false,verbose_opt=false,verbose_steps=1,num_monte=50)\nresult = Optim.optimize(obj, [1.0,0.5], Optim.BFGS())\nResults of Optimization Algorithm\n * Algorithm: BFGS\n * Starting Point: [1.0,0.5]\n * Minimizer: [1.5010687426045128,1.0023453619050238]\n * Minimum: 1.166650e-01\n * Iterations: 16\n * Convergence: false\n   * |x - x'| ≤ 0.0e+00: false\n     |x - x'| = 6.84e-09\n   * |f(x) - f(x')| ≤ 0.0e+00 |f(x)|: false\n     |f(x) - f(x')| = 5.85e-06 |f(x)|\n   * |g(x)| ≤ 1.0e-08: false\n     |g(x)| = 1.81e-01\n   * Stopped by an increasing objective: true\n   * Reached Maximum Number of Iterations: false\n * Objective Calls: 118\n * Gradient Calls: 118","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Here, we see that we successfully recovered the drift parameter, and got close to the original noise parameter after searching a two-orders-of-magnitude range.","category":"page"},{"location":"modules/NeuralOperators/apis/#APIs","page":"APIs","title":"APIs","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/#Transforms","page":"APIs","title":"Transforms","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"AbstractTransform","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.AbstractTransform","page":"APIs","title":"NeuralOperators.AbstractTransform","text":"AbstractTransform\n\nInterface\n\nBase.ndims(<:AbstractTransform): N dims of modes\ntransform(<:AbstractTransform, 𝐱::AbstractArray): Apply the transform to 𝐱\ntruncate_modes(<:AbstractTransform, 𝐱_transformed::AbstractArray): Truncate modes that contribute to the noise\ninverse(<:AbstractTransform, 𝐱_transformed::AbstractArray): Apply the inverse transform to 𝐱_transformed\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralOperators/apis/#Layers","page":"APIs","title":"Layers","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/#Operator-convolutional-layer","page":"APIs","title":"Operator convolutional layer","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"F(s) = mathcalF  v(x)  \nF(s) = g(F(s)) \nv(x) = mathcalF^-1  F(s) ","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"where v(x) and v(x) denotes input and output function, mathcalF  cdot , mathcalF^-1  cdot  are transform, inverse transform, respectively. Function g is a linear transform for lowering spectrum modes.","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"OperatorConv","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.OperatorConv","page":"APIs","title":"NeuralOperators.OperatorConv","text":"OperatorConv(ch, modes, transform;\n             init=c_glorot_uniform, permuted=false, T=ComplexF32)\n\nArguments\n\nch: A Pair of input and output channel size ch_in=>ch_out, e.g. 64=>64.\nmodes: The modes to be preserved. A tuple of length d,   where d is the dimension of data.\nTransform: The trafo to operate the transformation.\n\nKeyword Arguments\n\ninit: Initial function to initialize parameters.\npermuted: Whether the dim is permuted. If permuted=true, layer accepts   data in the order of (ch, x_1, ... , x_d , batch),   otherwise the order is (x_1, ... , x_d, ch, batch).\nT: Data type of parameters.\n\nExample\n\njulia> OperatorConv(2=>5, (16, ), FourierTransform)\nOperatorConv(2 => 5, (16,), FourierTransform, permuted=false)\n\njulia> OperatorConv(2=>5, (16, ), FourierTransform, permuted=true)\nOperatorConv(2 => 5, (16,), FourierTransform, permuted=true)\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Reference: FNO2021","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#Operator-kernel-layer","page":"APIs","title":"Operator kernel layer","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"v_t+1(x) = sigma(W v_t(x) + mathcalK  v_t(x)  )","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"where v_t(x) is the input function for t-th layer and mathcalK  cdot  denotes spectral convolutional layer. Activation function sigma can be arbitrary non-linear function.","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"OperatorKernel","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.OperatorKernel","page":"APIs","title":"NeuralOperators.OperatorKernel","text":"OperatorKernel(ch, modes, σ=identity; permuted=false)\n\nArguments\n\nch: A Pair of input and output channel size for spectral convolution in_ch=>out_ch,   e.g. 64=>64.\nmodes: The modes to be preserved for spectral convolution. A tuple of length d,   where d is the dimension of data.\nσ: Activation function.\n\nKeyword Arguments\n\npermuted: Whether the dim is permuted. If permuted=true, layer accepts   data in the order of (ch, x_1, ... , x_d , batch),   otherwise the order is (x_1, ... , x_d, ch, batch).\n\nExample\n\njulia> OperatorKernel(2=>5, (16, ), FourierTransform)\nOperatorKernel(2 => 5, (16,), FourierTransform, σ=identity, permuted=false)\n\njulia> using Flux\n\njulia> OperatorKernel(2=>5, (16, ), FourierTransform, relu)\nOperatorKernel(2 => 5, (16,), FourierTransform, σ=relu, permuted=false)\n\njulia> OperatorKernel(2=>5, (16, ), FourierTransform, relu, permuted=true)\nOperatorKernel(2 => 5, (16,), FourierTransform, σ=relu, permuted=true)\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Reference: FNO2021","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#Graph-kernel-layer","page":"APIs","title":"Graph kernel layer","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"v_t+1(x_i) = sigma(W v_t(x_i) + frac1mathcalN(x_i) sum_x_j in mathcalN(x_i) kappa  v_t(x_i) v_t(x_j)  )","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"where v_t(x_i) is the input function for t-th layer, x_i is the node feature for i-th node and mathcalN(x_i) represents the neighbors for x_i. Activation function sigma can be arbitrary non-linear function.","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"GraphKernel","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.GraphKernel","page":"APIs","title":"NeuralOperators.GraphKernel","text":"GraphKernel(κ, ch, σ=identity)\n\nGraph kernel layer.\n\nArguments\n\nκ: A neural network layer for approximation, e.g. a Dense layer or a MLP.\nch: Channel size for linear transform, e.g. 32.\nσ: Activation function.\n\nKeyword Arguments\n\ninit: Initial function to initialize parameters.\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Reference: NO2020","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#Models","page":"APIs","title":"Models","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/#Fourier-neural-operator","page":"APIs","title":"Fourier neural operator","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"FourierNeuralOperator","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.FourierNeuralOperator","page":"APIs","title":"NeuralOperators.FourierNeuralOperator","text":"FourierNeuralOperator(;\n                      ch = (2, 64, 64, 64, 64, 64, 128, 1),\n                      modes = (16, ),\n                      σ = gelu)\n\nFourier neural operator is a operator learning model that uses Fourier kernel to perform spectral convolutions. It is a promissing way for surrogate methods, and can be regarded as a physics operator.\n\nThe model is comprised of a Dense layer to lift (d + 1)-dimensional vector field to n-dimensional vector field, and an integral kernel operator which consists of four Fourier kernels, and two Dense layers to project data back to the scalar field of interest space.\n\nThe role of each channel size described as follow:\n\n[1] input channel number\n ↓ Dense\n[2] lifted channel number\n ↓ OperatorKernel\n[3] mapped cahnnel number\n ↓ OperatorKernel\n[4] mapped cahnnel number\n ↓ OperatorKernel\n[5] mapped cahnnel number\n ↓ OperatorKernel\n[6] mapped cahnnel number\n ↓ Dense\n[7] projected channel number\n ↓ Dense\n[8] projected channel number\n\nKeyword Arguments\n\nch: A Tuple or Vector of the 8 channel size.\nmodes: The modes to be preserved. A tuple of length d,   where d is the dimension of data.\nσ: Activation function for all layers in the model.\n\nExample\n\njulia> using NNlib\n\njulia> FourierNeuralOperator(;\n                             ch = (2, 64, 64, 64, 64, 64, 128, 1),\n                             modes = (16,),\n                             σ = gelu)\nChain(\n  Dense(2 => 64),                       # 192 parameters\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (16,), FourierTransform, permuted=false),  # 65_536 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (16,), FourierTransform, permuted=false),  # 65_536 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (16,), FourierTransform, permuted=false),  # 65_536 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (16,), FourierTransform, permuted=false),  # 65_536 parameters\n    identity,\n  ),\n  Dense(64 => 128, gelu),               # 8_320 parameters\n  Dense(128 => 1),                      # 129 parameters\n)                   # Total: 18 arrays, 287_425 parameters, 2.098 MiB.\n\n\n\n\n\n","category":"function"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Reference: FNO2021","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#Markov-neural-operator","page":"APIs","title":"Markov neural operator","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"MarkovNeuralOperator","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.MarkovNeuralOperator","page":"APIs","title":"NeuralOperators.MarkovNeuralOperator","text":"MarkovNeuralOperator(;\n                     ch = (1, 64, 64, 64, 64, 64, 1),\n                     modes = (24, 24),\n                     σ = gelu)\n\nMarkov neural operator learns a neural operator with Fourier operators. With only one time step information of learning, it can predict the following few steps with low loss by linking the operators into a Markov chain.\n\nThe model is comprised of a Dense layer to lift d-dimensional vector field to n-dimensional vector field, and an integral kernel operator which consists of four Fourier kernels, and a Dense layers to project data back to the scalar field of interest space.\n\nThe role of each channel size described as follow:\n\n[1] input channel number\n ↓ Dense\n[2] lifted channel number\n ↓ OperatorKernel\n[3] mapped cahnnel number\n ↓ OperatorKernel\n[4] mapped cahnnel number\n ↓ OperatorKernel\n[5] mapped cahnnel number\n ↓ OperatorKernel\n[6] mapped cahnnel number\n ↓ Dense\n[7] projected channel number\n\nKeyword Arguments\n\nch: A Tuple or Vector of the 7 channel size.\nmodes: The modes to be preserved. A tuple of length d,   where d is the dimension of data.\nσ: Activation function for all layers in the model.\n\nExample\n\njulia> using NNlib\n\njulia> MarkovNeuralOperator(;\n                            ch = (1, 64, 64, 64, 64, 64, 1),\n                            modes = (24, 24),\n                            σ = gelu)\nChain(\n  Dense(1 => 64),                       # 128 parameters\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (24, 24), FourierTransform, permuted=false),  # 2_359_296 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (24, 24), FourierTransform, permuted=false),  # 2_359_296 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (24, 24), FourierTransform, permuted=false),  # 2_359_296 parameters\n    NNlib.gelu,\n  ),\n  OperatorKernel(\n    Dense(64 => 64),                    # 4_160 parameters\n    OperatorConv(64 => 64, (24, 24), FourierTransform, permuted=false),  # 2_359_296 parameters\n    NNlib.gelu,\n  ),\n  Dense(64 => 1),                       # 65 parameters\n)                   # Total: 16 arrays, 9_454_017 parameters, 72.066 MiB.\n\n\n\n\n\n\n","category":"function"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Reference: MNO2021","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#DeepONet","page":"APIs","title":"DeepONet","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"DeepONet\nNeuralOperators.construct_subnet","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.DeepONet","page":"APIs","title":"NeuralOperators.DeepONet","text":"DeepONet(architecture_branch::Tuple, architecture_trunk::Tuple,                         act_branch = identity, act_trunk = identity;                         init_branch = Flux.glorot_uniform,                         init_trunk = Flux.glorot_uniform,                         bias_branch=true, bias_trunk=true) DeepONet(branch_net::Flux.Chain, trunk_net::Flux.Chain)\n\nCreate an (unstacked) DeepONet architecture as proposed by Lu et al. arXiv:1910.03193\n\nThe model works as follows:\n\nx –- branch –                |                 -⊠–u-                | y –- trunk –-\n\nWhere x represents the input function, discretely evaluated at its respective sensors. So the ipnut is of shape [m] for one instance or [m x b] for a training set. y are the probing locations for the operator to be trained. It has shape [N x n] for N different variables in the PDE (i.e. spatial and temporal coordinates) with each n distinct evaluation points. u is the solution of the queried instance of the PDE, given by the specific choice of parameters.\n\nBoth inputs x and y are multiplied together via dot product Σᵢ bᵢⱼ tᵢₖ.\n\nYou can set up this architecture in two ways:\n\nBy Specifying the architecture and all its parameters as given above. This always creates\n\nDense layers for the branch and trunk net and corresponds to the DeepONet proposed by Lu et al.\n\nBy passing two architectures in the form of two Chain structs directly. Do this if you want more\n\nflexibility and e.g. use an RNN or CNN instead of simple Dense layers.\n\nStrictly speaking, DeepONet does not imply either of the branch or trunk net to be a simple  DNN. Usually though, this is the case which is why it's treated as the default case here.\n\nExample\n\nConsider a transient 1D advection problem ∂ₜu + u ⋅ ∇u = 0, with an IC u(x,0) = g(x). We are given several (b = 200) instances of the IC, discretized at 50 points each and want  to query the solution for 100 different locations and times [0;1].\n\nThat makes the branch input of shape [50 x 200] and the trunk input of shape [2 x 100]. So the  input for the branch net is 50 and 100 for the trunk net.\n\nUsage\n\njulia> model = DeepONet((32,64,72), (24,64,72))\nDeepONet with\nbranch net: (Chain(Dense(32, 64), Dense(64, 72)))\nTrunk net: (Chain(Dense(24, 64), Dense(64, 72)))\n\njulia> model = DeepONet((32,64,72), (24,64,72), σ, tanh; init_branch=Flux.glorot_normal, bias_trunk=false)\nDeepONet with\nbranch net: (Chain(Dense(32, 64, σ), Dense(64, 72, σ)))\nTrunk net: (Chain(Dense(24, 64, tanh; bias=false), Dense(64, 72, tanh; bias=false)))\n\njulia> branch = Chain(Dense(2,128),Dense(128,64),Dense(64,72))\nChain(\n  Dense(2, 128),                        # 384 parameters\n  Dense(128, 64),                       # 8_256 parameters\n  Dense(64, 72),                        # 4_680 parameters\n)                   # Total: 6 arrays, 13_320 parameters, 52.406 KiB.\n\njulia> trunk = Chain(Dense(1,24),Dense(24,72))\nChain(\n  Dense(1, 24),                         # 48 parameters\n  Dense(24, 72),                        # 1_800 parameters\n)                   # Total: 4 arrays, 1_848 parameters, 7.469 KiB.\n\njulia> model = DeepONet(branch,trunk)\nDeepONet with\nbranch net: (Chain(Dense(2, 128), Dense(128, 64), Dense(64, 72)))\nTrunk net: (Chain(Dense(1, 24), Dense(24, 72)))\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.construct_subnet","page":"APIs","title":"NeuralOperators.construct_subnet","text":"Construct a Chain of Dense layers from a given tuple of integers.\n\nInput: A tuple (m,n,o,p) of integer type numbers that each describe the width of the i-th Dense layer to Construct\n\nOutput: A Flux Chain with length of the input tuple and individual width given by the tuple elements\n\nExample\n\njulia> model = NeuralOperators.construct_subnet((2,128,64,32,1))\nChain(\n  Dense(2, 128),                        # 384 parameters\n  Dense(128, 64),                       # 8_256 parameters\n  Dense(64, 32),                        # 2_080 parameters\n  Dense(32, 1),                         # 33 parameters\n)                   # Total: 8 arrays, 10_753 parameters, 42.504 KiB.\n\njulia> model([2,1])\n1-element Vector{Float32}:\n -0.7630446\n\n\n\n\n\n","category":"function"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"","category":"page"},{"location":"modules/NeuralOperators/apis/#NOMAD","page":"APIs","title":"NOMAD","text":"","category":"section"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"Nonlinear manifold decoders for operator learning","category":"page"},{"location":"modules/NeuralOperators/apis/","page":"APIs","title":"APIs","text":"NOMAD","category":"page"},{"location":"modules/NeuralOperators/apis/#NeuralOperators.NOMAD","page":"APIs","title":"NeuralOperators.NOMAD","text":"NOMAD(architecture_approximator::Tuple, architecture_decoder::Tuple,\n      act_approximator = identity, act_decoder=true;\n      init_approximator = Flux.glorot_uniform,\n      init_decoder = Flux.glorot_uniform,\n      bias_approximator=true, bias_decoder=true)\nNOMAD(approximator_net::Flux.Chain, decoder_net::Flux.Chain)\n\nCreate a Nonlinear Manifold Decoders for Operator Learning (NOMAD) as proposed by Lu et al. arXiv:2206.03551\n\nThe decoder is defined as follows:\n\ntilde D (β y) = f(β y)\n\nUsage\n\njulia> model = NOMAD((16,32,16), (24,32))\nNOMAD with\nApproximator net: (Chain(Dense(16 => 32), Dense(32 => 16)))\nDecoder net: (Chain(Dense(24 => 32, true)))\n\njulia> model = NeuralOperators.NOMAD((32,64,32), (64,72), σ, tanh; init_approximator=Flux.glorot_normal, bias_decoder=false)\nNOMAD with\nApproximator net: (Chain(Dense(32 => 64, σ), Dense(64 => 32, σ)))\nDecoder net: (Chain(Dense(64 => 72, tanh; bias=false)))\n\njulia> approximator = Chain(Dense(2,128),Dense(128,64))\nChain(\n  Dense(2 => 128),                      # 384 parameters\n  Dense(128 => 64),                     # 8_256 parameters\n)                   # Total: 4 arrays, 8_640 parameters, 34.000 KiB.\n\njulia> decoder = Chain(Dense(72,24),Dense(24,12))\nChain(\n  Dense(72 => 24),                      # 1_752 parameters\n  Dense(24 => 12),                      # 300 parameters\n)                   # Total: 4 arrays, 2_052 parameters, 8.266 KiB.\n\njulia> model = NOMAD(approximator, decoder)\nNOMAD with\nApproximator net: (Chain(Dense(2 => 128), Dense(128 => 64)))\nDecoder net: (Chain(Dense(72 => 24), Dense(24 => 12)))\n\n\n\n\n\n","category":"type"},{"location":"modules/Optimization/tutorials/constraints/#constraints","page":"Using Equality and Inequality Constraints","title":"Using Equality and Inequality Constraints","text":"","category":"section"},{"location":"highlevels/machine_learning/#SciML-Machine-Learning-Libraries-Overview","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"While SciML is not an ecosystem for machine learning, SciML has many libraries for doing machine learning with its equation solver libraries and machine learning libraries which are integrated into the equation solvers.","category":"page"},{"location":"highlevels/machine_learning/#DiffEqFlux.jl:-High-Level-Pre-Built-Architectures-for-Implicit-Deep-Learning","page":"SciML Machine Learning Libraries Overview","title":"DiffEqFlux.jl: High Level Pre-Built Architectures for Implicit Deep Learning","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"DiffEqFlux.jl is a library of pre-built architectures for implicit deep learning, including layer definitions for methods like:","category":"page"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"Neural Ordinary Differential Equations (Neural ODEs)\nCollocation-Based Neural ODEs (Neural ODEs without a solver, by far the fastest way!)\nMultiple Shooting Neural Ordinary Differential Equations\nNeural Stochastic Differential Equations (Neural SDEs)\nNeural Differential-Algebriac Equations (Neural DAEs)\nNeural Delay Differential Equations (Neural DDEs)\nAugmented Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nContinuous Normalizing Flows (CNF) and FFJORD","category":"page"},{"location":"highlevels/machine_learning/#ReservoirComputing.jl:-Fast-and-Flexible-Reservoir-Computing-Methods","page":"SciML Machine Learning Libraries Overview","title":"ReservoirComputing.jl: Fast and Flexible Reservoir Computing Methods","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"ReservoirComputing.jl is a library for doing machine learning using reservoir computing techniques, such as with methods like Echo State Networks (ESNs). Its reservoir computing methods make it stabilized for usage with difficult equations like stiff dynamics, chaotic equations, and more.","category":"page"},{"location":"highlevels/machine_learning/#DeepEquilibriumNetworks.jl:-Deep-Equilibrium-Models-Made-Fast","page":"SciML Machine Learning Libraries Overview","title":"DeepEquilibriumNetworks.jl: Deep Equilibrium Models Made Fast","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"FastDEQ.jl is a library of optimized layer implementations for Deep Equilibrium Models (DEQs). It uses special training techniques such as implicit-explicit regularization in order to accelerate the convergence over traditional implementations, all while using the optimized and flexible SciML libraries under the hood.","category":"page"},{"location":"highlevels/machine_learning/#Third-Party-Libraries-to-Note","page":"SciML Machine Learning Libraries Overview","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/machine_learning/#Flux.jl:-the-ML-library-that-doesn't-make-you-tensor","page":"SciML Machine Learning Libraries Overview","title":"Flux.jl: the ML library that doesn't make you tensor","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"Flux.jl is the most popular machine learning library in the Julia programming language. SciML's libraries are heavily tested with it and its automatic differentiation engine Zygote.jl for composability and compatibility.","category":"page"},{"location":"highlevels/machine_learning/#Lux.jl:-Explicitly-Parameterized-Neural-Networks-in-Julia","page":"SciML Machine Learning Libraries Overview","title":"Lux.jl: Explicitly Parameterized Neural Networks in Julia","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"Lux.jl is a library for fully explicitly parameterized neural networks. Thus while alternative interfaces are required to use Flux with many equation solvers (i.e. Flux.destructure), Lux.jl's explicit design merries very easily with the SciML equation solver libraries. For this reason, SciML's library are also heavily tested with Lux to ensure compatibility with neural network definitions from here.","category":"page"},{"location":"highlevels/machine_learning/#SimpleChains.jl:-Fast-Small-Scale-Machine-Learning","page":"SciML Machine Learning Libraries Overview","title":"SimpleChains.jl: Fast Small-Scale Machine Learning","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"SimpleChains.jl is a library specialized for small-scale machine learning. It uses non-allocating mutating forms to be highly efficient for the cases where matrix multiplication kernels are not able to overcome the common overheads of machine learning libraries. Thus for SciML cases with small neural networks (<100 node layers)  and non-batched usage (many/most use cases), SimpleChains.jl can be the fastest choice for the neural network definitions.","category":"page"},{"location":"highlevels/machine_learning/#NNLib.jl:-Neural-Network-Primitives-with-Multiple-Backends","page":"SciML Machine Learning Libraries Overview","title":"NNLib.jl: Neural Network Primitives with Multiple Backends","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"NNLib.jl is the core library which defines the handling of common functions, like conv and how they map to device accelerators such as the NVIDA cudnn. This library can thus be used to directly grab many of the core functions used in machine learning, such as common activation functions and gather/scatter operations, without depending on the given style of any machine learning library.","category":"page"},{"location":"highlevels/machine_learning/#GeometricFlux.jl:-Geometric-Deep-Learning-and-Graph-Neural-Networks","page":"SciML Machine Learning Libraries Overview","title":"GeometricFlux.jl: Geometric Deep Learning and Graph Neural Networks","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"GeometricFlux.jl is a library for graph neural networks and geometric deep learning. It is the one that is used and tested by the SciML developers for mixing with equation solver applications.","category":"page"},{"location":"highlevels/machine_learning/#AbstractGPs.jl:-Fast-and-Flexible-Gaussian-Processes","page":"SciML Machine Learning Libraries Overview","title":"AbstractGPs.jl: Fast and Flexible Gaussian Processes","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"AbstractGPs.jl is the fast and flexible Gaussian Process library that is used by the SciML packages and recommended for downstream usage.","category":"page"},{"location":"highlevels/machine_learning/#MLDatasets.jl:-Common-Machine-Learning-Datasets","page":"SciML Machine Learning Libraries Overview","title":"MLDatasets.jl: Common Machine Learning Datasets","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"MLDatasets.jl  is a common interface for accessing common machine learning datasets. For example, if you want to run a test on MNIST data, MLDatasets is the quicket way to obtain it.","category":"page"},{"location":"highlevels/machine_learning/#MLUtils.jl:-Utility-Functions-for-Machine-Learning-Pipelines","page":"SciML Machine Learning Libraries Overview","title":"MLUtils.jl: Utility Functions for Machine Learning Pipelines","text":"","category":"section"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"MLUtils.jl is a library of utility functions for making writing common machine learning pipelines easier. This includes functionality for:","category":"page"},{"location":"highlevels/machine_learning/","page":"SciML Machine Learning Libraries Overview","title":"SciML Machine Learning Libraries Overview","text":"An extensible dataset interface  (numobs and getobs).\nData iteration and dataloaders (eachobs and DataLoader).\nLazy data views (obsview). \nResampling procedures (undersample and oversample).\nTrain/test splits (splitobs) \nData partitioning and aggregation tools (batch, unbatch, chunk, group_counts, group_indices).\nFolds for cross-validation (kfolds, leavepout).\nDatasets lazy tranformations (mapobs, filterobs, groupobs, joinobs, shuffleobs).\nToy datasets for demonstration purpose. \nOther data handling utilities (flatten, normalise, unsqueeze, stack, unstack).","category":"page"},{"location":"modules/QuasiMonteCarlo/samplers/#Sampler-APIs","page":"Sampler APIs","title":"Sampler APIs","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/samplers/#Sample","page":"Sampler APIs","title":"Sample","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/samplers/","page":"Sampler APIs","title":"Sampler APIs","text":"sample","category":"page"},{"location":"modules/QuasiMonteCarlo/samplers/#Samplers","page":"Sampler APIs","title":"Samplers","text":"","category":"section"},{"location":"modules/QuasiMonteCarlo/samplers/","page":"Sampler APIs","title":"Sampler APIs","text":"GridSample\nUniformSample\nSobolSample\nLatinHypercubeSample\nLatticeRuleSample\nLowDiscrepancySample\nGoldenSample\nKroneckerSample\nSectionSample","category":"page"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.GridSample","page":"Sampler APIs","title":"QuasiMonteCarlo.GridSample","text":"GridSample{T}\n\nThe grid is given by lb:dx[i]:ub in the ith direction.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.UniformSample","page":"Sampler APIs","title":"QuasiMonteCarlo.UniformSample","text":"struct UniformSample <: SamplingAlgorithm end\n\nUniformly distributed random numbers.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.SobolSample","page":"Sampler APIs","title":"QuasiMonteCarlo.SobolSample","text":"struct SobolSample <: SamplingAlgorithm end\n\nSamples using the Sobol sequence.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.LatinHypercubeSample","page":"Sampler APIs","title":"QuasiMonteCarlo.LatinHypercubeSample","text":"struct LatinHypercubeSample <: SamplingAlgorithm end\n\nSamples using a Latin Hypercube.\n\nLatinHypercubeSample(threading=false)\n\nKeyword arguments:\n\nthreading: whether to use threading. Default is false, i.e. serial.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.LatticeRuleSample","page":"Sampler APIs","title":"QuasiMonteCarlo.LatticeRuleSample","text":"struct LatticeRuleSample <: SamplingAlgorithm end\n\nSamples using a randomly-shifted rank-1 lattice rule.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.LowDiscrepancySample","page":"Sampler APIs","title":"QuasiMonteCarlo.LowDiscrepancySample","text":"struct LowDiscrepancySample{T} <: SamplingAlgorithm\n\nbase[i] is the base in the ith direction.\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.GoldenSample","page":"Sampler APIs","title":"QuasiMonteCarlo.GoldenSample","text":"struct GoldenSample <: SamplingAlgorithm end\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.KroneckerSample","page":"Sampler APIs","title":"QuasiMonteCarlo.KroneckerSample","text":"struct KroneckerSample{A,B} <: SamplingAlgorithm\n\nKroneckerSample(alpha, s0) for a Kronecker sequence, where alpha is an length-d vector of irrational numbers (often sqrt(d)) and s0 is a length-d seed vector (often 0).\n\n\n\n\n\n","category":"type"},{"location":"modules/QuasiMonteCarlo/samplers/#QuasiMonteCarlo.SectionSample","page":"Sampler APIs","title":"QuasiMonteCarlo.SectionSample","text":"struct SectionSample{T} <: SamplingAlgorithm\n\nSectionSample(x0, sampler) where sampler is any sampler above and x0 is a vector of either NaN for a free dimension or some scalar for a constrained dimension.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/#Multiple-Shooting","page":"Multiple Shooting","title":"Multiple Shooting","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"In Multiple Shooting, the training data is split into overlapping intervals. The solver is then trained on individual intervals. If the end conditions of any interval coincide with the initial conditions of the next immediate interval, then the joined/combined solution is same as solving on the whole dataset (without splitting).","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"To ensure that the overlapping part of two consecutive intervals coincide, we add a penalizing term, continuity_term * absolute_value_of(prediction of last point of group i - prediction of first point of group i+1), to the loss.","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Note that the continuity_term should have a large positive value to add high penalties in case the solver predicts discontinuous values.","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"The following is a working demo, using Multiple Shooting","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"using Lux, DiffEqFlux, Optimization, OptimizationPolyalgorithms, DifferentialEquations, Plots\nusing DiffEqFlux: group_ranges\n\nusing Random\nrng = Random.default_rng()\n\n# Define initial conditions and time steps\ndatasize = 30\nu0 = Float32[2.0, 0.0]\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\n\n# Get the data\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n\n# Define the Neural Network\nnn = Lux.Chain(x -> x.^3,\n                  Lux.Dense(2, 16, tanh),\n                  Lux.Dense(16, 2))\np_init, st = Lux.setup(rng, nn)\n\nneuralode = NeuralODE(nn, tspan, Tsit5(), saveat = tsteps)\nprob_node = ODEProblem((u,p,t)->nn(u,p,st)[1], u0, tspan, Lux.ComponentArray(p_init))\n\n\nfunction plot_multiple_shoot(plt, preds, group_size)\n\tstep = group_size-1\n\tranges = group_ranges(datasize, group_size)\n\n\tfor (i, rg) in enumerate(ranges)\n\t\tplot!(plt, tsteps[rg], preds[i][1,:], markershape=:circle, label=\"Group $(i)\")\n\tend\nend\n\n# Animate training, cannot make animation on CI server\n# anim = Plots.Animation()\niter = 0\ncallback = function (p, l, preds; doplot = false)\n  display(l)\n  global iter\n  iter += 1\n  if doplot && iter%1 == 0\n    # plot the original data\n    plt = scatter(tsteps, ode_data[1,:], label = \"Data\")\n\n    # plot the different predictions for individual shoot\n    plot_multiple_shoot(plt, preds, group_size)\n\n    frame(anim)\n    display(plot(plt))\n  end\n  return false\nend\n\n# Define parameters for Multiple Shooting\ngroup_size = 3\ncontinuity_term = 200\n\nfunction loss_function(data, pred)\n\treturn sum(abs2, data - pred)\nend\n\nfunction loss_multiple_shooting(p)\n    return multiple_shoot(p, ode_data, tsteps, prob_node, loss_function, Tsit5(),\n                          group_size; continuity_term)\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_multiple_shooting(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(p_init))\nres_ms = Optimization.solve(optprob, PolyOpt(),\n                                callback = callback)\n#gif(anim, \"multiple_shooting.gif\", fps=15)","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here's the animation that we get from above when doplot=true and the animation code is uncommented:","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic) The connected lines show the predictions of each group (Notice that there are overlapping points as well. These are the points we are trying to coincide.)","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"Here is an output with group_size = 30 (which is same as solving on the whole interval without splitting also called single shooting)","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"(Image: pic_single_shoot3)","category":"page"},{"location":"modules/DiffEqFlux/examples/multiple_shooting/","page":"Multiple Shooting","title":"Multiple Shooting","text":"It is clear from the above picture, a single shoot doesn't perform very well with the ODE Problem we have and gets stuck in a local minima.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#The-Reaction-DSL","page":"The Reaction DSL","title":"The Reaction DSL","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"This tutorial covers some of the basic syntax for building chemical reaction network models using Catalyst's domain specific language (DSL). Examples showing how to both construct and solve ODE, SDE, and jump models are provided in Basic Chemical Reaction Network Examples. To learn more about the symbolic ReactionSystems generated by the DSL, and how to use them directly, see the tutorial on Programmatic Construction of Symbolic Reaction Systems.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Basic-syntax","page":"The Reaction DSL","title":"Basic syntax","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"The @reaction_network macro allows the (symbolic) specification of reaction networks with a simple format. Its input is a set of chemical reactions, and from them it generates a symbolic ReactionSystem reaction network object. The ReactionSystem can be used as input to ModelingToolkit ODEProblem, NonlinearProblem, SteadyStateProblem, SDEProblem, JumpProblem, and more. ReactionSystems can also be incrementally extended as needed, allowing for programmatic construction of networks and network composition.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"The basic syntax is:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  2.0, X + Y --> XY\n  1.0, XY --> Z1 + Z2\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"where each line corresponds to a chemical reaction. Each reaction consists of a reaction rate (the expression on the left hand side of  ,), a set of substrates (the expression in-between , and -->), and a set of products (the expression on the right hand side of -->). The substrates and the products may contain one or more reactants, separated by +. The naming convention for these are the same as for normal variables in Julia.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"The chemical reaction model is generated by the @reaction_network macro and stored in the rn variable (a normal Julia variable, which does not need to be called rn). It corresponds to a ReactionSystem, a symbolic representation of the chemical network.  The generated ReactionSystem can be converted to a symbolic differential equation model via","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"osys  = convert(ODESystem, rn)","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"We can then convert the symbolic ODE model into a compiled, optimized representation for use in the SciML ODE solvers by constructing an ODEProblem. Creating an ODEProblem also requires our specifying the initial conditions for the model. We do this by creating a mapping from each symbolic variable representing a chemical species to its initial value","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"# define the symbolic variables\n@variables t, X(t), Y(t), Z(t), XY(t), Z1(t), Z2(t)\n\n# create the mapping\nu0 = [X => 1.0, Y => 1.0, XY => 1.0, Z1 => 1.0, Z2 => 1.0]","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Alternatively, we can create a mapping use Julia Symbols for each variable, and then convert them to a mapping involving symbolic variables like","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"u0 = symmap_to_varmap(rn, [:X => 1.0, :Y => 1.0, :XY => 1.0, :Z1 => 1.0, :Z2 => 1.0])","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Given the mapping, we can then create an ODEProblem from our symbolic ODESystem","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"oprob = ODEProblem(osys, u0, tspan, [])","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Catalyst provides a shortcut to avoid having to explicitly convert to an ODESystem and/or use symmap_to_varmap, allowing direct construction of the ODEProblem like","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"u0 = [:X => 1.0, :Y => 1.0, :XY => 1.0, :Z1 => 1.0, :Z2 => 1.0]\noprob = ODEProblem(rn, u0, tspan, [])","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"For more detailed examples, see the Basic Chemical Reaction Network Examples. The generated differential equations use the law of mass action. For the above example, the ODEs are then","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"fracdXdt = -2 X Y\nfracdYdt = -2 X Y\nfracdXYdt = 2 X Y - XY\nfracdZ1dt= XY\nfracdZ2dt = XY","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Defining-parameters-and-species","page":"The Reaction DSL","title":"Defining parameters and species","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Parameter values do not need to be set when the model is created. Components can be designated as symbolic parameters by declaring them at the end:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  p, ∅ --> X\n  d, X --> ∅\nend p d","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Parameters can only exist in the reaction rates (where they can be mixed with reactants). All variables not declared after end will be treated as a chemical species.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Production,-Destruction-and-Stoichiometry","page":"The Reaction DSL","title":"Production, Destruction and Stoichiometry","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Sometimes reactants are produced/destroyed from/to nothing. This can be designated using either 0 or ∅:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  2.0, 0 --> X\n  1.0, X --> ∅\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"If several molecules of the same reactant are involved in a reaction, the stoichiometry of a reactant in a reaction can be set using a number. Here, two molecules of species X form the dimer X2:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  1.0, 2X --> X2\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"this corresponds to the differential equation:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"fracdXdt = -X^2\nfracdX2dt = frac12 X^2","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Other numbers than 2 can be used, and parenthesis can be used to reuse the same stoichiometry for several reactants:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  1.0, X + 2(Y + Z) --> XY2Z2\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Note, one can explicitly multiply by integer coefficients too, i.e.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  1.0, X + 2*(Y + Z) --> XY2Z2\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Arrow-variants","page":"The Reaction DSL","title":"Arrow variants","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"A variety of unicode arrows are accepted by the DSL in addition to -->. All of these work:  >, → ↣, ↦, ⇾, ⟶, ⟼, ⥟, ⥟, ⇀, ⇁. Backwards arrows can also be used to write the reaction in the opposite direction. For example, these reactions are equivalent:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  1.0, X + Y --> XY\n  1.0, X + Y → XY\n  1.0, XY ← X + Y\n  1.0, XY <-- X + Y\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Bi-directional-arrows-for-reversible-reactions","page":"The Reaction DSL","title":"Bi-directional arrows for reversible reactions","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Bi-directional arrows, including bidirectional unicode arrows like ↔, can be used to designate a reversible reaction. For example, these two models are equivalent:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  2.0, X + Y --> XY\n  2.0, X + Y <-- XY\nend\nrn = @reaction_network begin\n  (2.0,2.0), X + Y <--> XY\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"If the reaction rates in the backward and forward directions are different, they can be designated in the following way:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  (2.0,1.0) X + Y <--> XY\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"which is identical to","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  2.0, X + Y --> XY\n  1.0, X + Y <-- XY\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Combining-several-reactions-in-one-line","page":"The Reaction DSL","title":"Combining several reactions in one line","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Several similar reactions can be combined in one line by providing a tuple of reaction rates and/or substrates and/or products. If several tuples are provided, they must all be of identical length. These pairs of reaction networks are all identical:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n  1.0, S --> (P1,P2)\nend\nrn2 = @reaction_network begin\n  1.0, S --> P1\n  1.0, S --> P2\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n  (1.0,2.0), (S1,S2) --> P\nend\nrn2 = @reaction_network begin\n  1.0, S1 --> P\n  2.0, S2 --> P\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n  (1.0,2.0,3.0), (S1,S2,S3) → (P1,P2,P3)\nend\nrn2 = @reaction_network begin\n  1.0, S1 --> P1\n  2.0, S2 --> P2\n  3.0, S3 --> P3\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"This can also be combined with bi-directional arrows, in which case separate tuples can be provided for the backward and forward reaction rates. These reaction networks are identical","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n (1.0,(1.0,2.0)), S <--> (P1,P2)\nend\nrn2 = @reaction_network begin\n  1.0, S --> P1\n  1.0, S --> P2\n  1.0, P1 --> S\n  2.0, P2 --> S\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Variable-reaction-rates","page":"The Reaction DSL","title":"Variable reaction rates","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Reaction rates do not need to be a single parameter or a number, but can also be expressions depending on time or the current concentration of other species (when, for example, one species can activate the production of another). For instance, this is a valid notation:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  k*X, Y --> ∅\nend k","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"and will have Y degraded at rate","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"fracdYdt = -kXY","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Note, this is actually equivalent to the reaction","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network begin\n  k, X + Y --> X\nend k","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"in the resulting generated ODESystem, however, the latter Reaction will be classified as ismassaction and the former will not, which can impact optimizations used in generating JumpSystems. For this reason, it is recommended to use the latter representation when possible.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Most expressions and functions are valid reaction rates, e.g.:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"using SpecialFunctions\nrn = @reaction_network begin\n  2.0*X^2, 0 --> X + Y\n  t*gamma(Y), X --> ∅\n  pi*X/Y, Y → ∅\nend","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"where here t always denotes Catalyst's time variable. Please note that many user-defined functions can be called directly, but others will require registration with Symbolics.jl (see the faq).","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Naming-the-generated-ReactionSystem","page":"The Reaction DSL","title":"Naming the generated ReactionSystem","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"ModelingToolkit uses system names to allow for compositional and hierarchical models. To specify a name for the generated ReactionSystem via the reaction_network macro, just place the name before begin:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn = @reaction_network production_degradation begin\n  p, ∅ --> X\n  d, X --> ∅\nend p d\nModelingToolkit.nameof(rn) == :production_degradation","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Pre-defined-functions","page":"The Reaction DSL","title":"Pre-defined functions","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Hill functions and a Michaelis-Menten function are pre-defined and can be used as rate laws. Below, the pair of reactions within rn1 are equivalent, as are the pair of reactions within rn2:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n  hill(X,v,K,n), ∅ --> X\n  v*X^n/(X^n+K^n), ∅ --> X\nend v K n\nrn2 = @reaction_network begin\n  mm(X,v,K), ∅ --> X\n  v*X/(X+K), ∅ --> X\nend v K","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Repressor Hill (hillr) and Michaelis-Menten (mmr) functions are also provided:","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"rn1 = @reaction_network begin\n  hillr(X,v,K,n), ∅ --> X\n  v*K^n/(X^n+K^n), ∅ --> X\nend v K n\nrn2 = @reaction_network begin\n  mmr(X,v,K), ∅ --> X\n  v*K/(X+K), ∅ --> X\nend v K","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Please see the API Rate Laws section for more details.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/#Interpolation-of-Julia-Variables","page":"The Reaction DSL","title":"Interpolation of Julia Variables","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"The DSL allows Julia variables to be interpolated for the network name, within rate constant expressions, or for species within the reaction. For example,","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"@parameters k\n@variables t, A(t)\nspec = A\nrate = k*A\nname = :network\nrn = @reaction_network $name begin\n    $rate*B, 2*$spec + B --> $spec + C\n  end","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"gives","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Model network with 1 equations\nStates (3):\n  A(t)\n  B(t)\n  C(t)\nParameters (1):\n  k","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"with","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"reactions(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"giving","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"1-element Vector{Reaction}:\n k*A(t)*B(t), 2A + B --> A + C","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"As the parameter k was pre-defined and appears via interpolation, we did not need to declare it at the end of the @reaction_network macro.","category":"page"},{"location":"modules/Catalyst/tutorials/dsl/","page":"The Reaction DSL","title":"The Reaction DSL","text":"Note, when using interpolation expressions like 2$spec won't work; the multiplication symbol must be explicitly included like 2*$spec.","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/#Solving-Integro-Differential-Equations-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The integral of function u(x),","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"int_0^tu(x)dx","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"where x is variable of integral and t is variable of integro differential equation,","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"is defined as","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using ModelingToolkit\n@parameters t\n@variables i(..)\nIi = Symbolics.Integral(t in DomainSets.ClosedInterval(0, t))","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"In multidimensional case,","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x,y) in DomainSets.UnitSquare())","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The UnitSquare domain ranges both x and y from 0 to 1. Similarly a rectangular or cuboidal domain can be defined using ProductDomain of ClosedIntervals.","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x,y) in DomainSets.ProductDomain(ClosedInterval(0 ,1), ClosedInterval(0 ,x)))","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/#dimensional-example","page":"Solving Integro Differential Equations","title":"1-dimensional example","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Lets take an example of an integro differential equation:","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"fract u(t)  + 2u(t) + 5 int_0^tu(x)dx = 1  textfor  t geq 0","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"and boundary condition","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"u(0) = 0","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using NeuralPDE, Flux, ModelingToolkit, Optimization, OptimizationOptimJL, DomainSets\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t\n@variables i(..)\nDi = Differential(t)\nIi = Integral(t in DomainSets.ClosedInterval(0, t))\neq = Di(i(t)) + 2*i(t) + 5*Ii(i(t)) ~ 1\nbcs = [i(0.) ~ 0.0]\ndomains = [t ∈ Interval(0.0,2.0)]\nchain = Chain(Dense(1,15,Flux.σ),Dense(15,1)) |> f64\n\nstrategy_ = GridTraining(0.05)\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy_)\n@named pde_system = PDESystem(eq,bcs,domains,[t],[i(t)])\nprob = NeuralPDE.discretize(pde_system,discretization)\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=100)","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Plotting the final solution and analytical solution","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nphi = discretization.phi\nu_predict  = [first(phi([t],res.u)) for t in ts]\n\nanalytic_sol_func(t) = 1/2*(exp(-t))*(sin(2*t))\nu_real  = [analytic_sol_func(t) for t in ts]\nusing Plots\nplot(ts ,u_real, label = \"Analytical Solution\")\nplot!(ts, u_predict, label = \"PINN Solution\")","category":"page"},{"location":"modules/NeuralPDE/tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"(Image: IDE)","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/#The-SciML-init-and-solve-Functions","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"solve function has the default definition","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"solve(args...; kwargs...) = solve!(init(args...; kwargs...))","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"The interface for the three functions is as follows:","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"init(::ProblemType, args...; kwargs...) :: IteratorType\nsolve!(::SolverType) :: SolutionType","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"where ProblemType, IteratorType, and SolutionType are the types defined in your package.","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"To avoid method ambiguity, the first argument of solve, solve!, and init must be dispatched on the type defined in your package.  For example, do not define a method such as","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"init(::AbstractVector, ::AlgorithmType)","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/#init-and-the-Iterator-Interface","page":"The SciML init and solve Functions","title":"init and the Iterator Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"init's return gives an IteratorType which is designed to allow the user to have more direct handling over the internal solving process. Because of this internal nature, the IteratorType has a less unified interface across problem types than other portions like ProblemType and SolutionType. For example, for differential equations this is the  Integrator Interface designed for mutating solutions in a manner for callback implementation, which is distinctly different from the  LinearSolve init interface which is designed for caching efficiency with reusing factorizations.","category":"page"},{"location":"modules/SciMLBase/interfaces/Init_Solve/#__solve-and-High-Level-Handling","page":"The SciML init and solve Functions","title":"__solve and High-Level Handling","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Init_Solve/","page":"The SciML init and solve Functions","title":"The SciML init and solve Functions","text":"While init and solve are the common entry point for users, solver packages will mostly define dispatches on SciMLBase.__init and SciMLBase.__solve. The reason is because this allows for SciMLBase.init and SciMLBase.solve to have common implementations across all solvers for doing things such as checking for common errors and throwing high level messages. Solvers can opt-out of the high level error handling by directly defining SciMLBase.init and SciMLBase.solve instead, though this is not recommended in order to allow for uniformity of the error messages.","category":"page"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#Finding-Input-Output-Equations","page":"Input-Output Equation tools","title":"Finding Input-Output Equations","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/","page":"Input-Output Equation tools","title":"Input-Output Equation tools","text":"find_ioequations","category":"page"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#StructuralIdentifiability.find_ioequations","page":"Input-Output Equation tools","title":"StructuralIdentifiability.find_ioequations","text":"find_ioequations(ode, [var_change_policy=:default])\n\nFinds the input-output equations of an ODE system Input:\n\node - the ODE system\nvar_change_policy - whether to perform automatic variable change, can be one of :default, :yes, :no\n\nOutput:\n\na dictionary from \"leaders\" to the corresponding input-output equations\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#Reducing-with-respect-to-Input-Output-Equations","page":"Input-Output Equation tools","title":"Reducing with respect to Input-Output Equations","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/","page":"Input-Output Equation tools","title":"Input-Output Equation tools","text":"PBRepresentation\npseudodivision\ndiffreduce\nio_switch!","category":"page"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#StructuralIdentifiability.PBRepresentation","page":"Input-Output Equation tools","title":"StructuralIdentifiability.PBRepresentation","text":"The structure for storing a projection-based representation of differential ideal (see Section 2.3 https://arxiv.org/abs/2111.00991). Contains the following fields:\n\ny_names - the names of the variables with finite order in the profile (typically, outputs)\nu_names - the names of the variables with infinite order in the profile (typically, inputs)\nparam_names - the names of the parameters\nprofile - the profile of the PB-representation (see Definiton 2.13) as a dict from y_names with finite orders to the orders\nprojections - the corresponding projections (see Definition 2.15) as a dict from y_names to the projections\n\n\n\n\n\n","category":"type"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#StructuralIdentifiability.pseudodivision","page":"Input-Output Equation tools","title":"StructuralIdentifiability.pseudodivision","text":"pseudodivision(f, g, x)\n\nComputes the result of pseudodivision of f by g as univariate polynomials in x Input:\n\nf - the polynomail to be divided\ng - the polynomial to divide by\nx - the variable for the division\n\nOutput: the pseudoreminder of f divided by g w.r.t. x\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#StructuralIdentifiability.diffreduce","page":"Input-Output Equation tools","title":"StructuralIdentifiability.diffreduce","text":"diffreduce(diffpoly, pbr)\n\nComputes the result of differential reduction of a differential polynomial diffpoly with respect to the charset defined by a PB-representation pbr Input:\n\ndiffpoly - a polynomial representing a differential polynomial to be reduced\npbr - a projection-based representation\n\nOutput: the result of differential reduction of diffpoly by pbr considered as a characteristic set (see Remark 2.20 in the paper)\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/ioequations/ioequations/#StructuralIdentifiability.io_switch!","page":"Input-Output Equation tools","title":"StructuralIdentifiability.io_switch!","text":"io_switch(pbr)\n\nIn a single-output pb-representation pbr makes the leading variable to be the first of the inputs\n\n\n\n\n\n","category":"function"},{"location":"modules/Optimization/API/optimization_function/#optfunction","page":"OptimizationFunction","title":"OptimizationFunction","text":"","category":"section"},{"location":"modules/Optimization/API/optimization_function/","page":"OptimizationFunction","title":"OptimizationFunction","text":"OptimizationFunction","category":"page"},{"location":"modules/Optimization/API/optimization_function/#SciMLBase.OptimizationFunction","page":"OptimizationFunction","title":"SciMLBase.OptimizationFunction","text":"OptimizationFunction{iip,AD,F,G,H,HV,C,CJ,CH,HP,CJP,CHP,S,HCV,CJCV,CHCV} <: AbstractOptimizationFunction{iip}\n\nA representation of an optimization of an objective function f, defined by:\n\nmin_u f(up)\n\nand all of its related functions, such as the gradient of f, its Hessian, and more. For all cases, u is the state and p are the parameters.\n\nConstructor\n\nOptimizationFunction{iip}(f,adtype::AbstractADType=NoAD();                           grad=nothing,hess=nothing,hv=nothing,                           cons=nothing, consj=nothing,consh=nothing,                           hessprototype=nothing,consjacprototype=nothing,                           conshessprototype = nothing,                           syms = nothing, hesscolorvec = nothing,                           consjaccolorvec = nothing,                           conshesscolorvec = nothing)\n\nadtype: see the section \"Defining Optimization Functions via AD\"\ngrad(G,u,p) or G=grad(u,p): the gradient of f with respect to u\nhess(H,u,p) or H=hess(u,p): the Hessian of f with respect to u\nhv(Hv,u,v,p) or Hv=hv(u,v,p): the Hessian-vector product racd^2 fdu^2 v.\ncons(res,x,p) or cons(x,p) : the constraints function, should mutate or return a vector   with value of the ith constraint, evaluated at the current values of variables   inside the optimization routine. This takes just the function evaluations   and the equality or inequality assertion is applied by the solver based on the constraint   bounds passed as lcons and ucons to OptimizationProblem, in case of equality   constraints lcons and ucons should be passed equal values.\ncons_j(res,x,p) or res=cons_j(x,p): the Jacobian of the constraints.\ncons_h(res,x,p) or res=cons_h(x,p): the Hessian of the constraints, provided as  an array of Hessians with res[i] being the Hessian with respect to the ith output on cons.\nparamjac(pJ,u,p): returns the parameter Jacobian racdfdp.\nhess_prototype: a prototype matrix matching the type that matches the Hessian. For example, if the Hessian is tridiagonal, then an appropriately sized Hessian matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Hessian. The default is nothing, which means a dense Hessian.\ncons_jac_prototype: a prototype matrix matching the type that matches the constraint Jacobian. The default is nothing, which means a dense constraint Jacobian.\ncons_hess_prototype: a prototype matrix matching the type that matches the constraint Hessian. This is defined as an array of matrices, where hess[i] is the Hessian w.r.t. the ith output. For example, if the Hessian is sparse, then hess is a Vector{SparseMatrixCSC}. The default is nothing, which means a dense constraint Hessian.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nhess_colorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the hess_prototype. This specializes the Hessian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\ncons_jac_colorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the cons_jac_prototype.\ncons_hess_colorvec: an array of color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the cons_hess_prototype.\n\nDefining Optimization Functions Via AD\n\nWhile using the keyword arguments gives the user control over defining all of the possible functions, the simplest way to handle the generation of an OptimizationFunction is by specifying an AD type. By doing so, this will automatically fill in all of the extra functions. For example,\n\nOptimizationFunction(f,AutoZygote())\n\nwill use Zygote.jl to define all of the necessary functions. Note that if any functions are defined directly, the auto-AD definition does not overwrite the user's choice.\n\nEach of the AD-based constructors are documented separately via their own dispatches.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the OptimizationFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"modules/Optimization/API/optimization_function/#Automatic-Differentiation-Construction-Choice-Recommendations","page":"OptimizationFunction","title":"Automatic Differentiation Construction Choice Recommendations","text":"","category":"section"},{"location":"modules/Optimization/API/optimization_function/","page":"OptimizationFunction","title":"OptimizationFunction","text":"The choices for the auto-AD fill-ins with quick descriptions are:","category":"page"},{"location":"modules/Optimization/API/optimization_function/","page":"OptimizationFunction","title":"OptimizationFunction","text":"AutoForwardDiff(): The fastest choice for small optimizations\nAutoReverseDiff(compile=false): A fast choice for large scalar optimizations\nAutoTracker(): Like ReverseDiff but GPU-compatible\nAutoZygote(): The fastest choice for non-mutating array-based (BLAS) functions\nAutoFiniteDiff(): Finite differencing, not optimal but always applicable\nAutoModelingToolkit(): The fastest choice for large scalar optimizations","category":"page"},{"location":"modules/Optimization/API/optimization_function/#Automatic-Differentiation-Choice-API","page":"OptimizationFunction","title":"Automatic Differentiation Choice API","text":"","category":"section"},{"location":"modules/Optimization/API/optimization_function/","page":"OptimizationFunction","title":"OptimizationFunction","text":"The following sections describe the Auto-AD choices in detail.","category":"page"},{"location":"modules/Optimization/API/optimization_function/","page":"OptimizationFunction","title":"OptimizationFunction","text":"AutoForwardDiff\nAutoFiniteDiff\nAutoReverseDiff\nAutoZygote\nAutoTracker\nAutoModelingToolkit","category":"page"},{"location":"modules/DiffEqDocs/types/rode_types/#rode_problem","page":"RODE Problems","title":"RODE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"RODEProblem\nRODEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/rode_types/#SciMLBase.RODEProblem","page":"RODE Problems","title":"SciMLBase.RODEProblem","text":"Defines a random ordinary differential equation (RODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/rode_types/\n\nMathematical Specification of a RODE Problem\n\nTo define a RODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nfracdudt = f(uptW(t))\n\nwhere W(t) is a random process. f should be specified as f(u,p,t,W) (or in-place as f(du,u,p,t,W)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nConstructors\n\nRODEProblem(f::RODEFunction,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing)\nRODEProblem{isinplace}(f,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing,mass_matrix=I) : Defines the RODE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The drift function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page\nrand_prototype: A prototype type instance for the noise vector. It defaults to nothing, which means the problem should be interpreted as having a noise vector whose size matches u0.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/rode_types/#SciMLBase.RODEFunction","page":"RODE Problems","title":"SciMLBase.RODEFunction","text":"RODEFunction{iip,F,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractRODEFunction{iip}\n\nA representation of an RODE function f, defined by:\n\nM fracdudt = f(uptW)dt\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nRODEFunction{iip,recompile}(f;\n                           mass_matrix=I,\n                           analytic=nothing,\n                           tgrad=nothing,\n                           jac=nothing,\n                           jvp=nothing,\n                           vjp=nothing,\n                           jac_prototype=nothing,\n                           sparsity=jac_prototype,\n                           paramjac = nothing,\n                           syms = nothing,\n                           indepsym = nothing,\n                           colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the RODEFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/rode_types/#Solution-Type","page":"RODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"RODESolution","category":"page"},{"location":"modules/DiffEqDocs/types/rode_types/#SciMLBase.RODESolution","page":"RODE Problems","title":"SciMLBase.RODESolution","text":"struct RODESolution{T, N, uType, uType2, DType, tType, randType, P, A, IType, DE} <: SciMLBase.AbstractRODESolution{T, N, uType}\n\nRepresentation of the solution to an stochastic differential equation defined by an SDEProblem, or of a random ordinary differential equation defined by an RODEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://diffeq.sciml.ai/stable/basics/solution/\n\nFields\n\nu: the representation of the SDE or RODE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution  Handling page of the DifferentialEquations.jl documentation.\nt: the time points corresponding to the saved values of the ODE solution.\nW: the representation of the saved noise process from the solution. See the Noise Processes page of the DifferentialEquations.jl documentation for more details:  https://diffeq.sciml.ai/stable/features/noiseprocess/ . Note that this noise is only saved in full if `savenoise=true` in the solver.\nprob: the original SDEProblem/RODEProblem that was solved.\nalg: the algorithm type used by the solver.\ndestats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully (sol.retcode === :Success), whether it terminated due to a user-defined callback (sol.retcode === :Terminated), or whether it exited due to an error. For more details, see the return code section of the DifferentialEquations.jl documentation.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/CNFLayer/#CNF-Layer-Functions","page":"Continuous Normalizing Flows Layer","title":"CNF Layer Functions","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"The following layers are helper functions for easily building neural differential equation architectures specialized for the task of density estimation through Continuous Normalizing Flows (CNF).","category":"page"},{"location":"modules/DiffEqFlux/layers/CNFLayer/","page":"Continuous Normalizing Flows Layer","title":"Continuous Normalizing Flows Layer","text":"DeterministicCNF\nFFJORD\nFFJORDDistribution","category":"page"},{"location":"modules/DiffEqFlux/layers/CNFLayer/#DiffEqFlux.DeterministicCNF","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.DeterministicCNF","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a direct computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\n!!!note     This layer has been deprecated in favour of FFJORD. Use FFJORD with monte_carlo=false instead.\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nDeterministicCNF(model, tspan, basedist=nothing, monte_carlo=false, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/CNFLayer/#DiffEqFlux.FFJORD","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORD","text":"Constructs a continuous-time recurrent neural network, also known as a neural ordinary differential equation (neural ODE), with fast gradient calculation via adjoints [1] and specialized for density estimation based on continuous normalizing flows (CNF) [2] with a stochastic approach [2] for the computation of the trace of the dynamics' jacobian. At a high level this corresponds to the following steps:\n\nParameterize the variable of interest x(t) as a function f(z, θ, t) of a base variable z(t) with known density p_z;\nUse the transformation of variables formula to predict the density px as a function of the density pz and the trace of the Jacobian of f;\nChoose the parameter θ to minimize a loss function of p_x (usually the negative likelihood of the data);\n\nAfter these steps one may use the NN model and the learned θ to predict the density p_x for new values of x.\n\nFFJORD(model, basedist=nothing, monte_carlo=false, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain neural network that defines the dynamics of the model.\nbasedist: Distribution of the base variable. Set to the unit normal by default.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the Common Solver Arguments documentation for more details.\n\nReferences:\n\n[1] Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC press, 1987.\n\n[2] Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. \"Neural ordinary differential equations.\" In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp. 6572-6583. 2018.\n\n[3] Grathwohl, Will, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. \"Ffjord: Free-form continuous dynamics for scalable reversible generative models.\" arXiv preprint arXiv:1810.01367 (2018).\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/CNFLayer/#DiffEqFlux.FFJORDDistribution","page":"Continuous Normalizing Flows Layer","title":"DiffEqFlux.FFJORDDistribution","text":"FFJORD can be used as a distribution to generate new samples by rand or estimate densities by pdf or logpdf (from Distributions.jl).\n\nArguments:\n\nmodel: A FFJORD instance\nregularize: Whether we use regularization (default: false)\nmonte_carlo: Whether we use monte carlo (default: true)\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/examples/neural_sde/#Neural-Stochastic-Differential-Equations-With-Method-of-Moments","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"With neural stochastic differential equations, there is once again a helper form neural_dmsde which can be used for the multiplicative noise case (consult the layers API documentation, or this full example using the layer function).","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"However, since there are far too many possible combinations for the API to support, in many cases you will want to performantly define neural differential equations for non-ODE systems from scratch. For these systems, it is generally best to use TrackerAdjoint with non-mutating (out-of-place) forms. For example, the following defines a neural SDE with neural networks for both the drift and diffusion terms:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt(u, p, t) = model(u)\ng(u, p, t) = model2(u)\nprob = SDEProblem(dudt, g, x, tspan, nothing)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"where model and model2 are different neural networks. The same can apply to a neural delay differential equation. Its out-of-place formulation is f(u,h,p,t). Thus for example, if we want to define a neural delay differential equation which uses the history value at p.tau in the past, we can define:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"dudt!(u, h, p, t) = model([u; h(t - p.tau)])\nprob = DDEProblem(dudt_, u0, h, tspan, nothing)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"First let's build training data from the same example as the neural ODE:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"using Plots, Statistics\nusing Flux, Optimization, OptimizationFlux, DiffEqFlux, StochasticDiffEq, SciMLBase.EnsembleAnalysis\n\nu0 = Float32[2.; 0.]\ndatasize = 30\ntspan = (0.0f0, 1.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"function trueSDEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nmp = Float32[0.2, 0.2]\nfunction true_noise_func(du, u, p, t)\n    du .= mp.*u\nend\n\nprob_truesde = SDEProblem(trueSDEfunc, true_noise_func, u0, tspan)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"For our dataset we will use DifferentialEquations.jl's parallel ensemble interface to generate data from the average of 10,000 runs of the SDE:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Take a typical sample from the mean\nensemble_prob = EnsembleProblem(prob_truesde)\nensemble_sol = solve(ensemble_prob, SOSRI(), trajectories = 10000)\nensemble_sum = EnsembleSummary(ensemble_sol)\n\nsde_data, sde_data_vars = Array.(timeseries_point_meanvar(ensemble_sol, tsteps))","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we build a neural SDE. For simplicity we will use the NeuralDSDE neural SDE with diagonal noise layer function:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"drift_dudt = Flux.Chain(x -> x.^3,\n                       Flux.Dense(2, 50, tanh),\n                       Flux.Dense(50, 2))\np1, re1 = Flux.destructure(drift_dudt)\n\ndiffusion_dudt = Flux.Chain(Flux.Dense(2, 2))\np2, re2 = Flux.destructure(diffusion_dudt)\n\nneuralsde = NeuralDSDE(drift_dudt, diffusion_dudt, tspan, SOSRI(),\n                       saveat = tsteps, reltol = 1e-1, abstol = 1e-1)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Let's see what that looks like:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"# Get the prediction using the correct initial condition\nprediction0 = neuralsde(u0)\n\ndrift_(u, p, t) = re1(p[1:neuralsde.len])(u)\ndiffusion_(u, p, t) = re2(p[neuralsde.len+1:end])(u)\n\nprob_neuralsde = SDEProblem(drift_, diffusion_, u0,(0.0f0, 1.2f0), neuralsde.p)\n\nensemble_nprob = EnsembleProblem(prob_neuralsde)\nensemble_nsol = solve(ensemble_nprob, SOSRI(), trajectories = 100,\n                      saveat = tsteps)\nensemble_nsum = EnsembleSummary(ensemble_nsol)\n\nplt1 = plot(ensemble_nsum, title = \"Neural SDE: Before Training\")\nscatter!(plt1, tsteps, sde_data', lw = 3)\n\nscatter(tsteps, sde_data[1,:], label = \"data\")\nscatter!(tsteps, prediction0[1,:], label = \"prediction\")","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now just as with the neural ODE we define a loss function that calculates the mean and variance from n runs at each time point and uses the distance from the data values:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"function predict_neuralsde(p, u = u0)\n  return Array(neuralsde(u, p))\nend\n\nfunction loss_neuralsde(p; n = 100)\n  u = repeat(reshape(u0, :, 1), 1, n)\n  samples = predict_neuralsde(p, u)\n  means = mean(samples, dims = 2)\n  vars = var(samples, dims = 2, mean = means)[:, 1, :]\n  means = means[:, 1, :]\n  loss = sum(abs2, sde_data - means) + sum(abs2, sde_data_vars - vars)\n  return loss, means, vars\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"list_plots = []\niter = 0\n\n# Callback function to observe training\ncallback = function (p, loss, means, vars; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  # loss against current data\n  display(loss)\n\n  # plot current prediction against data\n  plt = Plots.scatter(tsteps, sde_data[1,:], yerror = sde_data_vars[1,:],\n                     ylim = (-4.0, 8.0), label = \"data\")\n  Plots.scatter!(plt, tsteps, means[1,:], ribbon = vars[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n\n  if doplot\n    display(plt)\n  end\n  return false\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Now we train using this loss function. We can pre-train a little bit using a smaller n and then decrease it after it has had some time to adjust towards the right mean behavior:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"opt = ADAM(0.025)\n\n# First round of training with n = 10\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_neuralsde(x, n=10), adtype)\noptprob = Optimization.OptimizationProblem(optf, neuralsde.p)\nresult1 = Optimization.solve(optprob, opt,\n                                 callback = callback, maxiters = 100)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"We resume the training with a larger n. (WARNING - this step is a couple of orders of magnitude longer than the previous one).","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"optf2 = Optimization.OptimizationFunction((x,p) -> loss_neuralsde(x, n=100), adtype)\noptprob2 = Optimization.OptimizationProblem(optf2, result1.u)\nresult2 = Optimization.solve(optprob2, opt,\n                                 callback = callback, maxiters = 100)","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"And now we plot the solution to an ensemble of the trained neural SDE:","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"_, means, vars = loss_neuralsde(result2.u, n = 1000)\n\nplt2 = Plots.scatter(tsteps, sde_data', yerror = sde_data_vars',\n                     label = \"data\", title = \"Neural SDE: After Training\",\n                     xlabel = \"Time\")\nplot!(plt2, tsteps, means', lw = 8, ribbon = vars', label = \"prediction\")\n\nplt = plot(plt1, plt2, layout = (2, 1))\nsavefig(plt, \"NN_sde_combined.png\"); nothing # sde","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"(Image: )","category":"page"},{"location":"modules/DiffEqFlux/examples/neural_sde/","page":"Neural Stochastic Differential Equations With Method of Moments","title":"Neural Stochastic Differential Equations With Method of Moments","text":"Try this with GPUs as well!","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/#Type-Traits","page":"Type Traits","title":"Type Traits","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"Many of the DiffEqBase abstract types have associated traits. These can be used to check compatibility and apply separate code paths. For example, a parameter estimation algorithm can set the default for using autodifferentiation by checking if the algorithm is compatible with autodifferentiation.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"Below are the abstract types along with the associated trait functions. These are listed as:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"f(x)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"where f is the trait function and x is the any type which subtypes the abstract type.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/#AbstractODEProblem","page":"Type Traits","title":"AbstractODEProblem","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"isinplace : Returns true if the problem uses in-place functions","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/#AbstractRODEProblem","page":"Type Traits","title":"AbstractRODEProblem","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"is_diagonal_noise : Returns true if the noise is diagonal.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/#DEAlgorithm","page":"Type Traits","title":"DEAlgorithm","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/type_traits/","page":"Type Traits","title":"Type Traits","text":"isautodifferentiable : Returns true if the algorithm is autodifferentiable.","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/#Delay-Differential-Equations","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Other differential equation problem types from DifferentialEquations.jl are supported. For example, we can build a layer with a delay differential equation like:","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using DifferentialEquations, Optimization, SciMLSensitivity,\n      OptimizationPolyalgorithms\n\n\n# Define the same LV equation, but including a delay parameter\nfunction delay_lotka_volterra!(du, u, h, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = (α   - β*y) * h(p, t-0.1)[1]\n  du[2] = dy = (δ*x - γ)   * y\nend\n\n# Initial parameters\np = [2.2, 1.0, 2.0, 0.4]\n\n# Define a vector containing delays for each variable (although only the first\n# one is used)\nh(p, t) = ones(eltype(p), 2)\n\n# Initial conditions\nu0 = [1.0, 1.0]\n\n# Define the problem as a delay differential equation\nprob_dde = DDEProblem(delay_lotka_volterra!, u0, h, (0.0, 10.0),\n                      constant_lags = [0.1])\n\nfunction predict_dde(p)\n  return Array(solve(prob_dde, MethodOfSteps(Tsit5()),\n                              u0=u0, p=p, saveat = 0.1,\n                              sensealg = ReverseDiffAdjoint()))\nend\n\nloss_dde(p) = sum(abs2, x-1 for x in predict_dde(p))\n\nusing Plots\ncallback = function (p,l...;doplot=false)\n  display(loss_dde(p))\n  doplot && display(plot(solve(remake(prob_dde,p=p),MethodOfSteps(Tsit5()),saveat=0.1),ylim=(0,6)))\n  return false\nend\n\ncallback(p,loss_dde(p)...)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_dde(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\nresult_dde = Optimization.solve(optprob, PolyOpt(), maxiters = 300, callback=callback)","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Notice that we chose sensealg = ReverseDiffAdjoint() to utilize the ReverseDiff.jl reverse-mode to handle the delay differential equation.","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We define a callback to display the solution at the current parameters for each step of the training:","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using Plots\ncallback = function (p,l...;doplot=false)\n  display(loss_dde(p))\n  doplot && display(plot(solve(remake(prob_dde,p=p),MethodOfSteps(Tsit5()),saveat=0.1),ylim=(0,6)))\n  return false\nend\n\ncallback(p,loss_dde(p)...)","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We use Optimization.solve to optimize the parameters for our loss function:","category":"page"},{"location":"modules/SciMLSensitivity/dde_fitting/delay_diffeq/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_dde(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\nresult_dde = Optimization.solve(optprob, PolyOpt(), callback=callback)","category":"page"},{"location":"highlevels/equation_solvers/#Equation-Solvers-Overview","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"The SciML Equation Solvers cover a large set of SciMLProblems with SciMLAlgorithms that are efficient, numerically stable, and flexible. These methods tie into libraries like SciMLSensitivity.jl to be fully differentiable and compatible with machine learning pipelines, and are designed for integration with applications like parameter estimation, global sensitivity analysis, and more.","category":"page"},{"location":"highlevels/equation_solvers/#LinearSolve.jl:-Unified-Interface-for-Linear-Solvers","page":"Equation Solvers Overview","title":"LinearSolve.jl: Unified Interface for Linear Solvers","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"LinearSolve.jl is the canonical library for solving LinearProblems. It includes:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Fast pure Julia LU factorizations which outperform standard BLAS\nKLU for faster sparse LU factorization on unstructured matrices\nUMFPACK for faster sparse LU factorization on matrices with some repeated structure\nMKLPardiso wrappers for handling many sparse matrices faster than SuiteSparse (KLU, UMFPACK) methods\nGPU-offloading for large dense matrices\nWrappers to all of the Krylov implementations (Krylov.jl, IterativeSolvers.jl, KrylovKit.jl) for easy testing of all of them. LinearSolve.jl handles the API differences, especially with the preconditioner definitions\nA polyalgorithm that smartly chooses between these methods\nA caching interface which automates caching of symbolic factorizations and numerical factorizations as optimally as possible\nCompatible with arbitrary AbstractArray and Number types, such as GPU-based arrays, uncertainty quantification number types, and more.","category":"page"},{"location":"highlevels/equation_solvers/#NonlinearSolve.jl:-Unified-Interface-for-Nonlinear-Solvers","page":"Equation Solvers Overview","title":"NonlinearSolve.jl: Unified Interface for Nonlinear Solvers","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"NonlinearSolve.jl is the canonical library for solving NonlinearProblems. It includes:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Fast non-allocating implementations on static arrays of common methods (Newton-Rhapson)\nBracketing methods (Bisection, Falsi) for methods with known upper and lower bounds\nWrappers to common other solvers (NLsolve.jl, MINPACK, KINSOL from Sundials) for trust region methods, line search based approaches, etc.\nBuilt over the LinearSolve.jl API for maximum flexibility and performance in the solving approach\nCompatible with arbitrary AbstractArray and Number types, such as GPU-based arrays, uncertainty quantification number types, and more.","category":"page"},{"location":"highlevels/equation_solvers/#DifferentialEquations.jl:-Unified-Interface-for-Differential-Equation-Solvers","page":"Equation Solvers Overview","title":"DifferentialEquations.jl: Unified Interface for Differential Equation Solvers","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"DifferentialEquations.jl is the canonical library for solving DEProblems. This includes:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay  differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions)  (DEProblems with callbacks and JumpProblem)","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"The well-optimized DifferentialEquations solvers benchmark as some of the fastest implementations of classic algorithms. It also includes algorithms from recent research which routinely outperform the \"standard\" C/Fortran methods, and algorithms optimized for high-precision and HPC applications. Simultaneously, it wraps the classic C/Fortran methods, making it easy to switch over to them whenever necessary. Solving differential equations with different methods from different languages and packages can be done by changing one line of code, allowing for easy benchmarking to ensure you are using the fastest method possible.","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"DifferentialEquations.jl integrates with the Julia package sphere with:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"GPU acceleration through CUDAnative.jl and CuArrays.jl\nAutomated sparsity detection with Symbolics.jl\nAutomatic Jacobian coloring with SparseDiffTools.jl,  allowing for fast solutions to problems with sparse or structured (Tridiagonal, Banded,  BlockBanded, etc.) Jacobians\nAllowing the specification of linear solvers for maximal efficiency\nProgress meter integration with the Juno IDE for estimated time to solution\nAutomatic plotting of time series and phase plots\nBuilt-in interpolations\nWraps for common C/Fortran methods, like Sundials and Hairer's radau\nArbitrary precision with BigFloats and Arbfloats\nArbitrary array types, allowing the definition of differential equations on matrices and distributed arrays\nUnit-checked arithmetic with Unitful","category":"page"},{"location":"highlevels/equation_solvers/#Optimization.jl:-Unified-Interface-for-Optimization","page":"Equation Solvers Overview","title":"Optimization.jl: Unified Interface for Optimization","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Optimization.jl is the canonical library for solving OptimizationProblems. It includes wrappers of most of the Julia nonlinear optimization ecosystem, allowing one syntax to use all packages in a uniform manner. This covers:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"OptimizationBBO for BlackBoxOptim.jl\nOptimizationEvolutionary for Evolutionary.jl  (see also this documentation)\nOptimizationGCMAES for GCMAES.jl\nOptimizationMOI for MathOptInterface.jl  (usage of algorithm via MathOptInterface API; see also the API  documentation)\nOptimizationMetaheuristics for Metaheuristics.jl  (see also this documentation)\nOptimizationMultistartOptimization for MultistartOptimization.jl  (see also this documentation)\nOptimizationNLopt for NLopt.jl  (usage via the NLopt API; see also the available  algorithms)\nOptimizationNOMAD for NOMAD.jl (see also  this documentation)\nOptimizationNonconvex for Nonconvex.jl (see  also this documentation)\nOptimizationQuadDIRECT for QuadDIRECT.jl\nOptimizationSpeedMapping for SpeedMapping.jl  (see also this documentation)","category":"page"},{"location":"highlevels/equation_solvers/#Integrals.jl:-Unified-Interface-for-Numerical-Integration","page":"Equation Solvers Overview","title":"Integrals.jl: Unified Interface for Numerical Integration","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Integrals.jl is the canonical library for solving IntegralsProblems. It includes wrappers of most of the Julia quadrature  ecosystem, allowing one syntax to use all packages in a uniform manner. This covers:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Gauss-Kronrod quadrature\nCubature methods (both h and p cubature)\nAdaptive Monte Carlo methods","category":"page"},{"location":"highlevels/equation_solvers/#JumpProcesses.jl:-Unified-Interface-for-Jump-Processes","page":"Equation Solvers Overview","title":"JumpProcesses.jl: Unified Interface for Jump Processes","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"JumpProcesses.jl is the library for Poisson jump processes, also known as chemical master equations or Gillespie simulations, for simulating chemical reaction networks and other applications. It allows for solving with many methods, including:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Direct: the Gillespie Direct method SSA.\nRDirect: A variant of Gillespie's Direct method that uses rejection to sample the next reaction.\nDirectCR: The Composition-Rejection Direct method of Slepoy et al. For large networks and linear chain-type networks it will often give better performance than Direct. (Requires dependency graph, see below.)\nDirectFW: the Gillespie Direct method SSA with FunctionWrappers. This aggregator uses a different internal storage format for collections of ConstantRateJumps. \nFRM: the Gillespie first reaction method SSA. Direct should generally offer better performance and be preferred to FRM.\nFRMFW: the Gillespie first reaction method SSA with FunctionWrappers.\nNRM: The Gibson-Bruck Next Reaction Method. For some reaction network  structures this may offer better performance than Direct (for example,  large, linear chains of reactions). (Requires dependency graph, see below.) \nRSSA: The Rejection SSA (RSSA) method of Thanh et al. With RSSACR, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nRSSACR: The Rejection SSA (RSSA) with Composition-Rejection method of Thanh et al. With RSSA, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nSortingDirect: The Sorting Direct Method of McCollum et al. It will usually offer performance as good as Direct, and for some systems can offer substantially better performance. (Requires dependency graph, see below.)","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"The design of JumpProcesses.jl composes with DifferentialEquations.jl, allowing for discrete stochastic chemical reactions to be easily mixed with differential equation models, allowing for simulation of hybrid systems, jump diffusions, and differential equations driven by Levy processes.","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"In addition, JumpProcesses's interfaces allow for solving with regular jump methods,  such as adaptive Tau-Leaping. ","category":"page"},{"location":"highlevels/equation_solvers/#Third-Party-Libraries-to-Note","page":"Equation Solvers Overview","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/equation_solvers/#JuMP.jl:-Julia-for-Mathematical-Programming","page":"Equation Solvers Overview","title":"JuMP.jl: Julia for Mathematical Programming","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"While Optimization.jl is the preferred library for nonlinear optimization, for all other forms of optimization  Julia for Mathematical Programming (JuMP) is the star. JuMP is the leading choice in Julia for doing:","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"Linear Programming\nQuadratic Programming\nConvex Programming\nConic Programming\nSemidefinite Programming\nMixed-Complementarity Programming\nInteger Programming\nMixed Integer (nonlinear/linear) Programming\n(Mixed Integer) Second Order Conic Programming","category":"page"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"JuMP can also be used for some nonlinear programming, though the Optimization.jl bindings to the JuMP solvers (via MathOptInterface.jl) is generally preferred.","category":"page"},{"location":"highlevels/equation_solvers/#FractionalDiffEq.jl:-Fractional-Differential-Equation-Solvers","page":"Equation Solvers Overview","title":"FractionalDiffEq.jl: Fractional Differential Equation Solvers","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"FractionalDiffEq.jl is a set of high-performance solvers for fractional differential equations.","category":"page"},{"location":"highlevels/equation_solvers/#ManifoldDiffEq.jl:-Solvers-for-Differential-Equations-on-Manifolds","page":"Equation Solvers Overview","title":"ManifoldDiffEq.jl: Solvers for Differential Equations on Manifolds","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"ManifoldDiffEq.jl is a set of high-performance solvers for differential equations on manifolds using methods such as Lie Group actions and frozen coefficients (Crouch-Grossman methods). These solvers can in many cases out-perform the OrdinaryDiffEq.jl nonautonomous operator ODE solvers by using methods specialized on manifold definitions of ManifoldsBase.","category":"page"},{"location":"highlevels/equation_solvers/#Manopt.jl:-Optimization-on-Manifolds","page":"Equation Solvers Overview","title":"Manopt.jl: Optimization on Manifolds","text":"","category":"section"},{"location":"highlevels/equation_solvers/","page":"Equation Solvers Overview","title":"Equation Solvers Overview","text":"ManOpt.jl allows for easy and efficient solving of nonlinear optimization problems on manifolds.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/#Handling-Divergent-and-Unstable-Trajectories","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"It is not uncommon for a set of parameters in an ODE model to simply give a divergent trajectory. If the rate of growth compounds and outpaces the rate of decay, you will end up at infinity in finite time. This it is not uncommon to see divergent trajectories in the optimization of parameters, as many times an optimizer can take an excursion into a parameter regime which simply gives a model with an infinite solution.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"This can be addressed by using the retcode system. In DifferentialEquations.jl, RetCodes detail the status of the returned solution. Thus if the retcode corresponds to a failure, we can use this to give an infinite loss and effectively discard the parameters. This is shown in the loss function:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"function loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if tmp_sol.retcode == :Success\n    return sum(abs2,Array(tmp_sol) - dataset)\n  else\n    return Inf\n  end\nend","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"A full example making use of this trick is:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"using DifferentialEquations, SciMLSensitivity, Optimization, OptimizationFlux, OptimizationOptimJL, Plots\n\nfunction lotka_volterra!(du,u,p,t)\n    rab, wol = u\n    α,β,γ,δ=p\n    du[1] = drab = α*rab - β*rab*wol\n    du[2] = dwol = γ*rab*wol - δ*wol\n    nothing\nend\n\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(lotka_volterra!,u0,tspan,p)\nsol = solve(prob,saveat=0.1)\nplot(sol)\n\ndataset = Array(sol)\nscatter!(sol.t,dataset')\n\ntmp_prob = remake(prob, p=[1.2,0.8,2.5,0.8])\ntmp_sol = solve(tmp_prob)\nplot(tmp_sol)\nscatter!(sol.t,dataset')\n\nfunction loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if tmp_sol.retcode == :Success\n    return sum(abs2,Array(tmp_sol) - dataset)\n  else\n    return Inf\n  end\nend\n\n\npinit = [1.2,0.8,2.5,0.8]\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, pinit)\nres = Optimization.solve(optprob,ADAM(), maxiters = 1000)\n\n# res = Optimization.solve(optprob,BFGS(), maxiters = 1000) ### errors!\n\n#try Newton method of optimization\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), Optimization.AutoForwardDiff())\noptprob = Optimization.OptimizationProblem(optf, pinit)\nres = Optimization.solve(optprob,Newton(), maxiters=1000)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"You might notice that AutoZygote (default) fails for the above Optimization.solve call with Optim's optimizers which happens because of Zygote's behaviour for zero gradients in which case it returns nothing. To avoid such issue you can just use a different version of the same check which compares the size of the obtained  solution and the data we have, shown below, which is easier to AD.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/divergence/","page":"Handling Divergent and Unstable Trajectories","title":"Handling Divergent and Unstable Trajectories","text":"function loss(p)\n  tmp_prob = remake(prob, p=p)\n  tmp_sol = solve(tmp_prob,Tsit5(),saveat=0.1)\n  if size(tmp_sol) == size(dataset)\n    return sum(abs2,Array(tmp_sol) .- dataset)\n  else\n    return Inf\n  end\nend","category":"page"},{"location":"modules/ParameterizedFunctions/#ParameterizedFunctions.jl:-Simple-High-Level-ODE-Definitions","page":"Home","title":"ParameterizedFunctions.jl: Simple High Level ODE Definitions","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/","page":"Home","title":"Home","text":"ParameterizedFunctions.jl is a component of the SciML ecosystem which allows for easily defining parameterized ODE models in a simple syntax.","category":"page"},{"location":"modules/ParameterizedFunctions/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/","page":"Home","title":"Home","text":"To install ParameterizedFunctions.jl, use the Julia package manager:","category":"page"},{"location":"modules/ParameterizedFunctions/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"ParameterizedFunctions\")","category":"page"},{"location":"modules/ParameterizedFunctions/#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/","page":"Home","title":"Home","text":"using DifferentialEquations, ParameterizedFunctions\n\n# Non-Stiff ODE\n\nlotka_volterra = @ode_def begin\n  d🐁  = α*🐁  - β*🐁*🐈\n  d🐈 = -γ*🐈 + δ*🐁*🐈\nend α β γ δ\n\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(lotka_volterra,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5(),reltol=1e-6,abstol=1e-6)\n\n# Stiff ODE\n\nrober = @ode_def begin\n  dy₁ = -k₁*y₁+k₃*y₂*y₃\n  dy₂ =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  dy₃ =  k₂*y₂^2\nend k₁ k₂ k₃\n\nprob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),[0.04,3e7,1e4])\nsol = solve(prob)","category":"page"},{"location":"modules/ParameterizedFunctions/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"using PolyChaos","category":"page"},{"location":"modules/PolyChaos/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"PolyChaos is a collection of numerical routines for orthogonal polynomials written in the Julia programming language. Starting from some non-negative weight (aka an absolutely continuous nonnegative measure), PolyChaos allows","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"to compute the coefficients for the monic three-term recurrence relation,\nto evaluate the orthogonal polynomials at arbitrary points,\nto compute the quadrature rule,\nto compute tensors of scalar products,\nto do all of the above in a multivariate setting (aka product measures).","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"If the weight function is a probability density function, PolyChaos further provides routines to compute polynomial chaos expansions (PCEs) of random variables with this very density function. These routines allow","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"to compute affine PCE coefficients for arbitrary densities,\nto compute moments,\nto compute the tensors of scalar products.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"PolyChaos contains several canonical orthogonal polynomials such as Jacobi or Hermite polynomials. For these, closed-form expressions and state-of-the art quadrature rules are used whenever possible. However, a cornerstone principle of PolyChaos is to provide all the functionality for user-specific, arbitrary weights.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"note: Note\nWhat PolyChaos is not (at least currently):a self-contained introduction to orthogonal polynomials, quadrature rules and/or polynomial chaos expansions. We assume the user brings some experience to the table. However, over time we will focus on strengthening the tutorial charater of the package.\na symbolic toolbox\na replacement for FastGaussQuadrature.jl","category":"page"},{"location":"modules/PolyChaos/#Installation","page":"Overview","title":"Installation","text":"","category":"section"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"The package requires Julia 1.3 or newer. In Julia switch to the package manager","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"julia> ]\n(v1.03 pkg> add PolyChaos","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"This will install PolyChaos and its dependencies. Once that is done, load the package:","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"julia> using PolyChaos","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"That's it.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Let's take a look at a simple example. We would like to solve the integral","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"int_0^1 6 x^5 mathrmdx","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Exploiting the underlying uniform measure, the integration can be done exactly with a 3-point quadrature rule.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"julia> using PolyChaos\n\njulia> opq = Uniform01OrthoPoly(3, addQuadrature = true)\nUniform01OrthoPoly{Array{Float64,1},Uniform01Measure,Quad{Float64,Array{Float64,1}}}(3, [0.5, 0.5, 0.5, 0.5], [1.0, 0.08333333333333333, 0.06666666666666667, 0.06428571428571428], Uniform01Measure(PolyChaos.w_uniform01, (0.0, 1.0), true), Quad{Float64,Array{Float64,1}}(\"golubwelsch\", 3, [0.11270166537925838, 0.49999999999999994, 0.8872983346207417], [0.2777777777777777, 0.4444444444444444, 0.27777777777777757]))\n\njulia> integrate(x -> 6x^5, opq)\n0.9999999999999993\n\njulia> show(opq)\n\nUnivariate orthogonal polynomials\ndegree:         3\n#coeffs:        4\nα =             [0.5, 0.5, 0.5, 0.5]\nβ =             [1.0, 0.08333333333333333, 0.06666666666666667, 0.06428571428571428]\n\nMeasure dλ(t)=w(t)dt\nw:      w_uniform01\ndom:    (0.0, 1.0)\nsymmetric:      true","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"To get going with PolyChaos check out the tutorials such as the one on numerical integration. In case you are unfamiliar with orthogonal polynomials, perhaps this background information is of help.","category":"page"},{"location":"modules/PolyChaos/#References","page":"Overview","title":"References","text":"","category":"section"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"The code base of PolyChaos is partially based on Walter Gautschi's Matlab suite of programs for generating orthogonal polynomials and related quadrature rules, with much of the theory presented in his book Orthogonal Polynomials: Computation and Approximation published in 2004 by the Oxford University Press.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"For the theory of polynomial chaos expansion we mainly consulted T. J. Sullivan. Introduction to Uncertainty Quantification. Springer International Publishing Switzerland. 2015.","category":"page"},{"location":"modules/PolyChaos/#Contributing","page":"Overview","title":"Contributing","text":"","category":"section"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"We are always looking for contributors. If you are interested, just get in touch: tillmann [dot] muehlpfordt [at] kit [dot] edu.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Or just fork and/or star the repository:","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Julia's package manager works nicely with Github: simply install the hosted package via Pkg.clone and the repository's URL. A fork is created with","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Pkg.clone(\"https://github.com/timueh/PolyChaos.jl\")","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"The fork will replace the original package.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Call","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Pkg.dir(\"PolyChaos\")","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"to figure out where the package was cloned to. Go to that location and figure out what branch you are on via git branch.","category":"page"},{"location":"modules/PolyChaos/#Citing","page":"Overview","title":"Citing","text":"","category":"section"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"If you found the software useful and applied it to your own research, we'd appreciate a citation. Add the following to your BibTeX file","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"@ARTICLE{2020arXiv200403970M,\n       author = {{M{\\\"u}hlpfordt}, Tillmann and {Zahn}, Frederik and {Hagenmeyer}, Veit and\n         {Faulwasser}, Timm},\n        title = \"{PolyChaos.jl -- A Julia Package for Polynomial Chaos in Systems and Control}\",\n      journal = {arXiv e-prints},\n     keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},\n         year = 2020,\n        month = apr,\n          eid = {arXiv:2004.03970},\n        pages = {arXiv:2004.03970},\narchivePrefix = {arXiv},\n       eprint = {2004.03970},\n}","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Of course you are more than welcome to partake in GitHub's gamification: starring and forking is much appreciated.","category":"page"},{"location":"modules/PolyChaos/","page":"Overview","title":"Overview","text":"Enjoy.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Using-Catalyst","page":"Using Catalyst","title":"Using Catalyst","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"In this tutorial we provide an introduction to using Catalyst to specify chemical reaction networks, and then to solve ODE, jump, and SDE models generated from them. At the end we show what mathematical rate laws and transition rate functions (i.e. intensities or propensities) are generated by Catalyst for ODE, SDE and jump process models.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Let's start by using the Catalyst @reaction_network macro to specify a simple chemical reaction network: the well-known repressilator.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We first import the basic packages we'll need:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"# If not already installed, first hit \"]\" within a Julia REPL. Then type:\n# add Catalyst DifferentialEquations Plots Latexify\n\nusing Catalyst, DifferentialEquations, Plots, Latexify","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We now construct the reaction network. The basic types of arrows and predefined rate laws one can use are discussed in detail within the tutorial, The Reaction DSL. Here, we use a mix of first order, zero order, and repressive Hill function rate laws. Note, varnothing corresponds to the empty state, and is used for zeroth order production and first order degradation reactions:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"repressilator = @reaction_network Repressilator begin\n    hillr(P₃,α,K,n), ∅ --> m₁\n    hillr(P₁,α,K,n), ∅ --> m₂\n    hillr(P₂,α,K,n), ∅ --> m₃\n    (δ,γ), m₁ <--> ∅\n    (δ,γ), m₂ <--> ∅\n    (δ,γ), m₃ <--> ∅\n    β, m₁ --> m₁ + P₁\n    β, m₂ --> m₂ + P₂\n    β, m₃ --> m₃ + P₃\n    μ, P₁ --> ∅\n    μ, P₂ --> ∅\n    μ, P₃ --> ∅\nend α K n δ γ β μ","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"which gives","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Model Repressilator with 15 equations\nStates (6):\n  m₁(t)\n  m₂(t)\n  m₃(t)\n  P₁(t)\n  P₂(t)\n  P₃(t)\nParameters (7):\n  α\n  K\n  n\n  δ\n  γ\n  β\n  μ","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"showing that we've created a new network model named Repressilator with the listed chemical species and states. @reaction_network returns a ReactionSystem, which we saved in the repressilator variable. It can be converted to a variety of other mathematical models represented as ModelingToolkit.AbstractSystems, or analyzed in various ways using the Catalyst.jl API. For example, to see the chemical species, parameters, and reactions we can use","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"species(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"which gives","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"6-element Array{Term{Real},1}:\n m₁(t)\n m₂(t)\n m₃(t)\n P₁(t)\n P₂(t)\n P₃(t)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"parameters(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"which gives","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"7-element Array{Sym{ModelingToolkit.Parameter{Real}},1}:\n α\n K\n n\n δ\n γ\n β\n μ","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"and","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"reactions(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"which gives","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"15-element Vector{Reaction}:\n Catalyst.hillr(P₃(t), α, K, n), ∅ --> m₁\n Catalyst.hillr(P₁(t), α, K, n), ∅ --> m₂\n Catalyst.hillr(P₂(t), α, K, n), ∅ --> m₃\n δ, m₁ --> ∅\n γ, ∅ --> m₁\n δ, m₂ --> ∅\n γ, ∅ --> m₂\n δ, m₃ --> ∅\n γ, ∅ --> m₃\n β, m₁ --> m₁ + P₁\n β, m₂ --> m₂ + P₂\n β, m₃ --> m₃ + P₃\n μ, P₁ --> ∅\n μ, P₂ --> ∅\n μ, P₃ --> ∅","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We can also use Latexify to see the corresponding reactions, which shows what the hillr terms correspond to mathematically","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"latexify(repressilator, starred=true)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"beginalign*\nrequiremhchem\nce varnothing -fracalpha K^nK^n + P_3^n m_1\nce varnothing -fracalpha K^nK^n + P_1^n m_2\nce varnothing -fracalpha K^nK^n + P_2^n m_3\nce m_1 =deltagamma varnothing\nce m_2 =deltagamma varnothing\nce m_3 =deltagamma varnothing\nce m_1 -beta m_1 + P_1\nce m_2 -beta m_2 + P_2\nce m_3 -beta m_3 + P_3\nce P_1 -mu varnothing\nce P_2 -mu varnothing\nce P_3 -mu varnothing\nendalign*","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Assuming Graphviz is installed and commandline accessible, within a Jupyter notebook we can also graph the reaction network by","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"g = Graph(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"giving","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"(Image: Repressilator solution)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"The network graph shows a variety of information, representing each species as a blue node, and each reaction as an orange dot. Black arrows from species to reactions indicate reactants, and are labelled with their input stoichiometry. Similarly, black arrows from reactions to species indicate products, and are labelled with their output stoichiometry. In contrast, red arrows from a species to reactions indicate the species is used within the reactions' rate expressions. For the repressilator, the reactions","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"hillr(P₃,α,K,n), ∅ --> m₁\nhillr(P₁,α,K,n), ∅ --> m₂\nhillr(P₂,α,K,n), ∅ --> m₃","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"have rates that depend on the proteins, and hence lead to red arrows from each Pᵢ.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Note, from the REPL or scripts one can always use savegraph to save the graph (assuming Graphviz is installed).","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Mass-Action-ODE-Models","page":"Using Catalyst","title":"Mass Action ODE Models","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Let's now use our ReactionSystem to generate and solve a corresponding mass action ODE model. We first convert the system to a ModelingToolkit.ODESystem by","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"odesys = convert(ODESystem, repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We can once again use Latexify to look at the corresponding ODE model","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"latexify(odesys)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"beginaligned\nfracdm_1(t)dt = fracalpha K^nK^n + left( mathrmP_3left( t right) right)^n - delta mathrmm_1left( t right) + gamma \nfracdm_2(t)dt = fracalpha K^nK^n + left( mathrmP_1left( t right) right)^n - delta mathrmm_2left( t right) + gamma \nfracdm_3(t)dt = fracalpha K^nK^n + left( mathrmP_2left( t right) right)^n - delta mathrmm_3left( t right) + gamma \nfracdP_1(t)dt = beta mathrmm_1left( t right) - mu mathrmP_1left( t right) \nfracdP_2(t)dt = beta mathrmm_2left( t right) - mu mathrmP_2left( t right) \nfracdP_3(t)dt = beta mathrmm_3left( t right) - mu mathrmP_3left( t right)\nendaligned","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"(Note, there is currently a Latexify bug that causes different fonts to be used for the species symbols on each side of the equations.)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Before we can solve the ODEs, we need to specify the values of the parameters in the model, the initial condition, and the time interval to solve the model on. To do this we need to build mappings from the symbolic parameters and the species to the corresponding numerical values for parameters and initial conditions. We can build such mappings in several ways. One is to use Julia Symbols to specify the values like","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"pmap  = (:α => .5, :K => 40, :n => 2, :δ => log(2)/120,\n         :γ => 5e-3, :β => log(2)/6, :μ => log(2)/60)\nu₀map = [:m₁ => 0., :m₂ => 0., :m₃ => 0., :P₁ => 20., :P₂ => 0., :P₃ => 0.]","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Alternatively, we can use ModelingToolkit symbolic variables to specify these mappings like","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"@parameters  α K n δ γ β μ\n@variables t m₁(t) m₂(t) m₃(t) P₁(t) P₂(t) P₃(t)\npmap  = (α => .5, K => 40, n => 2, δ => log(2)/120,\n         γ => 5e-3, β => 20*log(2)/120, μ => log(2)/60)\nu₀map = [m₁ => 0., m₂ => 0., m₃ => 0., P₁ => 20., P₂ => 0., P₃ => 0.]\n","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Knowing these mappings we can set up the ODEProblem we want to solve:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"# time interval to solve on\ntspan = (0., 10000.)\n\n# create the ODEProblem we want to solve\noprob = ODEProblem(repressilator, u₀map, tspan, pmap)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Note, by passing repressilator directly to the ODEProblem, Catalyst has to (internally) call convert(ODESystem, repressilator) again to generate the symbolic ODEs. We could instead pass odesys directly like","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"oprob2 = ODEProblem(odesys, u₀map, tspan, pmap)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"oprob and oprob2 are functionally equivalent, each representing the same underlying problem.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"At this point we are all set to solve the ODEs. We can now use any ODE solver from within the DifferentialEquations.jl package. We'll use the recommended default explicit solver, Tsit5(), and then plot the solutions:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"sol = solve(oprob, Tsit5(), saveat=10.)\nplot(sol)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"(Image: Repressilator ODE Solutions)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We see the well-known oscillatory behavior of the repressilator! For more on the choices of ODE solvers, see the DifferentialEquations.jl documentation.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Stochastic-Simulation-Algorithms-(SSAs)-for-Stochastic-Chemical-Kinetics","page":"Using Catalyst","title":"Stochastic Simulation Algorithms (SSAs) for Stochastic Chemical Kinetics","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Let's now look at a stochastic chemical kinetics model of the repressilator, modeling it with jump processes. Here, we will construct a JumpProcesses JumpProblem that uses Gillespie's Direct method, and then solve it to generate one realization of the jump process:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"# redefine the initial condition to be integer valued\nu₀map = [:m₁ => 0, :m₂ => 0, :m₃ => 0, :P₁ => 20, :P₂ => 0, :P₃ => 0]\n\n# next we create a discrete problem to encode that our species are integer valued:\ndprob = DiscreteProblem(repressilator, u₀map, tspan, pmap)\n\n# now, we create a JumpProblem, and specify Gillespie's Direct Method as the solver:\njprob = JumpProblem(repressilator, dprob, Direct(), save_positions=(false,false))\n\n# now, let's solve and plot the jump process:\nsol = solve(jprob, SSAStepper(), saveat=10.)\nplot(sol)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"(Image: Repressilator SSA Solutions)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We see that oscillations remain, but become much noisier. Note, in constructing the JumpProblem we could have used any of the SSAs that are part of JumpProcesses instead of the Direct method, see the list of SSAs (i.e., constant rate jump aggregators) in the documentation.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"Common questions that arise in using the JumpProcesses SSAs (i.e. Gillespie methods) are collated in the JumpProcesses FAQ.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Chemical-Langevin-Equation-(CLE)-Stochastic-Differential-Equation-(SDE)-Models","page":"Using Catalyst","title":"Chemical Langevin Equation (CLE) Stochastic Differential Equation (SDE) Models","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"At an intermediate physical scale between macroscopic ODE models and microscopic stochastic chemical kinetics models lies the CLE, given by a system of SDEs that add to each ODE above a noise term. As the repressilator has species that get very close to zero in size, it is not a good candidate to model with the CLE (where solutions can then go negative and become unphysical). Let's create a simpler reaction network for a birth-death process that will stay non-negative:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"bdp = @reaction_network begin\n  c₁, X --> 2X\n  c₂, X --> 0\n  c₃, 0 --> X\nend c₁ c₂ c₃\np = (:c₁ => 1.0, :c₂ => 2.0, :c₃ => 50.)\nu₀ = [:X => 5.]\ntspan = (0.,4.)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"The corresponding Chemical Langevin Equation SDE is then","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"dX(t) = left( c_1 Xleft( t right) - c_2 Xleft( t right) + c_3 right) dt + sqrtc_1 X(t) dW_1(t) - sqrtc_2 X(t) dW_2(t) + sqrtc_3 dW_3(t)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"where each W_i(t) denotes an independent Brownian Motion. We can solve the CLE model by creating an SDEProblem and solving it similarly to what we did for ODEs above:","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"# SDEProblem for CLE\nsprob = SDEProblem(bdp, u₀, tspan, p)\n\n# solve and plot, tstops is used to specify enough points\n# that the plot looks well-resolved\nsol = solve(sprob, LambaEM(), tstops=range(0., step=4e-3, length=1001))\nplot(sol)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"(Image: CLE Solution)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"We again have complete freedom to select any of the StochasticDiffEq.jl SDE solvers, see the documentation.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Reaction-rate-laws-used-in-simulations","page":"Using Catalyst","title":"Reaction rate laws used in simulations","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"In generating mathematical models from a ReactionSystem, reaction rates are treated as microscopic rates. That is, for a general mass action reaction of the form n_1 S_1 + n_2 S_2 + dots n_M S_M to dots with stoichiometric substrate coefficients n_i_i=1^M and rate constant k, the corresponding ODE and SDE rate laws are taken to be","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"k prod_i=1^M frac(S_i)^n_in_i","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"while the jump process transition rate (i.e., the propensity or intensity function) is","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"k prod_i=1^M fracS_i (S_i-1) dots (S_i-n_i+1)n_i","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"For example, the rate law of the reaction 2X + 3Y to Z with rate constant k would be","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"k fracX^22 fracY^33 ","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"giving the ODE model","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"beginalign*\nfracdXdt =  -2 k fracX^22 fracY^33 \nfracdYdt =  -3 k fracX^22 fracY^33 \nfracdZdt = k fracX^22 fracY^33\nendalign*","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"This implicit rescaling of rate constants can be disabled through explicit conversion of a ReactionSystem to another system via Base.convert using the combinatoric_ratelaws=false keyword argument, i.e.","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"rn = @reaction_network ...\nconvert(ODESystem, rn; combinatoric_ratelaws=false)","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"For the previous example using this keyword argument would give the rate law","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"k X^2 Y^3","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"and the ODE model","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"beginalign*\nfracdXdt =  -2 k X^2 Y^3 \nfracdYdt =  -3 k X^2 Y^3 \nfracdZdt = k X^2 Y^3\nendalign*","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"","category":"page"},{"location":"modules/Catalyst/tutorials/using_catalyst/#Notes","page":"Using Catalyst","title":"Notes","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/using_catalyst/","page":"Using Catalyst","title":"Using Catalyst","text":"For each of the preceding models we converted the ReactionSystem to, i.e., ODEs, jumps, or SDEs, we had two paths for conversion:\na. Convert to the corresponding ModelingToolkit system and then use it in     creating the corresponding problem.\nb. Directly create the desired problem type from the ReactionSystem.\nThe latter is more convenient, however, the former will be more efficient if one needs to repeatedly create the associated Problem.\nModelingToolkit offers many options for optimizing the generated ODEs and SDEs, including options to build functions for evaluating Jacobians and/or multithreaded versions of derivative evaluation functions. See the options for ODEProblems and SDEProblems.","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/#Linear-parabolic-system-of-PDEs","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"We can use NeuralPDE to solve the linear parabolic system of PDEs:","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nfracpartial upartial t = a * fracpartial^2 upartial x^2 + b_1 u + c_1 w \nfracpartial wpartial t = a * fracpartial^2 wpartial x^2 + b_2 u + c_2 w \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with initial and boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nu(0 x) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot cos(fracxa) \nw(0 x) = 0 \nu(t 0) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t  w(t 0) = frace^lambda_1-e^lambda_2lambda_1 - lambda_2 \nu(t 1) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t * cos(fracxa) \nw(t 1) = frace^lambda_1 cos(fracxa)-e^lambda_2cos(fracxa)lambda_1 - lambda_2\nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with a physics-informed neural network.","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDxx = Differential(x)^2\nDt = Differential(t)\n\n# Constants\na  = 1\nb1 = 4\nb2 = 2\nc1 = 3\nc2 = 1\nλ1 = (b1 + c2 + sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\nλ2 = (b1 + c2 - sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\n\n# Analytic solution\nθ(t, x) = exp(-t) * cos(x / a)\nu_analytic(t, x) = (b1 - λ2) / (b2 * (λ1 - λ2)) * exp(λ1 * t) * θ(t, x) - (b1 - λ1) / (b2 * (λ1 - λ2)) * exp(λ2 * t) * θ(t, x)\nw_analytic(t, x) = 1 / (λ1 - λ2) * (exp(λ1 * t) * θ(t, x) - exp(λ2 * t) * θ(t, x))\n\n# Second-order constant-coefficient linear parabolic system\neqs = [Dt(u(x, t)) ~ a * Dxx(u(x, t)) + b1 * u(x, t) + c1 * w(x, t),\n       Dt(w(x, t)) ~ a * Dxx(w(x, t)) + b2 * u(x, t) + c2 * w(x, t)]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n       w(0, x) ~ w_analytic(0, x),\n       u(t, 0) ~ u_analytic(t, 0),\n       w(t, 0) ~ w_analytic(t, 0),\n       u(t, 1) ~ u_analytic(t, 1),\n       w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           t ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t,x], [u(t,x),w(t,x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=5000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u,:w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t,x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real  = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict  = [[phi[i]([t,x], minimizers_[i])[1] for t in ts  for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(ts, xs, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"modules/NeuralPDE/examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"(Image: linear_parabolic_sol_u1) (Image: linear_parabolic_sol_u2)","category":"page"},{"location":"modules/MethodOfLines/tutorials/heat/#heat","page":"Solving the Heat Equation","title":"Solving the Heat Equation","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/heat/","page":"Solving the Heat Equation","title":"Solving the Heat Equation","text":"In this tutorial we will use the symbolic interface to solve the heat equation.","category":"page"},{"location":"modules/MethodOfLines/tutorials/heat/#Dirichlet-boundary-conditions","page":"Solving the Heat Equation","title":"Dirichlet boundary conditions","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/heat/","page":"Solving the Heat Equation","title":"Solving the Heat Equation","text":"using OrdinaryDiffEq, ModelingToolkit, MethodOfLines, DomainSets\n# Method of Manufactured Solutions: exact solution\nu_exact = (x,t) -> exp.(-t) * cos.(x)\n\n# Parameters, variables, and derivatives\n@parameters t x\n@variables u(..)\nDt = Differential(t)\nDxx = Differential(x)^2\n\n# 1D PDE and boundary conditions\neq  = Dt(u(t, x)) ~ Dxx(u(t, x))\nbcs = [u(0, x) ~ cos(x),\n        u(t, 0) ~ exp(-t),\n        u(t, 1) ~ exp(-t) * cos(1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\n# PDE system\n@named pdesys = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\n\n# Method of lines discretization\ndx = 0.1\norder = 2\ndiscretization = MOLFiniteDifference([x => dx], t)\n\n# Convert the PDE problem into an ODE problem\nprob = discretize(pdesys,discretization)\n\n# Solve ODE problem\nusing OrdinaryDiffEq\nsol = solve(prob, Tsit5(), saveat=0.2)\n\n# Plot results and compare with exact solution\ngrid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\ndiscrete_t = sol[t]\n\nusing Plots\nplt = plot()\n\nfor i in 1:length(discrete_t)\n    plot!(discrete_x, map(d -> sol[d][i], grid[u(t, x)]), label=\"Numerical, t=$(discrete_t[i])\")\n    scatter!(discrete_x, u_exact(x, discrete_t[i]), label=\"Exact, t=$(discrete_t[i])\")\nend\ndisplay(plt)\nsavefig(\"plot.png\")","category":"page"},{"location":"modules/MethodOfLines/tutorials/heat/#Neumann-boundary-conditions","page":"Solving the Heat Equation","title":"Neumann boundary conditions","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/heat/","page":"Solving the Heat Equation","title":"Solving the Heat Equation","text":"using OrdinaryDiffEq, ModelingToolkit, MethodOfLines, DomainSets\n# Method of Manufactured Solutions: exact solution\nu_exact = (x,t) -> exp.(-t) * cos.(x)\n\n# Parameters, variables, and derivatives\n@parameters t x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n# 1D PDE and boundary conditions\neq  = Dt(u(t, x)) ~ Dxx(u(t, x))\nbcs = [u(0, x) ~ cos(x),\n        Dx(u(t, 0)) ~ 0.0,\n        Dx(u(t, 1)) ~ -exp(-t) * sin(1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n        x ∈ Interval(0.0, 1.0)]\n\n# PDE system\n@named pdesys = PDESystem(eq, bcs, domains,[t, x],[u(t, x)])\n\n# Method of lines discretization\n# Need a small dx here for accuracy\ndx = 0.01\norder = 2\ndiscretization = MOLFiniteDifference([x => dx],t)\n\n# Convert the PDE problem into an ODE problem\nprob = discretize(pdesys, discretization)\n\n# Solve ODE problem\nusing OrdinaryDiffEq\nsol = solve(prob, Tsit5(), saveat=0.2)\n\n# Plot results and compare with exact solution\ngrid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\ndiscrete_t = sol[t]\n\nusing Plots\nplt = plot()\n\nfor i in 1:length(discrete_t)\n    plot!(discrete_x, map(d -> sol[d][i], grid[u(t, x)]), label=\"Numerical, t=$(discrete_t[i])\")\n    scatter!(discrete_x, u_exact(x, discrete_t[i]), label=\"Exact, t=$(discrete_t[i])\")\nend\ndisplay(plt)\nsavefig(\"plot.png\")","category":"page"},{"location":"modules/MethodOfLines/tutorials/heat/#Robin-boundary-conditions","page":"Solving the Heat Equation","title":"Robin boundary conditions","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/heat/","page":"Solving the Heat Equation","title":"Solving the Heat Equation","text":"using ModelingToolkit, MethodOfLines, DomainSets, OrdinaryDiffEq\n# Method of Manufactured Solutions\nu_exact = (x,t) -> exp.(-t) * sin.(x)\n\n# Parameters, variables, and derivatives\n@parameters t x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n# 1D PDE and boundary conditions\neq  = Dt(u(t, x)) ~ Dxx(u(t, x))\nbcs = [u(0, x) ~ sin(x),\n        u(t, -1.0) + 3Dx(u(t, -1.0)) ~ exp(-t) * (sin(-1.0) + 3cos(-1.0)),\n        u(t, 1.0) + Dx(u(t, 1.0)) ~ exp(-t) * (sin(1.0) + cos(1.0))]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n        x ∈ Interval(-1.0, 1.0)]\n\n# PDE system\n@named pdesys = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\n\n# Method of lines discretization\n# Need a small dx here for accuracy\ndx = 0.05\norder = 2\ndiscretization = MOLFiniteDifference([x => dx], t)\n\n# Convert the PDE problem into an ODE problem\nprob = discretize(pdesys, discretization)\n\n# Solve ODE problem\nusing OrdinaryDiffEq\nsol = solve(prob, Tsit5(), saveat=0.2)\n\n# Plot results and compare with exact solution\ngrid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\ndiscrete_t = sol[t]\n\nusing Plots\nplt = plot()\n\nfor i in 1:length(discrete_t)\n    plot!(discrete_x, map(d -> sol[d][i], grid[u(t,x)]), label=\"Numerical, t=$(discrete_t[i])\")\n    scatter!(discrete_x, u_exact(x, discrete_t[i]), label=\"Exact, t=$(discrete_t[i])\")\nend\ndisplay(plt)\nsavefig(\"plot.png\")","category":"page"},{"location":"modules/LabelledArrays/Note_labelled_slices/#Note:-Labelled-slices","page":"Note: Labelled slices","title":"Note: Labelled slices","text":"","category":"section"},{"location":"modules/LabelledArrays/Note_labelled_slices/","page":"Note: Labelled slices","title":"Note: Labelled slices","text":"This functionality has been removed from LabelledArrays.jl, but can  replicated with the same compile-time performance and indexing syntax  using DimensionalData.jl.","category":"page"},{"location":"modules/PolyChaos/functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"note: Note\nThe core interface of all essential functions are not dependent on specialized types such as AbstractOrthoPoly. Having said that, for exactly those essential functions there exist overloaded functions that accept specialized types such as AbstractOrthoPoly as arguments.Too abstract? For example, the function evaluate that evaluates a polynomial of degree n at points x has the core interface    evaluate(n::Int,x::Array{<:Real},a::Vector{<:Real},b::Vector{<:Real})where a and b are the vectors of recurrence coefficients. For simplicity, there also exists the interface    evaluate(n::Int64,x::Vector{<:Real},op::AbstractOrthoPoly)So fret not upon the encounter of multiply-dispatched versions of the same thing. It's there to simplify your life.The idea of this approach is to make it simpler for others to copy and paste code snippets and use them in their own work.","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"List of all functions in PolyChaos.","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"","category":"page"},{"location":"modules/PolyChaos/functions/#Recurrence-Coefficients-for-Monic-Orthogonal-Polynomials","page":"Functions","title":"Recurrence Coefficients for Monic Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"The functions below provide analytic expressions for the recurrence coefficients of common orthogonal polynomials. All of these provide monic orthogonal polynomials relative to the weights.","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"note: Note\nThe number N of recurrence coefficients has to be positive for all functions below.","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"r_scale\nrm_compute\nrm_logistic\nrm_hermite\nrm_hermite_prob\nrm_laguerre\nrm_legendre\nrm_legendre01\nrm_jacobi\nrm_jacobi01\nrm_meixner_pollaczek\nstieltjes\nlanczos\nmcdiscretization","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.r_scale","page":"Functions","title":"PolyChaos.r_scale","text":"r_scale(c::Real,β::AbstractVector{<:Real},α::AbstractVector{<:Real})\n\nGiven the recursion coefficients (α,β) for a system of orthogonal polynomials that are orthogonal with respect to some positive weight m(t), this function returns the recursion coefficients (α_,β_) for the scaled measure c m(t) for some positive c.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_compute","page":"Functions","title":"PolyChaos.rm_compute","text":"rm_compute(weight::Function,lb::Real,ub::Real,Npoly::Int=4,Nquad::Int=10;quadrature::Function=clenshaw_curtis)\n\nGiven a positive weight function with domain (lb,ub), i.e. a function w lb ub  rightarrow mathbbR_geq 0, this function creates Npoly recursion coefficients (α,β).\n\nThe keyword quadrature specifies what quadrature rule is being used.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_logistic","page":"Functions","title":"PolyChaos.rm_logistic","text":"rm_logistic(N::Int)\n\nCreates N recurrence coefficients for monic polynomials that are orthogonal on (-inftyinfty) relative to w(t) = fracmathrme^-t(1 - mathrme^-t)^2\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_hermite","page":"Functions","title":"PolyChaos.rm_hermite","text":"rm_hermite(N::Int,mu::Real)\nrm_hermite(N::Int)\n\nCreates N recurrence coefficients for monic generalized Hermite polynomials that are orthogonal on (-inftyinfty) relative to w(t) = t^2 mu mathrme^-t^2\n\nThe call rm_hermite(N) is the same as rm_hermite(N,0).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_hermite_prob","page":"Functions","title":"PolyChaos.rm_hermite_prob","text":"rm_hermite_prob(N::Int)\n\nCreates N recurrence coefficients for monic probabilists' Hermite polynomials that are orthogonal on (-inftyinfty) relative to w(t) = mathrme^-05t^2\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_laguerre","page":"Functions","title":"PolyChaos.rm_laguerre","text":"rm_laguerre(N::Int,a::Real)\nrm_laguerre(N::Int)\n\nCreates N recurrence coefficients for monic generalized Laguerre polynomials that are orthogonal on (0infty) relative to w(t) = t^a mathrme^-t.\n\nThe call rm_laguerre(N) is the same as rm_laguerre(N,0).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_legendre","page":"Functions","title":"PolyChaos.rm_legendre","text":"rm_legendre(N::Int)\n\nCreates N recurrence coefficients for monic Legendre polynomials that are orthogonal on (-11) relative to w(t) = 1.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_legendre01","page":"Functions","title":"PolyChaos.rm_legendre01","text":"rm_legendre01(N::Int)\n\nCreates N recurrence coefficients for monic Legendre polynomials that are orthogonal on (01) relative to w(t) = 1.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_jacobi","page":"Functions","title":"PolyChaos.rm_jacobi","text":"rm_jacobi(N::Int,a::Real,b::Real)\nrm_jacobi(N::Int,a::Real)\nrm_jacobi(N::Int)\n\nCreates N recurrence coefficients for monic Jacobi polynomials that are orthogonal on (-11) relative to w(t) = (1-t)^a (1+t)^b.\n\nThe call rm_jacobi(N,a) is the same as rm_jacobi(N,a,a) and rm_jacobi(N) the same as rm_jacobi(N,0,0).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_jacobi01","page":"Functions","title":"PolyChaos.rm_jacobi01","text":"rm_jacobi01(N::Int,a::Real,b::Real)\nrm_jacobi01(N::Int,a::Real)\nrm_jacobi01(N::Int)\n\nCreates N recurrence coefficients for monic Jacobi polynomials that are orthogonal on (01) relative to w(t) = (1-t)^a t^b.\n\nThe call rm_jacobi01(N,a) is the same as rm_jacobi01(N,a,a) and rm_jacobi01(N) the same as rm_jacobi01(N,0,0).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.rm_meixner_pollaczek","page":"Functions","title":"PolyChaos.rm_meixner_pollaczek","text":"rm_meixner_pollaczek(N::Int,lambda::Real,phi::Real)\nrm_meixner_pollaczek(N::Int,lambda::Real)\n\nCreates N recurrence coefficients for monic  Meixner-Pollaczek polynomials with parameters λ and ϕ. These are orthogonal on  -inftyinfty relative to the weight function w(t)=(2 pi)^-1 exp(2 phi-pi)t Gamma(lambda+ i t)^2.\n\nThe call rm_meixner_pollaczek(n,lambda) is the same as rm_meixner_pollaczek(n,lambda,pi/2).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.stieltjes","page":"Functions","title":"PolyChaos.stieltjes","text":"stieltjes(N::Int,nodes_::AbstractVector{<:Real},weights_::AbstractVector{<:Real};removezeroweights::Bool=true)\n\nStieltjes procedure–-Given the nodes and weights the function generates the firstN recurrence coefficients of the corresponding discrete orthogonal polynomials.\n\nSet the Boolean removezeroweights to true if zero weights should be removed.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.lanczos","page":"Functions","title":"PolyChaos.lanczos","text":"lanczos(N::Int,nodes::AbstractVector{<:Real},weights::AbstractVector{<:Real};removezeroweights::Bool=true)\n\nLanczos procedure–-given the nodes and weights the function generates the first N recurrence coefficients of the corresponding discrete orthogonal polynomials.\n\nSet the Boolean removezeroweights to true if zero weights should be removed.\n\nThe script is adapted from the routine RKPW in W.B. Gragg and W.J. Harrod, The numerically stable reconstruction of Jacobi matrices from spectral data, Numer. Math. 44 (1984), 317-335.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.mcdiscretization","page":"Functions","title":"PolyChaos.mcdiscretization","text":"mcdiscretization(N::Int,quads::Vector{},discretemeasure::AbstractMatrix{<:Real}=zeros(0,2);discretization::Function=stieltjes,Nmax::Integer=300,ε::Float64=1e-8,gaussquad::Bool=false)\n\nThis routine returns N recurrence coefficients of the polynomials that are orthogonal relative to a weight function w that is decomposed as a sum of m weights w_i with domains a_ib_i for i=1dotsm,\n\nw(t) = sum_i^m w_i(t) quad textwith  operatornamedom(w_i) = a_i b_i\n\nFor each weight w_i and its domain a_i b_i the function mcdiscretization() expects a quadrature rule of the form     nodes::AbstractVector{<:Real}, weights::AbstractVector{<:Real} = myquadi(N::Int) all of which are stacked in the parameter quad     quad = [ myquad1, ..., myquadm ] If the weight function has a discrete part (specified by discretemeasure) it is added on to the discretized continuous weight function.\n\nThe function mcdiscretization() performs a sequence of discretizations of the given weight w(t), each discretization being followed by an application of the Stieltjes or Lanczos procedure (keyword discretization in [stieltjes, lanczos]) to produce approximations to the desired recurrence coefficients. The function applies to each subinterval i an N-point quadrature rule (the ith entry of quad) to discretize the weight function w_i on that subinterval. If the procedure converges to within a prescribed accuracy ε before N reaches its maximum allowed value Nmax. If the function does not converge, the function prompts an error message.\n\nThe keyword gaussquad should be set to true if Gauss quadrature rules are available for all m weights w_i(t) with i = 1 dots m.\n\nFor further information, please see W. Gautschi \"Orthogonal Polynomials: Approximation and Computation\", Section 2.2.4.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Show-Orthogonal-Polynomials","page":"Functions","title":"Show Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"To get a human-readable output of the orthognoal polynomials there is the function showpoly","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"showpoly","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.showpoly","page":"Functions","title":"PolyChaos.showpoly","text":"showpoly(coeffs::Vector{<:Real};sym::String,digits::Integer)\n\nShow the monic polynomial with coefficients coeffs in a human readable way. They keyword sym sets the name of the variable, and digits controls the number of shown digits.\n\njulia> using PolyChaos\n\njulia> showpoly([1.2, 2.3, 3.4456])\nx^3 + 3.45x^2 + 2.3x + 1.2\njulia> showpoly([1.2, 2.3, 3.4456], sym=\"t\", digits=2)\nt^3 + 3.45t^2 + 2.3t + 1.2\n\nshowpoly(d::Integer,α::Vector{<:Real},β::Vector{<:Real}; sym::String,digits::Integer)\nshowpoly(d::Range,α::Vector{<:Real},β::Vector{<:Real};sym::String,digits::Integer) where Range <: OrdinalRange\n\nShow the monic polynomial of degree/range d that has the recurrence coefficients α, β.\n\njulia> using PolyChaos\n\njulia> α, β = rm_hermite(10)\n([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.77245, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5])\njulia> showpoly(3,α,β)\nx^3 - 1.5x\n\njulia> showpoly(0:2:10,α,β)\n1\nx^2 - 0.5\nx^4 - 3.0x^2 + 0.75\nx^6 - 7.5x^4 + 11.25x^2 - 1.88\nx^8 - 14.0x^6 + 52.5x^4 - 52.5x^2 + 6.56\nx^10 - 22.5x^8 + 157.5x^6 - 393.75x^4 + 295.31x^2 - 29.53\n\nTailored to types from PolyChaos.jl\n\nshowpoly(d::Union{Integer,Range},op::AbstractOrthoPoly;sym::String,digits::Integer) where Range <: OrdinalRange\n\nShow the monic polynomial of degree/range d of an AbstractOrthoPoly.\n\njulia> using PolyChaos\n\njulia> op = GaussOrthoPoly(5);\n\njulia> showpoly(3,op)\nx^3 - 3.0x\n\njulia> showpoly(0:op.deg,op; sym=\"t\")\n1\nt\nt^2 - 1.0\nt^3 - 3.0t\nt^4 - 6.0t^2 + 3.0\nt^5 - 10.0t^3 + 15.0t\n\nThanks @pfitzseb for providing this functionality.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"In case you want to see the entire basis, just use showbasis","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"showbasis","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.showbasis","page":"Functions","title":"PolyChaos.showbasis","text":"showbasis(α::Vector{<:Real},β::Vector{<:Real};sym::String,digits::Integer)\n\nShow all basis polynomials given the recurrence coefficients α, β. They keyword sym sets the name of the variable, and digits controls the number of shown digits.\n\njulia> using PolyChaos\n\njulia> α, β = rm_hermite(5);\n\njulia> showbasis(α,β)\n1\nx\nx^2 - 0.5\nx^3 - 1.5x\nx^4 - 3.0x^2 + 0.75\nx^5 - 5.0x^3 + 3.75x\n\nTailored to types from PolyChaos.jl\n\nshowbasis(op::AbstractOrthoPoly;sym::String,digits::Integer)\n\nShow all basis polynomials of an AbstractOrthoPoly.\n\njulia> using PolyChaos\n\njulia> op = LegendreOrthoPoly(4);\n\njulia> showbasis(op)\n1\nx\nx^2 - 0.33\nx^3 - 0.6x\nx^4 - 0.86x^2 + 0.09\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"Both of these functions make excessive use of","category":"page"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"rec2coeff","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.rec2coeff","page":"Functions","title":"PolyChaos.rec2coeff","text":"rec2coeff(deg::Int,a::Vector{<:Real},b::Vector{<:Real})\nrec2coeff(a,b) = rec2coeff(length(a),a,b)\n\nGet the coefficients of the orthogonal polynomial of degree up to deg specified by its recurrence coefficients (a,b). The function returns the values c_i^(k) from\n\np_k (t) = t^d + sum_i=0^k-1 c_i t^i\n\nwhere k runs from 1 to deg.\n\nThe call rec2coeff(a,b) outputs all possible recurrence coefficients given (a,b).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Evaluate-Orthogonal-Polynomials","page":"Functions","title":"Evaluate Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"evaluate","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.evaluate","page":"Functions","title":"PolyChaos.evaluate","text":"Univariate\n\nevaluate(n::Int,x::Array{<:Real},a::AbstractVector{<:Real},b::AbstractVector{<:Real})\nevaluate(n::Int,x::Real,a::AbstractVector{<:Real},b::AbstractVector{<:Real})\nevaluate(n::Int,x::AbstractVector{<:Real},op::AbstractOrthoPoly)\nevaluate(n::Int,x::Real,op::AbstractOrthoPoly)\n\nEvaluate the n-th univariate basis polynomial at point(s) x The function is multiply dispatched to facilitate its use with the composite type AbstractOrthoPoly\n\nIf several basis polynomials (stored in ns) are to be evaluated at points x, then call\n\nevaluate(ns::AbstractVector{<:Int},x::AbstractVector{<:Real},op::AbstractOrthoPoly) = evaluate(ns,x,op.α,op.β)\nevaluate(ns::AbstractVector{<:Int},x::Real,op::AbstractOrthoPoly) = evaluate(ns,[x],op)\n\nIf all basis polynomials are to be evaluated at points x, then call\n\nevaluate(x::AbstractVector{<:Real},op::AbstractOrthoPoly) = evaluate(collect(0:op.deg),x,op)\nevaluate(x::Real,op::AbstractOrthoPoly) = evaluate([x],op)\n\nwhich returns an Array of dimensions (length(x),op.deg+1).\n\nnote: Note\nn is the degree of the univariate basis polynomial\nlength(x) = N, where N is the number of points\n(a,b) are the recursion coefficients\n\nMultivariate\n\nevaluate(n::AbstractVector{<:Int},x::AbstractMatrix{<:Real},a::Vector{<:AbstractVector{<:Real}},b::Vector{<:AbstractVector{<:Real}})\nevaluate(n::AbstractVector{<:Int},x::AbstractVector{<:Real},a::Vector{<:AbstractVector{<:Real}},b::Vector{<:AbstractVector{<:Real}})\nevaluate(n::AbstractVector{<:Int},x::AbstractMatrix{<:Real},op::MultiOrthoPoly)\nevaluate(n::AbstractVector{<:Int},x::AbstractVector{<:Real},op::MultiOrthoPoly)\n\nEvaluate the n-th p-variate basis polynomial at point(s) x The function is multiply dispatched to facilitate its use with the composite type MultiOrthoPoly\n\nIf several basis polynomials are to be evaluated at points x, then call\n\nevaluate(ind::AbstractMatrix{<:Int},x::AbstractMatrix{<:Real},a::Vector{<:AbstractVector{<:Real}},b::Vector{<:AbstractVector{<:Real}})\nevaluate(ind::AbstractMatrix{<:Int},x::AbstractMatrix{<:Real},op::MultiOrthoPoly)\n\nwhere ind is a matrix of multi-indices.\n\nIf all basis polynomials are to be evaluated at points x, then call\n\nevaluate(x::AbstractMatrix{<:Real},mop::MultiOrthoPoly) = evaluate(mop.ind,x,mop)\n\nwhich returns an array of dimensions (mop.dim,size(x,1)).\n\nnote: Note\nn is a multi-index\nlength(n) == p, i.e. a p-variate basis polynomial\nsize(x) = (N,p), where N is the number of points\nsize(a)==size(b)=p.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Scalar-Products","page":"Functions","title":"Scalar Products","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"computeSP2\ncomputeSP","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.computeSP2","page":"Functions","title":"PolyChaos.computeSP2","text":"computeSP2(n::Int,β::AbstractVector{<:Real})\ncomputeSP2(n::Int,op::AbstractOrthoPoly) = computeSP2(n,op.β)\ncomputeSP2(op::AbstractOrthoPoly) = computeSP2(op.deg,op.β)\n\nComputes the n regular scalar products aka 2-norms of the orthogonal polynomials, namely\n\nϕ_i^2 = langle phi_iphi_irangle quad forall i in  0dotsn \n\nNotice that only the values of β of the recurrence coefficients (α,β) are required. The computation is based on equation (1.3.7) from Gautschi, W. \"Orthogonal Polynomials: Computation and Approximation\". Whenever there exists an analytic expressions for β, this function should be used.\n\nThe function is multiply dispatched to facilitate its use with AbstractOrthoPoly.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.computeSP","page":"Functions","title":"PolyChaos.computeSP","text":"Univariate\n\ncomputeSP(a::AbstractVector{<:Int},α::AbstractVector{<:Real},β::AbstractVector{<:Real},nodes::AbstractVector{<:Real},weights::AbstractVector{<:Real};issymmetric::Bool=false)\ncomputeSP(a::AbstractVector{<:Int},op::AbstractOrthoPoly;issymmetric=issymmetric(op))\n\nMultivariate\n\ncomputeSP( a::AbstractVector{<:Int},\n           α::AbstractVector{<:AbstractVector{<:Real}},β::AbstractVector{<:AbstractVector{<:Real}},\n           nodes::AbstractVector{<:AbstractVector{<:Real}},weights::AbstractVector{<:AbstractVector{<:Real}},\n           ind::Matrix{<:Int};\n           issymmetric::BitArray=falses(length(α)))\ncomputeSP(a::AbstractVector{<:Int},op::AbstractVector,ind::Matrix{<:Int})\ncomputeSP(a::AbstractVector{<:Int},mOP::MultiOrthoPoly)\n\nComputes the scalar product\n\nlangle phi_a_1phi_a_2cdotsphi_a_n rangle\n\nwhere n = length(a). This requires to provide the recurrence coefficients (α,β) and the quadrature rule (nodes,weights), as well as the multi-index ind. If provided via the keyword issymmetric, symmetry of the weight function is exploited. All computations of the multivariate scalar products resort back to efficient computations of the univariate scalar products. Mathematically, this follows from Fubini's theorem.\n\nThe function is dispatched to facilitate its use with AbstractOrthoPoly and its quadrature rule Quad.\n\nnote: Note\nZero entries of a are removed automatically to simplify computations, which follows fromlangle phi_i phi_j phi_0cdotsphi_0 rangle = langle phi_i phi_j ranglebecause \\phi_0 = 1.It is checked whether enough quadrature points are supplied to solve the integral exactly.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Quadrature-Rules","page":"Functions","title":"Quadrature Rules","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"fejer\nfejer2\nclenshaw_curtis\nquadgp\ngauss\nradau\nlobatto","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.fejer","page":"Functions","title":"PolyChaos.fejer","text":"fejer(N::Int)\n\nFejer's first quadrature rule.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.fejer2","page":"Functions","title":"PolyChaos.fejer2","text":"fejer2(n::Int)\n\nFejer's second quadrature rule according to Waldvogel, J. Bit Numer Math (2006) 46: 195.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.clenshaw_curtis","page":"Functions","title":"PolyChaos.clenshaw_curtis","text":"clenshaw_curtis(n::Int)\n\nClenshaw-Curtis quadrature according to Waldvogel, J. Bit Numer Math (2006) 46: 195.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.quadgp","page":"Functions","title":"PolyChaos.quadgp","text":"quadgp(weight::Function,lb::Real,ub::Real,N::Int=10;quadrature::Function=clenshaw_curtis,bnd::Float64=Inf)\n\ngeneral purpose quadrature based on Gautschi, \"Orthogonal Polynomials: Computation and Approximation\", Section 2.2.2, pp. 93-95\n\nCompute the N-point quadrature rule for weight with support (lb, ub). The quadrature rule can be specified by the keyword quadrature. The keyword bnd sets the numerical value for infinity.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.gauss","page":"Functions","title":"PolyChaos.gauss","text":"gauss(N::Int,α::AbstractVector{<:Real},β::AbstractVector{<:Real})\ngauss(α::AbstractVector{<:Real},β::AbstractVector{<:Real})\ngauss(N::Int,op::Union{OrthoPoly,AbstractCanonicalOrthoPoly})\ngauss(op::Union{OrthoPoly,AbstractCanonicalOrthoPoly})\n\nGauss quadrature rule, also known as Golub-Welsch algorithm\n\ngauss() generates the N Gauss quadrature nodes and weights for a given weight function. The weight function is represented by the N recurrence coefficients for the monic polynomials orthogonal with respect to the weight function.\n\nnote: Note\nThe function gauss accepts at most N = length(α) - 1 quadrature points,     hence providing at most an (length(α) - 1)-point quadrature rule.\n\nnote: Note\nIf no N is provided, then N = length(α) - 1.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.radau","page":"Functions","title":"PolyChaos.radau","text":"radau(N::Int,α::AbstractVector{<:Real},β::AbstractVector{<:Real},end0::Real)\nradau(α::AbstractVector{<:Real},β::AbstractVector{<:Real},end0::Real)\nradau(N::Int,op::Union{OrthoPoly,AbstractCanonicalOrthoPoly},end0::Real)\nradau(op::Union{OrthoPoly,AbstractCanonicalOrthoPoly},end0::Real)\n\nGauss-Radau quadrature rule. Given a weight function encoded by the recurrence coefficients (α,β)for the associated orthogonal polynomials, the function generates the nodes and weights (N+1)-point Gauss-Radau quadrature rule for the weight function having a prescribed node end0 (typically at one of the end points of the support interval of w, or outside thereof).\n\nnote: Note\nThe function radau accepts at most N = length(α) - 2 as an input,     hence providing at most an (length(α) - 1)-point quadrature rule.\n\nnote: Note\nReference: OPQ: A MATLAB SUITE OF PROGRAMS FOR GENERATING ORTHOGONAL POLYNOMIALS AND RELATED QUADRATURE RULES by Walter Gautschi\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.lobatto","page":"Functions","title":"PolyChaos.lobatto","text":"lobatto(N::Int,α::AbstractVector{<:Real},β::AbstractVector{<:Real},endl::Real,endr::Real)\nlobatto(α::AbstractVector{<:Real},β::AbstractVector{<:Real},endl::Real,endr::Real)\nlobatto(N::Int,op::Union{OrthoPoly,AbstractCanonicalOrthoPoly},endl::Real,endr::Real)\nlobatto(op::Union{OrthoPoly,AbstractCanonicalOrthoPoly},endl::Real,endr::Real)\n\nGauss-Lobatto quadrature rule. Given a weight function encoded by the recurrence coefficients for the associated orthogonal polynomials, the function generates the nodes and weights of the (N+2)-point Gauss-Lobatto quadrature rule for the weight function, having two prescribed nodes endl, endr (typically the left and right end points of the support interval, or points to the left resp. to the right thereof).\n\nnote: Note\nThe function radau accepts at most N = length(α) - 3 as an input, hence providing at most an (length(α) - 1)-point quadrature rule.\n\nnote: Note\nReference: OPQ: A MATLAB SUITE OF PROGRAMS FOR GENERATING ORTHOGONAL POLYNOMIALS AND RELATED QUADRATURE RULES by Walter Gautschi\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Polynomial-Chaos","page":"Functions","title":"Polynomial Chaos","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"mean\nvar\nstd\nsampleMeasure\nevaluatePCE\nsamplePCE\ncalculateAffinePCE\nconvert2affinePCE","category":"page"},{"location":"modules/PolyChaos/functions/#Statistics.mean","page":"Functions","title":"Statistics.mean","text":"Univariate\n\nmean(x::AbstractVector,op::AbstractOrthoPoly)\n\nMultivariate\n\nmean(x::AbstractVector,mop::MultiOrthoPoly)\n\ncompute mean of random variable with PCE x\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Statistics.var","page":"Functions","title":"Statistics.var","text":"Univariate\n\nvar(x::AbstractVector,op::AbstractOrthoPoly)\nvar(x::AbstractVector,t2::Tensor)\n\nMultivariate\n\nvar(x::AbstractVector,mop::MultiOrthoPoly)\nvar(x::AbstractVector,t2::Tensor)\n\ncompute variance of random variable with PCE x\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Statistics.std","page":"Functions","title":"Statistics.std","text":"Univariate\n\nstd(x::AbstractVector,op::AbstractOrthoPoly)\n\nMultivariate\n\nstd(x::AbstractVector,mop::MultiOrthoPoly)\n\ncompute standard deviation of random variable with PCE x\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.sampleMeasure","page":"Functions","title":"PolyChaos.sampleMeasure","text":"Univariate\n\nsampleMeasure(n::Int,name::String,w::Function,dom::Tuple{<:Real,<:Real},symm::Bool,d::Dict;method::String=\"adaptiverejection\")\nsampleMeasure(n::Int,m::Measure;method::String=\"adaptiverejection\")\nsampleMeasure(n::Int,op::AbstractOrthoPoly;method::String=\"adaptiverejection\")\n\nDraw n samples from the measure m described by its\n\nname\nweight function w,\ndomain dom,\nsymmetry property symm,\nand, if applicable, parameters stored in the dictionary d.\n\nBy default an adaptive rejection sampling method is used (from AdaptiveRejectionSampling.jl), unless it is a common random variable for which Distributions.jl is used.\n\nThe function is dispatched to accept AbstractOrthoPoly.\n\nMultivariate\n\nsampleMeasure(n::Int,m::ProductMeasure;method::Vector{String}=[\"adaptiverejection\" for i=1:length(m.name)])\nsampleMeasure(n::Int,mop::MultiOrthoPoly;method::Vector{String}=[\"adaptiverejection\" for i=1:length(mop.meas.name)])\n\nMultivariate extension which provides array of samples with n rows and as many columns as the multimeasure has univariate measures.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.evaluatePCE","page":"Functions","title":"PolyChaos.evaluatePCE","text":"evaluatePCE(x::AbstractVector{<:Real},ξ::AbstractVector{<:Real},α::AbstractVector{<:Real},β::AbstractVector{<:Real})\n\nEvaluation of polynomial chaos expansion\n\nmathsfx = sum_i=0^L x_i phi_ixi_j\n\nwhere L+1 = length(x) and x_j is the jth sample where j=1dotsm with m = length(ξ).\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.samplePCE","page":"Functions","title":"PolyChaos.samplePCE","text":"Univariate\n\nsamplePCE(n::Int,x::AbstractVector{<:Real},op::AbstractOrthoPoly;method::String=\"adaptiverejection\")\n\nCombines sampleMeasure and evaluatePCE, i.e. it first draws n samples from the measure, then evaluates the PCE for those samples.\n\nMultivariate\n\nsamplePCE(n::Int,x::AbstractVector{<:Real},mop::MultiOrthoPoly;method::Vector{String}=[\"adaptiverejection\" for i=1:length(mop.meas.name)])\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.calculateAffinePCE","page":"Functions","title":"PolyChaos.calculateAffinePCE","text":"calculateAffinePCE(α::AbstractVector{<:Real})\n\nComputes the affine PCE coefficients x_0 and x_1 from recurrence coefficients lpha.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.convert2affinePCE","page":"Functions","title":"PolyChaos.convert2affinePCE","text":"convert2affinePCE(mu::Real, sigma::Real, op::AbstractCanonicalOrthoPoly; kind::String)\n\nComputes the affine PCE coefficients x_0 and x_1 from\n\nX = a_1 + a_2 Xi = x_0 + x_1 phi_1(Xi)\n\nwhere phi_1(t) = t-alpha_0 is the first-order monic basis polynomial.\n\nWorks for subtypes of AbstractCanonicalOrthoPoly. The keyword kind in [\"lbub\", \"μσ\"] specifies whether p1 and p2 have the meaning of lower/upper bounds or mean/standard deviation.\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#Auxiliary-Functions","page":"Functions","title":"Auxiliary Functions","text":"","category":"section"},{"location":"modules/PolyChaos/functions/","page":"Functions","title":"Functions","text":"nw\ncoeffs\nintegrate\nPolyChaos.issymmetric","category":"page"},{"location":"modules/PolyChaos/functions/#PolyChaos.nw","page":"Functions","title":"PolyChaos.nw","text":"nw(q::EmptyQuad)\nnw(q::AbstractQuad)\nnw(opq::AbstractOrthoPoly)\nnw(opq::AbstractVector)\nnw(mop::MultiOrthoPoly)\n\nreturns nodes and weights in matrix form\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#PolyChaos.coeffs","page":"Functions","title":"PolyChaos.coeffs","text":"coeffs(op::AbstractOrthoPoly)\ncoeffs(op::AbstractVector)\ncoeffs(mop::MultiOrthoPoly)\n\nreturns recurrence coefficients of in matrix form\n\n\n\n\n\n","category":"function"},{"location":"modules/PolyChaos/functions/#LinearAlgebra.issymmetric","page":"Functions","title":"LinearAlgebra.issymmetric","text":"issymmetric(m::AbstractMeasure)\nissymmetric(op::AbstractOrthoPoly)\n\nIs the measure symmetric (around any point in the domain)?\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLOperators/tutorials/fftw/#Wrap-a-Fourier-transform-with-SciMLOperators","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"","category":"section"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"In this tutorial, we will wrap a Fast Fourier Transform (FFT) in a SciMLOperator via the FunctionOperator interface. FFTs are commonly used algorithms for performing numerical interpolation and differentiation. In this example, we will use the FFT to compute the derivative of a function.","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/#Copy-Paste-Code","page":"Wrap a Fourier transform with SciMLOperators","title":"Copy-Paste Code","text":"","category":"section"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"using SciMLOperators\nusing LinearAlgebra, FFTW\n\nL  = 2π\nn  = 256\ndx = L / n\nx  = range(start=-L/2, stop=L/2-dx, length=n) |> Array\n\nu  = @. sin(5x)cos(7x);\ndu = @. 5cos(5x)cos(7x) - 7sin(5x)sin(7x);\n\ntransform = plan_rfft(x)\nk = Array(rfftfreq(n, 2π*n/L))\n\nop_transform = FunctionOperator(\n                                (du,u,p,t) -> mul!(du, transform, u);\n                                isinplace=true,\n                                T=ComplexF64,\n                                size=(length(k),n),\n\n                                input_prototype=x,\n                                output_prototype=im*k,\n\n                                op_inverse = (du,u,p,t) -> ldiv!(du, transform, u)\n                               )\n\nik = im * DiagonalOperator(k)\nDx = op_transform \\ ik * op_transform\n\nDx = cache_operator(Dx, x)\n\n@show ≈(Dx * u, du; atol=1e-8)\n@show ≈(mul!(copy(u), Dx, u), du; atol=1e-8)","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/#Explanation","page":"Wrap a Fourier transform with SciMLOperators","title":"Explanation","text":"","category":"section"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"We load SciMLOperators, LinearAlgebra, and FFTW (short for Fastest Fourier Transform in the West), a common Fourier transform library. Next, we define an equispaced grid from -π to π, and write the function u that we intend to differentiate. Since this is a trivial example, we already know the derivative, du and write it down to later test our FFT wrapper.","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"using SciMLOperators\nusing LinearAlgebra, FFTW\n\nL  = 2π\nn  = 256\ndx = L / n\nx  = range(start=-L/2, stop=L/2-dx, length=n) |> Array\n\nu  = @. sin(5x)cos(7x);\ndu = @. 5cos(5x)cos(7x) - 7sin(5x)sin(7x);\n","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"Now, we define the Fourier transform. Since our input is purely Real, we use the real Fast Fourier Transform. The funciton plan_rfft outputs a real fast fourier transform object that can be applied to inputs that are like x as follows: xhat = transform * x, and LinearAlgebra.mul!(xhat, transform, x).  We also get k, the frequency modes sampled by our finite grid, via the function rfftfreq.","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"transform = plan_rfft(x)\nk = Array(rfftfreq(n, 2π*n/L))","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"Now we are ready to define our wrapper for the FFT object. To FunctionOperator, we pass the in-place forward application of the transform, (du,u,p,t) -> mul!(du, transform, u), its inverse application, (du,u,p,t) -> ldiv!(du, transform, u), as well as input and output prototype vectors. We also set the flag isinplace to true to signal that we intend to use the operator in a non-allocating way, and pass in the element-type and size of the operator.","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"op_transform = FunctionOperator(\n                                (du,u,p,t) -> mul!(du, transform, u);\n                                isinplace=true,\n                                T=ComplexF64,\n                                size=(length(k),n),\n\n                                input_prototype=x,\n                                output_prototype=im*k,\n\n                                op_inverse = (du,u,p,t) -> ldiv!(du, transform, u)\n                               )","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"After wrapping the FFT with FunctionOperator, we are ready to compose it with other SciMLOperators. Below we form the derivative operator, and cache it via the function cache_operator that requires an input prototype. We can test our derivative operator both in-place, and out-of-place by comparing its output to the analytical derivative.","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"ik = im * DiagonalOperator(k)\nDx = op_transform \\ ik * op_transform\n\n@show ≈(Dx * u, du; atol=1e-8)\n@show ≈(mul!(copy(u), Dx, u), du; atol=1e-8)","category":"page"},{"location":"modules/SciMLOperators/tutorials/fftw/","page":"Wrap a Fourier transform with SciMLOperators","title":"Wrap a Fourier transform with SciMLOperators","text":"≈(Dx * u, du; atol = 1.0e-8) = true\n≈(mul!(copy(u), Dx, u), du; atol = 1.0e-8) = true","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#mnist","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Training a classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"(Step-by-step description below)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs)\n\t# Use MLDataUtils LabelEnc for natural onehot conversion\n  \tonehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n\t# Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n\t# Process images into (H,W,C,BS) batches\n\tx_train = Float32.(reshape(imgs,size(imgs,1),size(imgs,2),1,size(imgs,3))) |> gpu\n\tx_train = batchview(x_train,batchsize)\n\t# Onehot and batch the labels\n\ty_train = onehot(labels_raw) |> gpu\n\ty_train = batchview(y_train,batchsize)\n\treturn x_train, y_train\nend\n\n# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)\n\ndown = Flux.Chain(Flux.flatten, Flux.Dense(784, 20, tanh)) |> gpu\n\nnn = Flux.Chain(Flux.Dense(20, 10, tanh),\n           Flux.Dense(10, 10, tanh),\n           Flux.Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Flux.Chain(Flux.Dense(20, 10)) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend\n\n# Build our overall model topology\nmodel = Flux.Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncallback() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\n# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = callback)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Step-by-Step-Description","page":"GPU-based MNIST Neural ODE Classifier","title":"Step-by-Step Description","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Load-Packages","page":"GPU-based MNIST Neural ODE Classifier","title":"Load Packages","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using DiffEqFlux, DifferentialEquations, NNlib, MLDataUtils, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#GPU","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"A good trick used here:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"GPU-based MNIST Neural ODE Classifier","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function loadmnist(batchsize = bs)\n\t# Use MLDataUtils LabelEnc for natural onehot conversion\n  \tonehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw, LabelEnc.NativeLabels(collect(0:9)))\n\t# Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n\t# Process images into (H,W,C,BS) batches\n\tx_train = Float32.(reshape(imgs,size(imgs,1),size(imgs,2),1,size(imgs,3))) |> gpu\n\tx_train = batchview(x_train,batchsize)\n\t# Onehot and batch the labels\n\ty_train = onehot(labels_raw) |> gpu\n\ty_train = batchview(y_train,batchsize)\n\treturn x_train, y_train\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"and then loaded from main:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Main\nconst bs = 128\nx_train, y_train = loadmnist(bs)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Layers","page":"GPU-based MNIST Neural ODE Classifier","title":"Layers","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down = Flux.Chain(Flux.flatten, Flux.Dense(784, 20, tanh)) |> gpu\n\nnn = Flux.Chain(Flux.Dense(20, 10, tanh),\n           Flux.Dense(10, 10, tanh),\n           Flux.Dense(10, 20, tanh)) |> gpu\n\n\nnn_ode = NeuralODE(nn, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfc  = Flux.Chain(Flux.Dense(20, 10)) |> gpu","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"down: This layer downsamples our images into a 20 dimensional feature vector.         It takes a 28 x 28 image, flattens it, and then passes it through a fully connected         layer with tanh activation","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn: A 3 layers Deep Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"nn_ode: ODE solver layer","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"fc: The final fully connected layer which maps our learned feature vector to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"|> gpu: An utility function which transfers our model to GPU, if it is available","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Array-Conversion","page":"GPU-based MNIST Neural ODE Classifier","title":"Array Conversion","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"When using NeuralODE, this function converts the ODESolution's DiffEqArray to a Matrix (CuArray), and reduces the matrix from 3 to 2 dimensions for use in the next layer.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return reshape(xarr, size(xarr)[1:2])\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) to Array(x).","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Build-Topology","page":"GPU-based MNIST Neural ODE Classifier","title":"Build Topology","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Build our overall model topology\nmodel = Flux.Chain(down,\n              nn_ode,\n              DiffEqArray_to_Array,\n              fc) |> gpu;","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Flux.Chain(down,\n                 nn,\n                 fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Prediction","page":"GPU-based MNIST Neural ODE Classifier","title":"Prediction","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Accuracy","page":"GPU-based MNIST Neural ODE Classifier","title":"Accuracy","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(collect(data))\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(m, train_dataloader)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Training-Parameters","page":"GPU-based MNIST Neural ODE Classifier","title":"Training Parameters","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Loss","page":"GPU-based MNIST Neural ODE Classifier","title":"Loss","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Optimizer","page":"GPU-based MNIST Neural ODE Classifier","title":"Optimizer","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"opt = ADAM(0.05)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#CallBack","page":"GPU-based MNIST Neural ODE Classifier","title":"CallBack","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"callback() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Train","page":"GPU-based MNIST Neural ODE Classifier","title":"Train","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params( down, nn_ode.p, fc), zip( x_train, y_train ), opt, callback = callback)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/#Expected-Output","page":"GPU-based MNIST Neural ODE Classifier","title":"Expected Output","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_neural_ode/","page":"GPU-based MNIST Neural ODE Classifier","title":"GPU-based MNIST Neural ODE Classifier","text":"Iter:   1 || Train Accuracy: 16.203 || Test Accuracy: 16.933\nIter:  11 || Train Accuracy: 64.406 || Test Accuracy: 64.900\nIter:  21 || Train Accuracy: 76.656 || Test Accuracy: 76.667\nIter:  31 || Train Accuracy: 81.758 || Test Accuracy: 81.683\nIter:  41 || Train Accuracy: 81.078 || Test Accuracy: 81.967\nIter:  51 || Train Accuracy: 83.953 || Test Accuracy: 84.417\nIter:  61 || Train Accuracy: 85.266 || Test Accuracy: 85.017\nIter:  71 || Train Accuracy: 85.938 || Test Accuracy: 86.400\nIter:  81 || Train Accuracy: 84.836 || Test Accuracy: 85.533\nIter:  91 || Train Accuracy: 86.148 || Test Accuracy: 86.583\nIter: 101 || Train Accuracy: 83.859 || Test Accuracy: 84.500\nIter: 111 || Train Accuracy: 86.227 || Test Accuracy: 86.617\nIter: 121 || Train Accuracy: 87.508 || Test Accuracy: 87.200\nIter: 131 || Train Accuracy: 86.227 || Test Accuracy: 85.917\nIter: 141 || Train Accuracy: 84.453 || Test Accuracy: 84.850\nIter: 151 || Train Accuracy: 86.063 || Test Accuracy: 85.650\nIter: 161 || Train Accuracy: 88.375 || Test Accuracy: 88.033\nIter: 171 || Train Accuracy: 87.398 || Test Accuracy: 87.683\nIter: 181 || Train Accuracy: 88.070 || Test Accuracy: 88.350\nIter: 191 || Train Accuracy: 86.836 || Test Accuracy: 87.150\nIter: 201 || Train Accuracy: 89.266 || Test Accuracy: 88.583\nIter: 211 || Train Accuracy: 86.633 || Test Accuracy: 85.550\nIter: 221 || Train Accuracy: 89.313 || Test Accuracy: 88.217\nIter: 231 || Train Accuracy: 88.641 || Test Accuracy: 89.417\nIter: 241 || Train Accuracy: 88.617 || Test Accuracy: 88.550\nIter: 251 || Train Accuracy: 88.211 || Test Accuracy: 87.950\nIter: 261 || Train Accuracy: 87.742 || Test Accuracy: 87.317\nIter: 271 || Train Accuracy: 89.070 || Test Accuracy: 89.217\nIter: 281 || Train Accuracy: 89.703 || Test Accuracy: 89.067\nIter: 291 || Train Accuracy: 88.484 || Test Accuracy: 88.250\nIter: 301 || Train Accuracy: 87.898 || Test Accuracy: 88.367\nIter: 311 || Train Accuracy: 88.438 || Test Accuracy: 88.633\nIter: 321 || Train Accuracy: 88.664 || Test Accuracy: 88.567\nIter: 331 || Train Accuracy: 89.906 || Test Accuracy: 89.883\nIter: 341 || Train Accuracy: 88.883 || Test Accuracy: 88.667\nIter: 351 || Train Accuracy: 89.609 || Test Accuracy: 89.283\nIter: 361 || Train Accuracy: 89.516 || Test Accuracy: 89.117\nIter: 371 || Train Accuracy: 89.898 || Test Accuracy: 89.633\nIter: 381 || Train Accuracy: 89.055 || Test Accuracy: 89.017\nIter: 391 || Train Accuracy: 89.445 || Test Accuracy: 89.467\nIter: 401 || Train Accuracy: 89.156 || Test Accuracy: 88.250\nIter: 411 || Train Accuracy: 88.977 || Test Accuracy: 89.083\nIter: 421 || Train Accuracy: 90.109 || Test Accuracy: 89.417","category":"page"},{"location":"modules/Integrals/basics/solve/#Common-Solver-Options-(Solve-Keyword-Arguments)","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"","category":"section"},{"location":"modules/Integrals/basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"reltol: Relative tolerance\nabstol: Absolute tolerance\nmaxiters: The maximum number of iterations","category":"page"},{"location":"modules/Integrals/basics/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"Additionally, the extra keyword arguments are splatted to the library calls, so see the documentation of the integrator library for all of the extra details. These extra keyword arguments are not guaranteed to act uniformly.","category":"page"},{"location":"modules/PolyChaos/math/#MathematicalBackground","page":"Mathematical Background","title":"Mathematical Background","text":"","category":"section"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"This section is heavily based on the book \"Orthogonal Polynomials: Computation and Approximation\" by Walter Gautschi (Oxford University Press)","category":"page"},{"location":"modules/PolyChaos/math/#Orthogonal-Polynomials","page":"Mathematical Background","title":"Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/math/#Basic-Theory","page":"Mathematical Background","title":"Basic Theory","text":"","category":"section"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"We always work with absolutely continuous measures for which we write mathrmd lambda (t) = w(t) mathrmdt, where the so-called weight function w","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"is a nonnegative integrable function on the real line mathbbR, i.e. w mathcalW subseteq mathbbR rightarrow mathbbR_geq 0\nhas finite limits in case mathcalW = mathbbR, i.e.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"lim_t to pm infty w(t)  infty","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"has finite moments of all orders","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"mu_r(mathrmdlambda) = int_mathcalW t^r mathrmd lambda (t) quad r = 0 1 2 dots quad textwith mu_0  0","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"For any pair of integrable functions u v, their scalar product relative to mathrmd lambda is defined as","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"langle u v rangle_mathrmd lambda = int_mathcalW u(t) v(t) mathrmd lambda(t)","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Let mathcalP be the set of real polynomials and mathcalP_d subset mathcalP be the set of real polynomials of degree at most d on mathcalW, respectively. Monic real polynomials are real polynomials with leading coefficient equal to one, i.e. pi_k(t) = t^k + dots for k = 0 1 dots","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"The polynomials uv in mathcalP with u neq v are said to be orthogonal if","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"langle u v rangle_mathrmd lambda = int_mathcalW u(t) v(t) mathrmd lambda(t) = 0","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"The norm of u is given by","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":" u _ mathrmdlambda = sqrtlangle u u rangle","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"If the polynomials u in mathcalP has unit length  u _ mathrmdlambda = 1, it is called orthonormal.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Monic orthogonal polynomials are polynomials that are monic and orthogonal, hence satisfy","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"pi_k(t) = pi_k(t mathrmd lambda) = t^k + dots\nfor k = 0 1 dots, and\nlangle pi_k pi_l rangle_mathrmdlambda = 0\nfor k neq l and k l = 0 1 dots, and langle pi_k pi_k rangle_mathrmdlambda =  pi_k ^2_ mathrmdlambda  0 for k = 0 1 dots.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"note: Note\nThe support mathcalW of mathrmd lambda can be an interval (finite, half-finite, infinite), or a finite number of disjoint intervals. If the support consists of a finite or denumerably infinite number of distinct points t_k at which lambda has positive jumps w_k, the measure is called a discrete measure. For a finite number N of points, the discrete measure is denoted by mathrmdlambda_N, and it is characterized by its nodes and weights  t_k w_k _k=1^N according tomathrmd lambda_N (t) = sum_k=1^N w_k delta(t-t_k)where delta is the delta-function.The inner product associated with mathrmd lambda_N islangle u v rangle_mathrmdlambda_N = int_mathcalW u(t) v(t) mathrmd lambda_N (t) = sum_k=1^N w_k u(t_k) v(t_k)There exist only N orthogonal polynomials  pi_k(mathrmd lambda_N) _k=0^N-1 that are orthogonal relative to the discrete measure mathrmd lambda_N in the senselangle pi_k(mathrmd lambda_N) pi_l(mathrmd lambda_N) rangle_mathrmdlambda_N =  pi_k(mathrmd lambda_N) _mathrmd lambda_N delta_klwhere delta_kl is the Dirac-delta, for kl = 0 1 dots N-1.","category":"page"},{"location":"modules/PolyChaos/math/#Properties","page":"Mathematical Background","title":"Properties","text":"","category":"section"},{"location":"modules/PolyChaos/math/#Symmetry","page":"Mathematical Background","title":"Symmetry","text":"","category":"section"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"An absolutely continuous measure mathrmd lambda(t) = w(t) mathrmd t is symmetric (with respect to the origin) if its support is mathcalW = -aa for some 0  a leq infty, and if w(-t) = w(t) for all t in mathcalW.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Similarly, a discrete measure mathrmd lambda_N (t) = sum_k=1^N w_k delta(t-t_k) is symmetric if t_k = - t_N+1-k, and w_k = w_N+1-k for k=1 2 dots N.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Theorem 1.17 states that: If mathrmd lambda is symmetric, then","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"pi_k(-t mathrmd lambda) = (-1)^k pi_k(t mathrmd lambda) quad k=01 dots","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"hence the parity of k decides whether pi_k is even/odd.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"note: Note\nSymmetry is exploited in computeSP, where symmetry need not be relative to the origin, but some arbitrary point of the support.","category":"page"},{"location":"modules/PolyChaos/math/#Three-term-Recurrence-Relation","page":"Mathematical Background","title":"Three-term Recurrence Relation","text":"","category":"section"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"The fact that orthogonal polynomials can be represented in terms of a three-term recurrence formula is at the heart of all numerical methods of the package. The importance of the three-term recurrence relation is difficult to overestimate. It provides","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"efficient means of evaluating polynomials (and derivatives),\nzeros of orthogonal polynomials by means of a eigenvalues of a symmetric, tridiagonal matrix\nacces to quadrature rules,\nnormalization constants to create orthonormal polynomials.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Theorem 1.27 states:","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Let pi_k(cdot) = pi_k(cdot mathrmdlambda) for k = 0 1 dots be the monic orthogonal polynomials with respect to the measure mathrmd lambda. Then","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"beginaligned\npi_k+1(t) = (t - alpha_k) pi_k(t) - beta_k pi_k-1(t) quad k= 0 1 dots \npi_o(t) = 1 \npi_-1(t) = 0\nendaligned","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"where","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"beginaligned\nalpha = alpha_k(mathrmd lambda) = fraclangle t pi_k pi_k rangle_mathrmd lambdalangle pi_k pi_k rangle_mathrmd lambda  k=012 dots \nbeta = beta_k(mathrmd lambda) = fraclangle pi_k pi_k rangle_mathrmd lambdalangle pi_k-1 pi_k-1 rangle_mathrmd lambda  k=12dots\nendaligned","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Let tildepi_k(cdot) = tildepi_k(cdot mathrmd lambda t) denote the orthonormal polynomials, then","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"beginaligned\nsqrtbeta_k+1 tildepi_k(t) = (t - alpha_k) tildepi_k(t) - sqrtbeta_k tildepi_k-1(t) quad k = 0 1 dots \ntildepi_0(t) = 1 \ntildepi_-1(t) = 0\nendaligned","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"note: Note\nWithin the package, the coefficients (α,β) are the building block to represent (monic) orthogonal polynomials.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Notice that beta_0 is arbitrary. Nevertheless, it is convenient to define it as","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"beta_0(mathrmdlambda) = langle pi_0 pi_0 rangle_mathrmd lambda = int_mathcalW mathrmd lambda (t)","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"because it allows to compute the norms of the polynomials based on beta_k alone","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":" pi_n _mathrmd lambda = beta_n(mathrmd lambda) beta_n-1(mathrmd lambda) cdots beta_0(mathrmd lambda) quad n = 01 dots","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"Let the support be mathcalW = ab for 0  ab  infty, then","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"beginaligned\n a  alpha_k(mathrmd lambda)  b  k = 012 dots \n 0  beta_k(mathrmd lambda)  max(a^2 b^2)  k = 1 2 dots\nendaligned","category":"page"},{"location":"modules/PolyChaos/math/#Quadrature-Rules","page":"Mathematical Background","title":"Quadrature Rules","text":"","category":"section"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"An n-point quadrature rule for the measure mathrmd lambda t is a formula of the form","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"int_mathcalW f(t) mathrmd lambda(t) = sum_nu = 1^n w_nu f(tau_nu) + R_n(f)","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"The quadrature rule  (tau_nu w_nu) _nu=1^n composed of (mutually distinct) nodes tau_nu and weights w_nu provides an approximation to the integral. The respective error is given by R_n(f). If, for polynomials p in mathcalP_d, the error R_n(p) vanishes, the respective quadrature rule is said to have a degree of exactness d. Gauss quadrature rule are special quadrature rules that have a degree of exactness d = 2n - 1. That means, taking a n =3-point quadrature rule, polynomials up to degree 5 can be integrated exactly. The nodes and weights for the Gauss quadrature rules have some remarkable properties:","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"all Gauss nodes are mutually distinct and contained in the interior of the support of mathrmd lambda;\nthe n Gauss nodes are the zeros of pi_n, the monic orthogonal polynomial of degree n relative to the measure mathrmd lambda;\nall Gauss weights are positive.","category":"page"},{"location":"modules/PolyChaos/math/","page":"Mathematical Background","title":"Mathematical Background","text":"The Gauss nodes and weights can be computed using the Golub-Welsch algorithm. This means to solve an eigenvalue problem of a symmetric tridiagonal matrix.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Partial-Differential-Equation-(PDE)-Constrained-Optimization","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"This example uses a prediction model to optimize the one-dimensional Heat Equation. (Step-by-step description below)","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DifferentialEquations, Optimization, OptimizationPolyalgorithms, Zygote, OptimizationOptimJL\n\n# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;\n\n## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend\n\n## ODE description of the Physics:\nfunction heat(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend\n\n# Testing Solver on linear PDE\nprob = ODEProblem(heat,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")\n\nps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend\n\n## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes\n\nLOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncallback = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncallback(ps,loss(ps)...) # Testing callback function\n\n# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, ps)\nres = Optimization.solve(optprob, PolyOpt(), callback = callback)\n@show res.u # returns [0.999999999613485, 0.9999999991343996]","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Step-by-step-Description","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Step-by-step Description","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Load-Packages","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Load Packages","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"using DelimitedFiles,Plots\nusing DifferentialEquations, Optimization, OptimizationPolyalgorithms, \n      Zygote, OptimizationOptimJL","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Parameters","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Parameters","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"First, we setup the 1-dimensional space over which our equations will be evaluated. x spans from 0.0 to 10.0 in steps of 0.01; t spans from 0.00 to 0.04 in steps of 4.0e-5.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Problem setup parameters:\nLx = 10.0\nx  = 0.0:0.01:Lx\ndx = x[2] - x[1]\nNx = size(x)\n\nu0 = exp.(-(x.-3.0).^2) # I.C\n\n## Problem Parameters\np        = [1.0,1.0]    # True solution parameters\nxtrs     = [dx,Nx]      # Extra parameters\ndt       = 0.40*dx^2    # CFL condition\nt0, tMax = 0.0 ,1000*dt\ntspan    = (t0,tMax)\nt        = t0:dt:tMax;","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"In plain terms, the quantities that were defined are:","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"x (to Lx) spans the specified 1D space\ndx = distance between two points\nNx = total size of space\nu0 = initial condition\np = true solution\nxtrs = convenient grouping of dx and Nx into Array\ndt = time distance between two points\nt (t0 to tMax) spans the specified time frame\ntspan = span of t","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Auxiliary-Functions","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Auxiliary Functions","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We then define two functions to compute the derivatives numerically. The Central Difference is used in both the 1st and 2nd degree derivatives.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## Definition of Auxiliary functions\nfunction ddx(u,dx)\n    \"\"\"\n    2nd order Central difference for 1st degree derivative\n    \"\"\"\n    return [[zero(eltype(u))] ; (u[3:end] - u[1:end-2]) ./ (2.0*dx) ; [zero(eltype(u))]]\nend\n\n\nfunction d2dx(u,dx)\n    \"\"\"\n    2nd order Central difference for 2nd degree derivative\n    \"\"\"\n    return [[zero(eltype(u))]; (u[3:end] - 2.0.*u[2:end-1] + u[1:end-2]) ./ (dx^2); [zero(eltype(u))]]\nend","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Heat-Differential-Equation","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Heat Differential Equation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Next, we setup our desired set of equations in order to define our problem.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## ODE description of the Physics:\nfunction heat(u,p,t)\n    # Model parameters\n    a0, a1 = p\n    dx,Nx = xtrs #[1.0,3.0,0.125,100]\n    return 2.0*a0 .* u +  a1 .* d2dx(u, dx)\nend","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Solve-and-Plot-Ground-Truth","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Solve and Plot Ground Truth","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We then solve and plot our partial differential equation. This is the true solution which we will compare to further on.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Testing Solver on linear PDE\nprob = ODEProblem(heat,u0,tspan,p)\nsol = solve(prob,Tsit5(), dt=dt,saveat=t);\n\nplot(x, sol.u[1], lw=3, label=\"t0\", size=(800,500))\nplot!(x, sol.u[end],lw=3, ls=:dash, label=\"tMax\")","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Building-the-Prediction-Model","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Building the Prediction Model","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Now we start building our prediction model to try to obtain the values p. We make an initial guess for the parameters and name it ps here. The predict function is a non-linear transformation in one layer using solve. If unfamiliar with the concept, refer to here.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"ps  = [0.1, 0.2];   # Initial guess for model parameters\nfunction predict(θ)\n    Array(solve(prob,Tsit5(),p=θ,dt=dt,saveat=t))\nend","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Train-Parameters","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Train Parameters","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"Training our model requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Loss","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Loss","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We first make our predictions based on the current values of our parameters ps, then take the difference between the predicted solution and the truth above. For the loss, we use the Mean squared error.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"## Defining Loss function\nfunction loss(θ)\n    pred = predict(θ)\n    l = predict(θ)  - sol\n    return sum(abs2, l), pred # Mean squared error\nend\n\nl,pred   = loss(ps)\nsize(pred), size(sol), size(t) # Checking sizes","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Optimizer","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Optimizer","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The optimizers ADAM with a learning rate of 0.01 and BFGS are directly passed in training (see below)","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Callback","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Callback","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The callback function displays the loss during training. We also keep a history of the loss, the previous predictions and the previous parameters with LOSS, PRED and PARS accumulators.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"LOSS  = []                              # Loss accumulator\nPRED  = []                              # prediction accumulator\nPARS  = []                              # parameters accumulator\n\ncallback = function (θ,l,pred) #callback function to observe training\n  display(l)\n  append!(PRED, [pred])\n  append!(LOSS, l)\n  append!(PARS, [θ])\n  false\nend\n\ncallback(ps,loss(ps)...) # Testing callback function","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Plotting-Prediction-vs-Ground-Truth","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Plotting Prediction vs Ground Truth","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The scatter points plotted here are the ground truth obtained from the actual solution we solved for above. The solid line represents our prediction. The goal is for both to overlap almost perfectly when the PDE finishes its training and the loss is close to 0.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"# Let see prediction vs. Truth\nscatter(sol[:,end], label=\"Truth\", size=(800,500))\nplot!(PRED[end][:,end], lw=2, label=\"Prediction\")","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/#Train","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Train","text":"","category":"section"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"The parameters are trained using Optimization.solve and adjoint sensitivities. The resulting best parameters are stored in res and res.u returns the parameters that minimizes the cost function.","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, ps)\nres = Optimization.solve(optprob, PolyOpt(), callback = callback)\n@show res.u # returns [0.999999999613485, 0.9999999991343996]","category":"page"},{"location":"modules/SciMLSensitivity/pde_fitting/pde_constrained/","page":"Partial Differential Equation (PDE) Constrained Optimization","title":"Partial Differential Equation (PDE) Constrained Optimization","text":"We successfully predict the final ps to be equal to [0.999999999999975, 1.0000000000000213] vs the true solution of p = [1.0, 1.0]","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/#Vector-Calculus-Operators","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"A good way to represent physical vectors is by storing them as a N+1 dimensional matrix for a N-dimensional physical vector, with each last index i storing the iᵗʰ component of it at grid point specified by the indices prior to it. For e.g., u[p,q,r,2] stores the 2ⁿᵈ component at x[p], y[q], z[r]. ","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"Various operators and functions have been introduced here to carry out common calculus operations like  Gradient, Curl , square_norm etc. for them.","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/#Operators","page":"Vector Calculus Operators","title":"Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"All operators store CenteredDifference operators along various axes for computing the underlying  derivatives lazily. They differ in the way convolutions are performed.","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"Following are the constructors :","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"Gradient(approximation_order :: Int,\n        dx::Union{NTuple{N,AbstractVector},NTuple{N,T}},\n        len::NTuple{N,Int}, coeff_func=nothing)\n\nCurl(approximation_order :: Int,\n     dx::Union{NTuple{3,AbstractVector},NTuple{3,T}},\n     len::NTuple{3,Int}, coeff_func = nothing )\n\nDivergence(approximation_order :: Int,\n           dx::Union{NTuple{N,AbstractVector},NTuple{N,T}},\n           len::NTuple{N,Int}, coeff_func=nothing)","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"These can then be used as A*u, A holding our constructor and u being the input Array, either representing a multi-variable function in a N-dim Tensor, which would be compatible with Gradient or the N+1-dim Tensor representation desribed earlier, holding our physical vector compatible with Divergence and Curl.  ","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"The arguments are :","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"approximation_order : the order of the discretization in terms of O(dx^order).\ndx : tuple containing the spacing of the discretization in order of dimensions.  When dx has eltype <: Number, that would imply uniform discretization, while it also  supports eltype <: Array{Number} for non-uniform grids.\nlen: tuple storing length of the discretization in the respective directions.\ncoeff_func: An operational argument which sets the coefficients of the operator. If coeff_func is a Number, then the coefficients are set to be constant with that number. If coeff_func is an AbstractArray with length matching len, then the coefficients are constant but spatially dependent.","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/#Functions","page":"Vector Calculus Operators","title":"Functions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"Some common functions used in Vector calculus that have been made available are :","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"dot_product(A::AbstractArray{T1,N},B::AbstractArray{T2,N})\ndot_product!(u::AbstractArray{T1,N}, A::AbstractArray{T2,N2},B::AbstractArray{T3,N2})\n\ncross_product(A::AbstractArray{T1,4},B::AbstractArray{T2,4})\ncross_product!(u::AbstractArray{T1,4},A::AbstractArray{T2,4},B::AbstractArray{T3,4})\n\nsquare_norm(A::AbstractArray{T,N})\nsquare_norm!(u::AbstractArray{T1,N1},A::AbstractArray{T2,N2})","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"A and  B are N+1-dim Tensors of same sizes. The output would be a N-dim Tensor storing the corresponding value of operation at each grid point. All of these support inplace operations with ! notation as described above.","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_calculus_operators/","page":"Vector Calculus Operators","title":"Vector Calculus Operators","text":"dot_product translates to A ⋅ B, cross_product to A × B and square_norm to L2-norm in real sense.","category":"page"},{"location":"modules/DiffEqOperators/#DiffEqOperators.jl","page":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","title":"DiffEqOperators.jl","text":"","category":"section"},{"location":"modules/DiffEqOperators/","page":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","title":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","text":"DiffEqOperators.jl is a package for finite difference discretization of partial differential equations. It is for building fast lazy operators for high order non-uniform finite differences.","category":"page"},{"location":"modules/DiffEqOperators/","page":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","title":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","text":"note: Note\nFor automated finite difference discretization of symbolically-defined PDEs, see MethodOfLines.jl.","category":"page"},{"location":"modules/DiffEqOperators/","page":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","title":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","text":"warn: Warn\nThis library is not complete, especially for higher dimensional operators. Use with caution.","category":"page"},{"location":"modules/DiffEqOperators/","page":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","title":"DiffEqOperators.jl: Linear operators for Scientific Machine Learning","text":"For the operators, both centered and upwind operators are provided, for domains of any dimension, arbitrarily spaced grids, and for any order of accuracy. The cases of 1, 2, and 3 dimensions with an evenly spaced grid are optimized with a convolution routine from NNlib.jl. Care is taken to give efficiency by avoiding unnecessary allocations, using purpose-built stencil compilers, allowing GPUs and parallelism, etc. Any operator can be concretized as an Array, a BandedMatrix or a sparse matrix.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/#poisson_proc_tutorial","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"In this tutorial we show how to simulate several Poisson jump processes, for several types of intensities and jump distributions. Readers interested primarily in chemical or population process models, where several types of jumps may occur, can skip directly to the second tutorial for a tutorial covering similar material but focused on the SIR model.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"JumpProcesses allows the simulation of jump processes where the transition rate, i.e. intensity or propensity, can be a function of the current solution, current parameters, and current time. Throughout this tutorial these are denoted by u, p and t. Likewise, when a jump occurs any DifferentialEquations.jl-compatible change to the current system state, as encoded by a DifferentialEquations.jl integrator, is allowed. This includes changes to the current state or to parameter values.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"This tutorial requires several packages, which can be added if not already installed via","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using Pkg\nPkg.add(\"JumpProcesses\")\nPkg.add(\"Plots)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Let's also load our packages and set some defaults for our plot formatting","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using JumpProcesses, Plots\ndefault(; lw = 2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/#ConstantRateJumps","page":"Simple Poisson Processes in JumpProcesses","title":"ConstantRateJumps","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Our first example will be to simulate a simple Poission counting process, N(t), with a constant transition rate of λ. We can interpret this as a birth process where new individuals are created at the constant rate λ. N(t) then gives the current population size. In terms of a unit Poisson counting process, Y_b(t), we have","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"N(t) = Y_bleft( lambda t right)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"(Here by a unit Poisson counting process we just mean a Poisson counting process with a constant rate of one.)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"In the remainder of this tutorial we will use transition rate, rate, propensity, and intensity interchangeably. Here is the full program listing we will subsequently explain line by line","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using JumpProcesses, Plots\n\nrate(u,p,t) = p.λ\naffect!(integrator) = (integrator.u[1] += 1)\ncrj = ConstantRateJump(rate, affect!)\n\nu₀ = [0]\np = (λ = 2.0, )\ntspan = (0.0, 10.0)\n\ndprob = DiscreteProblem(u₀, tspan, p)\njprob = JumpProblem(dprob, Direct(), crj)\n\nsol = solve(jprob, SSAStepper())\nplot(sol, label=\"N(t)\", xlabel=\"t\", legend=:bottomright)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We can define and simulate our jump process using JumpProcesses. We first load our packages","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using JumpProcesses, Plots","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"To specify our jump process we need to define two functions. One that given the current state of the system, u, the parameters, p, and the time, t, can determine the current transition rate (intensity)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"rate(u,p,t) = p.λ","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"This corresponds to the instantaneous probability per time a jump occurs when the current state is u, current parameters are p, and the time is t. We also give a function that updates the system state when a jump is known to have occurred (at time integrator.t)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"affect!(integrator) = (integrator.u[1] += 1)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Here the convention is to take a DifferentialEquations.jl integrator, and directly modify the current solution value it stores. i.e. integrator.u is the current solution vector, with integrator.u[1] the first component of this vector. In our case we will only have one unknown, so this will be the current value of the counting process. As our jump process's transition rate is constant between jumps, we can use a ConstantRateJump to encode it","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"crj = ConstantRateJump(rate, affect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We then specify the parameters needed to simulate our jump process","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"# the initial condition vector, notice we make it an integer\n# since we have a discrete counting process\nu₀ = [0]\n\n# the parameters of the model, in this case a named tuple storing the rate, λ\np = (λ = 2.0, )\n\n# the time interval to solve over\ntspan = (0.0, 10.0)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Finally, we construct the associated SciML problem types and generate one realization of the process. We first create a DiscreteProblem to encode that we are simulating a process that evolves in discrete time steps. Note, this currently requires that the process has constant transition rates between jumps","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"dprob = DiscreteProblem(u₀, tspan, p)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We next create a JumpProblem that wraps the discrete problem, and specifies which algorithm to use for determining next jump times (and in the case of multiple possible jumps the next jump type). Here we use the classical Direct method, proposed by Gillespie in the chemical reaction context, but going back to earlier work by Doob and others (and also known as Kinetic Monte Carlo in the physics literature)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"# a jump problem, specifying we will use the Direct method to sample\n# jump times and events, and that our jump is encoded by crj\njprob = JumpProblem(dprob, Direct(), crj)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We are finally ready to simulate one realization of our jump process","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"# now we simulate the jump process in time, using the SSAStepper time-stepper\nsol = solve(jprob, SSAStepper())\n\nplot(sol, label=\"N(t)\", xlabel=\"t\", legend=:bottomright)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/#More-general-ConstantRateJumps","page":"Simple Poisson Processes in JumpProcesses","title":"More general ConstantRateJumps","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"The previous counting process could be interpreted as a birth process, where new individuals were created with a constant transition rate λ. Suppose we also allow individuals to be killed with a death rate of μ. The transition rate at time t for some individual to die, assuming the death of individuals are independent, is just mu N(t). Suppose we also wish to keep track of the number of deaths, D(t), that have occurred. We can store these as an auxiliary variable in u[2]. Our processes are then given mathematically by","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"beginalign*\nN(t) = Y_b(lambda t) - Y_d left(int_0^t mu N(s^-)  ds right) \nD(t) = Y_d left(int_0^t mu N(s^-)  ds right)\nendalign*","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"where Y_d(t) denotes a second, independent, unit Poisson counting process.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We can encode this as a second jump for our system like","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"deathrate(u,p,t) = p.μ * u[1]\ndeathaffect!(integrator) = (integrator.u[1] -= 1; integrator.u[2] += 1)\ndeathcrj = ConstantRateJump(deathrate, deathaffect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"As the death rate is constant between jumps we can encode this process as a second ConstantRateJump. We then construct the corresponding problems, passing both jumps to JumpProblem, and can solve as before","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"p = (λ = 2.0, μ = 1.5)\nu₀ = [0,0]   # (N(0), D(0))\ndprob = DiscreteProblem(u₀, tspan, p)\njprob = JumpProblem(dprob, Direct(), crj, deathcrj)\nsol = solve(jprob, SSAStepper())\nplot(sol, label=[\"N(t)\" \"D(t)\"], xlabel=\"t\", legend=:topleft)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"In the next tutorial we will also introduce MassActionJumps, which are a special type of ConstantRateJumps that require a more specialized form of transition rate and state update, but can offer better computational performance. They can encode any mass action reaction, as commonly arise in chemical and population process models, and essentially require that rate(u,p,t) be a monomial in the components of u and state changes be given by adding or subtracting a constant vector from u.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/#VariableRateJumps-for-processes-that-are-not-constant-between-jumps","page":"Simple Poisson Processes in JumpProcesses","title":"VariableRateJumps for processes that are not constant between jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"So far we have assumed that our jump processes have transition rates that are constant in between jumps. In many applications this may be a limiting assumption. To support such models JumpProcesses has the VariableRateJump type, which represents jump processes that have an arbitrary time dependence in the calculation of the transition rate, including transition rates that depend on states which can change in between ConstantRateJumps. Let's consider the previous example, but now let the birth rate be time dependent, b(t) = lambda left(sin(pi t  2) + 1right), so that our model becomes","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"beginalign*\nN(t) = Y_bleft(int_0^t left( lambda sinleft(tfracpi s2right) + 1 right)  d sright) - Y_d left(int_0^t mu N(s^-)  ds right) \nD(t) = Y_d left(int_0^t mu N(s^-)  ds right)\nendalign*","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We'll then re-encode the first jump as a VariableRateJump","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"rate1(u,p,t) = p.λ * (sin(pi*t/2) + 1)\naffect1!(integrator) = (integrator.u[1] += 1)\nvrj = VariableRateJump(rate1, affect1!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Because this new jump can modify the value of u[1] between death events, and the death transition rate depends on this value, we must also update our death jump process to also be a VariableRateJump","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"deathvrj = VariableRateJump(deathrate, deathaffect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Note, if the death rate only depended on values that were unchanged by a variable rate jump, then it could have remained a ConstantRateJump. This would have been the case if, for example, it depended on u[2] instead of u[1].","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"To simulate our jump process we now need to use a continuous problem type to properly handle determining the jump times. We do this by constructing an ordinary differential equation problem, ODEProblem, but setting the ODE derivative to preserve the state (i.e. to zero). We are essentially defining a combined ODE-jump process, i.e. a piecewise deterministic Markov process, but one where the ODE is trivial and does not change the state. To use this problem type and the ODE solvers we first load OrdinaryDiffEq.jl or DifferentialEquations.jl. If neither is installed, we first","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using Pkg\nPkg.add(\"OrdinaryDiffEq\")\n# or Pkg.add(\"DifferentialEquations\")","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"and then load it via","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"using OrdinaryDiffEq\n# or using DifferentialEquations","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We can then construct our ODE problem with a trivial ODE derivative component. Note, to work with the ODE solver time stepper we must change our initial condition to be floating point valued","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"function f!(du, u, p, t)\n    du .= 0\n    nothing\nend\nu₀ = [0.0, 0.0]\noprob = ODEProblem(f!, u₀, tspan, p)\njprob = JumpProblem(oprob, Direct(), vrj, deathvrj)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We simulate our jump process, using the Tsit5 ODE solver as the time stepper in place of SSAStepper","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"sol = solve(jprob, Tsit5())\nplot(sol, label=[\"N(t)\" \"D(t)\"], xlabel=\"t\", legend=:topleft)","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/#Having-a-Random-Jump-Distribution","page":"Simple Poisson Processes in JumpProcesses","title":"Having a Random Jump Distribution","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Suppose we want to simulate a compound Poisson process, G(t), where","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"G(t) = sum_i=1^N(t) C_i","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"with N(t) a Poisson counting process with constant transition rate lambda, and the C_i independent and identical samples from a uniform distribution over -11. We can simulate such a process as follows.","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"We first ensure that we use the same random number generator as JumpProcesses. We can either pass one as an input to JumpProblem via the rng keyword argument, and make sure it is the same one we use in our affect! function, or we can just use the default generator chosen by JumpProcesses if one is not specified, JumpProcesses.DEFAULT_RNG. Let's do the latter","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"rng = JumpProcesses.DEFAULT_RNG","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"Let's assume u[1] is N(t) and u[2] is G(t). We now proceed as in the previous examples","category":"page"},{"location":"modules/JumpProcesses/tutorials/simple_poisson_process/","page":"Simple Poisson Processes in JumpProcesses","title":"Simple Poisson Processes in JumpProcesses","text":"rate3(u,p,t) = p.λ\n\n# define the affect function via a closure\naffect3! = integrator -> let rng=rng\n    # N(t) <-- N(t) + 1\n    integrator.u[1] += 1\n\n    # G(t) <-- G(t) + C_{N(t)}\n    integrator.u[2] += rand(rng, (-1,1))\n    nothing\nend\ncrj = ConstantRateJump(rate3, affect3!)\n\nu₀ = [0, 0]\np = (λ = 1.0,)\ntspan = (0.0, 100.0)\ndprob = DiscreteProblem(u₀, tspan, p)\njprob = JumpProblem(dprob, Direct(), crj)\nsol = solve(jprob, SSAStepper())\nplot(sol, label=[\"N(t)\" \"G(t)\"], xlabel=\"t\")","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/#Benchmark-Suite","page":"Benchmark Suite","title":"Benchmark Suite","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"DifferentialEquations.jl provides a benchmarking suite to be able to test the difference in error, speed, and efficiency between algorithms. DifferentialEquations.jl includes current benchmarking notebooks to help users understand the performance of the methods. These benchmarking notebooks use the included benchmarking suite. There are two parts to the benchmarking suite: shootouts and work-precision. The Shootout tests methods head-to-head for timing and error on the same problem. A WorkPrecision draws a work-precision diagram for the algorithms in question on the chosen problem.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/#Using-the-Benchmarking-Notebooks","page":"Benchmark Suite","title":"Using the Benchmarking Notebooks","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"To use the benchmarking notebooks, IJulia is required. The commands are as follows:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"]add \"www.github.com/JuliaDiffEq/DiffEqBenchmarks\"\nusing IJulia\nnotebook(dir = Pkg.dir(\"DiffEqBenchmarks\"))","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/#Shootout","page":"Benchmark Suite","title":"Shootout","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"A shootout is where you compare between algorithms. For example, to see how different Runge-Kutta algorithms fair against each other, one can define a setup which is a dictionary of Symbols to Any, where the symbol is the keyword argument. Then you call Shootout on that setup. The code is as follows:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"using OrdinaryDiffEq,DiffEqProblemLibrary.ODEProblemLibrary,DiffEqDevTools,ODE,ODEInterface,ODEInterfaceDiffEq\n\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_2Dlinear\nsetups = [Dict(:alg=>DP5())\n          Dict(:abstol=>1e-3,:reltol=>1e-6,:alg=>ode45()) # Fix ODE to be normal\n          Dict(:alg=>dopri5())]\nnames = [\"DifferentialEquations\";\"ODE\";\"ODEInterface\"]\nshoot = Shootout(prob,setups;dt=1/2^(10),names=names)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"Note that keyword arguments applied to Shootout are applied to every run, so in this example every run has the same starting timestep.  Here we explicitly chose names. If you don't, then the algorithm name is the default. This returns a Shootout type which holds the times it took for each algorithm and the errors. Using these, it calculates the efficiency defined as 1/(error*time), i.e. if the error is low or the run was quick then it's efficient. print(shoot) will show all of this information, and plot(shoot) will show the efficiencies of the algorithms in comparison to each other.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"For every benchmark function there is a special keyword numruns which controls the number of runs used in the time estimate. To be more precise, these functions by default run the algorithm 20 times on the problem and take the average time. This amount can be increased and decreased as needed.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"The keyword appxsol allows for specifying a reference against which the error is computed. The method of error computation can be specified by the keyword error_estimate with values :L2 for the L2 error over the solution time interval, :l2 calculates the l2 error at the actual steps and the default :final only compares the endpoints.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"A ShootoutSet is a where you define a vector of probs and tspans and run a shootout on each of these values.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/#WorkPrecision","page":"Benchmark Suite","title":"WorkPrecision","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"A WorkPrecision calculates the necessary componnets of a work-precision plot. This shows how time scales with the user chosen tolerances on a given problem. To make a WorkPrecision, you give it a vector of absolute and relative tolerances:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"abstols = 1 ./10 .^ (3:10)\nreltols = 1 ./10 .^ (3:10)\nwp = WorkPrecision(prob,DP5(),abstols,reltols;name=\"Dormand-Prince 4/5\")","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"If we want to plot many WorkPrecisions together in order to compare between algorithms, you can make a WorkPrecisionSet. To do so, you pass the setups into the function as well:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"wp_set = WorkPrecisionSet(prob,tspan,abstols,reltols,setups;numruns=2)\nsetups = [Dict(:alg=>RK4());Dict(:alg=>Euler());Dict(:alg=>BS3());\n          Dict(:alg=>Midpoint());Dict(:alg=>BS5());Dict(:alg=>DP5())]    \nwp_set = WorkPrecisionSet(prob,abstols,reltols,setups;dt=1/2^4,numruns=2)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/benchmarks/","page":"Benchmark Suite","title":"Benchmark Suite","text":"Both of these types have a plot recipe to produce a work-precision diagram, and a print which will show some relevant information.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Stochastic-Differential-Equations","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"This tutorial will introduce you to the functionality for solving SDEs. Other introductions can be found by checking out SciMLTutorials.jl.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example-1:-Scalar-SDEs","page":"Stochastic Differential Equations","title":"Example 1: Scalar SDEs","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du = f(upt)dt + g(upt)dW","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"where f(upt)=αu and g(upt)=βu. We know via Stochastic Calculus that the solution to this equation is","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"u(tWₜ)=u₀exp((α-fracβ^22)t+βWₜ)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"To solve this numerically, we define a problem type by giving it the equation and the initial condition:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"using DifferentialEquations\nα=1\nβ=1\nu₀=1/2\nf(u,p,t) = α*u\ng(u,p,t) = β*u\ndt = 1//2^(4)\ntspan = (0.0,1.0)\nprob = SDEProblem(f,g,u₀,(0.0,1.0))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The solve interface is then the same as with ODEs. Here we will use the classic Euler-Maruyama algorithm EM and plot the solution:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,EM(),dt=dt)\nusing Plots; plotly() # Using the Plotly backend\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Basic Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Using-Higher-Order-Methods","page":"Stochastic Differential Equations","title":"Using Higher Order Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"One unique feature of DifferentialEquations.jl is that higher-order methods for stochastic differential equations are included. For reference, let's also give the SDEProblem the analytical solution. We can do this by making a test problem. This can be a good way to judge how accurate the algorithms are, or is used to test convergence of the algorithms for methods developers. Thus we define the problem object with:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f_analytic(u₀,p,t,W) = u₀*exp((α-(β^2)/2)*t+β*W)\nff = SDEFunction(f,g,analytic=f_analytic)\nprob = SDEProblem(ff,g,u₀,(0.0,1.0))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"and then we pass this information to the solver and plot:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"#We can plot using the classic Euler-Maruyama algorithm as follows:\nsol = solve(prob,EM(),dt=dt)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: SDE Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"We can choose a higher-order solver for a more accurate result:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1(),dt=dt,adaptive=false)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better SDE Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"By default, the higher order methods have adaptivity. Thus one can use","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1())\nplot(sol,plot_analytic=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better Automatic Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Here we allowed the solver to automatically determine a starting dt. This estimate at the beginning is conservative (small) to ensure accuracy. We can instead start the method with a larger dt by passing in a value for the starting dt:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1(),dt=dt)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better Automatic Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Ensemble-Simulations","page":"Stochastic Differential Equations","title":"Ensemble Simulations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Instead of solving single trajectories, we can turn our problem into a EnsembleProblem to solve many trajectories all at once. This is done by the EnsembleProblem constructor:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"ensembleprob = EnsembleProblem(prob)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The solver commands are defined at the Parallel Ensemble Simulations page. For example we can choose to have 1000 trajectories via trajectories=1000. In addition, this will automatically parallelize using Julia native parallelism if extra processes are added via addprocs(), but we can change this to use multithreading via EnsembleThreads(). Together, this looks like:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(ensembleprob,EnsembleThreads(),trajectories=1000)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"warn: Warn\nIf you use a custom noise process, you might need to specify it in a custom prob_func in the EnsembleProblem constructor, as each trajectory needs its own noise process.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Many more controls are defined at the Ensemble simulations page,  including analysis tools. A very simple analysis can be done with the EnsembleSummary, which builds mean/var statistics and has an associated plot recipe. For example, we can get the statistics at every 0.01 timesteps and plot the average + error using:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"using DifferentialEquations.EnsembleAnalysis\nsumm = EnsembleSummary(sol,0:0.01:1)\nplot(summ,labels=\"Middle 95%\")\nsumm = EnsembleSummary(sol,0:0.01:1;quantiles=[0.25,0.75])\nplot!(summ,labels=\"Middle 50%\",legend=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: sde_tutorial_monte)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Additionally we can easily calculate the correlation between the values at t=0.2 and t=0.7 via","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"timepoint_meancor(sim,0.2,0.7) # Gives both means and then the correlation coefficient","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example-2:-Systems-of-SDEs-with-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 2: Systems of SDEs with Diagonal Noise","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"More generally, an SDE","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du = f(upt)dt + g(upt)dW","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"generalizes to systems of equations is done in the same way as ODEs. Here, g is now a matrix of values. One common case, and the default for DifferentialEquations.jl, is diagonal noise where g is a diagonal matrix. This means that every function in the system gets a different random number. Instead of handling matrices in this case, we simply define both f and g as in-place functions. Thus f(du,u,p,t) gives a vector of du which is the deterministic change, and g(du2,u,p,t) gives a vector du2 for which du2.*W is the stochastic portion of the equation.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"For example, the Lorenz equation with additive noise has the same deterministic portion as the Lorenz equations, but adds an additive noise, which is simply 3*N(0,dt) where N is the normal distribution dt is the time step, to each step of the equation. This is done via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function lorenz(du,u,p,t)\n  du[1] = 10.0(u[2]-u[1])\n  du[2] = u[1]*(28.0-u[3]) - u[2]\n  du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nfunction σ_lorenz(du,u,p,t)\n  du[1] = 3.0\n  du[2] = 3.0\n  du[3] = 3.0\nend\n\nprob_sde_lorenz = SDEProblem(lorenz,σ_lorenz,[1.0,0.0,0.0],(0.0,10.0))\nsol = solve(prob_sde_lorenz)\nplot(sol,vars=(1,2,3))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: stochastic_3d_lorenz)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Note that it's okay for the noise function to mix terms. For example","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function σ_lorenz(du,u,p,t)\n  du[1] = sin(u[3])*3.0\n  du[2] = u[2]*u[1]*3.0\n  du[3] = 3.0\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"is a valid noise function, which will once again give diagonal noise by du2.*W.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example-3:-Systems-of-SDEs-with-Scalar-Noise","page":"Stochastic Differential Equations","title":"Example 3: Systems of SDEs with Scalar Noise","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this example we'll solve a system of SDEs with scalar noise. This means that the same noise process is applied to all SDEs. First we need to define a scalar noise process using the Noise Process interface. Since we want a WienerProcess that starts at 0.0 at time 0.0, we use the command W = WienerProcess(0.0,0.0,0.0) to define the Brownian motion we want, and then give this to the noise option in the SDEProblem. For a full example, let's solve a linear SDE with scalar noise using a high order algorithm:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f(du,u,p,t) = (du .= u)\ng(du,u,p,t) = (du .= u)\nu0 = rand(4,2)\n\nW = WienerProcess(0.0,0.0,0.0)\nprob = SDEProblem(f,g,u0,(0.0,1.0),noise=W)\nsol = solve(prob,SRIW1())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Scalar Noise)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example-4:-Systems-of-SDEs-with-Non-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 4: Systems of SDEs with Non-Diagonal Noise","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In the previous examples we had diagonal noise, that is a vector of random numbers dW whose size matches the output of g where the noise is applied element-wise, and scalar noise where a single random variable is applied to all dependent variables. However, a more general type of noise allows for the terms to linearly mixed via g being a matrix.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Note that nonlinear mixings are not SDEs but fall under the more general class of random ordinary differential equations (RODEs) which have a separate set of solvers.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Let's define a problem with four Wiener processes and two dependent random variables. In this case, we will want the output of g to be a 2x4 matrix, such that the solution is g(u,p,t)*dW, the matrix multiplication. For example, we can do the following:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f(du,u,p,t) = du .= 1.01u\nfunction g(du,u,p,t)\n  du[1,1] = 0.3u[1]\n  du[1,2] = 0.6u[1]\n  du[1,3] = 0.9u[1]\n  du[1,4] = 0.12u[1]\n  du[2,1] = 1.2u[2]\n  du[2,2] = 0.2u[2]\n  du[2,3] = 0.3u[2]\n  du[2,4] = 1.8u[2]\nend\nprob = SDEProblem(f,g,ones(2),(0.0,1.0),noise_rate_prototype=zeros(2,4))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In our g we define the functions for computing the values of the matrix. We can now think of the SDE that this solves as the system of equations","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du_1 = f_1(upt)dt + g_11(upt)dW_1 + g_12(upt)dW_2 + g_13(upt)dW_3 + g_14(upt)dW_4 \ndu_2 = f_2(upt)dt + g_21(upt)dW_1 + g_22(upt)dW_2 + g_23(upt)dW_3 + g_24(upt)dW_4","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"meaning that for example du[1,1] and du[2,1] correspond to stochastic changes with the same random number in the first and second SDEs.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"note: Note\nThis problem can only be solved my SDE methods which are compatible with non-diagonal noise. This is discussed in the SDE solvers page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The matrix itself is determined by the keyword argument noise_rate_prototype in the SDEProblem constructor. This is a prototype for the type that du will be in g. This can be any AbstractMatrix type. Thus for example, we can define the problem as","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"\n# Define a sparse matrix by making a dense matrix and setting some values as not zero\nA = zeros(2,4)\nA[1,1] = 1\nA[1,4] = 1\nA[2,4] = 1\nA=sparse(A)\n\n# Make `g` write the sparse matrix values\nfunction g(du,u,p,t)\n  du[1,1] = 0.3u[1]\n  du[1,4] = 0.12u[2]\n  du[2,4] = 1.8u[2]\nend\n\n# Make `g` use the sparse matrix\nprob = SDEProblem(f,g,ones(2),(0.0,1.0),noise_rate_prototype=A)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"and now g(u,p,t) writes into a sparse matrix, and g(u,p,t)*dW is sparse matrix multiplication.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example-4:-Colored-Noise","page":"Stochastic Differential Equations","title":"Example 4: Colored Noise","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Colored noise can be defined using the Noise Process interface. In that portion of the docs, it is shown how to define your own noise process my_noise, which can be passed to the SDEProblem","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"SDEProblem(f,g,u0,tspan,noise=my_noise)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Note that general colored noise problems are only compatible with the EM and EulerHeun methods. This is discussed in the SDE solvers page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/#Example:-Spatially-Colored-Noise-in-the-Heston-Model","page":"Stochastic Differential Equations","title":"Example: Spatially-Colored Noise in the Heston Model","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Let's define the Heston equation from financial mathematics:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"dS = μSdt + sqrtvSdW_1 \ndv = κ(Θ-v)dt + σsqrtvdW_2 \ndW_1 dW_2 = ρ dt","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this problem, we have a diagonal noise problem given by:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function f(du,u,p,t)\n  du[1] = μ*u[1]\n  du[2] = κ*(Θ-u[2])\nend\nfunction g(du,u,p,t)\n  du[1] = √u[2]*u[1]\n  du[2] = Θ*√u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"However, our noise has a correlation matrix for some constant ρ. Choosing ρ=0.2:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Γ = [1 ρ;ρ 1]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"To solve this, we can define a CorrelatedWienerProcess which starts at zero (W(0)=0) via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"heston_noise = CorrelatedWienerProcess!(Γ,tspan[1],zeros(2),zeros(2))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"This is then used to build the SDE:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"SDEProblem(f,g,u0,tspan,noise=heston_noise)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Of course, to fully define this problem we need to define our constants. Constructors for making common models like this easier to define can be found in the modeling toolkits. For example, the HestonProblem is pre-defined as part of the financial modeling tools.","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/#DAE-Problems","page":"DAE Problems","title":"DAE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"DAEProblem\nDAEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/#SciMLBase.DAEProblem","page":"DAE Problems","title":"SciMLBase.DAEProblem","text":"Defines an implicit ordinary differential equation (ODE) or  differential-algebraic equation (DAE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/dae_types/\n\nMathematical Specification of an DAE Problem\n\nTo define a DAE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\n0 = f(duupt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(resid,du,u,p,t)). Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nDAEProblem(f::DAEFunction,du0,u0,tspan,p=NullParameters();kwargs...)\nDAEProblem{isinplace}(f,du0,u0,tspan,p=NullParameters();kwargs...) : Defines the DAE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the ODE.\ndu0: The initial condition for the derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ndifferential_vars: A logical array which declares which variables are the differential (non algebraic) vars (i.e. du' is in the equations for this variable). Defaults to nothing. Some solvers may require this be set if an initial condition needs to be determined.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nExample Problems\n\nExamples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_dae_resrob, you can do something like:\n\n#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.DAEProblemLibrary\n# load problems\nDAEProblemLibrary.importdaeproblems()\nprob = DAEProblemLibrary.prob_dae_resrob\nsol = solve(prob,IDA())\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dae_types/#SciMLBase.DAEFunction","page":"DAE Problems","title":"SciMLBase.DAEFunction","text":"DAEFunction{iip,F,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractDAEFunction{iip}\n\nA representation of an implicit DAE function f, defined by:\n\n0 = f(fracdudtupt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDAEFunction{iip,recompile}(f;\n                           analytic=nothing,\n                           jac=nothing,\n                           jvp=nothing,\n                           vjp=nothing,\n                           jac_prototype=nothing,\n                           sparsity=jac_prototype,\n                           syms = nothing,\n                           indepsym = nothing,\n                           colorvec = nothing)\n\nNote that only the function f itself is required. This function should be given as f!(out,du,u,p,t) or out = f(du,u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\njac(J,du,u,p,gamma,t) or J=jac(du,u,p,gamma,t): returns the implicit DAE Jacobian defined as gamma fracdGd(du) + fracdGdu\njvp(Jv,v,du,u,p,gamma,t) or Jv=jvp(v,du,u,p,gamma,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,du,u,p,gamma,t) or Jv=vjp(v,du,u,p,gamma,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DAEFunction type directly match the names of the inputs.\n\nExamples\n\nDeclaring Explicit Jacobians for DAEs\n\nFor fully implicit ODEs (DAEProblems), a slightly different Jacobian function is necessary. For the DAE\n\nG(duupt) = res\n\nThe Jacobian should be given in the form gamma*dG/d(du) + dG/du where gamma is given by the solver. This means that the signature is:\n\nf(J,du,u,p,gamma,t)\n\nFor example, for the equation\n\nfunction testjac(res,du,u,p,t)\n  res[1] = du[1] - 2.0 * u[1] + 1.2 * u[1]*u[2]\n  res[2] = du[2] -3 * u[2] - u[1]*u[2]\nend\n\nwe would define the Jacobian as:\n\nfunction testjac(J,du,u,p,gamma,t)\n  J[1,1] = gamma - 2.0 + 1.2 * u[2]\n  J[1,2] = 1.2 * u[1]\n  J[2,1] = - 1 * u[2]\n  J[2,2] = gamma - 3 - u[1]\n  nothing\nend\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dae_types/#Solution-Type","page":"DAE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"DAESolution","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/#SciMLBase.DAESolution","page":"DAE Problems","title":"SciMLBase.DAESolution","text":"struct DAESolution{T, N, uType, duType, uType2, DType, tType, P, A, ID, DE} <: SciMLBase.AbstractDAESolution{T, N, uType}\n\nRepresentation of the solution to an differential-algebraic equation defined by an DAEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://diffeq.sciml.ai/stable/basics/solution/\n\nFields\n\nu: the representation of the DAE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution  Handling page of the DifferentialEquations.jl documentation.\ndu: the representation fo the derivatives of the DAE solution.\nt: the time points corresponding to the saved values of the DAE solution.\nprob: the original DAEProblem that was solved.\nalg: the algorithm type used by the solver.\ndestats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully (sol.retcode === :Success), whether it terminated due to a user-defined callback (sol.retcode === :Terminated), or whether it exited due to an error. For more details, see the return code section of the DifferentialEquations.jl documentation.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dae_types/#Example-Problems","page":"DAE Problems","title":"Example Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"To use a sample problem, such as prob_dae_resrob, you can do something like:","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.DAEProblemLibrary\n# load problems\nDAEProblemLibrary.importdaeproblems()\nprob = DAEProblemLibrary.prob_dae_resrob\nsol = solve(prob,IDA())","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"CurrentModule = DAEProblemLibrary","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"prob_dae_resrob","category":"page"},{"location":"modules/DiffEqDocs/types/dae_types/#DiffEqProblemLibrary.DAEProblemLibrary.prob_dae_resrob","page":"DAE Problems","title":"DiffEqProblemLibrary.DAEProblemLibrary.prob_dae_resrob","text":"The Robertson biochemical reactions in DAE form\n\nfracdy₁dt = -k₁y₁+k₃y₂y₃\n\nfracdy₂dt =  k₁y₁-k₂y₂^2-k₃y₂y₃\n\n1 = y₁ + y₂ + y₃\n\nwhere k₁=004, k₂=3times10^7, k₃=10^4. For details, see: Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129 Usually solved on 01e11\n\n\n\n","category":"constant"},{"location":"modules/DiffEqDocs/types/split_ode_types/#split_ode_prob","page":"Split ODE Problems","title":"Split ODE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"SplitODEProblem\nSplitFunction","category":"page"},{"location":"modules/DiffEqDocs/types/split_ode_types/#SciMLBase.SplitODEProblem","page":"Split ODE Problems","title":"SciMLBase.SplitODEProblem","text":"Defines a split ordinary differential equation (ODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/splitodetypes/\n\nMathematical Specification of a Split ODE Problem\n\nTo define a SplitODEProblem, you simply need to give a two functions  f_1 and f_2 along with an initial condition u_0 which define an ODE:\n\nfracdudt =  f_1(upt) + f_2(upt)\n\nf should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nMany splits are at least partially linear. That is the equation:\n\nfracdudt =  Au + f_2(upt)\n\nFor how to define a linear function A, see the documentation for the DiffEqOperators.\n\nConstructors\n\nSplitODEProblem(f::SplitFunction,u0,tspan,p=NullParameters();kwargs...)\nSplitODEProblem{isinplace}(f1,f2,u0,tspan,p=NullParameters();kwargs...)\n\nThe isinplace parameter can be omitted and will be determined using the signature of f2. Note that both f1 and f2 should support the in-place style if isinplace is true or they should both support the out-of-place style if isinplace is false. You cannot mix up the two styles.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nUnder the hood, a SplitODEProblem is just a regular ODEProblem whose f is a SplitFunction. Therefore you can solve a SplitODEProblem using the same solvers for ODEProblem. For solvers dedicated to split problems, see Split ODE Solvers.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf1, f2: The functions in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/split_ode_types/#SciMLBase.SplitFunction","page":"Split ODE Problems","title":"SciMLBase.SplitFunction","text":"SplitFunction{iip,F1,F2,TMM,C,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractODEFunction{iip}\n\nA representation of a split ODE function f, defined by:\n\nM fracdudt = f_1(upt) + f_2(upt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nGenerally, for ODE integrators the f_1 portion should be considered the \"stiff portion of the model\" with larger time scale separation, while the f_2 portion should be considered the \"non-stiff portion\". This interpretation is directly used in integrators like IMEX (implicit-explicit integrators) and exponential integrators.\n\nConstructor\n\nSplitFunction{iip,recompile}(f1,f2;\n                             mass_matrix=I,\n                             analytic=nothing,\n                             tgrad=nothing,\n                             jac=nothing,\n                             jvp=nothing,\n                             vjp=nothing,\n                             jac_prototype=nothing,\n                             sparsity=jac_prototype,\n                             paramjac = nothing,\n                             syms = nothing,\n                             indepsym = nothing,\n                             colorvec = nothing)\n\nNote that only the functions f_i themselves are required. These functions should be given as f_i!(du,u,p,t) or du = f_i(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracpartial f_1(upt)partial t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdf_1du\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdf_1du v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdf_1du^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdf_1dp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\nNote on the Derivative Definition\n\nThe derivatives, such as the Jacobian, are only defined on the f1 portion of the split ODE. This is used to treat the f1 implicit while keeping the f2 portion explicit.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the SplitFunction type directly match the names of the inputs.\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions. See ModelingToolkit.SplitODEProblem for information on generating the SplitFunction from this symbolic engine.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/split_ode_types/#Solution-Type","page":"Split ODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"SplitODEProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"page"},{"location":"modules/Surrogates/Salustowicz/#Salustowicz-Benchmark-Function","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark Function","text":"","category":"section"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The true underlying function HyGP had to approximate is the 1D Salustowicz function. The function can be evaluated in the given domain: x in 0 10.","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"The Salustowicz benchmark function is as follows:","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"f(x) = e^(-x) x^3 cos(x) sin(x) (cos(x) sin^2(x) - 1)","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's import these two packages  Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's define our objective function:","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"function salustowicz(x)\r\n    term1 = 2.72^(-x) * x^3 * cos(x) * sin(x);\r\n    term2 = (cos(x) * sin(x)*sin(x) - 1);\r\n    y = term1 * term2;\r\nend","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Let's sample f in 30 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"n_samples = 30\r\nlower_bound = 0\r\nupper_bound = 10\r\nnum_round = 2\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = salustowicz.(x)\r\nxs = lower_bound:0.001:upper_bound\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Now, let's fit Salustowicz Function with different Surrogates:","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"InverseDistance = InverseDistanceSurrogate(x, y, lower_bound, upper_bound)\r\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\r\nplot!(xs, InverseDistance.(xs), label=\"InverseDistanceSurrogate\", legend=:topright)\r\nplot!(xs, lobachevsky_surrogate.(xs), label=\"Lobachevsky\", legend=:topright)","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"Not's let's see Kriging Surrogate with different hyper parameter:","category":"page"},{"location":"modules/Surrogates/Salustowicz/","page":"Salustowicz Benchmark","title":"Salustowicz Benchmark","text":"kriging_surrogate1 = Kriging(x, y, lower_bound, upper_bound, p=0.9);\r\nkriging_surrogate2 = Kriging(x, y, lower_bound, upper_bound, p=1.5);\r\nkriging_surrogate3 = Kriging(x, y, lower_bound, upper_bound, p=1.9);\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:topright)\r\nplot!(xs, salustowicz.(xs), label=\"True function\", legend=:topright)\r\nplot!(xs, kriging_surrogate1.(xs), label=\"kriging_surrogate1\", ribbon=p->std_error_at_point(kriging_surrogate1, p), legend=:topright)\r\nplot!(xs, kriging_surrogate2.(xs), label=\"kriging_surrogate2\", ribbon=p->std_error_at_point(kriging_surrogate2, p), legend=:topright)\r\nplot!(xs, kriging_surrogate3.(xs), label=\"kriging_surrogate3\", ribbon=p->std_error_at_point(kriging_surrogate3, p), legend=:topright)","category":"page"},{"location":"modules/NeuralPDE/manual/ode/#ODE-Specialized-Physics-Informed-Neural-Network-(PINN)-Solver","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/ode/","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"NNODE","category":"page"},{"location":"modules/NeuralPDE/manual/ode/#NeuralPDE.NNODE","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"NeuralPDE.NNODE","text":"NNODE(chain, opt=OptimizationPolyalgorithms.PolyOpt(), init_params = nothing;\n                          autodiff=false, batch=0, kwargs...)\n\nAlgorithm for solving ordinary differential equations using a neural network. This is a specialization of the physics-informed neural network which is used as a solver for a standard ODEProblem.\n\nwarn: Warn\nNote that NNODE only supports ODEs which are written in the out-of-place form, i.e. du = f(u,p,t), and not f(du,u,p,t). If not declared out-of-place then the NNODE will exit with an error.\n\nPositional Arguments\n\nchain: A neural network architecture, defined as either a Flux.Chain or a Lux.AbstractExplicitLayer.\nopt: The optimizer to train the neural network. Defaults to OptimizationPolyalgorithms.PolyOpt()\ninit_params: The initial parameter of the neural network. By default this is nothing which thus uses the random initialization provided by the neural network library.\n\nKeyword Arguments\n\nautodiff: The switch between automatic and numerical differentiation for             the PDE operators. The reverse mode of the loss function is always             automatic differentation (via Zygote), this is only for the derivative             in the loss function (the derivative with respect to time).\nbatch: The batch size to use for the internal quadrature. Defaults to 0, which means the application of the neural network is done at individual time points one at a time. batch>0 means the neural network is applied at a row vector of values t simultaniously, i.e. it's the batch size for the neural network evaluations. This requires a neural network compatible with batched data.\nstrategy: The training strategy used to choose the points for the evaluations. Default of nothing means that QuadratureTraining with QuadGK is used if no dt is given, and GridTraining is used with dt if given.\nkwargs: Extra keyword arguments are splatted to the Optimization.jl solve call.\n\nExample\n\nf(u,p,t) = cos(2pi*t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0 ,tspan)\nchain = Flux.Chain(Dense(1,5,σ),Dense(5,1))\nopt = Flux.ADAM(0.1)\nsol = solve(prob, NeuralPDE.NNODE(chain,opt), dt=1/20f0, verbose = true,\n            abstol=1e-10, maxiters = 200)\n\nSolution Notes\n\nNote that the solution is evaluated at fixed time points according to standard output handlers such as saveat and dt. However, the neural network is a fully continuous solution so sol(t) is an accuate interpolation (up to the neural network training result). In addition, the OptimizationSolution is returned as sol.k for further analysis.\n\nReferences\n\nLagaris, Isaac E., Aristidis Likas, and Dimitrios I. Fotiadis. \"Artificial neural networks for solving ordinary and partial differential equations.\" IEEE Transactions on Neural Networks 9, no. 5 (1998): 987-1000.\n\n\n\n\n\n","category":"type"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Nonconvex.jl","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"Nonconvex is a is a Julia package implementing and wrapping nonconvex constrained optimization algorithms.","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Installation:-OptimizationNonconvex.jl","page":"Nonconvex.jl","title":"Installation: OptimizationNonconvex.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"To use this package, install the OptimizationNonconvex package:","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"import Pkg; Pkg.add(\"OptimizationNonconvex\")","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Global-Optimizer","page":"Nonconvex.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Without-Constraint-Equations","page":"Nonconvex.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"A Nonconvex algorithm is called using one of the following:","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"Method of moving asymptotes (MMA):\nMMA87()\nMMA02()\nIpopt:\nIpoptAlg()\nNLopt:\nNLoptAlg(solver) where solver can be any of the NLopt algorithms\nAugmented Lagrangian algorithm:\nAugLag()\nonly works with constraints\nMixed integer nonlinear programming (MINLP):\nJuniper + Ipopt: JuniperIpoptAlg()\nPavito + Ipopt + Cbc: PavitoIpoptCbcAlg()\nMulti-start optimization:\nHyperoptAlg(subsolver) where subalg can be any of the described Nonconvex algorithm\nSurrogate-assisted Bayesian optimization\nBayesOptAlg(subsolver) where subalg can be any of the described Nonconvex algorithm\nMultiple Trajectory Search\nMTSAlg()","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"When performing optimizing a combination of integer and floating-point parameters the integer keyword has to be set. It takes a boolean vector indicating which parameter is an integer.","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Notes","page":"Nonconvex.jl","title":"Notes","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"Some optimizer may require further options to be defined in order to work.","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"The currently available algorithms are listed here","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"The algorithms in Nonconvex are performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/#Examples","page":"Nonconvex.jl","title":"Examples","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"The Rosenbrock function can optimized using the Method of moving asymptotes algorithm MMA02() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, MMA02(), maxiters=100000, maxtime=1000.0)","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"The options of for a sub-algorithm are passed simply as a NamedTuple and GalactcOptim infers the correct Nonconvex options struct:","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, HyperoptAlg(IpoptAlg()), sub_options=(;max_iter=100))","category":"page"},{"location":"modules/Optimization/optimization_packages/nonconvex/#With-Constraint-Equations","page":"Nonconvex.jl","title":"With Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nonconvex/","page":"Nonconvex.jl","title":"Nonconvex.jl","text":"While Nonconvex.jl supports such constraints, Optimization.jl currently does not relay these constraints.","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/#RuntimeGeneratedFunctions.jl","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"","category":"section"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"(Image: Build Status)","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"RuntimeGeneratedFunctions are functions generated at runtime without world-age issues and with the full performance of a standard Julia anonymous function. This builds functions in a way that avoids eval.","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"Note that RuntimeGeneratedFunction does not handle closures. Please use the GeneralizedGenerated.jl package for more fixable staged programming. While GeneralizedGenerated.jl is more powerful, RuntimeGeneratedFunctions.jl handles large expressions better.","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/#Simple-Example","page":"RuntimeGeneratedFunctions.jl","title":"Simple Example","text":"","category":"section"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"Here's an example showing how to construct and immediately call a runtime generated function:","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"using RuntimeGeneratedFunctions\nRuntimeGeneratedFunctions.init(@__MODULE__)\n\nfunction no_worldage()\n    ex = :(function f(_du,_u,_p,_t)\n        @inbounds _du[1] = _u[1]\n        @inbounds _du[2] = _u[2]\n        nothing\n    end)\n    f1 = @RuntimeGeneratedFunction(ex)\n    du = rand(2)\n    u = rand(2)\n    p = nothing\n    t = nothing\n    f1(du,u,p,t)\nend\nno_worldage()","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/#Changing-how-global-symbols-are-looked-up","page":"RuntimeGeneratedFunctions.jl","title":"Changing how global symbols are looked up","text":"","category":"section"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"If you want to use helper functions or global variables from a different module within your function expression you'll need to pass a context_module to the @RuntimeGeneratedFunction constructor. For example","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"RuntimeGeneratedFunctions.init(@__MODULE__)\n\nmodule A\n    using RuntimeGeneratedFunctions\n    RuntimeGeneratedFunctions.init(A)\n    helper_function(x) = x + 1\nend\n\nfunction g()\n    expression = :(f(x) = helper_function(x))\n    # context module is `A` so that `helper_function` can be found.\n    f = @RuntimeGeneratedFunction(A, expression)\n    @show f(1)\nend","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/#Precompilation-and-setting-the-function-expression-cache","page":"RuntimeGeneratedFunctions.jl","title":"Precompilation and setting the function expression cache","text":"","category":"section"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"For technical reasons RuntimeGeneratedFunctions needs to cache the function expression in a global variable within some module. This is normally transparent to the user, but if the RuntimeGeneratedFunction is evaluated during module precompilation, the cache module must be explicitly set to the module currently being precompiled. This is relevant for helper functions in some module which construct a RuntimeGeneratedFunction on behalf of the user. For example, in the following code, any third party user of HelperModule.construct_rgf() user needs to pass their own module as the cache_module if they want the returned function to work after precompilation:","category":"page"},{"location":"modules/RuntimeGeneratedFunctions/","page":"RuntimeGeneratedFunctions.jl","title":"RuntimeGeneratedFunctions.jl","text":"RuntimeGeneratedFunctions.init(@__MODULE__)\n\n# Imagine HelperModule is in a separate package and will be precompiled\n# separately.\nmodule HelperModule\n    using RuntimeGeneratedFunctions\n    RuntimeGeneratedFunctions.init(HelperModule)\n\n    function construct_rgf(cache_module, context_module, ex)\n        ex = :((x)->$ex^2 + x)\n        RuntimeGeneratedFunction(cache_module, context_module, ex)\n    end\nend\n\nfunction g()\n    ex = :(x + 1)\n    # Here cache_module is set to the module currently being compiled so that\n    # the returned RGF works with Julia's module precompilation system.\n    HelperModule.construct_rgf(@__MODULE__, @__MODULE__, ex)\nend\n\nf = g()\n@show f(1)","category":"page"},{"location":"modules/NeuralPDE/#NeuralPDE.jl:-Scientific-Machine-Learning-for-Partial-Differential-Equations","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Scientific Machine Learning for Partial Differential Equations","text":"","category":"section"},{"location":"modules/NeuralPDE/","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"NeuralPDE.jl NeuralPDE.jl is a solver package which consists of neural network solvers for partial differential equations using physics-informed neural networks (PINNs).","category":"page"},{"location":"modules/NeuralPDE/#Features","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Features","text":"","category":"section"},{"location":"modules/NeuralPDE/","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Physics-Informed Neural Networks for ODE, SDE, RODE, and PDE solving\nAbility to define extra loss functions to mix xDE solving with data fitting (scientific machine learning)\nAutomated construction of Physics-Informed loss functions from a high level symbolic interface\nSophisticated techniques like quadrature training strategies, adaptive loss functions, and neural adapters to accelerate training\nIntegrated logging suite for handling connections to TensorBoard\nHandling of (partial) integro-differential equations and various stochastic equations\nSpecialized forms for solving ODEProblems with neural networks\nCompatability with Flux.jl and Lux.jl for all of the GPU-powered machine learning layers available from those libraries.\nCompatability with NeuralOperators.jl for mixing DeepONets and other neural operators (Fourier Neural Operators, Graph Neural Operators, etc.) with physics-informed loss functions","category":"page"},{"location":"modules/NeuralPDE/#Citation","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Citation","text":"","category":"section"},{"location":"modules/NeuralPDE/","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"If you use NeuralPDE.jl in your research, please cite this paper:","category":"page"},{"location":"modules/NeuralPDE/","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"@misc{https://doi.org/10.48550/arxiv.2107.09443,\n  doi = {10.48550/ARXIV.2107.09443},\n  url = {https://arxiv.org/abs/2107.09443},\n  author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},\n  keywords = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/#Optimising-Parameters-(Solving-Inverse-Problems)-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems) with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Consider a Lorenz System,","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"beginalign*\n    fracmathrmd xmathrmdt = sigma (y -x)  \n    fracmathrmd ymathrmdt = x (rho - z) - y  \n    fracmathrmd zmathrmdt = x y - beta z  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimise the parameters \\sigma, \\beta, and \\rho.","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We start by defining the the problem,","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, OrdinaryDiffEq, Plots\nimport ModelingToolkit: Interval, infimum, supremum\n@parameters t ,σ_ ,β, ρ\n@variables x(..), y(..), z(..)\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ_*(y(t) - x(t)),\n       Dt(y(t)) ~ x(t)*(ρ - z(t)) - y(t),\n       Dt(z(t)) ~ x(t)*y(t) - β*z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0,1.0)]\ndt = 0.01","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And the neural networks as,","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"input_ = length(domains)\nn = 8\nchain1 = Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1))\nchain2 = Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1))\nchain3 = Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1))","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We will add an additional loss term based on the data that we have in order to optimise the parameters.","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Here we simply calculate the solution of the lorenz system with OrdinaryDiffEq.jl based on the adaptivity of the ODE solver. This is used to introduce non-uniformity to the time series.","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"function lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,1.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob, Tsit5(), dt=0.1)\nts = [infimum(d.domain):dt:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us,ts_]\nend\ndata = getData(sol)\n\n(u_ , t_) = data\nlen = length(data[2])","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then we define the additional loss funciton additional_loss(phi, θ , p), the function has three arguments:","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"phi the trial solution\nθ the parameters of neural networks\nthe hyperparameters p .","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"For a Lux neural network, the composed function will present itself as having θ as a ComponentArray subsets θ.x, which can also be dereferenced like θ[:x]. Thus the additional loss looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"depvars = [:x,:y,:z]\nfunction additional_loss(phi, θ , p)\n    return sum(sum(abs2, phi[i](t_ , θ[depvars[i]]) .- u_[[i], :])/len for i in 1:1:3)\nend","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/#Note-about-Flux","page":"Optimising Parameters (Solving Inverse Problems)","title":"Note about Flux","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"If Flux neural network is used, then the subsetting must be computed manually as θ is simply a vector. This looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"init_params = [Flux.destructure(c)[1] for c in [chain1,chain2,chain3]]\nacum =  [0;accumulate(+, length.(init_params))]\nsep = [acum[i]+1 : acum[i+1] for i in 1:length(acum)-1]\n(u_ , t_) = data\nlen = length(data[2])\n\nfunction additional_loss(phi, θ , p)\n    return sum(sum(abs2, phi[i](t_ , θ[sep[i]]) .- u_[[i], :])/len for i in 1:1:3)\nend","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/#Back-to-our-originally-scheduled-programming","page":"Optimising Parameters (Solving Inverse Problems)","title":"Back to our originally scheduled programming","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then finally defining and optimising using the PhysicsInformedNN interface.","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"discretization = NeuralPDE.PhysicsInformedNN([chain1 , chain2, chain3],NeuralPDE.GridTraining(dt), param_estim=true, additional_loss=additional_loss)\n@named pde_system = PDESystem(eqs,bcs,domains,[t],[x(t), y(t), z(t)],[σ_, ρ, β], defaults=Dict([p .=> 1.0 for p in [σ_, ρ, β]]))\nprob = NeuralPDE.discretize(pde_system,discretization)\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=5000)\np_ = res.u[end-2:end] # p_ = [9.93, 28.002, 2.667]","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And then finally some analyisis by plotting.","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"minimizers = [res.u.depvar[depvars[i]] for i in 1:3]\nts = [infimum(d.domain):dt/10:supremum(d.domain) for d in domains][1]\nu_predict  = [[discretization.phi[i]([t],minimizers[i])[1] for t in ts] for i in 1:3]\nplot(sol)\nplot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])","category":"page"},{"location":"modules/NeuralPDE/tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"(Image: Plot_Lorenz)","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"using PolyChaos, DifferentialEquations\nx0 = 2.0\nμ, σ = -0.5, 0.05\ntend, Δt = 3.0, 0.01\nusing PolyChaos\nL, Nrec = 6, 40\nopq = GaussOrthoPoly(L; Nrec=Nrec, addQuadrature=true)\nusing DifferentialEquations\n\na = [ convert2affinePCE(μ, σ, opq); zeros(Float64,L-1) ] # PCE coefficients of a\nxinit = [ x0; zeros(Float64,L) ] # PCE coefficients of initial condition\n\nt2 = Tensor(2, opq); # \\langle \\phi_i, \\phi_j \\rangle\nt3 = Tensor(3, opq); # \\langle \\phi_i \\phi_j, \\phi_k \\rangle\n\n# Galerkin-projected random differential equation\nfunction ODEgalerkin(du,u,p,t)\n   du[:] = [ sum( p[j+1]*u[k+1]*t3.get([j,k,m])/t2.get([m,m]) for j=0:L for k=0:L) for m=0:L ]\nend\n\nprobgalerkin = ODEProblem(ODEgalerkin,xinit,(0,tend),a)\nsolgalerkin = solve(probgalerkin;saveat=0:Δt:tend)\nt, x = solgalerkin.t, solgalerkin.u;\n# an advantage of PCE is that moments can be computed from the PCE coefficients alone; no sampling required\nmean_pce = [ mean(x_, opq) for x_ in x]  \nstd_pce = [ std(x_, opq) for x_ in x]\nusing Statistics\nNsmpl = 5000\nξ = sampleMeasure(Nsmpl,opq)     # sample from Gaussian measure; effectively randn() here    \nasmpl = evaluatePCE(a,ξ,opq)     # sample random variable with PCE coefficients a; effectively μ + σ*randn() here\n# or: asmpl = samplePCE(Nsmpl,a,opq)\nxmc = [ solve(ODEProblem((u,p,t)->aa*u,x0,(0,tend));saveat=0:Δt:tend).u for aa in asmpl]\nxmc = hcat(xmc...);\n[ mean(xmc,dims=2)-mean_pce std(xmc,dims=2)-std_pce]\nlogx_pce = [ log.(evaluatePCE(x[i],ξ,opq)) for i=1:length(t)]\n[mean.(logx_pce)-(log(x0) .+ μ*t) std.(logx_pce)-σ*t ]","category":"page"},{"location":"modules/PolyChaos/random_ode/#Galerkin-based-Solution-of-Random-Differential-Equation","page":"Random ODE","title":"Galerkin-based Solution of Random Differential Equation","text":"","category":"section"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"This tutorial demonstrates how random differential equations can be solved using polynomial chaos expansions (PCE).","category":"page"},{"location":"modules/PolyChaos/random_ode/#Theory","page":"Random ODE","title":"Theory","text":"","category":"section"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"A random differential equation is an ordinary differential equation that has random parameters, hence its solution is itself a (time-varying) random variable. Perhaps the simplest non-trivial example is the following scalar, linear ordinary differential equation","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"dotx(t) = a x(t) quad x(0) = x_0","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"where a is the realization of a Gaussian random variable mathsfa sim mathcalN(mu sigma^2) with mean mu and variance sigma^2. Arguably, for every realization a we can solve the differential equation and obtain","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"x(t) = x_0 mathrme^a t","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"from which we find that","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"ln (x(t)) = ln (x_0) + at sim mathcalN(ln(x_0) + mu t (sigma t)^2)","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"In other words, the logarithm of the solution is normally distributed (so-called log-normal distribution).","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"We'd like to obtain this result numerically with the help of PCE. The first step is to define the (truncated) PCE for the random variable mathsfa","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"mathsfa = sum_i=0^L a_i phi_i","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"where a_i are the so-called PCE coefficients, and phi_i are the orthogonal basis polynomials. As the solution to the random differential equation is itself a random variable, we treat x(t) as the realization of the random variable mathsfx(t), and define its PCE","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"mathsfx(t) = sum_i=0^L x_i(t) phi_i","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"The question is how to obtain the unknown PCE coefficients x_i(t) from the known PCE coefficients a_i relative to the orthogonal basis polynomials phi_i. This can be done using Galerkin projection, which is nothing else than projecting onto the orthogonal basis. Think of a three-dimensional space, in which you have placed some three-dimensional object. If you know project the silhouett of the object onto every axis of the three-dimensional space, then you are doing a Galerkin projection. With PCE the concept is equivalent, but the imagination has a harder time. The first step for Galerkin projection is to insert the PCEs","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"sum_i=0^L dotx_i(t) phi_i = sum_j=0^L a_j phi_j sum_k=0^L x_k(t) phi_k","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"the second step is to project onto every basis polynomial phi_m for m = 0 1 dots L, and to exploit orthogonality of the basis. This gives","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"dotx_m(t) langle phi_m phi_m rangle = sum_j=0^L sum_k=0^L a_j x_k(t) langle phi_l phi_k phi_m rangle quad m = 0 1 dots L","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Of course, the initial condition must not be forgotten:","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"x_0(0) = x_0 quad x_m(0) = 0 quad m = 1 dots L","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"If we can solve this enlarged system of ordinary random differential equations, we can reconstruct the analytic solution.","category":"page"},{"location":"modules/PolyChaos/random_ode/#Practice","page":"Random ODE","title":"Practice","text":"","category":"section"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"We begin by defining the random differential equation","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"x0 = 2.0\nμ, σ = -0.5, 0.05\ntend, Δt = 3.0, 0.01","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Next, we define an orthogonal basis (and its quadrature rule) relative to the Gaussian measure using PolyChaos. We choose a maximum degree of L.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"using PolyChaos\nL, Nrec = 6, 40\nopq = GaussOrthoPoly(L; Nrec=Nrec, addQuadrature=true)","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Now we can define the PCE for mathsfa and solve the Galerkin-projected ordinary differential equation using DifferentialEquations.jl.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"using DifferentialEquations\n\na = [ convert2affinePCE(μ, σ, opq); zeros(Float64,L-1) ] # PCE coefficients of a\nxinit = [ x0; zeros(Float64,L) ] # PCE coefficients of initial condition\n\nt2 = Tensor(2, opq); # \\langle \\phi_i, \\phi_j \\rangle\nt3 = Tensor(3, opq); # \\langle \\phi_i \\phi_j, \\phi_k \\rangle\n\n# Galerkin-projected random differential equation\nfunction ODEgalerkin(du,u,p,t)\n   du[:] = [ sum( p[j+1]*u[k+1]*t3.get([j,k,m])/t2.get([m,m]) for j=0:L for k=0:L) for m=0:L ]\nend\n\nprobgalerkin = ODEProblem(ODEgalerkin,xinit,(0,tend),a)\nsolgalerkin = solve(probgalerkin;saveat=0:Δt:tend)\nt, x = solgalerkin.t, solgalerkin.u;","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"For later purposes we compute the expected value and the standard deviation at all time instants using PCE.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"# an advantage of PCE is that moments can be computed from the PCE coefficients alone; no sampling required\nmean_pce = [ mean(x_, opq) for x_ in x]  \nstd_pce = [ std(x_, opq) for x_ in x]","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"We compare the solution from PCE to a Monte-Carlo-based solution. That means to solve the ordinary differential equation for many samples of mathsfa. We first sample from the measure using sampleMeasure, and then generate samples of mathsfa using evaluatePCE. After that we solve the ODE and store the results in xmc.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"using Statistics\nNsmpl = 5000\nξ = sampleMeasure(Nsmpl, opq)     # sample from Gaussian measure; effectively randn() here    \nasmpl = evaluatePCE(a, ξ, opq)     # sample random variable with PCE coefficients a; effectively μ + σ*randn() here\n# or: asmpl = samplePCE(Nsmpl,a,opq)\nxmc = [ solve(ODEProblem((u,p,t)->aa*u,x0,(0,tend));saveat=0:Δt:tend).u for aa in asmpl]\nxmc = hcat(xmc...);","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Now we can compare the Monte Carlo mean and standard deviation to the expression from PCE for every time instant.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"[ mean(xmc,dims=2)-mean_pce std(xmc,dims=2)-std_pce]","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Clearly, the accuracy of PCE deteriorates over time. Possible remedies are to increase the dimension of PCE, and to tweak the tolerances of the integrator.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"Finally, we compare whether the samples follow a log-normal distribution, and compare the result to the analytic mean and standard deviation.","category":"page"},{"location":"modules/PolyChaos/random_ode/","page":"Random ODE","title":"Random ODE","text":"logx_pce = [ log.(evaluatePCE(x_,ξ,opq)) for x_ in x]\n[ mean.(logx_pce)-(log(x0) .+ μ*t) std.(logx_pce)-σ*t ]","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#DiffEqOperators","page":"DiffEqOperators","title":"DiffEqOperators","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"The AbstractDiffEqOperator interface is an interface for declaring parts of a differential equation as linear or affine. This then allows the solvers to exploit linearity to achieve maximal performance.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#Using-DiffEqOperators","page":"DiffEqOperators","title":"Using DiffEqOperators","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"AbstractDiffEqOperators act like functions. When defined, A has function calls A(u,p,t) and A(du,u,p,t) that act like A*u. These operators update via a function update_coefficients!(A,u,p,t).","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#Constructors","page":"DiffEqOperators","title":"Constructors","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#Wrapping-an-Array:-DiffEqArrayOperator","page":"DiffEqOperators","title":"Wrapping an Array: DiffEqArrayOperator","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"DiffEqArrayOperator is for defining an operator directly from an array. The operator is of the form:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"A(upt)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"for some scalar α and time plus possibly state dependent A. The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"DiffEqArrayOperator(A::AbstractMatrix{T},update_func = DEFAULT_UPDATE_FUNC)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"A is the operator array. update_func(A,u,p,t) is the function called by  update_coefficients!(A,u,p,t). If left as its default, then update_func  is trivial which signifies A is a constant.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#AffineDiffEqOperator","page":"DiffEqOperators","title":"AffineDiffEqOperator","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"For As = (A1,A2,...,An) and Bs = (B1,B2,...,Bm) where each of the Ai and Bi are AbstractDiffEqOperators, the following constructor:","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"function AffineDiffEqOperator{T}(As,Bs,u_cache=nothing)","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"builds an operator L = (A1 + A2 + ... An)*u + B1 + B2 + ... + Bm. u_cache is for designating a type of internal cache for non-allocating evaluation of L(du,u,p,t). If not given, the function L(du,u,p,t) is not available. Note that in solves which exploit this structure, this function call is not necessary. It's only used as the fallback in ODE solvers which were not developed for this structure.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#Formal-Properties-of-DiffEqOperators","page":"DiffEqOperators","title":"Formal Properties of DiffEqOperators","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"These are the formal properties that an AbstractDiffEqOperator should obey for it to work in the solvers.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#AbstractDiffEqOperator-Interface-Description","page":"DiffEqOperators","title":"AbstractDiffEqOperator Interface Description","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"Function call and multiplication: L(du,u,p,t) for inplace and du = L(u,p,t) for out-of-place, meaning L*u and mul!.\nIf the operator is not a constant, update it with (u,p,t). A mutating form, i.e. update_coefficients!(A,u,p,t) that changes the internal coefficients, and a out-of-place form B = update_coefficients(A,u,p,t).\nisconstant(A) trait for whether the operator is constant or not.","category":"page"},{"location":"modules/DiffEqDocs/features/diffeq_operator/#AbstractDiffEqLinearOperator-Interface-Description","page":"DiffEqOperators","title":"AbstractDiffEqLinearOperator Interface Description","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"AbstractDiffEqLinearOperator <: AbstractDiffEqOperator\nCan absorb under multiplication by a scalar. In all algorithms things like dt*L show up all the time, so the linear operator must be able to absorb such constants.\nisconstant(A) trait for whether the operator is constant or not.\nOptional: diagonal, symmetric, etc traits from LinearMaps.jl.\nOptional: exp(A). Required for simple exponential integration.\nOptional: expv(A,u,t) = exp(t*A)*u and expv!(v,A::DiffEqOperator,u,t) Required for sparse-saving exponential integration.\nOptional: factorizations. ldiv!, factorize et. al. This is only required for algorithms which use the factorization of the operator (Crank-Nicolson), and only for when the default linear solve is used.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/#Global-Optimization-via-NLopt","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"The build_loss_objective function builds an objective function which is able to be used with MathOptInterface-associated solvers. This includes packages like IPOPT, NLopt, MOSEK, etc. Building off of the previous example, we can build a cost function for the single parameter optimization problem like:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"function f(du,u,p,t)\n  dx = p[1]*u[1] - u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())\n\nt = collect(range(0,stop=10,length=200))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\ndata = convert(Array,randomized)\n\nobj = build_loss_objective(prob,Tsit5(),L2Loss(t,data),maxiters=10000)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can now use this obj as the objective function with MathProgBase solvers. For our example, we will use NLopt. To use the local derivative-free Constrained Optimization BY Linear Approximations algorithm, we can simply do:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_COBYLA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"This finds a minimum at [1.49997]. For a modified evolutionary algorithm, we can use:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ESCH, 1)\nmin_objective!(opt, obj)\nlower_bounds!(opt,[0.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can even use things like the Improved Stochastic Ranking Evolution Strategy (and add constraints if needed). This is done via:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ISRES, 1)\nmin_objective!(opt, obj.cost_function2)\nlower_bounds!(opt,[-1.0])\nupper_bounds!(opt,[5.0])\nxtol_rel!(opt,1e-3)\nmaxeval!(opt, 100000)\n(minf,minx,ret) = NLopt.optimize(opt,[0.2])","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"which is very robust to the initial condition. The fastest result comes from the following:","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using NLopt\nopt = Opt(:LN_BOBYQA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"For more information, see the NLopt documentation for more details. And give IPOPT or MOSEK a try!","category":"page"},{"location":"modules/DiffEqOperators/operators/matrix_free_operators/#Matrix-Free-Operators","page":"Matrix-Free Operators","title":"Matrix-Free Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/matrix_free_operators/","page":"Matrix-Free Operators","title":"Matrix-Free Operators","text":"MatrixFreeOperator(f::F, args::N;\n                   size=nothing, opnorm=true, ishermitian=false) where {F,N}","category":"page"},{"location":"modules/DiffEqOperators/operators/matrix_free_operators/","page":"Matrix-Free Operators","title":"Matrix-Free Operators","text":"A MatrixFreeOperator is a linear operator A*u where the action of A is explicitly defined by an in-place function f(du, u, p, t).","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/#Delta-Moment-Independent-Method","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"struct DeltaMoment{T} <: GSAMethod\n    nboot::Int\n    conf_level::Float64\n    Ygrid_length::Int\n    num_classes::T\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"DeltaMoment has the following keyword arguments:","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"nboot: number of bootstrap repetions. Defaults to 500.\nconf_level: the level used for confidence interval calculation with bootstrap. Default value of 0.95.\nYgrid_length: number of quadrature points to consider when performing the kernel density estimation and the integration steps. Should be a power of 2 for efficient FFT in kernel density estimates. Defaults to 2048.\nnum_classes: Determine how many classes to split each factor into to when generating distributions of model output conditioned on class.","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/#Method-Details","page":"Delta Moment-Independent Method","title":"Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"The Delta moment-independent method relies on new estimators for  density-based statistics.  It allows for the estimation of both  distribution-based sensitivity measures and of sensitivity measures that  look at contributions to a specific moment. One of the primary advantage  of this method is the independence of computation cost from the number of  parameters.","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"note: Note\nDeltaMoment only works for scalar output.","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/#API","page":"Delta Moment-Independent Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"function gsa(f, method::DeltaMoment, p_range; N, batch = false, rng::AbstractRNG = Random.default_rng(), kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/delta/#Example","page":"Delta Moment-Independent Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/delta/","page":"Delta Moment-Independent Method","title":"Delta Moment-Independent Method","text":"using GlobalSensitivity, Test\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nm = gsa(ishi,DeltaMoment(),fill([lb[1], ub[1]], 3), N=1000)","category":"page"},{"location":"modules/ModelingToolkit/#ModelingToolkit.jl:-High-Performance-Symbolic-Numeric-Equation-Based-Modeling","page":"Home","title":"ModelingToolkit.jl: High-Performance Symbolic-Numeric Equation-Based Modeling","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"ModelingToolkit.jl is a modeling language for high-performance symbolic-numeric computation in scientific computing and scientific machine learning. It then mixes ideas from symbolic computational algebra systems with causal and acausal equation-based modeling frameworks to give an extendable and parallel modeling system. It allows for users to give a high-level description of a model for symbolic preprocessing to analyze and enhance the model. Automatic transformations, such as index reduction, can be applied to the model before solving in order to make it easily handle equations would could not be solved when modeled without symbolic intervention.","category":"page"},{"location":"modules/ModelingToolkit/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"To install ModelingToolkit.jl, use the Julia package manager:","category":"page"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"ModelingToolkit\")","category":"page"},{"location":"modules/ModelingToolkit/#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"If you use ModelingToolkit in your work, please cite the following:","category":"page"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"@misc{ma2021modelingtoolkit,\n      title={ModelingToolkit: A Composable Graph Transformation System For Equation-Based Modeling},\n      author={Yingbo Ma and Shashi Gowda and Ranjan Anantharaman and Chris Laughman and Viral Shah and Chris Rackauckas},\n      year={2021},\n      eprint={2103.05244},\n      archivePrefix={arXiv},\n      primaryClass={cs.MS}\n}","category":"page"},{"location":"modules/ModelingToolkit/#Feature-Summary","page":"Home","title":"Feature Summary","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"ModelingToolkit.jl is a symbolic-numeric modeling package. Thus it combines some of the features from symbolic computing packages like SymPy or Mathematica with the ideas of equation-based modeling systems like the causal Simulink and the acausal Modelica. It bridges the gap between many different kinds of equations, allowing one to quickly and easily transform systems of DAEs into optimization problems, or vice-versa, and then simplify and parallelize the resulting expressions before generating code.","category":"page"},{"location":"modules/ModelingToolkit/#Feature-List","page":"Home","title":"Feature List","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"Causal and acausal modeling (Simulink/Modelica)\nAutomated model transformation, simplification, and composition\nAutomatic conversion of numerical models into symbolic models\nComposition of models through the components, a lazy connection system, and tools for expanding/flattening\nPervasive parallelism in symbolic computations and generated functions\nTransformations like alias elimination and tearing of nonlinear systems for efficiently numerically handling large-scale systems of equations\nThe ability to use the entire Symbolics.jl Computer Algebra System (CAS) as part of the modeling process.\nImport models from common formats like SBML, CellML, BioNetGen, and more.\nExtendability: the whole system is written in pure Julia, so adding new functions, simplification rules, and model transformations has no barrier.","category":"page"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"For information on how to use the Symbolics.jl CAS system that ModelingToolkit.jl is built on, consult the Symbolics.jl documentation","category":"page"},{"location":"modules/ModelingToolkit/#Equation-Types","page":"Home","title":"Equation Types","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"Ordinary differential equations\nStochastic differential equations\nPartial differential equations\nNonlinear systems\nOptimization problems\nContinuous-Time Markov Chains\nChemical Reactions (via Catalyst.jl)\nNonlinear Optimal Control","category":"page"},{"location":"modules/ModelingToolkit/#Standard-Library","page":"Home","title":"Standard Library","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"For quick development, ModelingToolkit.jl includes  ModelingToolkitStandardLibrary.jl, a standard library of prebuilt components for the ModelingToolkit ecosystem.","category":"page"},{"location":"modules/ModelingToolkit/#Model-Import-Formats","page":"Home","title":"Model Import Formats","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"CellMLToolkit.jl: Import CellML models into ModelingToolkit\nRepository of more than a thousand pre-made models\nFocus on biomedical models in areas such as: Calcium Dynamics, Cardiovascular Circulation, Cell Cycle, Cell Migration, Circadian Rhythms, Electrophysiology, Endocrine, Excitation-Contraction Coupling, Gene Regulation, Hepatology, Immunology, Ion Transport, Mechanical Constitutive Laws, Metabolism, Myofilament Mechanics, Neurobiology, pH Regulation, PKPD, Protein Modules, Signal Transduction, and Synthetic Biology.\nSBMLToolkit.jl: Import SBML models into ModelingToolkit\nUses the robust libsbml library for parsing and transforming the SBML\nReactionNetworkImporters.jl: Import various models into ModelingToolkit\nSupports the BioNetGen .net file\nSupports importing networks specified by stoichiometric matrices","category":"page"},{"location":"modules/ModelingToolkit/#Extension-Libraries","page":"Home","title":"Extension Libraries","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"Because ModelingToolkit.jl is the core foundation of a equation-based modeling ecosystem, there is a large set of libraries adding features to this system. Below is an incomplete list of extension libraries one may want to be aware of:","category":"page"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"Catalyst.jl: Symbolic representations of chemical reactions\nSymbolically build and represent large systems of chemical reactions\nGenerate code for ODEs, SDEs, continuous-time Markov Chains, and more\nSimulate the models using the SciML ecosystem with O(1) Gillespie methods\nDataDrivenDiffEq.jl: Automatic identification of equations from data\nAutomated construction of ODEs and DAEs from data\nRepresentations of Koopman operators and Dynamic Mode Decomposition (DMD)\nMomentClosure.jl: Automatic transformation of ReactionSystems into deterministic systems\nGenerates ODESystems for the moment closures\nAllows for geometrically-distributed random reaction rates\nReactionMechanismSimulator.jl: Simulating and analyzing large chemical reaction mechanisms\nIdeal gas and dilute liquid phases.\nConstant T and P and constant V adiabatic ideal gas reactors.\nConstant T and V dilute liquid reactors.\nDiffusion limited rates. Sensitivity analysis for all reactors.\nFlux diagrams with molecular images (if molecular information is provided).\nNumCME.jl: High-performance simulation of chemical master equations (CME)\nTransient solution of the CME\nDynamic state spaces\nAccepts reaction systems defined using Catalyst.jl DSL.\nFiniteStateProjection.jl: High-performance simulation of  chemical master equations (CME) via finite state projections\nAccepts reaction systems defined using Catalyst.jl DSL.","category":"page"},{"location":"modules/ModelingToolkit/#Compatible-Numerical-Solvers","page":"Home","title":"Compatible Numerical Solvers","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"All of the symbolic systems have a direct conversion to a numerical system which can then be handled through the SciML interfaces. For example, after building a model and performing symbolic manipulations, an ODESystem can be converted into an ODEProblem to then be solved by a numerical ODE solver. Below is a list of the solver libraries which are the numerical targets of the ModelingToolkit system:","category":"page"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"DifferentialEquations.jl\nMulti-package interface of high performance numerical solvers for ODESystem, SDESystem, and JumpSystem\nNonlinearSolve.jl\nHigh performance numerical solving of NonlinearSystem\nOptimization.jl\nMulti-package interface for numerical solving OptimizationSystem\nNeuralPDE.jl\nPhysics-Informed Neural Network (PINN) training on PDESystem\nMethodOfLines.jl\nAutomated finite difference method (FDM) discretization of PDESystem","category":"page"},{"location":"modules/ModelingToolkit/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/ModelingToolkit/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nThe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums (look for the modelingtoolkit tag\nSee also SciML Community page","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/#CMAEvolutionStrategy.jl","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"CMAEvolutionStrategy is a Julia package implementing the Covariance Matrix Adaptation Evolution Strategy algorithm. ","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"The CMAEvolutionStrategy algorithm is called by CMAEvolutionStrategyOpt()","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/#Installation:-OptimizationCMAEvolutionStrategy.jl","page":"CMAEvolutionStrategy.jl","title":"Installation: OptimizationCMAEvolutionStrategy.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"To use this package, install the OptimizationCMAEvolutionStrategy package:","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"import Pkg; Pkg.add(\"OptimizationCMAEvolutionStrategy\")","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/#Global-Optimizer","page":"CMAEvolutionStrategy.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/#Without-Constraint-Equations","page":"CMAEvolutionStrategy.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"The method in CMAEvolutionStrategy is performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/#Example","page":"CMAEvolutionStrategy.jl","title":"Example","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"The Rosenbrock function can optimized using the CMAEvolutionStrategyOpt() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/cmaevolutionstrategy/","page":"CMAEvolutionStrategy.jl","title":"CMAEvolutionStrategy.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, CMAEvolutionStrategyOpt())","category":"page"},{"location":"modules/Catalyst/tutorials/advanced_examples/#Advanced-Chemical-Reaction-Network-Examples","page":"Advanced Chemical Reaction Network Examples","title":"Advanced Chemical Reaction Network Examples","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/advanced_examples/","page":"Advanced Chemical Reaction Network Examples","title":"Advanced Chemical Reaction Network Examples","text":"For additional flexibility, we can convert the generated ReactionSystem first to another ModelingToolkit.AbstractSystem, e.g., an ODESystem, SDESystem, JumpSystem, etc. These systems can then be used in problem generation. Please also see the ModelingToolkit docs, which give many options for optimized problem generation (i.e., generating dense or sparse Jacobians with or without threading and/or parallelization), creating LaTeX representations for systems, etc.","category":"page"},{"location":"modules/Catalyst/tutorials/advanced_examples/","page":"Advanced Chemical Reaction Network Examples","title":"Advanced Chemical Reaction Network Examples","text":"Note, when generating problems from other system types, u0 and p must provide vectors, tuples or dictionaries of Pairs that map each the symbolic variables for each species or parameter to their numerical value. E.g., for the Michaelis-Menten example above we'd use","category":"page"},{"location":"modules/Catalyst/tutorials/advanced_examples/","page":"Advanced Chemical Reaction Network Examples","title":"Advanced Chemical Reaction Network Examples","text":"rs = @reaction_network begin\n  c1, X --> 2X\n  c2, X --> 0\n  c3, 0 --> X\nend c1 c2 c3\np     = (:c1 => 1.0, :c2 => 2.0, :c3 => 50.)\npmap  = symmap_to_varmap(rs,p)   # convert Symbol map to symbolic variable map\ntspan = (0.,4.)\nu0    = [:X => 5.]   \nu0map = symmap_to_varmap(rs,u0)  # convert Symbol map to symbolic variable map\nosys  = convert(ODESystem, rs)\noprob = ODEProblem(osys, u0map, tspan, pmap)\nsol   = solve(oprob, Tsit5())","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/#sensitivity_math","page":"Sensitivity Math Details","title":"Mathematics of Sensitivity Analysis","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sensitivity_math/#Forward-Sensitivity-Analysis","page":"Sensitivity Math Details","title":"Forward Sensitivity Analysis","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"The local sensitivity is computed using the sensitivity ODE:","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"fracddtfracpartial upartial p_j=fracpartial fpartial ufracpartial upartial p_j+fracpartial fpartial p_j=Jcdot S_j+F_j","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"where","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"J=left(beginarraycccc\nfracpartial f_1partial u_1  fracpartial f_1partial u_2  cdots  fracpartial f_1partial u_k\nfracpartial f_2partial u_1  fracpartial f_2partial u_2  cdots  fracpartial f_2partial u_k\ncdots  cdots  cdots  cdots\nfracpartial f_kpartial u_1  fracpartial f_kpartial u_2  cdots  fracpartial f_kpartial u_k\nendarrayright)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"is the Jacobian of the system,","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"F_j=left(beginarrayc\nfracpartial f_1partial p_j\nfracpartial f_2partial p_j\nvdots\nfracpartial f_kpartial p_j\nendarrayright)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"are the parameter derivatives, and","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"S_j=left(beginarrayc\nfracpartial u_1partial p_j\nfracpartial u_2partial p_j\nvdots\nfracpartial u_kpartial p_j\nendarrayright)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"is the vector of sensitivities. Since this ODE is dependent on the values of the independent variables themselves, this ODE is computed simultaneously with the actual ODE system.","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"Note that the Jacobian-vector product","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"fracpartial fpartial ufracpartial upartial p_j","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"can be computed without forming the Jacobian. With finite differences, this through using the following formula for the directional derivative","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"Jv approx fracf(x+v epsilon) - f(x)epsilon","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"or, alternatively and without truncation error, by using a dual number with a single partial dimension, d = x + v epsilon we get that","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"f(d) = f(x) + Jv epsilon","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"as a fast way to calcuate Jv. Thus, except when a sufficiently good function for J is given by the user, the Jacobian is never formed. For more details, consult the MIT 18.337 lecture notes on forward mode AD.","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/#Adjoint-Sensitivity-Analysis","page":"Sensitivity Math Details","title":"Adjoint Sensitivity Analysis","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"This adjoint requires the definition of some scalar functional g(up) where u(tp) is the (numerical) solution to the differential equation ddt u(tp)=f(tup) with tin 0T and u(t_0p)=u_0. Adjoint sensitivity analysis finds the gradient of","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"G(up)=G(u(cdotp))=int_t_0^Tg(u(tp)p)dt","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"some integral of the solution. It does so by solving the adjoint problem","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"fracdlambda^stardt=g_u(u(tp)p)-lambda^star(t)f_u(tu(tp)p)thinspacethinspacethinspacelambda^star(T)=0","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"where f_u is the Jacobian of the system with respect to the state u while f_p is the Jacobian with respect to the parameters. The adjoint problem's solution gives the sensitivities through the integral:","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"fracdGdp=int_t_0^Tlambda^star(t)f_p(t)+g_p(t)dt+lambda^star(t_0)u_p(t_0)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"Notice that since the adjoints require the Jacobian of the system at the state, it requires the ability to evaluate the state at any point in time. Thus it requires the continuous forward solution in order to solve the adjoint solution, and the adjoint solution is required to be continuous in order to calculate the resulting integral.","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"There is one extra detail to consider. In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution. One canonical function is the L2 loss against some data points, that is:","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"L(up)=sum_i=1^nVerttildeu(t_i)-u(t_ip)Vert^2","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"In this case, we can reinterpret our summation as the distribution integral:","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"G(up)=int_0^Tsum_i=1^nVerttildeu(t_i)-u(t_ip)Vert^2delta(t_i-t)dt","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"where δ is the Dirac distribution. In this case, the integral is continuous except at finitely many points. Thus it can be calculated between each t_i. At a given t_i, given that the t_i are unique, we have that","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"g_u(t_i)=2left(tildeu(t_i)-u(t_ip)right)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"Thus the adjoint solution lambda^star(t) is given by integrating between the integrals and applying the jump function g_u at every data point t_i.","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"We note that","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"lambda^star(t)f_u(t)","category":"page"},{"location":"modules/SciMLSensitivity/sensitivity_math/","page":"Sensitivity Math Details","title":"Sensitivity Math Details","text":"is a vector-transpose Jacobian product, also known as a vjp, which can be efficiently computed using the pullback of backpropogation on the user function f with a forward pass at u with a pullback vector lambda^star. For more information, consult the MIT 18.337 lecture notes on reverse mode AD","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/#QuadDIRECT.jl","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"QuadDIRECT is a Julia package implementing QuadDIRECT algorithm (inspired by DIRECT and MCS). ","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"The QuadDIRECT algorithm is called using QuadDirect(). ","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/#Installation:-OptimizationQuadDIRECT.jl","page":"QuadDIRECT.jl","title":"Installation: OptimizationQuadDIRECT.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"To use this package, install the OptimizationQuadDIRECT package:","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"import Pkg; Pkg.add(\"OptimizationQuadDIRECT\")","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"Also note that QuadDIRECT should (for now) be installed by doing:","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"] add https://github.com/timholy/QuadDIRECT.jl.git","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/#Global-Optimizer","page":"QuadDIRECT.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/quaddirect/#Without-Constraint-Equations","page":"QuadDIRECT.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"The algorithm in QuadDIRECT is performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"Furthermore, QuadDirect requires splits which is a list of 3-vectors with initial locations at which to evaluate the function (the values must be in strictly increasing order and lie within the specified bounds) such that solve(problem, QuadDirect(), splits).","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/#Example","page":"QuadDIRECT.jl","title":"Example","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"The Rosenbrock function can optimized using the QuadDirect() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/quaddirect/","page":"QuadDIRECT.jl","title":"QuadDIRECT.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsolve(prob, QuadDirect(), splits = ([-0.9, 0, 0.9], [-0.8, 0, 0.8]))","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/#Evolutionary.jl","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"Evolutionary is a Julia package implementing various evolutionary and genetic algorithm.","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/#Installation:-OptimizationCMAEvolutionStrategy.jl","page":"Evolutionary.jl","title":"Installation: OptimizationCMAEvolutionStrategy.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"To use this package, install the OptimizationCMAEvolutionStrategy package:","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"import Pkg; Pkg.add(\"OptimizationCMAEvolutionStrategy\")","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/#Global-Optimizer","page":"Evolutionary.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/evolutionary/#Without-Constraint-Equations","page":"Evolutionary.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"The methods in Evolutionary are performing global optimization on problems without constraint equations. These methods work both with and without lower and upper constraints set by lb and ub in the OptimizationProblem.","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"A Evolutionary algorithm is called by one of the following:","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"Evolutionary.GA(): Genetic Algorithm optimizer\nEvolutionary.DE(): Differential Evolution optimizer\nEvolutionary.ES(): Evolution Strategy algorithm\nEvolutionary.CMAES(): Covariance Matrix Adaptation Evolution Strategy algorithm","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"Algorithm specific options are defined as kwargs. See the respective documentation for more detail.","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/#Example","page":"Evolutionary.jl","title":"Example","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"The Rosenbrock function can optimized using the Evolutionary.CMAES() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/evolutionary/","page":"Evolutionary.jl","title":"Evolutionary.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, Evolutionary.CMAES(μ =40 , λ = 100))","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#NLopt.jl","page":"NLopt.jl","title":"NLopt.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt is Julia package interfacing to the free/open-source NLopt library which implements many optimization methods both global and local NLopt Documentation.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#Installation:-OptimizationNLopt.jl","page":"NLopt.jl","title":"Installation: OptimizationNLopt.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"To use this package, install the OptimizationNLopt package:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"import Pkg; Pkg.add(\"OptimizationNLopt\")","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#Methods","page":"NLopt.jl","title":"Methods","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.jl algorithms are chosen either via NLopt.Opt(:algname, nstates) where nstates is the number of states to be optimized but preferably via NLopt.AlgorithmName() where `AlgorithmName can be one of the following:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.GN_DIRECT()\nNLopt.GN_DIRECT_L()\nNLopt.GN_DIRECT_L_RAND()\nNLopt.GN_DIRECT_NOSCAL()\nNLopt.GN_DIRECT_L_NOSCAL()\nNLopt.GN_DIRECT_L_RAND_NOSCAL()\nNLopt.GN_ORIG_DIRECT()\nNLopt.GN_ORIG_DIRECT_L()\nNLopt.GD_STOGO()\nNLopt.GD_STOGO_RAND()\nNLopt.LD_LBFGS_NOCEDAL()\nNLopt.LD_LBFGS()\nNLopt.LN_PRAXIS()\nNLopt.LD_VAR1()\nNLopt.LD_VAR2()\nNLopt.LD_TNEWTON()\nNLopt.LD_TNEWTON_RESTART()\nNLopt.LD_TNEWTON_PRECOND()\nNLopt.LD_TNEWTON_PRECOND_RESTART()\nNLopt.GN_CRS2_LM()\nNLopt.GN_MLSL()\nNLopt.GD_MLSL()\nNLopt.GN_MLSL_LDS()\nNLopt.GD_MLSL_LDS()\nNLopt.LD_MMA()\nNLopt.LN_COBYLA()\nNLopt.LN_NEWUOA()\nNLopt.LN_NEWUOA_BOUND()\nNLopt.LN_NELDERMEAD()\nNLopt.LN_SBPLX()\nNLopt.LN_AUGLAG()\nNLopt.LD_AUGLAG()\nNLopt.LN_AUGLAG_EQ()\nNLopt.LD_AUGLAG_EQ()\nNLopt.LN_BOBYQA()\nNLopt.GN_ISRES()\nNLopt.AUGLAG()\nNLopt.AUGLAG_EQ()\nNLopt.G_MLSL()\nNLopt.G_MLSL_LDS()\nNLopt.LD_SLSQP()\nNLopt.LD_CCSAQ()\nNLopt.GN_ESCH()\nNLopt.GN_AGS()","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"See the NLopt Documentation for more details on each optimizer.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"Beyond the common arguments the following optimizer parameters can be set as kwargs:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"stopval\nxtol_rel\nxtol_abs\nconstrtol_abs\ninitial_step\npopulation\nvector_storage","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#Local-Optimizer","page":"NLopt.jl","title":"Local Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/#Derivative-Free","page":"NLopt.jl","title":"Derivative-Free","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"Derivative-free optimizers are optimizers that can be used even in cases where no derivatives or automatic differentiation is specified. While they tend to be less efficient than derivative-based optimizers, they can be easily applied to cases where defining derivatives is difficult. Note that while these methods do not support general constraints, all support bounds constraints via lb and ub in the OptimizationProblem.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt derivative-free optimizers are:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.LN_PRAXIS()\nNLopt.LN_COBYLA()\nNLopt.LN_NEWUOA()\nNLopt.LN_NEWUOA_BOUND()\nNLopt.LN_NELDERMEAD()\nNLopt.LN_SBPLX()\nNLopt.LN_AUGLAG()\nNLopt.LN_AUGLAG_EQ()\nNLopt.LN_BOBYQA()","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The Rosenbrock function can optimized using the NLopt.LN_NELDERMEAD() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, NLopt.LN_NELDERMEAD())","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#Gradient-Based","page":"NLopt.jl","title":"Gradient-Based","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"Gradient-based optimizers are optimizers which utilise the gradient information based on derivatives defined or automatic differentiation.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt gradient-based optimizers are:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.LD_LBFGS_NOCEDAL()\nNLopt.LD_LBFGS()\nNLopt.LD_VAR1()\nNLopt.LD_VAR2()\nNLopt.LD_TNEWTON()\nNLopt.LD_TNEWTON_RESTART()\nNLopt.LD_TNEWTON_PRECOND()\nNLopt.LD_TNEWTON_PRECOND_RESTART()\nNLopt.LD_MMA()\nNLopt.LD_AUGLAG()\nNLopt.LD_AUGLAG_EQ()\nNLopt.LD_SLSQP()\nNLopt.LD_CCSAQ()","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The Rosenbrock function can optimized using NLopt.LD_LBFGS() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, NLopt.LD_LBFGS())","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#Global-Optimizer","page":"NLopt.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/#Without-Constraint-Equations","page":"NLopt.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The following algorithms in NLopt are performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt global optimizers which fall into this category are:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.GN_DIRECT()\nNLopt.GN_DIRECT_L()\nNLopt.GN_DIRECT_L_RAND()\nNLopt.GN_DIRECT_NOSCAL()\nNLopt.GN_DIRECT_L_NOSCAL()\nNLopt.GN_DIRECT_L_RAND_NOSCAL()\nNLopt.GD_STOGO()\nNLopt.GD_STOGO_RAND()\nNLopt.GN_CRS2_LM()\nNLopt.GN_MLSL()\nNLopt.GD_MLSL()\nNLopt.GN_MLSL_LDS()\nNLopt.GD_MLSL_LDS()\nNLopt.G_MLSL()\nNLopt.G_MLSL_LDS()\nNLopt.GN_ESCH()","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The Rosenbrock function can optimized using NLopt.GN_DIRECT() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, NLopt.GN_DIRECT())","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"Algorithms such as NLopt.G_MLSL() or NLopt.G_MLSL_LDS() also require a local optimiser to be selected which via the local_method argument of solve.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The Rosenbrock function can optimized using NLopt.G_MLSL_LDS() with NLopt.LN_NELDERMEAD() as the local optimizer. The local optimizer maximum iterations are set via local_maxiters:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, NLopt.G_MLSL_LDS(), local_method = NLopt.LD_LBFGS(), local_maxiters=10000)","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/#With-Constraint-Equations","page":"NLopt.jl","title":"With Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"The following algorithms in NLopt are performing global optimization on problems with constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"note: Constraints with NLopt\nEquality and inequality equation support for NLopt via Optimization is not supported directly. However, you can use the MOI wrapper to use constraints with NLopt optimisers.","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt global optimizers which fall into this category are:","category":"page"},{"location":"modules/Optimization/optimization_packages/nlopt/","page":"NLopt.jl","title":"NLopt.jl","text":"NLopt.GN_ORIG_DIRECT()\nNLopt.GN_ORIG_DIRECT_L()\nNLopt.GN_ISRES()\nNLopt.GN_AGS()","category":"page"},{"location":"modules/Integrals/basics/IntegralProblem/#Integral-Problems","page":"Integral Problems","title":"Integral Problems","text":"","category":"section"},{"location":"modules/Integrals/basics/IntegralProblem/","page":"Integral Problems","title":"Integral Problems","text":"IntegralProblem","category":"page"},{"location":"modules/Integrals/basics/IntegralProblem/#SciMLBase.IntegralProblem","page":"Integral Problems","title":"SciMLBase.IntegralProblem","text":"Defines an integral problem. Documentation Page: https://github.com/SciML/Integrals.jl\n\nMathematical Specification of a Integral Problem\n\nIntegral problems are multi-dimensional integrals defined as:\n\nint_lb^ub f(up) du\n\nwhere p are parameters. u is a Number or AbstractArray whose geometry matches the space being integrated.\n\nProblem Type\n\nConstructors\n\nIntegralProblem{iip}(f,lb,ub,p=NullParameters();                   nout=1, batch = 0, kwargs...)\n\nf: the integrand, dx=f(x,p) for out-of-place or f(dx,x,p) for in-place.\nlb: Either a number or vector of lower bounds.\nub: Either a number or vector of upper bounds.\np: The parameters associated with the problem.\nnout: The output size of the function f. Defaults to 1, i.e., a scalar integral output.\nbatch: The preferred number of points to batch. This allows user-side parallelization of the integrand. If batch != 0, then each x[:,i] is a different point of the integral to calculate, and the output should be nout x batchsize. Note that batch is a suggestion for the number of points, and it is not necessarily true that batch is the same as batchsize in all algorithms.\nkwargs:: Keyword arguments copied to the solvers.\n\nAdditionally, we can supply iip like IntegralProblem{iip}(...) as true or false to declare at compile time whether the integrator function is in-place.\n\nFields\n\nThe fields match the names of the constructor arguments.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/#Steady-State-Solvers","page":"Steady State Solvers","title":"Steady State Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"solve(prob::SteadyStateProblem,alg;kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Solves for the steady states in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/#Recommended-Methods","page":"Steady State Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"DynamicSS is a good choice if you think you may have multiple steady states or a bad initial guess. SSRootfind can be faster if you have a good initial guess. For DynamicSS, in many cases an adaptive stiff solver, like a Rosenbrock or BDF method (Rodas5 or QNDF), is a good way to allow for very large time steps as the steady state approaches.","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Note that if you use CVODE_BDF you may need to give a starting dt via dt=.....","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/#Full-List-of-Methods","page":"Steady State Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/#SteadyStateDiffEq.jl","page":"Steady State Solvers","title":"SteadyStateDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"SSRootfind : Uses a rootfinding algorithm to find a steady state. Defaults to using NLsolve.jl. A different algorithm can be specified via the nlsolve keyword argument.\nDynamicSS : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state. DynamicSS(alg;abstol=1e-8,reltol=1e-6,tspan=Inf) requires that an ODE algorithm is given as the first argument.  The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero.  This internally uses the TerminateSteadyState callback from the Callback Library.  The simulated time for which given ODE is solved can be limited by tspan.  If tspan is a number, it is equivalent to passing (zero(tspan), tspan).","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Example usage:","category":"page"},{"location":"modules/DiffEqDocs/solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"sol = solve(prob,SSRootfind())\nsol = solve(prob,DynamicSS(Tsit5()))\nusing Sundials\nsol = solve(prob,DynamicSS(CVODE_BDF()),dt=1.0)","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/#Training-a-Neural-Ordinary-Differential-Equation-with-Mini-Batching","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"using DifferentialEquations, Flux, Random, Plots\nusing IterTools: ncycle \n\nrng = Random.default_rng()\n\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k*(temp-temp_m) \n  end\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2)/8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend\n\n\nann = Chain(Dense(1,8,tanh), Dense(8,1,tanh))\nθ, re = Flux.destructure(ann)\n\nfunction dudt_(u,p,t)           \n    re(p)(u)[1].* u\nend\n\nfunction predict_adjoint(time_batch)\n    _prob = remake(prob,u0=u0,p=θ)\n    Array(solve(_prob, Tsit5(), saveat = time_batch)) \nend\n\nfunction loss_adjoint(batch, time_batch)\n    pred = predict_adjoint(time_batch)\n    sum(abs2, batch - pred)#, pred\nend\n\n\nu0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 3.0f0)\n\nt = range(tspan[1], tspan[2], length=datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat=t))\n\nprob = ODEProblem{false}(dudt_, u0, tspan, θ)\n\nk = 10\ntrain_loader = Flux.Data.DataLoader((ode_data, t), batchsize = k)\n\nfor (x, y) in train_loader\n    @show x\n    @show y\nend\n\nnumEpochs = 300\nlosses=[]\ncb() = begin\n    l=loss_adjoint(ode_data, t)\n    push!(losses, l)\n    @show l\n    pred=predict_adjoint(t)\n    pl = scatter(t,ode_data[1,:],label=\"data\", color=:black, ylim=(150,200))\n    scatter!(pl,t,pred[1,:],label=\"prediction\", color=:darkgreen)\n    display(plot(pl))\n    false\nend \n\nopt=ADAM(0.05)\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader,numEpochs), opt, cb=Flux.throttle(cb, 10))\n\n#Now lets see how well it generalizes to new initial conditions \n\nstarting_temp=collect(10:30:250)\ntrue_prob_func(u0)=ODEProblem(true_sol, [u0], tspan)\ncolor_cycle=palette(:tab10)\npl=plot()\nfor (j,temp) in enumerate(starting_temp)\n    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0,10.0f0)), Tsit5(), saveat=0.0:0.5:10.0)\n    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0,10.0f0), θ))\n    scatter!(pl, ode_test_sol, var=(0,1), label=\"\", color=color_cycle[j])\n    plot!(pl, ode_nn_sol, var=(0,1), label=\"\", color=color_cycle[j], lw=2.0)\nend\ndisplay(pl) \ntitle!(\"Neural ODE for Newton's Law of Cooling: Test Data\")\nxlabel!(\"Time\")\nylabel!(\"Temp\") ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"When training a neural network we need to find the gradient with respect to our data set. There are three main ways to partition our data when using a training algorithm like gradient descent: stochastic, batching and mini-batching. Stochastic gradient descent trains on a single random data point each epoch. This allows for the neural network to better converge to the global minimum even on noisy data but is computationally inefficient. Batch gradient descent trains on the whole data set each epoch and while computationally efficient is prone to converging to local minima. Mini-batching combines both of these advantages and by training on a small random \"mini-batch\" of the data each epoch can converge to the global minimum while remaining more computationally efficient than stochastic descent. Typically we do this by randomly selecting subsets of the data each epoch and use this subset to train on. We can also pre-batch the data by creating an iterator holding these randomly selected batches before beginning to train. The proper size for the batch can be determined experimentally. Let us see how to do this with Julia. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"For this example we will use a very simple ordinary differential equation, newtons law of cooling. We can represent this in Julia like so. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"using DifferentialEquations, Flux, Random, Plots\nusing IterTools: ncycle \n\nrng = Random.default_rng()\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k*(temp-temp_m) \n  end\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2)/8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Now we define a neural-network using a linear approximation with 1 hidden layer of 8 neurons.  ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"ann = Chain(Dense(1,8,tanh), Dense(8,1,tanh))\nθ, re = Flux.destructure(ann)\n\nfunction dudt_(u,p,t)           \n    re(p)(u)[1].* u\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"From here we build a loss function around it. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"function predict_adjoint(time_batch)\n    _prob = remake(prob, u0=u0, p=θ)\n    Array(solve(_prob, Tsit5(), saveat = time_batch)) \nend\n\nfunction loss_adjoint(batch, time_batch)\n    pred = predict_adjoint(time_batch)\n    sum(abs2, batch - pred)#, pred\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"To add support for batches of size k we use Flux.Data.DataLoader. To use this we pass in the ode_data and t as the 'x' and 'y' data to batch respectively. The parameter batchsize controls the size of our batches. We check our implementation by iterating over the batched data. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"u0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 3.0f0)\n\nt = range(tspan[1], tspan[2], length=datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat=t))\n\nprob = ODEProblem{false}(dudt_, u0, tspan, θ)\n\nk = 10\ntrain_loader = Flux.Data.DataLoader((ode_data, t), batchsize = k)\n\nfor (x, y) in train_loader\n    @show x\n    @show y\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Now we train the neural network with a user defined call back function to display loss and the graphs with a maximum of 300 epochs. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"numEpochs = 300\nlosses=[]\ncb() = begin\n    l=loss_adjoint(ode_data, t)\n    push!(losses, l)\n    @show l\n    pred=predict_adjoint(t)\n    pl = scatter(t,ode_data[1,:],label=\"data\", color=:black, ylim=(150,200))\n    scatter!(pl,t,pred[1,:],label=\"prediction\", color=:darkgreen)\n    display(plot(pl))\n    false\nend \n\nopt=ADAM(0.05)\nFlux.train!(loss_adjoint, Flux.params(θ), ncycle(train_loader,numEpochs), opt, cb=Flux.throttle(cb, 10))","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"Finally we can see how well our trained network will generalize to new initial conditions. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/minibatch/","page":"Training a Neural Ordinary Differential Equation with Mini-Batching","title":"Training a Neural Ordinary Differential Equation with Mini-Batching","text":"starting_temp=collect(10:30:250)\ntrue_prob_func(u0)=ODEProblem(true_sol, [u0], tspan)\ncolor_cycle=palette(:tab10)\npl=plot()\nfor (j,temp) in enumerate(starting_temp)\n    ode_test_sol = solve(ODEProblem(true_sol, [temp], (0.0f0,10.0f0)), Tsit5(), saveat=0.0:0.5:10.0)\n    ode_nn_sol = solve(ODEProblem{false}(dudt_, [temp], (0.0f0,10.0f0), θ))\n    scatter!(pl, ode_test_sol, var=(0,1), label=\"\", color=color_cycle[j])\n    plot!(pl, ode_nn_sol, var=(0,1), label=\"\", color=color_cycle[j], lw=2.0)\nend\ndisplay(pl) \ntitle!(\"Neural ODE for Newton's Law of Cooling: Test Data\")\nxlabel!(\"Time\")\nylabel!(\"Temp\") ","category":"page"},{"location":"modules/Surrogates/multi_objective_opt/#Multi-objective-optimization-benchmarks","page":"Multi objective optimization","title":"Multi objective optimization benchmarks","text":"","category":"section"},{"location":"modules/Surrogates/multi_objective_opt/#Case-1:-Non-colliding-objective-functions","page":"Multi objective optimization","title":"Case 1: Non colliding objective functions","text":"","category":"section"},{"location":"modules/Surrogates/multi_objective_opt/","page":"Multi objective optimization","title":"Multi objective optimization","text":"using Surrogates\n#EGO\nm = 10\nf  = x -> [x^i for i = 1:m]\nlb = 1.0\nub = 10.0\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub, rad = linearRadial)\npareto_set, pareto_front = surrogate_optimize(f,EGO(),lb,ub,my_radial_basis_ego,SobolSample())\n\nm = 5\nf  = x -> [x^i for i =1:m]\nlb = 1.0\nub = 10.0\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_rtea = RadialBasis(x, y, lb, ub, rad = linearRadial)\nZ = 0.8\nK = 2\np_cross = 0.5\nn_c = 1.0\nsigma = 1.5\nsurrogate_optimize(f,RTEA(Z,K,p_cross,n_c,sigma),lb,ub,my_radial_basis_rtea,SobolSample())","category":"page"},{"location":"modules/Surrogates/multi_objective_opt/#Case-2:-objective-functions-with-conflicting-minima","page":"Multi objective optimization","title":"Case 2: objective functions with conflicting minima","text":"","category":"section"},{"location":"modules/Surrogates/multi_objective_opt/","page":"Multi objective optimization","title":"Multi objective optimization","text":"#EGO\nf  = x -> [sqrt((x[1] - 4)^2 + 25*(x[2])^2),\n           sqrt((x[1]+4)^2 + 25*(x[2])^2),\n           sqrt((x[1]-3)^2 + (x[2]-1)^2)]\nlb = [2.5,-0.5]\nub = [3.5,0.5]\nx  = sample(100, lb, ub, SobolSample())\ny  = f.(x)\nmy_radial_basis_ego = RadialBasis(x, y, lb, ub, rad = linearRadial)\n#I can find my pareto set and pareto front by calling again the surrogate_optimize function:\npareto_set, pareto_front = surrogate_optimize(f,EGO(),lb,ub,my_radial_basis_ego,SobolSample(),maxiters=30);","category":"page"},{"location":"modules/Surrogates/surrogate/#Surrogate","page":"Surrogates","title":"Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Every surrogate has a different definition depending on the parameters needed. However, they have in common:","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"add_point!(::AbstractSurrogate,x_new,y_new)\nAbstractSurrogate(value)","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"The first function adds a sample point to the surrogate, thus changing the internal coefficients. The second one calculates the approximation at value.","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Linear surrogate","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"LinearSurrogate(x,y,lb,ub)","category":"page"},{"location":"modules/Surrogates/surrogate/#Surrogates.LinearSurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LinearSurrogate","text":"LinearSurrogate(x,y,lb,ub)\n\nBuilds a linear surrogate using GLM.jl\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Radial basis function surrogate","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"RadialBasis(x, y, lb, ub; rad::RadialFunction = linearRadial, scale_factor::Real=1.0, sparse = false)","category":"page"},{"location":"modules/Surrogates/surrogate/#Surrogates.RadialBasis-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.RadialBasis","text":"RadialBasis(x,y,lb,ub,rad::RadialFunction, scale_factor::Float = 1.0)\n\nConstructor for RadialBasis surrogate\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Kriging surrogate","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Kriging(x,y,p,theta)","category":"page"},{"location":"modules/Surrogates/surrogate/#Surrogates.Kriging-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.Kriging","text":"Kriging(x,y,lb,ub;p=collect(one.(x[1])),theta=collect(one.(x[1])))\n\nConstructor for Kriging surrogate.\n\n(x,y): sampled points\np: array of values 0<=p<2 modeling the    smoothness of the function being approximated in the i-th variable.    low p -> rough, high p -> smooth\ntheta: array of values > 0 modeling how much the function is         changing in the i-th variable.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Lobachevsky surrogate","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"LobachevskySurrogate(x,y,lb,ub; alpha = collect(one.(x[1])),n::Int = 4, sparse = false)\nlobachevsky_integral(loba::LobachevskySurrogate,lb,ub)","category":"page"},{"location":"modules/Surrogates/surrogate/#Surrogates.LobachevskySurrogate-NTuple{4, Any}","page":"Surrogates","title":"Surrogates.LobachevskySurrogate","text":"LobachevskySurrogate(x,y,alpha,n::Int,lb,ub,sparse = false)\n\nBuild the Lobachevsky surrogate with parameters alpha and n.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/surrogate/#Surrogates.lobachevsky_integral-Tuple{LobachevskySurrogate, Any, Any}","page":"Surrogates","title":"Surrogates.lobachevsky_integral","text":"lobachevsky_integral(loba::LobachevskySurrogate,lb,ub)\n\nCalculates the integral of the Lobachevsky surrogate, which has a closed form.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Support vector machine surrogate, requires using LIBSVM","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"SVMSurrogate(x,y,lb::Number,ub::Number)","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Random forest surrogate, requires using XGBoost","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"RandomForestSurrogate(x,y,lb,ub;num_round::Int = 1)","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Neural network surrogate, requires using Flux","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"NeuralSurrogate(x,y,lb,ub; model = Chain(Dense(length(x[1]),1), first), loss = (x,y) -> Flux.mse(model(x), y),opt = Descent(0.01),n_echos::Int = 1)","category":"page"},{"location":"modules/Surrogates/surrogate/#Creating-another-surrogate","page":"Surrogates","title":"Creating another surrogate","text":"","category":"section"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"It's great that you want to add another surrogate to the library! You will need to:","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Define a new mutable struct and a constructor function\nDefine add_point!(your_surrogate::AbstactSurrogate,x_new,y_new)\nDefine your_surrogate(value) for the approximation","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"Example","category":"page"},{"location":"modules/Surrogates/surrogate/","page":"Surrogates","title":"Surrogates","text":"mutable struct NewSurrogate{X,Y,L,U,C,A,B} <: AbstractSurrogate\n  x::X\n  y::Y\n  lb::L\n  ub::U\n  coeff::C\n  alpha::A\n  beta::B\nend\n\nfunction NewSurrogate(x,y,lb,ub,parameters)\n    ...\n    return NewSurrogate(x,y,lb,ub,calculated\\_coeff,alpha,beta)\nend\n\nfunction add_point!(NewSurrogate,x\\_new,y\\_new)\n\n  nothing\nend\n\nfunction (s::NewSurrogate)(value)\n  return s.coeff*value + s.alpha\nend","category":"page"},{"location":"modules/NonlinearSolve/basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/NonlinearSolve/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Ask more questions.","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/#Note-on-benchmarking-and-getting-the-best-performance-out-of-the-SciML-stack's-adjoints","page":"Benchmarks","title":"Note on benchmarking and getting the best performance out of the SciML stack's adjoints","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"From our recent papers it's clear that EnzymeVJP is the fastest, especially when the program is setup to be fully non-allocating mutating functions. Thus for all benchmarking, especially with PDEs, this should be done. Neural network libraries don't make use of mutation effectively except for SimpleChains.jl, so we recommend creating a neural ODE / universal ODE with ZygoteVJP and Flux first, but then check the correctness by moving the implementation over to SimpleChains and if possible EnzymeVJP. This can be an order of magnitude improvement (or more) in many situations over all of the previous benchmarks using Zygote and Flux, and thus it's highly recommended in scenarios that require performance.","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/#Vs-Torchdiffeq-1-million-and-less-ODEs","page":"Benchmarks","title":"Vs Torchdiffeq 1 million and less ODEs","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"A raw ODE solver benchmark showcases >30x performance advantage for DifferentialEquations.jl for ODEs ranging in size from 3 to nearly 1 million.","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/#Vs-Torchdiffeq-on-neural-ODE-training","page":"Benchmarks","title":"Vs Torchdiffeq on neural ODE training","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"A training benchmark using the spiral ODE from the original neural ODE paper demonstrates a 100x performance advantage for DiffEqFlux in training neural ODEs.","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/#Vs-torchsde-on-small-SDEs","page":"Benchmarks","title":"Vs torchsde on small SDEs","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Using the code from torchsde's README we demonstrated a >70,000x performance advantage over torchsde. Further benchmarking is planned but was found to be computationally infeasible for the time being.","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/#A-bunch-of-adjoint-choices-on-neural-ODEs","page":"Benchmarks","title":"A bunch of adjoint choices on neural ODEs","text":"","category":"section"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"Quick summary:","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"BacksolveAdjoint can be the fastest (but use with caution!); about 25% faster\nUsing ZygoteVJP is faster than other vjp choices with FastDense due to the overloads","category":"page"},{"location":"modules/SciMLSensitivity/Benchmark/","page":"Benchmarks","title":"Benchmarks","text":"using DiffEqFlux, OrdinaryDiffEq, Flux, Optim, Plots, SciMLSensitivity,\n      Zygote, BenchmarkTools, Random\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = FastChain((x, p) -> x.^3,\n                  FastDense(2, 50, tanh),\n                  FastDense(50, 2))\nRandom.seed!(100)\np = initial_params(dudt2)\n\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction loss_neuralode(p)\n    pred = Array(prob_neuralode(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode,p)\n# 2.709 ms (56506 allocations: 6.62 MiB)\n\nprob_neuralode_interpolating = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_interpolating(p)\n    pred = Array(prob_neuralode_interpolating(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating,p)\n# 5.501 ms (103835 allocations: 2.57 MiB)\n\nprob_neuralode_interpolating_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=InterpolatingAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_interpolating_zygote(p)\n    pred = Array(prob_neuralode_interpolating_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_interpolating_zygote,p)\n# 2.899 ms (56150 allocations: 6.61 MiB)\n\nprob_neuralode_backsolve = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_backsolve(p)\n    pred = Array(prob_neuralode_backsolve(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve,p)\n# 4.871 ms (85855 allocations: 2.20 MiB)\n\nprob_neuralode_quad = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true)))\n\nfunction loss_neuralode_quad(p)\n    pred = Array(prob_neuralode_quad(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_quad,p)\n# 11.748 ms (79549 allocations: 3.87 MiB)\n\nprob_neuralode_backsolve_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=TrackerVJP()))\n\nfunction loss_neuralode_backsolve_tracker(p)\n    pred = Array(prob_neuralode_backsolve_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_tracker,p)\n# 27.604 ms (186143 allocations: 12.22 MiB)\n\nprob_neuralode_backsolve_zygote = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ZygoteVJP()))\n\nfunction loss_neuralode_backsolve_zygote(p)\n    pred = Array(prob_neuralode_backsolve_zygote(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_zygote,p)\n# 2.091 ms (49883 allocations: 6.28 MiB)\n\nprob_neuralode_backsolve_false = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP(false)))\n\nfunction loss_neuralode_backsolve_false(p)\n    pred = Array(prob_neuralode_backsolve_false(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_backsolve_false,p)\n# 4.822 ms (9956 allocations: 1.03 MiB)\n\nprob_neuralode_tracker = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps, sensealg=TrackerAdjoint())\n\nfunction loss_neuralode_tracker(p)\n    pred = Array(prob_neuralode_tracker(u0, p))\n    loss = sum(abs2, ode_data .- pred)\n    return loss\nend\n\n@btime Zygote.gradient(loss_neuralode_tracker,p)\n# 12.614 ms (76346 allocations: 3.12 MiB)","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/#Bouncing-Ball-Hybrid-ODE-Optimization","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"","category":"section"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"The bouncing ball is a classic hybrid ODE which can be represented in the DifferentialEquations.jl event handling system. This can be applied to ODEs, SDEs, DAEs, DDEs, and more. Let's now add the DiffEqFlux machinery to this problem in order to optimize the friction that's required to match data. Assume we have data for the ball's height after 15 seconds. Let's first start by implementing the ODE:","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"using Optimization, OptimizationPolyalgorithms, SciMLSensitivity, DifferentialEquations\n\nfunction f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]\nend\n\nfunction condition(u,t,integrator) # Event when event_f(u,t) == 0\n  u[1]\nend\n\nfunction affect!(integrator)\n  integrator.u[2] = -integrator.p[2]*integrator.u[2]\nend\n\ncallback = ContinuousCallback(condition,affect!)\nu0 = [50.0,0.0]\ntspan = (0.0,15.0)\np = [9.8, 0.8]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=callback)","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"Here we have a friction coefficient of 0.8. We want to refine this coefficient to find the value so that the predicted height of the ball at the endpoint is 20. We do this by minimizing a loss function against the value 20:","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"function loss(θ)\n  sol = solve(prob,Tsit5(),p=[9.8,θ[1]],callback=callback)\n  target = 20.0\n  abs2(sol[end][1] - target)\nend\n\nloss([0.8])\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, [0.8])\n@time res = Optimization.solve(optprob, PolyOpt(), maxiters = 300)\n@show res.u # [0.866554105436901]","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"This runs in about 0.091215 seconds (533.45 k allocations: 80.717 MiB) and finds an optimal drag coefficient.","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/#Note-on-Sensitivity-Methods","page":"Bouncing Ball Hybrid ODE Optimization","title":"Note on Sensitivity Methods","text":"","category":"section"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"The continuous adjoint sensitivities BacksolveAdjoint, InterpolatingAdjoint, and QuadratureAdjoint are compatible with events for ODEs. BacksolveAdjoint and InterpolatingAdjoint can also handle events for SDEs. Use BacksolveAdjoint if the event terminates the time evolution and several states are saved. Currently, the continuous adjoint sensitivities do not support multiple events per time point.","category":"page"},{"location":"modules/SciMLSensitivity/hybrid_jump_fitting/bouncing_ball/","page":"Bouncing Ball Hybrid ODE Optimization","title":"Bouncing Ball Hybrid ODE Optimization","text":"All methods based on discrete sensitivity analysis via automatic differentiation, like ReverseDiffAdjoint, TrackerAdjoint, or ForwardDiffSensitivity are the methods to use (and ReverseDiffAdjoint is demonstrated above), are compatible with events. This applies to SDEs, DAEs, and DDEs as well.","category":"page"},{"location":"modules/JumpProcesses/faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/#My-simulation-is-really-slow-and/or-using-a-lot-of-memory,-what-can-I-do?","page":"FAQ","title":"My simulation is really slow and/or using a lot of memory, what can I do?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"To reduce memory use, use save_positions=(false,false) in the JumpProblem constructor as described earlier to turn off saving the system state before and after every jump. Combined with use of saveat in the call to solve this can dramatically reduce memory usage.","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"While Direct is often fastest for systems with 10 or less ConstantRateJumps or MassActionJumps, if your system has many jumps or one jump occurs most frequently, other stochastic simulation algorithms may be faster. See Constant Rate Jump Aggregators and the subsequent sections there for guidance on choosing different SSAs (called aggregators in JumpProcesses).","category":"page"},{"location":"modules/JumpProcesses/faq/#When-running-many-consecutive-simulations,-for-example-within-an-EnsembleProblem-or-loop,-how-can-I-update-JumpProblems?","page":"FAQ","title":"When running many consecutive simulations, for example within an EnsembleProblem or loop, how can I update JumpProblems?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"In Remaking JumpProblems we show how to modify parameters, the initial condition, and other components of a generated JumpProblem. This can be useful when trying to call solve many times while avoiding reallocations of the internal aggregators for each new parameter value or initial condition.","category":"page"},{"location":"modules/JumpProcesses/faq/#How-can-I-define-collections-of-many-different-jumps-and-pass-them-to-JumpProblem?","page":"FAQ","title":"How can I define collections of many different jumps and pass them to JumpProblem?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"We can use JumpSets to collect jumps together, and then pass them into JumpProblems directly. For example, using the MassActionJump and ConstantRateJump defined earlier we can write","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"jset = JumpSet(mass_act_jump, birth_jump)\njump_prob = JumpProblem(prob, Direct(), jset)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"If you have many jumps in tuples or vectors it is easiest to use the keyword argument-based constructor:","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"cj1 = ConstantRateJump(rate1, affect1!)\ncj2 = ConstantRateJump(rate2, affect2!)\ncjvec = [cj1, cj2]\n\nvj1 = VariableRateJump(rate3, affect3!)\nvj2 = VariableRateJump(rate4, affect4!)\nvjtuple = (vj1, vj2)\n\njset = JumpSet(; constant_jumps=cjvec, variable_jumps=vjtuple,\n                 massaction_jumps=mass_act_jump)","category":"page"},{"location":"modules/JumpProcesses/faq/#How-can-I-set-the-random-number-generator-used-in-the-jump-process-sampling-algorithms-(SSAs)?","page":"FAQ","title":"How can I set the random number generator used in the jump process sampling algorithms (SSAs)?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"Random number generators can be passed to JumpProblem via the rng keyword argument. Continuing the previous example:","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"#] add RandomNumbers\nusing RandomNumbers\njprob = JumpProblem(dprob, Direct(), maj,\n                    rng = Xorshifts.Xoroshiro128Star(rand(UInt64)))","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"uses the Xoroshiro128Star generator from RandomNumbers.jl.","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"On version 1.7 and up JumpProcesses uses Julia's builtin random number generator by default. On versions below 1.7 it uses Xoroshiro128Star.","category":"page"},{"location":"modules/JumpProcesses/faq/#What-are-these-aggregators-and-aggregations-in-JumpProcesses?","page":"FAQ","title":"What are these aggregators and aggregations in JumpProcesses?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"JumpProcesses provides a variety of methods for sampling the time the next ConstantRateJump or MassActionJump occurs, and which jump type happens at that time. These methods are examples of stochastic simulation algorithms (SSAs), also known as Gillespie methods, Doob's method, or Kinetic Monte Carlo methods. In the JumpProcesses terminology we call such methods \"aggregators\", and the cache structures that hold their basic data \"aggregations\". See Constant Rate Jump Aggregators for a list of the available SSA aggregators.","category":"page"},{"location":"modules/JumpProcesses/faq/#How-should-jumps-be-ordered-in-dependency-graphs?","page":"FAQ","title":"How should jumps be ordered in dependency graphs?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"Internally, JumpProcesses SSAs (aggregators) order all MassActionJumps first, then all ConstantRateJumps. i.e. in the example","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"using JumpProcesses\nrs = [[1 => 1], [2 => 1]]\nns = [[1 => -1, 2 => 1], [1 => 1, 2 => -1]]\np = [1.0, 0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1, 2])\nrate1(u, p, t) = u[1]\nfunction affect1!(integrator)\n    u[1] -= 1\nend\ncj1 = ConstantRateJump(rate1, affect1)\nrate2(u, p, t) = u[2]\nfunction affect2!(integrator)\n    u[2] -= 1\nend\ncj2 = ConstantRateJump(rate2, affect2)\njset = JumpSet(; constant_jumps=[cj1, cj2], massaction_jump=maj)","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"The four jumps would be ordered by the first jump in maj, the second jump in maj, cj1, and finally cj2. Any user-generated dependency graphs should then follow this ordering when assigning an integer id to each jump.","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"See also Constant Rate Jump Aggregators Requiring Dependency Graphs for more on dependency graphs needed for the various SSAs.","category":"page"},{"location":"modules/JumpProcesses/faq/#How-do-I-use-callbacks-with-ConstantRateJump-or-MassActionJump-systems?","page":"FAQ","title":"How do I use callbacks with ConstantRateJump or MassActionJump systems?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"Callbacks can be used with ConstantRateJumps and MassActionJumps. When solving a pure jump system with SSAStepper, only discrete callbacks can be used (otherwise a different time stepper is needed). When using an ODE or SDE time stepper any callback should work.","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"Note, when modifying u or p within a callback, you must call reset_aggregated_jumps! after making updates. This ensures that the underlying jump simulation algorithms know to reinitialize their internal data structures. Leaving out this call will lead to incorrect behavior!","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"A simple example that uses a MassActionJump and changes the parameters at a specified time in the simulation using a DiscreteCallback is","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"using JumpProcesses\nrs = [[1 => 1], [2 => 1]]\nns = [[1 => -1, 2 => 1], [1 => 1, 2 => -1]]\np = [1.0, 0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1, 2])\nu₀ = [100, 0]\ntspan = (0.0, 40.0)\ndprob = DiscreteProblem(u₀, tspan, p)\njprob = JumpProblem(dprob, Direct(), maj)\npcondit(u, t, integrator) = t == 20.0\nfunction paffect!(integrator)\n    integrator.p[1] = 0.0\n    integrator.p[2] = 1.0\n    reset_aggregated_jumps!(integrator)\n    nothing\nend\nsol = solve(jprob, SSAStepper(), tstops=[20.0], callback=DiscreteCallback(pcondit, paffect!))","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"Here at time 20.0 we turn off production of u[2] while activating production of u[1], giving","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"(Image: callback_gillespie)","category":"page"},{"location":"modules/JumpProcesses/faq/#How-can-I-access-earlier-solution-values-in-callbacks?","page":"FAQ","title":"How can I access earlier solution values in callbacks?","text":"","category":"section"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"When using an ODE or SDE time-stepper that conforms to the integrator interface one can simply use integrator.uprev. For efficiency reasons, the pure jump SSAStepper integrator does not have such a field. If one needs solution components at earlier times one can save them within the callback condition by making a functor:","category":"page"},{"location":"modules/JumpProcesses/faq/","page":"FAQ","title":"FAQ","text":"# stores the previous value of u[2] and represents the callback functions\nmutable struct UprevCondition{T}\n     u2::T\nend\n\n# condition\nfunction (upc::UprevCondition)(u, t, integrator)\n    # condition for the callback is that the new value of u[2]\n    # is smaller than the previous value\n    condit = u[2] - upc.u2 < 0\n\n    # save the new value as the previous value\n    upc.u2 = u[2]\n\n    condit\nend\n\n# affect!\nfunction (upc::UprevCondition)(integrator)\n    integrator.u[4] -= 1\n    nothing\nend\n\nupc = UprevCondition(u0[2])\ncb = DiscreteCallback(upc, upc)","category":"page"},{"location":"modules/Integrals/tutorials/differentiating_integrals/#Differentiating-Integrals","page":"Differentiating Integrals","title":"Differentiating Integrals","text":"","category":"section"},{"location":"modules/Integrals/tutorials/differentiating_integrals/","page":"Differentiating Integrals","title":"Differentiating Integrals","text":"Integrals.jl is a fully differentiable quadrature library. Thus, it adds the ability to perform automatic differentiation over any of the libraries that it calls. It integrates with ForwardDiff.jl for forward-mode automatic differentiation and Zygote.jl for reverse-mode automatic differentiation. For example:","category":"page"},{"location":"modules/Integrals/tutorials/differentiating_integrals/","page":"Differentiating Integrals","title":"Differentiating Integrals","text":"using Integrals, ForwardDiff, FiniteDiff, Zygote, Cuba\nf(x,p) = sum(sin.(x .* p))\nlb = ones(2)\nub = 3ones(2)\np = [1.5,2.0]\n\nfunction testf(p)\n    prob = IntegralProblem(f,lb,ub,p)\n    sin(solve(prob,CubaCuhre(),reltol=1e-6,abstol=1e-6)[1])\nend\ndp1 = Zygote.gradient(testf,p)\ndp2 = FiniteDiff.finite_difference_gradient(testf,p)\ndp3 = ForwardDiff.gradient(testf,p)\ndp1[1] ≈ dp2 ≈ dp3","category":"page"},{"location":"modules/Catalyst/#Catalyst.jl-for-Reaction-Network-Modeling","page":"Home","title":"Catalyst.jl for Reaction Network Modeling","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Catalyst.jl is a symbolic modeling package for analysis and high performance simulation of chemical reaction networks. Catalyst defines symbolic ReactionSystems, which can be created programmatically or easily specified using Catalyst's domain specific language (DSL). Leveraging ModelingToolkit and Symbolics.jl, Catalyst enables large-scale simulations through auto-vectorization and parallelism. Symbolic ReactionSystems can be used to generate ModelingToolkit-based models, allowing the easy simulation and parameter estimation of mass action ODE models, Chemical Langevin SDE models, stochastic chemical kinetics jump process models, and more. Generated models can be used with solvers throughout the broader SciML ecosystem, including higher level SciML packages (e.g. for sensitivity analysis, parameter estimation, machine learning applications, etc).","category":"page"},{"location":"modules/Catalyst/#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"DSL provides a simple and readable format for manually specifying chemical reactions.\nCatalyst ReactionSystems provide a symbolic representation of reaction networks, built on ModelingToolkit.jl and Symbolics.jl.\nNon-integer (e.g. Float64) stoichiometric coefficients are supported for generating ODE models, and symbolic expressions for stoichiometric coefficients are supported for all system types.\nThe Catalyst.jl API provides functionality for extending networks, building networks programmatically, network analysis, and for composing multiple networks together.\nReactionSystems generated by the DSL can be converted to a variety of ModelingToolkit.AbstractSystems, including symbolic ODE, SDE and jump process representations.\nConservation laws can be detected and applied to reduce system sizes, and generate non-singular Jacobians, during conversion to ODEs, SDEs, and steady-state equations.\nBy leveraging ModelingToolkit, users have a variety of options for generating optimized system representations to use in solvers. These include construction of dense or sparse Jacobians, multithreading or parallelization of generated derivative functions, automatic classification of reactions into optimized jump types for Gillespie type simulations, automatic construction of dependency graphs for jump systems, and more.\nGenerated systems can be solved using any DifferentialEquations.jl ODE/SDE/jump solver, and can be used within EnsembleProblems for carrying out parallelized parameter sweeps and statistical sampling. Plot recipes are available for visualizing the solutions.\nJulia Exprs can be obtained for all rate laws and functions determining the deterministic and stochastic terms within resulting ODE, SDE or jump models.\nLatexify can be used to generate LaTeX expressions corresponding to generated mathematical models or the underlying set of reactions.\nGraphviz can be used to generate and visualize reaction network graphs. (Reusing the Graphviz interface created in Catlab.jl.)","category":"page"},{"location":"modules/Catalyst/#Packages-Supporting-Catalyst","page":"Home","title":"Packages Supporting Catalyst","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Catalyst ReactionSystems can be imported from SBML files via SBMLToolkit.jl, and from BioNetGen .net files and various stoichiometric matrix network representations using ReactionNetworkImporters.jl.\nMomentClosure.jl allows generation of symbolic ModelingToolkit ODESystems, representing moment closure approximations to moments of the Chemical Master Equation, from reaction networks defined in Catalyst.\nFiniteStateProjection.jl allows the construction and numerical solution of Chemical Master Equation models from reaction networks defined in Catalyst.\nDelaySSAToolkit.jl can augment Catalyst reaction network models with delays, and can simulate the resulting stochastic chemical kinetics with delays models.","category":"page"},{"location":"modules/Catalyst/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Catalyst can be installed through the Julia package manager:","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"]add Catalyst\nusing Catalyst","category":"page"},{"location":"modules/Catalyst/#Illustrative-Example","page":"Home","title":"Illustrative Example","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Here is a simple example of generating, visualizing and solving an SIR ODE model. We first define the SIR reaction model using Catalyst","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"using Catalyst\nrn = @reaction_network begin\n    α, S + I --> 2I\n    β, I --> R\nend α β","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Assuming Graphviz and is installed and command line accessible, the network can be visualized using the Graph command","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Graph(rn)","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"which in Jupyter notebooks will give the figure","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"(Image: SIR Network Graph)","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"To generate and solve a mass action ODE version of the model we use","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"using OrdinaryDiffEq\np     = [:α => .1/1000, :β => .01]\ntspan = (0.0,250.0)\nu0    = [:S => 999.0, :I => 1.0, :R => 0.0]\nop    = ODEProblem(rn, u0, tspan, p)\nsol   = solve(op, Tsit5())       # use Tsit5 ODE solver","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"which we can plot as","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"using Plots\nplot(sol, lw=2)","category":"page"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"(Image: SIR Solution)","category":"page"},{"location":"modules/Catalyst/#Getting-Help","page":"Home","title":"Getting Help","text":"","category":"section"},{"location":"modules/Catalyst/","page":"Home","title":"Home","text":"Catalyst developers are active on the Julia Discourse, and the Julia Slack channels #sciml-bridged and #sciml-sysbio. For bugs or feature requests open an issue.","category":"page"},{"location":"modules/Surrogates/neural/#Neural-network-tutorial","page":"NeuralSurrogate","title":"Neural network tutorial","text":"","category":"section"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"note: Note\nThis surrogate requires the 'SurrogatesFlux' module which can be added by inputting \"]add SurrogatesFlux\" from the Julia command line. ","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"It's possible to define a neural network as a surrogate, using Flux. This is useful because we can call optimization methods on it.","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"First of all we will define the Schaffer function we are going to build surrogate for.","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"using Plots\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates\nusing Flux\nusing SurrogatesFlux\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"modules/Surrogates/neural/#Sampling","page":"NeuralSurrogate","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/neural/#Building-a-surrogate","page":"NeuralSurrogate","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"You can specify your own model, optimization function, loss functions and epochs. As always, getting the model right is hardest thing.","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"model1 = Chain(\n  Dense(2, 5, σ),\n  Dense(5,2,σ),\n  Dense(2, 1)\n)\nneural = NeuralSurrogate(xys, zs, lower_bound, upper_bound, model = model1, n_echos = 10)","category":"page"},{"location":"modules/Surrogates/neural/#Optimization","page":"NeuralSurrogate","title":"Optimization","text":"","category":"section"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"We can now call an optimization function on the neural network:","category":"page"},{"location":"modules/Surrogates/neural/","page":"NeuralSurrogate","title":"NeuralSurrogate","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, neural, SobolSample(), maxiters=20, num_new_samples=10)","category":"page"},{"location":"highlevels/simulation_analysis/#SciML-Simulation-Analysis-Utilities","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"","category":"section"},{"location":"highlevels/simulation_analysis/#GlobalSensitivity.jl:-Global-Sensitivity-Analysis","page":"SciML Simulation Analysis Utilities","title":"GlobalSensitivity.jl: Global Sensitivity Analysis","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"Derivatives calculate the local sensitivity of a model, i.e. the change in the simulation's outcome if one was to change the parameter with respect to some chosen part of the parameter space. But how does a simulation's output change \"in general\" with respect to a given parameter? That is what global sensitivity analysis (GSA) computes, and thus GlobalSensitivity.jl is the way to answer that question. GlobalSensitivity.jl includes a wide array of methods, including:","category":"page"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"Morris's method\nSobol's method\nRegression methods (PCC, SRC, Pearson)\neFAST\nDelta Moment-Independent method\nDerivative-based Global Sensitivity Measures (DGSM)\nEASI\nFractional Factorial method\nRandom Balance Design FAST method","category":"page"},{"location":"highlevels/simulation_analysis/#MinimallyDisruptiveCurves.jl","page":"SciML Simulation Analysis Utilities","title":"MinimallyDisruptiveCurves.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"MinimallyDisruptiveCurves.jl is a library for finding relationships between parameters of models, finding the curves on which the solution is constant.","category":"page"},{"location":"highlevels/simulation_analysis/#Third-Party-Libraries-to-Note","page":"SciML Simulation Analysis Utilities","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/simulation_analysis/#DynamicalSystems.jl:-A-Suite-of-Dynamical-Systems-Analysis","page":"SciML Simulation Analysis Utilities","title":"DynamicalSystems.jl: A Suite of Dynamical Systems Analysis","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"DynamicalSystems.jl is an entire ecosystem of dynamical systems analysis methods, for computing measures of chaos (dimension estimation, Lyapunov coefficients), generating delay embeddings, and much more. It uses the SciML tools for its internal equation solving and thus shares much of its API, adding a layer of new tools for extended analyses.","category":"page"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"For more information, watch the tutorial Introduction to DynamicalSystems.jl.","category":"page"},{"location":"highlevels/simulation_analysis/#BifurcationKit.jl","page":"SciML Simulation Analysis Utilities","title":"BifurcationKit.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"BifucationKit.jl is a tool for performing bifurcation analysis. It uses and composes with many SciML equation solvers.","category":"page"},{"location":"highlevels/simulation_analysis/#ReachabilityAnalysis.jl","page":"SciML Simulation Analysis Utilities","title":"ReachabilityAnalysis.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"ReachabilityAnalysis.jl is a library for performing reachability analysis of dynamical systems, determining for a given uncertainty interval the full set of possible outcomes from a dynamical system.","category":"page"},{"location":"highlevels/simulation_analysis/#ControlSystems.jl","page":"SciML Simulation Analysis Utilities","title":"ControlSystems.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"ControlSystems.jl is a library for building and analyzing control systems.","category":"page"},{"location":"highlevels/simulation_analysis/#Recommended-Plotting-and-Visualization-Libraries","page":"SciML Simulation Analysis Utilities","title":"Recommended Plotting and Visualization Libraries","text":"","category":"section"},{"location":"highlevels/simulation_analysis/#Plots.jl","page":"SciML Simulation Analysis Utilities","title":"Plots.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"Plots.jl is the current standard plotting system for the SciML ecosystem. SciML types attempt to include plot recipes for as many types as possible, allowing for automatic visualization with the Plots.jl system. All current tutorials and documentation default to using Plots.jl.","category":"page"},{"location":"highlevels/simulation_analysis/#Makie.jl","page":"SciML Simulation Analysis Utilities","title":"Makie.jl","text":"","category":"section"},{"location":"highlevels/simulation_analysis/","page":"SciML Simulation Analysis Utilities","title":"SciML Simulation Analysis Utilities","text":"Makie.jl is a high-performance interactive plotting system for the Julia programming language. It's planned to be the default plotting system used by the SciML organization in the near future.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/#linear_nonlinear","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"One of the key features of DifferentialEquations.jl is its flexibility. Keeping with this trend, many of the native Julia solvers provided by DifferentialEquations.jl allow you to choose the method for linear and nonlinear solving. This section details how to make that choice.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"note: Note\nWe highly recommend looking at the Solving Large Stiff Equations tutorial which goes through these options in a real-world example.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"warning: Warning\nThese options do not apply to the Sundials differential equation solvers (CVODE_BDF, CVODE_Adams, ARKODE, and IDA). For complete descriptions of similar functionality for Sundials, see the Sundials ODE solver documentation and Sundials DAE solver documentation.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/#Linear-Solvers:-linsolve-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Linear Solvers: linsolve Specification","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"For linear solvers, DifferentialEquations.jl uses LinearSolve.jl. Any LinearSolve.jl algorithm can be used as the linear solver simply by passing the algorithm choice to linsolve. For example, the following tells TRBDF2 to use KLU.jl","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"TRBDF2(linsolve = KLUFactorization())","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"Many choices exist, including GPU offloading, so consult the LinearSolve.jl documentation for more details on the choices.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/#Preconditioners:-precs-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Preconditioners: precs Specification","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"W: the current Jacobian of the nonlinear system. Specified as either I - gamma J or Igamma - J depending on the algorithm. This will commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy representation of the operator. Users can construct the W-matrix on demand by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since the last call to precs. It is recommended that this is checked to only update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function. Solver-dependent and subject to change.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"The return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used.","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"Additionally, precs must supply the dispatch:","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"Pl,Pr = precs(W,du,u,p,t,::Nothing,::Nothing,::Nothing,solverdata)","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"which is used in the solver setup phase in order to construct the integrator type with the preconditioners (Pl,Pr).","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"DEFAULT_PRECS(W,du,u,p,t,newW,Plprev,Prprev,solverdata) = nothing,nothing","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/#Nonlinear-Solvers:-nlsolve-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Nonlinear Solvers: nlsolve Specification","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"All of the Julia-based implicit solvers (OrdinaryDiffEq.jl, StochasticDiffEq.jl, etc.) allow for choosing the nonlinear solver that is used to handle the implicit system. While fully modifiable and customizable, most users should stick to the pre-defined nonlinear solver choices. These are:","category":"page"},{"location":"modules/DiffEqDocs/features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"NLNewton(; κ=1//100, max_iter=10, fast_convergence_cutoff=1//5, new_W_dt_cutoff=1//5): A quasi-Newton method. The default.\nNLAnderson(; κ=1//100, max_iter=10, max_history::Int=5, aa_start::Int=1, droptol=nothing, fast_convergence_cutoff=1//5): Anderson acceleration. While more stable than functional iteration, this method is less stable than Newton's method but does not require a Jacobian.\nNLFunctional(; κ=1//100, max_iter=10, fast_convergence_cutoff=1//5): This method is the least stable but does not require Jacobians. Should only be used for non-stiff ODEs.","category":"page"},{"location":"modules/Optimization/API/modelingtoolkit/#ModelingToolkit-Integration","page":"ModelingToolkit Integration","title":"ModelingToolkit Integration","text":"","category":"section"},{"location":"modules/Optimization/API/modelingtoolkit/","page":"ModelingToolkit Integration","title":"ModelingToolkit Integration","text":"Optimization.jl is heavily integrated with the ModelingToolkit.jl symbolic system for symbolic-numeric optimizations. It provides a front-end for automating the construction, parallelization, and optimization of code. Optimizers can better interface with the extra symbolic information provided by the system.","category":"page"},{"location":"modules/Optimization/API/modelingtoolkit/","page":"ModelingToolkit Integration","title":"ModelingToolkit Integration","text":"There are two ways that the user interacts with ModelingToolkit.jl. One can use OptimizationFunction with AutoModelingToolkit for automatically transforming numerical codes into symbolic codes. See the OptimizationFunction documentation for more details.","category":"page"},{"location":"modules/Optimization/API/modelingtoolkit/","page":"ModelingToolkit Integration","title":"ModelingToolkit Integration","text":"Secondly, one can generate OptimizationProblems for use in Optimization.jl from purely a symbolic front-end. This is the form users will encounter when using ModelingToolkit.jl directly, and its also the form supplied by domain-specific languages. For more information, see the OptimizationSystem documentation.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#DDE-Solvers","page":"DDE Solvers","title":"DDE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"solve(prob::AbstractDDEProblem, alg; kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Solves the DDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Recommended-Methods","page":"DDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"The recommended method for DDE problems are the MethodOfSteps algorithms. These are constructed from an OrdinaryDiffEq.jl algorithm as follows:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"MethodOfSteps(alg; constrained=false, fpsolve=NLFunctional(; max_iter=10))","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"where alg is an OrdinaryDiffEq.jl algorithm. Most algorithms should work.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Nonstiff-DDEs","page":"DDE Solvers","title":"Nonstiff DDEs","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"The standard algorithm choice is MethodOfSteps(Tsit5()). This is a highly efficient FSAL 5th order algorithm with free interpolants which should handle most problems. For fast solving where non-strict error control is needed, choosing MethodOfSteps(BS3()) can do well. Using BS3 is similar to the MATLAB dde23. For algorithms where strict error control is needed, it is recommended that one uses MethodOfSteps(Vern6()). Benchmarks show that going to higher order methods like MethodOfSteps(DP8()) may not be beneficial.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Stiff-DDEs-and-Differential-Algebraic-Delay-Equations-(DADEs)","page":"DDE Solvers","title":"Stiff DDEs and Differential-Algebraic Delay Equations (DADEs)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"For stiff DDEs, the SDIRK and Rosenbrock methods are very efficient as they will reuse the Jacobian in the unconstrained stepping iterations. One should choose from the methods which have stiff-aware interpolants for better stability. MethodOfSteps(Rosenbrock23()) is a good low order method choice. Additionally, the Rodas methods like MethodOfSteps(Rodas4()) are good choices because of their higher order stiff-aware interpolant.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Additionally, DADEs can be solved by specifying the problem in mass matrix form. The Rosenbrock methods are good choices in these situations.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Lag-Handling","page":"DDE Solvers","title":"Lag Handling","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Lags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate. Constant delays are propagated until the order is higher than the order of the integrator. If state-dependent delays are declared, the algorithm will detect discontinuities arising from these delays and adjust the step size such that these discontinuities are included in the mesh, if steps are rejected. This way, all discontinuities are treated exactly.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"If there are undeclared lags, the discontinuities due to delays are not tracked. In this case, one should only use residual control methods like MethodOfSteps(RK4()), which is the current best choice, as these will step more accurately. Still, residual control is an error-prone method. We recommend setting the tolerances lower in order to get accurate results, though this may be costly since it will use a rejection-based approach to adapt to the delay discontinuities.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Special-Keyword-Arguments","page":"DDE Solvers","title":"Special Keyword Arguments","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"discontinuity_interp_points - Number of interpolation points used to track discontinuities arising from dependent delays. Defaults to 10. Only relevant if dependent delays are declared.\ndiscontinuity_abstol and discontinuity_reltol - These are absolute and relative tolerances used by the check whether the time point at the beginning of the current step is a discontinuity arising from dependent delays. Defaults to 1/10^12 and 0. Only relevant if dependent delays are declared.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dde_solve/#Note","page":"DDE Solvers","title":"Note","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"If the method is having trouble, one may want to adjust the fixed-point iteration. Decreasing the absolute tolerance and the relative tolerance by specifying the keyword arguments abstol and reltol when solving the DDE problem, and increasing the maximal number of iterations by specifying the keyword argument max_iter in the MethodOfSteps algorithm, can help ensure that the steps are correct. If the problem still is not correctly converging, one should lower dtmax. For problems with only constant delays, in the worst case scenario, one may need to set constrained = true which will constrain timesteps to at most the size of the minimal lag and hence forces more stability at the cost of smaller timesteps.","category":"page"},{"location":"modules/Surrogates/rosenbrock/#Rosenbrock-function","page":"Rosenbrock","title":"Rosenbrock function","text":"","category":"section"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"The Rosenbrock function is defined as: f(x) = sum_i=1^d-1 (x_i+1-x_i)^2 + (x_i - 1)^2","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"I will treat the 2D version, which is commonly defined as: f(xy) = (1-x)^2 + 100(y-x^2)^2 Let's import Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"function f(x)\n    x1 = x[1]\n    x2 = x[2]\n    return (1-x1)^2 + 100*(x2-x1^2)^2\nend","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Let's plot it:","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"n = 100\nlb = [0.0,0.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Fitting different Surrogates:","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = LobachevskySurrogate(xys, zs, lb, ub)\ninver = InverseDistanceSurrogate(xys, zs,  lb, ub)","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"Plotting:","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"modules/Surrogates/rosenbrock/","page":"Rosenbrock","title":"Rosenbrock","text":"p1 = surface(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> inver([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance surrogate\")","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#solution","page":"Solution Handling","title":"Solution Handling","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/#Accessing-the-Values","page":"Solution Handling","title":"Accessing the Values","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution type has a lot of built in functionality to help analysis. For example, it has an array interface for accessing the values. Internally, the solution type has two important fields:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"u which holds the Vector of values at each timestep\nt which holds the times of each timestep.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Different solution types may add extra information as necessary, such as the derivative at each timestep du or the spatial discretization x, y, etc.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Array-Interface","page":"Solution Handling","title":"Array Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Instead of working on the Vector{uType} directly, we can use the provided array interface.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[j]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"to access the value at timestep j (if the timeseries was saved), and","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol.t[j]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"to access the value of t at timestep j. For multi-dimensional systems, this will address first by component and lastly by time, and thus","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,j]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"will be the ith component at timestep j. Hence, sol[j][i] == sol[i, j]. This is done because Julia is column-major, so the leading dimension should be contiguous in memory. If the independent variables had shape (for example, was a matrix), then i is the linear index. We can also access solutions with shape:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,k,j]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"gives the [i,k] component of the system at timestep j. The colon operator is supported, meaning that","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,:]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"gives the timeseries for the ith component.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Using-the-AbstractArray-Interface","page":"Solution Handling","title":"Using the AbstractArray Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The AbstractArray interface can be directly used. For example, for a vector system of variables sol[i,j] is a matrix with rows being the variables and columns being the timepoints. Operations like sol' will transpose the solution type. Functionality written for AbstractArrays can directly use this. For example, the Base cov function computes correlations amongst columns, and thus:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"cov(sol)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"computes the correlation of the system state in time, whereas","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"cov(sol,2)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"computes the correlation between the variables. Similarly, mean(sol,2) is the mean of the variable in time, and var(sol,2) is the variance. Other statistical functions and packages which work on AbstractArray types will work on the solution type.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"At anytime, a true Array can be created using Array(sol).","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Interpolations-and-Calculating-Derivatives","page":"Solution Handling","title":"Interpolations and Calculating Derivatives","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"If the solver allows for dense output and dense=true was set for the solving (which is the default), then we can access the approximate value at a time t using the command","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Note that the interpolating function allows for t to be a vector and uses this to speed up the interpolation calculations. The full API for the interpolations is","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,deriv=Val{0};idxs=nothing,continuity=:left)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The optional argument deriv lets you choose the number n derivative to solve the interpolation for, defaulting with n=0. Note that most of the derivatives have not yet been implemented (though it's not hard, it just has to be done by hand for each algorithm. Open an issue if there's a specific one you need). continuity describes whether to satisfy left or right continuity when a discontinuity is saved. The default is :left, i.e. grab the value before the callback's change, but can be changed to :right. idxs allows you to choose the indices the interpolation should solve for. For example,","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,idxs=1:2:5)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"will return a Vector of length 3 which is the interpolated values at t for components 1, 3, and 5. idxs=nothing, the default, means it will return every component. In addition, we can do","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,idxs=1)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"and it will return a Number for the interpolation of the single value. Note that this interpolation only computes the values which are requested, and thus it's much faster on large systems to use this rather than computing the full interpolation and using only a few values.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"In addition, there is an inplace form:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(out,t,deriv=Val{0};idxs=nothing,continuity=:left)","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"which will write the output to out. This allows one to use pre-allocated vectors for the output to improve the speed even more.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Comprehensions","page":"Solution Handling","title":"Comprehensions","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solver interface also gives tools for using comprehensions over the solution. Using the tuples(sol) function, we can get a tuple for the output at each timestep. This allows one to do the following:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"[t+2u for (u,t) in tuples(sol)]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"One can use the extra components of the solution object as well as using zip. For example, say the solution type holds du, the derivative at each timestep. One can comprehend over the values using:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"[t+3u-du for (t,u,du) in zip(sol.t,sol.u,sol.du)]","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Note that the solution object acts as a vector in time, and so its length is the number of saved timepoints.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Special-Fields","page":"Solution Handling","title":"Special Fields","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution interface also includes some special fields. The problem object prob and the algorithm used to solve the problem alg are included in the solution. Additionally, the field dense is a boolean which states whether the interpolation functionality is available. Further, the field destats contains the internal statistics for the solution process such as the number of linear solves and convergence failures. Lastly, there is a mutable state tslocation which controls the plot recipe behavior. By default, tslocation=0. Its values have different meanings between partial and ordinary differential equations:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"tslocation=0  for non-spatial problems (ODEs) means that the plot recipe will plot the full solution. tslocation=i means that it will only plot the timepoint i.\ntslocation=0 for spatial problems (PDEs) means the plot recipe will plot the final timepoint. tslocation=i means that the plot recipe will plot the ith timepoint.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"What this means is that for ODEs, the plots will default to the full plot and PDEs will default to plotting the surface at the final timepoint. The iterator interface simply iterates the value of tslocation, and the animate function iterates the solution calling solve at each step.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Differential-Equation-Solver-Statistics-(destats)","page":"Solution Handling","title":"Differential Equation Solver Statistics (destats)","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"DiffEqBase.DEStats","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#DiffEqBase.DEStats","page":"Solution Handling","title":"DiffEqBase.DEStats","text":"mutable struct DEStats\n\nStatistics from the differential equation solver about the solution process.\n\nFields\n\nnf: Number of function evaluations. If the differential equation is a split function, such as a SplitFunction for implicit-explicit (IMEX) integration, then nf is the number of function evaluations for the first function (the implicit function)\nnf2: If the differential equation is a split function, such as a SplitFunction for implicit-explicit (IMEX) integration, then nf2 is the number of function evaluations for the second function, i.e. the function treated explicitly. Otherwise it is zero.\nnw: The number of W=I-gamma*J (or W=I/gamma-J) matrices constructed during the solving process.\nnsolve: The number of linear solves W\b required for the integration.\nnjacs: Number of Jacobians calculated during the integration.\nnnonliniter: Total number of iterations for the nonlinear solvers.\nnnonlinconvfail: Number of nonlinear solver convergence failures.\nncondition: Number of calls to the condition function for callbacks.\nnaccept: Number of accepted steps.\nnreject: Number of rejected steps.\nmaxeig: Maximum eigenvalue over the solution. This is only computed if the method is an auto-switching algorithm.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/basics/solution/#retcodes","page":"Solution Handling","title":"Return Codes (RetCodes)","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution types have a retcode field which returns a symbol signifying the error state of the solution. The retcodes are as follows:","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":":Default: The solver did not set retcodes.\n:Success: The integration completed without erroring or the steady state solver from SteadyStateDiffEq found the steady state.\n:Terminated: The integration is terminated with terminate!(integrator). Note that this may occur by using TerminateSteadyState from the callback library DiffEqCallbacks.\n:MaxIters: The integration exited early because it reached its maximum number of iterations.\n:DtLessThanMin: The timestep method chose a stepsize which is smaller than the allowed minimum timestep, and exited early.\n:Unstable: The solver detected that the solution was unstable and exited early.\n:InitialFailure: The DAE solver could not find consistent initial conditions.\n:ConvergenceFailure: The internal implicit solvers failed to converge.\n:Failure: General uncategorized failures or errors.","category":"page"},{"location":"modules/DiffEqDocs/basics/solution/#Problem-Specific-Features","page":"Solution Handling","title":"Problem-Specific Features","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Extra fields for solutions of specific problems are specified in the appropriate problem definition page.  ","category":"page"},{"location":"modules/NeuralOperators/references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"modules/NeuralOperators/references/","page":"References","title":"References","text":"","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#speed","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"note: Note\nSee this FAQ for information on common pitfalls and how to improve performance.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Code-Optimization-in-Julia","page":"Code Optimization for Differential Equations","title":"Code Optimization in Julia","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Before starting this tutorial, we recommend the reader to check out one of the many tutorials for optimization Julia code. The following is an incomplete list:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"The Julia Performance Tips\nMIT 18.337 Course Notes on Optimizing Serial Code\nWhat scientists must know about hardware to write fast code","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"User-side optimizations are important because, for sufficiently difficult problems, most of the time will be spent inside of your f function, the function you are trying to solve. \"Efficient\" integrators are those that reduce the required number of f calls to hit the error tolerance. The main ideas for optimizing your DiffEq code, or any Julia function, are the following:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Make it non-allocating\nUse StaticArrays for small arrays\nUse broadcast fusion\nMake it type-stable\nReduce redundant calculations\nMake use of BLAS calls\nOptimize algorithm choice","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"We'll discuss these strategies in the context of differential equations. Let's start with small systems.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Example-Accelerating-a-Non-Stiff-Equation:-The-Lorenz-Equation","page":"Code Optimization for Differential Equations","title":"Example Accelerating a Non-Stiff Equation: The Lorenz Equation","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Let's take the classic Lorenz system. Let's start by naively writing the system in its out-of-place form:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function lorenz(u,p,t)\r\n dx = 10.0*(u[2]-u[1])\r\n dy = u[1]*(28.0-u[3]) - u[2]\r\n dz = u[1]*u[2] - (8/3)*u[3]\r\n [dx,dy,dz]\r\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Here, lorenz returns an object, [dx,dy,dz], which is created within the body of lorenz.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"This is a common code pattern from high-level languages like MATLAB, SciPy, or R's deSolve. However, the issue with this form is that it allocates a vector, [dx,dy,dz], at each step. Let's benchmark the solution process with this choice of function:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using DifferentialEquations, BenchmarkTools\r\nu0 = [1.0;0.0;0.0]\r\ntspan = (0.0,100.0)\r\nprob = ODEProblem(lorenz,u0,tspan)\r\n@benchmark solve(prob,Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"BenchmarkTools.Trial: 1350 samples with 1 evaluation.\r\n Range (min … max):  2.296 ms … 13.764 ms  ┊ GC (min … max):  0.00% … 67.48%\r\n Time  (median):     2.561 ms              ┊ GC (median):     0.00%\r\n Time  (mean ± σ):   3.699 ms ±  2.223 ms  ┊ GC (mean ± σ):  14.83% ± 17.79%\r\n\r\n  █▆▄       ▁ ▃▄▃▄▂\r\n  ████▆▆█▇▇▇█▇█████▆▄▄▄▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▆▇▆▇▇▆▇▇▆█▇▇▆ █\r\n  2.3 ms       Histogram: log(frequency) by time     11.3 ms <\r\n\r\n Memory estimate: 7.82 MiB, allocs estimate: 101102.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"The BenchmarkTools.jl package's @benchmark runs the code multiple times to get an accurate measurement. The minimum time is the time it takes when your OS and other background processes aren't getting in the way. Notice that in this case it takes about 5ms to solve and allocates around 11.11 MiB. However, if we were to use this inside of a real user code we'd see a lot of time spent doing garbage collection (GC) to clean up all of the arrays we made. Even if we turn off saving we have these allocations.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@benchmark solve(prob,Tsit5(),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 1490 samples with 1 evaluation.\r\n Range (min … max):  2.010 ms … 14.612 ms  ┊ GC (min … max):  0.00% … 65.56%\r\n Time  (median):     2.313 ms              ┊ GC (median):     0.00%\r\n Time  (mean ± σ):   3.350 ms ±  2.095 ms  ┊ GC (mean ± σ):  14.55% ± 17.48%\r\n\r\n  █▇▅▁       ▃▅▅▄▃\r\n  █████▇▇▇▆▇███████▅▁▄▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▅▇▇▆▆█▇▇▆▆▆▆ █\r\n  2.01 ms      Histogram: log(frequency) by time       11 ms <\r\n\r\n Memory estimate: 6.83 MiB, allocs estimate: 89529.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"The problem of course is that arrays are created every time our derivative function is called. This function is called multiple times per step and is thus the main source of memory usage. To fix this, we can use the in-place form to ***make our code non-allocating***:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function lorenz!(du,u,p,t)\r\n du[1] = 10.0*(u[2]-u[1])\r\n du[2] = u[1]*(28.0-u[3]) - u[2]\r\n du[3] = u[1]*u[2] - (8/3)*u[3]\r\n nothing\r\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Here, instead of creating an array each time, we utilized the cache array du. When the in-place form is used, DifferentialEquations.jl takes a different internal route that minimizes the internal allocations as well.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"note: Note\nNotice that nothing is returned. When in in-place form, the ODE solver ignores the return. Instead, make sure that the original du array is mutated instead of constructing a new array","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"When we benchmark this function, we will see quite a difference.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"u0 = [1.0;0.0;0.0]\r\ntspan = (0.0,100.0)\r\nprob = ODEProblem(lorenz!,u0,tspan)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 8180 samples with 1 evaluation.\r\n Range (min … max):  415.800 μs …  12.112 ms  ┊ GC (min … max):  0.00% … 93.21%\r\n Time  (median):     463.700 μs               ┊ GC (median):     0.00%\r\n Time  (mean ± σ):   605.847 μs ± 695.892 μs  ┊ GC (mean ± σ):  11.02% ±  9.07%\r\n\r\n  ▄█▇▅▃▂▂▂▂▁▁▄▅▅▃▂▂▂▁▂▁▁                                        ▂\r\n  ███████████████████████▇▇▇▇▇▆▆▆▅▅▄▄▅▃▅▄▅▄▅▄▁▅▁▃▄▁▃▁▁▁▁▃▄▁▁▄▁▃ █\r\n  416 μs        Histogram: log(frequency) by time       1.68 ms <\r\n\r\n Memory estimate: 1016.36 KiB, allocs estimate: 11641.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@benchmark solve(prob,Tsit5(),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\r\n Range (min … max):  197.900 μs … 315.800 μs  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     206.700 μs               ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   207.688 μs ±   6.241 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n          ▃▅▅▇█▅▃       ▁▁▂▁▃▂▁\r\n  ▁▁▁▂▃▃▅▇█████████▇▇▇▆█████████▆▆▅▅▃▂▃▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▃\r\n  198 μs           Histogram: frequency by time          227 μs <\r\n\r\n Memory estimate: 4.94 KiB, allocs estimate: 41.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"There is a 16x time difference just from that change! Notice there are still some allocations and this is due to the construction of the integration cache. But this doesn't scale with the problem size:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"tspan = (0.0,500.0) # 5x longer than before\r\nprob = ODEProblem(lorenz!,u0,tspan)\r\n@benchmark solve(prob,Tsit5(),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 4755 samples with 1 evaluation.\r\n Range (min … max):  1.014 ms …  1.485 ms  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     1.042 ms              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   1.048 ms ± 31.281 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n    ▁▆▆▅▇█▇▆▆▂▁ ▁            ▁\r\n  ▂▄█████████████▇▇▇▇▇▇█▇▇█▇███▇█▇▇▅▆▄▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁ ▄\r\n  1.01 ms        Histogram: frequency by time        1.12 ms <\r\n\r\n Memory estimate: 4.94 KiB, allocs estimate: 41.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Since that's all setup allocations the user-side optimization is complete.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Further-Optimizations-of-Small-Non-Stiff-ODEs-with-StaticArrays","page":"Code Optimization for Differential Equations","title":"Further Optimizations of Small Non-Stiff ODEs with StaticArrays","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Allocations are only expensive if they are \"heap allocations\". For a more in-depth definition of heap allocations, there are a lot of sources online. But a good working definition is that heap allocations are variable-sized slabs of memory which have to be pointed to, and this pointer indirection costs time. Additionally, the heap has to be managed and the garbage controllers has to actively keep track of what's on the heap.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"However, there's an alternative to heap allocations, known as stack allocations. The stack is statically-sized (known at compile time) and thus its accesses are quick. Additionally, the exact block of memory is known in advance by the compiler, and thus re-using the memory is cheap. This means that allocating on the stack has essentially no cost!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Arrays have to be heap allocated because their size (and thus the amount of memory they take up) is determined at runtime. But there are structures in Julia which are stack-allocated. structs for example are stack-allocated \"value-type\"s. Tuples are a stack-allocated collection. The most useful data structure for DiffEq though is the StaticArray from the package StaticArrays.jl. These arrays have their length determined at compile-time. They are created using macros attached to normal array expressions, for example:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using StaticArrays\r\nA = SA[2.0,3.0,5.0]\r\ntypeof(A) # SVector{3, Float64} (alias for SArray{Tuple{3}, Float64, 1, 3})","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Notice that the 3 after SVector gives the size of the SVector. It cannot be changed. Additionally, SVectors are immutable, so we have to create a new SVector to change values. But remember, we don't have to worry about allocations because this data structure is stack-allocated. SArrays have a lot of extra optimizations as well: they have fast matrix multiplication, fast QR factorizations, etc. which directly make use of the information about the size of the array. Thus, when possible they should be used.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Unfortunately static arrays can only be used for sufficiently small arrays. After a certain size, they are forced to heap allocate after some instructions and their compile time balloons. Thus static arrays shouldn't be used if your system has more than ~20 variables. Additionally, only the native Julia algorithms can fully utilize static arrays.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Let's ***optimize lorenz using static arrays***. Note that in this case, we want to use the out-of-place allocating form, but this time we want to output a static array:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function lorenz_static(u,p,t)\r\n dx = 10.0*(u[2]-u[1])\r\n dy = u[1]*(28.0-u[3]) - u[2]\r\n dz = u[1]*u[2] - (8/3)*u[3]\r\n SA[dx,dy,dz]\r\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"To make the solver internally use static arrays, we simply give it a static array as the initial condition:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"u0 = SA[1.0,0.0,0.0]\r\ntspan = (0.0,100.0)\r\nprob = ODEProblem(lorenz_static,u0,tspan)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\r\n Range (min … max):  196.600 μs …   6.310 ms  ┊ GC (min … max): 0.00% … 95.70%\r\n Time  (median):     220.900 μs               ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   265.006 μs ± 302.623 μs  ┊ GC (mean ± σ):  6.91% ±  5.82%\r\n\r\n    ▅█▄▄▄\r\n  ▁▄██████▅▄▃▂▂▁▁▂▁▁▁▁▁▁▁▂▃▄▅▅▆▆▆▅▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁ ▂\r\n  197 μs           Histogram: frequency by time          374 μs <\r\n\r\n Memory estimate: 394.50 KiB, allocs estimate: 1319.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@benchmark solve(prob,Tsit5(),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\r\n Range (min … max):  144.100 μs … 242.600 μs  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     151.000 μs               ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   151.875 μs ±   7.502 μs  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n      █▇▇▂   ▁▅▄▂▂\r\n  ▂▂▃█████▇▇▇██████▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂ ▃\r\n  144 μs           Histogram: frequency by time          185 μs <\r\n\r\n Memory estimate: 3.67 KiB, allocs estimate: 22.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"And that's pretty much all there is to it. With static arrays you don't have to worry about allocating, so use operations like * and don't worry about fusing operations (discussed in the next section). Do \"the vectorized code\" of R/MATLAB/Python and your code in this case will be fast, or directly use the numbers/values.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Example-Accelerating-a-Stiff-Equation:-the-Robertson-Equation","page":"Code Optimization for Differential Equations","title":"Example Accelerating a Stiff Equation: the Robertson Equation","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"For these next examples, let's solve the Robertson equations (also known as ROBER):","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"beginaligned\r\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \r\nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \r\nfracdy_3dt = 3*10^7 y_2^2 \r\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Given that these equations are stiff, non-stiff ODE solvers like Tsit5 or Vern9 will fail to solve these equations. The automatic algorithm will detect this and automatically switch to something more robust to handle these issues. For example:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using DifferentialEquations\r\nfunction rober!(du,u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  du[1] = -k₁*y₁+k₃*y₂*y₃\r\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\r\n  du[3] =  k₂*y₂^2\r\n  nothing\r\nend\r\nprob = ODEProblem(rober!,[1.0,0.0,0.0],(0.0,1e5),[0.04,3e7,1e4])\r\nsol = solve(prob)\r\nplot(sol,tspan=(1e-2,1e5),xscale=:log10)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"julia> using BenchmarkTools\r\njulia> @btime solve(prob)\r\n97.000 μs (1832 allocations: 132.30 KiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Choosing-a-Good-Solver","page":"Code Optimization for Differential Equations","title":"Choosing a Good Solver","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Choosing a good solver is required for getting top notch speed. General recommendations can be found on the solver page (for example, the ODE Solver Recommendations). The current recommendations can be simplified to a Rosenbrock method (Rosenbrock23 or Rodas5) for smaller (<50 ODEs) problems, ESDIRK methods for slightly larger (TRBDF2 or KenCarp4 for <2000 ODEs), and QNDF for even larger problems. lsoda from LSODA.jl is sometimes worth a try for the medium sized category.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"More details on the solver to choose can be found by benchmarking. See the SciMLBenchmarks to compare many solvers on many problems.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"From this, we try the recommendation of Rosenbrock23() for stiff ODEs at default tolerances:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@btime solve(prob,Rosenbrock23())\r\n# 61.200 μs (918 allocations: 78.72 KiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Declaring-Jacobian-Functions","page":"Code Optimization for Differential Equations","title":"Declaring Jacobian Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"In order to reduce the Jacobian construction cost, one can describe a Jacobian function by using the jac argument for the ODEFunction. First we have to derive the Jacobian fracdf_idu_j which is J[i,j]. From this we get:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function rober_jac!(J,u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  J[1,1] = k₁ * -1\r\n  J[2,1] = k₁\r\n  J[3,1] = 0\r\n  J[1,2] = y₃ * k₃\r\n  J[2,2] = y₂ * k₂ * -2 + y₃ * k₃ * -1\r\n  J[3,2] = y₂ * 2 * k₂\r\n  J[1,3] = k₃ * y₂\r\n  J[2,3] = k₃ * y₂ * -1\r\n  J[3,3] = 0\r\n  nothing\r\nend\r\nf! = ODEFunction(rober!, jac=rober_jac!)\r\nprob_jac = ODEProblem(f!,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"julia> @btime solve(prob_jac,Rosenbrock23())\r\n57.400 μs (978 allocations: 82.58 KiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Automatic-Derivation-of-Jacobian-Functions","page":"Code Optimization for Differential Equations","title":"Automatic Derivation of Jacobian Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"But that was hard! If you want to take the symbolic Jacobian of numerical code, we can make use of ModelingToolkit.jl to symbolic-ify the numerical code and do the symbolic calculation and return the Julia code for this.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using ModelingToolkit\r\nde = modelingtoolkitize(prob)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"We can tell it to compute the Jacobian if we want to see the code:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"julia> ModelingToolkit.generate_jacobian(de)[2] # Second is in-place\r\n:(function (ˍ₋out, ˍ₋arg1, ˍ₋arg2, t)\r\n      #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:303 =#\r\n      #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:304 =#\r\n      let var\"x₁(t)\" = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg1[1]), var\"x₂(t)\" = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg1[2]), var\"x₃(t)\" = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg1[3]), α₁ = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg2[1]), α₂ = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg2[2]), α₃ = #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:190 =# @inbounds(ˍ₋arg2[3])\r\n          #= C:\\Users\\accou\\.julia\\dev\\Symbolics\\src\\build_function.jl:378 =#\r\n          #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:350 =# @inbounds begin\r\n                  #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:346 =#\r\n                  ˍ₋out[1] = (*)(-1, α₁)\r\n                  ˍ₋out[2] = α₁\r\n                  ˍ₋out[3] = 0\r\n                  ˍ₋out[4] = (*)(α₃, var\"x₃(t)\")\r\n                  ˍ₋out[5] = (+)((*)((*)(-2, α₂), var\"x₂(t)\"), (*)((*)(-1, α₃), var\"x₃(t)\"))\r\n                  ˍ₋out[6] = (*)((*)(2, α₂), var\"x₂(t)\")\r\n                  ˍ₋out[7] = (*)(α₃, var\"x₂(t)\")\r\n                  ˍ₋out[8] = (*)((*)(-1, α₃), var\"x₂(t)\")\r\n                  ˍ₋out[9] = 0\r\n                  #= C:\\Users\\accou\\.julia\\packages\\SymbolicUtils\\0KTj4\\src\\code.jl:348 =#\r\n                  nothing\r\n              end\r\n      end\r\n  end)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Now let's use that to give the analytical solution Jacobian:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"prob_jac2 = ODEProblem(de,[],(0.0,1e5),jac=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"julia> @btime solve(prob_jac2)\r\n122.600 μs (1425 allocations: 99.34 KiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"See the ModelingToolkit.jl documentation for more details.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Accelerating-Small-ODE-Solves-with-Static-Arrays","page":"Code Optimization for Differential Equations","title":"Accelerating Small ODE Solves with Static Arrays","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"If the ODE is sufficiently small (<20 ODEs or so), using StaticArrays.jl for the state variables can greatly enhance the performance. This is done by making u0 a StaticArray and writing an out-of-place non-mutating dispatch for static arrays, for the ROBER problem, this looks like:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using DifferentialEquations, StaticArrays\r\nfunction rober_static(u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  du1 = -k₁*y₁+k₃*y₂*y₃\r\n  du2 =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\r\n  du3 =  k₂*y₂^2\r\n  SA[du1,du2,du3]\r\nend\r\nprob = ODEProblem(rober_static,SA[1.0,0.0,0.0],(0.0,1e5),SA[0.04,3e7,1e4])\r\nsol = solve(prob,Rosenbrock23())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"If we benchmark this we see a really fast solution with really low allocation counts:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@btime sol = solve(prob,Rosenbrock23())\r\n# 12.100 μs (87 allocations: 18.53 KiB)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"This version is thus very amenable to multithreading and other forms of parallelism.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Example-Accelerating-Linear-Algebra-PDE-Semi-Discretization","page":"Code Optimization for Differential Equations","title":"Example Accelerating Linear Algebra PDE Semi-Discretization","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"In this tutorial we will optimize the right-hand side definition of a PDE semi-discretization.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"note: Note\nWe highly recommend looking at the Solving Large Stiff Equations tutorial for details on customizing DifferentialEquations.jl for more efficient large-scale stiff ODE solving. This section will only focus on the user-side code.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Let's optimize the solution of a Reaction-Diffusion PDE's discretization. In its discretized form, this is the ODE:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"beginalign\r\ndu = D_1 (A_y u + u A_x) + fracau^2v + baru - alpha u\r\ndv = D_2 (A_y v + v A_x) + a u^2 + beta v\r\nendalign","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"where u, v, and A are matrices. Here, we will use the simplified version where A is the tridiagonal stencil 1-21, i.e. it's the 2D discretization of the LaPlacian. The native code would be something along the lines of:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using DifferentialEquations, LinearAlgebra\r\n# Generate the constants\r\np = (1.0,1.0,1.0,10.0,0.001,100.0) # a,α,ubar,β,D1,D2\r\nN = 100\r\nAx = Array(Tridiagonal([1.0 for i in 1:N-1],[-2.0 for i in 1:N],[1.0 for i in 1:N-1]))\r\nAy = copy(Ax)\r\nAx[2,1] = 2.0\r\nAx[end-1,end] = 2.0\r\nAy[1,2] = 2.0\r\nAy[end,end-1] = 2.0\r\n\r\nfunction basic_version!(dr,r,p,t)\r\n  a,α,ubar,β,D1,D2 = p\r\n  u = r[:,:,1]\r\n  v = r[:,:,2]\r\n  Du = D1*(Ay*u + u*Ax)\r\n  Dv = D2*(Ay*v + v*Ax)\r\n  dr[:,:,1] = Du .+ a.*u.*u./v .+ ubar .- α*u\r\n  dr[:,:,2] = Dv .+ a.*u.*u .- β*v\r\nend\r\n\r\na,α,ubar,β,D1,D2 = p\r\nuss = (ubar+β)/α\r\nvss = (a/β)*uss^2\r\nr0 = zeros(100,100,2)\r\nr0[:,:,1] .= uss.+0.1.*rand.()\r\nr0[:,:,2] .= vss\r\n\r\nprob = ODEProblem(basic_version!,r0,(0.0,0.1),p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"In this version we have encoded our initial condition to be a 3-dimensional array, with u[:,:,1] being the A part and u[:,:,2] being the B part.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 48 samples with 1 evaluation.\r\n Range (min … max):   96.001 ms … 130.443 ms  ┊ GC (min … max):  7.04% … 16.06%\r\n Time  (median):     104.225 ms               ┊ GC (median):    10.48%\r\n Time  (mean ± σ):   105.063 ms ±   6.812 ms  ┊ GC (mean ± σ):   9.42% ±  2.62%\r\n\r\n  ▃█▃   █▃█    ▃██   ▃█ ▃▃           ▃\r\n  ███▇▁▇███▇▇▁▇███▁▇▁██▇██▇▁▁▇▇▇▁▇▁▁▁█▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇ ▁\r\n  96 ms            Histogram: frequency by time          130 ms <\r\n\r\n Memory estimate: 186.83 MiB, allocs estimate: 7341.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"While this version isn't very efficient,","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#We-recommend-writing-the-\"high-level\"-code-first,-and-iteratively-optimizing-it!","page":"Code Optimization for Differential Equations","title":"We recommend writing the \"high-level\" code first, and iteratively optimizing it!","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"The first thing that we can do is get rid of the slicing allocations. The operation r[:,:,1] creates a temporary array instead of a \"view\", i.e. a pointer to the already existing memory. To make it a view, add @view. Note that we have to be careful with views because they point to the same memory, and thus changing a view changes the original values:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"A = rand(4)\r\n@show A\r\nB = @view A[1:3]\r\nB[2] = 2\r\n@show A","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Notice that changing B changed A. This is something to be careful of, but at the same time we want to use this since we want to modify the output dr. Additionally, the last statement is a purely element-wise operation, and thus we can make use of broadcast fusion there. Let's rewrite basic_version! to ***avoid slicing allocations*** and to ***use broadcast fusion***:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function gm2!(dr,r,p,t)\r\n  a,α,ubar,β,D1,D2 = p\r\n  u = @view r[:,:,1]\r\n  v = @view r[:,:,2]\r\n  du = @view dr[:,:,1]\r\n  dv = @view dr[:,:,2]\r\n  Du = D1*(Ay*u + u*Ax)\r\n  Dv = D2*(Ay*v + v*Ax)\r\n  @. du = Du + a.*u.*u./v + ubar - α*u\r\n  @. dv = Dv + a.*u.*u - β*v\r\nend\r\nprob = ODEProblem(gm2!,r0,(0.0,0.1),p)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 58 samples with 1 evaluation.\r\n Range (min … max):  80.456 ms … 106.830 ms  ┊ GC (min … max): 0.00% … 10.74%\r\n Time  (median):     85.858 ms               ┊ GC (median):    6.34%\r\n Time  (mean ± σ):   86.916 ms ±   4.214 ms  ┊ GC (mean ± σ):  6.46% ±  1.75%\r\n\r\n              █ ▄\r\n  ▅▃▁▁▁▁▁▁▁▆▃▆█▃██▆▆▅▃▃▅▃█▆▁▃▁▃▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃ ▁\r\n  80.5 ms         Histogram: frequency by time          102 ms <\r\n\r\n Memory estimate: 119.71 MiB, allocs estimate: 5871.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Now, most of the allocations are taking place in Du = D1*(Ay*u + u*Ax) since those operations are vectorized and not mutating. We should instead replace the matrix multiplications with mul!. When doing so, we will need to have cache variables to write into. This looks like:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Ayu = zeros(N,N)\r\nuAx = zeros(N,N)\r\nDu = zeros(N,N)\r\nAyv = zeros(N,N)\r\nvAx = zeros(N,N)\r\nDv = zeros(N,N)\r\nfunction gm3!(dr,r,p,t)\r\n  a,α,ubar,β,D1,D2 = p\r\n  u = @view r[:,:,1]\r\n  v = @view r[:,:,2]\r\n  du = @view dr[:,:,1]\r\n  dv = @view dr[:,:,2]\r\n  mul!(Ayu,Ay,u)\r\n  mul!(uAx,u,Ax)\r\n  mul!(Ayv,Ay,v)\r\n  mul!(vAx,v,Ax)\r\n  @. Du = D1*(Ayu + uAx)\r\n  @. Dv = D2*(Ayv + vAx)\r\n  @. du = Du + a*u*u./v + ubar - α*u\r\n  @. dv = Dv + a*u*u - β*v\r\nend\r\nprob = ODEProblem(gm3!,r0,(0.0,0.1),p)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 71 samples with 1 evaluation.\r\n Range (min … max):  66.051 ms … 78.626 ms  ┊ GC (min … max): 0.00% … 6.51%\r\n Time  (median):     69.778 ms              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   70.563 ms ±  3.392 ms  ┊ GC (mean ± σ):  1.68% ± 2.94%\r\n\r\n     ▂▆█    ▂\r\n  ▄▁▄███▁▆▄▁█▁▁▆█▁▆▆▁▄▁▆▁▄▁▄▄▆▁▄▆▁▁▁▆▁▄▄▁▄█▆▄▁▆▄▆▁▆▁▆▆▁▁▁▁▄▁▄ ▁\r\n  66.1 ms         Histogram: frequency by time        77.1 ms <\r\n\r\n Memory estimate: 29.98 MiB, allocs estimate: 4695.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"But our temporary variables are global variables. We need to either declare the caches as const or localize them. We can localize them by adding them to the parameters, p. It's easier for the compiler to reason about local variables than global variables. ***Localizing variables helps to ensure type stability***.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"p = (1.0,1.0,1.0,10.0,0.001,100.0,Ayu,uAx,Du,Ayv,vAx,Dv) # a,α,ubar,β,D1,D2\r\nfunction gm4!(dr,r,p,t)\r\n  a,α,ubar,β,D1,D2,Ayu,uAx,Du,Ayv,vAx,Dv = p\r\n  u = @view r[:,:,1]\r\n  v = @view r[:,:,2]\r\n  du = @view dr[:,:,1]\r\n  dv = @view dr[:,:,2]\r\n  mul!(Ayu,Ay,u)\r\n  mul!(uAx,u,Ax)\r\n  mul!(Ayv,Ay,v)\r\n  mul!(vAx,v,Ax)\r\n  @. Du = D1*(Ayu + uAx)\r\n  @. Dv = D2*(Ayv + vAx)\r\n  @. du = Du + a*u*u./v + ubar - α*u\r\n  @. dv = Dv + a*u*u - β*v\r\nend\r\nprob = ODEProblem(gm4!,r0,(0.0,0.1),p)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 75 samples with 1 evaluation.\r\n Range (min … max):  63.820 ms … 76.176 ms  ┊ GC (min … max): 0.00% … 6.03%\r\n Time  (median):     66.711 ms              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   67.396 ms ±  3.167 ms  ┊ GC (mean ± σ):  1.55% ± 2.78%\r\n\r\n  ▁▁█▁█▃▆         ▁ ▁        ▆         ▁\r\n  ███████▄▁▁▄▁▇▁▄▁█▄█▁▄▇▁▄▄▄▇█▁▁▁▁▄▁▄▁▇█▁▁▁▄▁▄▄▇▁▁▁▇▁▄▁▁▁▁▁▇▇ ▁\r\n  63.8 ms         Histogram: frequency by time          74 ms <\r\n\r\n Memory estimate: 29.66 MiB, allocs estimate: 1020.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"We could then use the BLAS gemmv to optimize the matrix multiplications some more, but instead let's devectorize the stencil.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"p = (1.0,1.0,1.0,10.0,0.001,100.0,N)\r\nfunction fast_gm!(du,u,p,t)\r\n  a,α,ubar,β,D1,D2,N = p\r\n\r\n  @inbounds for j in 2:N-1, i in 2:N-1\r\n    du[i,j,1] = D1*(u[i-1,j,1] + u[i+1,j,1] + u[i,j+1,1] + u[i,j-1,1] - 4u[i,j,1]) +\r\n              a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n  end\r\n\r\n  @inbounds for j in 2:N-1, i in 2:N-1\r\n    du[i,j,2] = D2*(u[i-1,j,2] + u[i+1,j,2] + u[i,j+1,2] + u[i,j-1,2] - 4u[i,j,2]) +\r\n            a*u[i,j,1]^2 - β*u[i,j,2]\r\n  end\r\n\r\n  @inbounds for j in 2:N-1\r\n    i = 1\r\n    du[1,j,1] = D1*(2u[i+1,j,1] + u[i,j+1,1] + u[i,j-1,1] - 4u[i,j,1]) +\r\n            a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n  end\r\n  @inbounds for j in 2:N-1\r\n    i = 1\r\n    du[1,j,2] = D2*(2u[i+1,j,2] + u[i,j+1,2] + u[i,j-1,2] - 4u[i,j,2]) +\r\n            a*u[i,j,1]^2 - β*u[i,j,2]\r\n  end\r\n  @inbounds for j in 2:N-1\r\n    i = N\r\n    du[end,j,1] = D1*(2u[i-1,j,1] + u[i,j+1,1] + u[i,j-1,1] - 4u[i,j,1]) +\r\n           a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n  end\r\n  @inbounds for j in 2:N-1\r\n    i = N\r\n    du[end,j,2] = D2*(2u[i-1,j,2] + u[i,j+1,2] + u[i,j-1,2] - 4u[i,j,2]) +\r\n           a*u[i,j,1]^2 - β*u[i,j,2]\r\n  end\r\n\r\n  @inbounds for i in 2:N-1\r\n    j = 1\r\n    du[i,1,1] = D1*(u[i-1,j,1] + u[i+1,j,1] + 2u[i,j+1,1] - 4u[i,j,1]) +\r\n              a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n  end\r\n  @inbounds for i in 2:N-1\r\n    j = 1\r\n    du[i,1,2] = D2*(u[i-1,j,2] + u[i+1,j,2] + 2u[i,j+1,2] - 4u[i,j,2]) +\r\n              a*u[i,j,1]^2 - β*u[i,j,2]\r\n  end\r\n  @inbounds for i in 2:N-1\r\n    j = N\r\n    du[i,end,1] = D1*(u[i-1,j,1] + u[i+1,j,1] + 2u[i,j-1,1] - 4u[i,j,1]) +\r\n             a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n  end\r\n  @inbounds for i in 2:N-1\r\n    j = N\r\n    du[i,end,2] = D2*(u[i-1,j,2] + u[i+1,j,2] + 2u[i,j-1,2] - 4u[i,j,2]) +\r\n             a*u[i,j,1]^2 - β*u[i,j,2]\r\n  end\r\n\r\n  @inbounds begin\r\n    i = 1; j = 1\r\n    du[1,1,1] = D1*(2u[i+1,j,1] + 2u[i,j+1,1] - 4u[i,j,1]) +\r\n              a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n    du[1,1,2] = D2*(2u[i+1,j,2] + 2u[i,j+1,2] - 4u[i,j,2]) +\r\n              a*u[i,j,1]^2 - β*u[i,j,2]\r\n\r\n    i = 1; j = N\r\n    du[1,N,1] = D1*(2u[i+1,j,1] + 2u[i,j-1,1] - 4u[i,j,1]) +\r\n             a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n    du[1,N,2] = D2*(2u[i+1,j,2] + 2u[i,j-1,2] - 4u[i,j,2]) +\r\n             a*u[i,j,1]^2 - β*u[i,j,2]\r\n\r\n    i = N; j = 1\r\n    du[N,1,1] = D1*(2u[i-1,j,1] + 2u[i,j+1,1] - 4u[i,j,1]) +\r\n             a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n    du[N,1,2] = D2*(2u[i-1,j,2] + 2u[i,j+1,2] - 4u[i,j,2]) +\r\n             a*u[i,j,1]^2 - β*u[i,j,2]\r\n\r\n    i = N; j = N\r\n    du[end,end,1] = D1*(2u[i-1,j,1] + 2u[i,j-1,1] - 4u[i,j,1]) +\r\n             a*u[i,j,1]^2/u[i,j,2] + ubar - α*u[i,j,1]\r\n    du[end,end,2] = D2*(2u[i-1,j,2] + 2u[i,j-1,2] - 4u[i,j,2]) +\r\n             a*u[i,j,1]^2 - β*u[i,j,2]\r\n   end\r\nend\r\nprob = ODEProblem(fast_gm!,r0,(0.0,0.1),p)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 700 samples with 1 evaluation.\r\n Range (min … max):  5.433 ms … 26.007 ms  ┊ GC (min … max):  0.00% … 56.34%\r\n Time  (median):     5.878 ms              ┊ GC (median):     0.00%\r\n Time  (mean ± σ):   7.138 ms ±  2.348 ms  ┊ GC (mean ± σ):  11.10% ± 13.88%\r\n\r\n  ▄▇█▆▃       ▁           ▁▁▁      ▁ ▁▁▂\r\n  ██████▆▆█▇▇▆███▇▆▆▄▆█▇█▇███▇▆▄█▇██████▆██▇▆▁▄▁▁▁▄▄▁▁▁▁▁▁▁▄ █\r\n  5.43 ms      Histogram: log(frequency) by time     13.4 ms <\r\n\r\n Memory estimate: 29.62 MiB, allocs estimate: 432.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Notice that in this case fusing the loops and avoiding the linear operators is a major improvement of about 10x! That's an order of magnitude faster than our original MATLAB/SciPy/R vectorized style code!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Since this is tedious to do by hand, we note that ModelingToolkit.jl's symbolic code generation can do this automatically from the basic version:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"function basic_version!(dr,r,p,t)\r\n  a,α,ubar,β,D1,D2 = p\r\n  u = r[:,:,1]\r\n  v = r[:,:,2]\r\n  Du = D1*(Ay*u + u*Ax)\r\n  Dv = D2*(Ay*v + v*Ax)\r\n  dr[:,:,1] = Du .+ a.*u.*u./v .+ ubar .- α*u\r\n  dr[:,:,2] = Dv .+ a.*u.*u .- β*v\r\nend\r\n\r\na,α,ubar,β,D1,D2 = p\r\nuss = (ubar+β)/α\r\nvss = (a/β)*uss^2\r\nr0 = zeros(100,100,2)\r\nr0[:,:,1] .= uss.+0.1.*rand.()\r\nr0[:,:,2] .= vss\r\n\r\nprob = ODEProblem(basic_version!,r0,(0.0,0.1),p)\r\nde = modelingtoolkitize(prob)\r\n\r\n# Note jac=true,sparse=true makes it automatically build sparse Jacobian code\r\n# as well!\r\n\r\nfastprob = ODEProblem(de,[],(0.0,0.1),jac=true,sparse=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Lastly, we can do other things like multithread the main loops. LoopVectorization.jl provides the @turbo macro for doing a lot of SIMD enhancements, and @tturbo is the multithreaded version.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/#Optimizing-Algorithm-Choices","page":"Code Optimization for Differential Equations","title":"Optimizing Algorithm Choices","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"The last thing to do is then ***optimize our algorithm choice***. We have been using Tsit5() as our test algorithm, but in reality this problem is a stiff PDE discretization and thus one recommendation is to use CVODE_BDF(). However, instead of using the default dense Jacobian, we should make use of the sparse Jacobian afforded by the problem. The Jacobian is the matrix fracdf_idr_j, where r is read by the linear index (i.e. down columns). But since the u variables depend on the v, the band size here is large, and thus this will not do well with a Banded Jacobian solver. Instead, we utilize sparse Jacobian algorithms. CVODE_BDF allows us to use a sparse Newton-Krylov solver by setting linear_solver = :GMRES.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"note: Note\nThe Solving Large Stiff Equations tutorial goes through these details. This is simply to give a taste of how much optimization opportunity is left on the table!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Let's see how our fast right-hand side scales as we increase the integration time.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"prob = ODEProblem(fast_gm!,r0,(0.0,10.0),p)\r\n@benchmark solve(prob,Tsit5())\r\n\r\nBenchmarkTools.Trial: 3 samples with 1 evaluation.\r\n Range (min … max):  1.578 s …    2.502 s  ┊ GC (min … max): 31.99% … 58.83%\r\n Time  (median):     1.580 s               ┊ GC (median):    35.16%\r\n Time  (mean ± σ):   1.887 s ± 532.716 ms  ┊ GC (mean ± σ):  44.74% ± 14.66%\r\n\r\n  █                                                        ▁\r\n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\r\n  1.58 s         Histogram: frequency by time          2.5 s <\r\n\r\n Memory estimate: 2.76 GiB, allocs estimate: 39323.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"using Sundials\r\n@benchmark solve(prob,CVODE_BDF(linear_solver=:GMRES))\r\n\r\nBenchmarkTools.Trial: 11 samples with 1 evaluation.\r\n Range (min … max):  450.051 ms … 476.904 ms  ┊ GC (min … max): 0.00% … 0.38%\r\n Time  (median):     460.246 ms               ┊ GC (median):    0.75%\r\n Time  (mean ± σ):   461.439 ms ±   9.264 ms  ┊ GC (mean ± σ):  0.56% ± 0.33%\r\n\r\n  █    █   █  █ █        █ ██                        █   █    █\r\n  █▁▁▁▁█▁▁▁█▁▁█▁█▁▁▁▁▁▁▁▁█▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁█ ▁\r\n  450 ms           Histogram: frequency by time          477 ms <\r\n\r\n Memory estimate: 120.93 MiB, allocs estimate: 20000.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"prob = ODEProblem(fast_gm!,r0,(0.0,100.0),p)\r\n# Will go out of memory if we don't turn off `save_everystep`!\r\n@benchmark solve(prob,Tsit5(),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 2 samples with 1 evaluation.\r\n Range (min … max):  3.075 s …   3.095 s  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     3.085 s              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   3.085 s ± 14.570 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n  █                                                       █\r\n  █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\r\n  3.07 s         Histogram: frequency by time         3.1 s <\r\n\r\n Memory estimate: 2.90 MiB, allocs estimate: 60.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"@benchmark solve(prob,CVODE_BDF(linear_solver=:GMRES),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 4 samples with 1 evaluation.\r\n Range (min … max):  1.342 s …   1.386 s  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     1.352 s              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   1.358 s ± 19.636 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n  █    █               █                                  █\r\n  █▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\r\n  1.34 s         Histogram: frequency by time        1.39 s <\r\n\r\n Memory estimate: 3.09 MiB, allocs estimate: 49880.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"prob = ODEProblem(fast_gm!,r0,(0.0,500.0),p)\r\n@benchmark solve(prob,CVODE_BDF(linear_solver=:GMRES),save_everystep=false)\r\n\r\nBenchmarkTools.Trial: 3 samples with 1 evaluation.\r\n Range (min … max):  1.817 s …   1.915 s  ┊ GC (min … max): 0.00% … 0.00%\r\n Time  (median):     1.825 s              ┊ GC (median):    0.00%\r\n Time  (mean ± σ):   1.853 s ± 54.179 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\r\n\r\n  █   █                                                   █\r\n  █▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\r\n  1.82 s         Histogram: frequency by time        1.91 s <\r\n\r\n Memory estimate: 3.83 MiB, allocs estimate: 66931.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Notice that we've eliminated almost all allocations, allowing the code to grow without hitting garbage collection and slowing down.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Why is CVODE_BDF doing well? What's happening is that, because the problem is stiff, the number of steps required by the explicit Runge-Kutta method grows rapidly, whereas CVODE_BDF is taking large steps. Additionally, the GMRES linear solver form is quite an efficient way to solve the implicit system in this case. This is problem-dependent, and in many cases using a Krylov method effectively requires a preconditioner, so you need to play around with testing other algorithms and linear solvers to find out what works best with your problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/faster_ode_example/","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"Now continue to the Solving Large Stiff Equations tutorial for more details on optimizing the algorithm choice for such codes.","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/#brusssys","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"","category":"section"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"Here's the generated system of equations for the Brusselator, with dx = dy = 1/4","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/#Equations-for-u","page":"Generated ODE system for the Brusselator Equation","title":"Equations for u","text":"","category":"section"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"[Differential(t)(u[2, 2](t)) ~ 1.0 + 160.0u[3, 2](t) + 160.0u[5, 2](t) - 320.0u[2, 2](t) + 160.0u[2, 3](t) + 160.0u[2, 5](t) - 320.0u[2, 2](t) + (u[2, 2](t)^2)*v[2, 2](t) - 4.4u[2, 2](t), \nDifferential(t)(u[3, 2](t)) ~ 1.0 + 160.0u[2, 2](t) + 160.0u[4, 2](t) - 320.0u[3, 2](t) + 160.0u[3, 3](t) + 160.0u[3, 5](t) - 320.0u[3, 2](t) + (u[3, 2](t)^2)*v[3, 2](t) - 4.4u[3, 2](t), \nDifferential(t)(u[4, 2](t)) ~ 1.0 + 160.0u[4, 3](t) + 160.0u[4, 5](t) - 320.0u[4, 2](t) + 160.0u[3, 2](t) + 160.0u[5, 2](t) - 320.0u[4, 2](t) + (u[4, 2](t)^2)*v[4, 2](t) - 4.4u[4, 2](t), \nDifferential(t)(u[5, 2](t)) ~ 1.0 + 160.0u[2, 2](t) + 160.0u[4, 2](t) - 320.0u[5, 2](t) + 160.0u[5, 3](t) + 160.0u[5, 5](t) - 320.0u[5, 2](t) + (u[5, 2](t)^2)*v[5, 2](t) - 4.4u[5, 2](t), \nDifferential(t)(u[2, 3](t)) ~ 1.0 + 160.0u[2, 2](t) + 160.0u[2, 4](t) - 320.0u[2, 3](t) + 160.0u[3, 3](t) + 160.0u[5, 3](t) - 320.0u[2, 3](t) + (u[2, 3](t)^2)*v[2, 3](t) - 4.4u[2, 3](t), \nDifferential(t)(u[3, 3](t)) ~ 1.0 + 160.0u[2, 3](t) + 160.0u[4, 3](t) - 320.0u[3, 3](t) + 160.0u[3, 2](t) + 160.0u[3, 4](t) - 320.0u[3, 3](t) + (u[3, 3](t)^2)*v[3, 3](t) - 4.4u[3, 3](t), \nDifferential(t)(u[4, 3](t)) ~ 1.0 + 160.0u[3, 3](t) + 160.0u[5, 3](t) - 320.0u[4, 3](t) + 160.0u[4, 2](t) + 160.0u[4, 4](t) - 320.0u[4, 3](t) + (u[4, 3](t)^2)*v[4, 3](t) - 4.4u[4, 3](t), \nDifferential(t)(u[5, 3](t)) ~ 1.0 + 160.0u[2, 3](t) + 160.0u[4, 3](t) - 320.0u[5, 3](t) + 160.0u[5, 2](t) + 160.0u[5, 4](t) - 320.0u[5, 3](t) + (u[5, 3](t)^2)*v[5, 3](t) - 4.4u[5, 3](t), \nDifferential(t)(u[2, 4](t)) ~ 1.0 + 160.0u[3, 4](t) + 160.0u[5, 4](t) - 320.0u[2, 4](t) + 160.0u[2, 3](t) + 160.0u[2, 5](t) - 320.0u[2, 4](t) + (u[2, 4](t)^2)*v[2, 4](t) - 4.4u[2, 4](t), \nDifferential(t)(u[3, 4](t)) ~ 1.0 + 160.0u[2, 4](t) + 160.0u[4, 4](t) - 320.0u[3, 4](t) + 160.0u[3, 3](t) + 160.0u[3, 5](t) - 320.0u[3, 4](t) + (u[3, 4](t)^2)*v[3, 4](t) - 4.4u[3, 4](t), \nDifferential(t)(u[4, 4](t)) ~ 1.0 + 160.0u[4, 3](t) + 160.0u[4, 5](t) - 320.0u[4, 4](t) + 160.0u[3, 4](t) + 160.0u[5, 4](t) - 320.0u[4, 4](t) + (u[4, 4](t)^2)*v[4, 4](t) - 4.4u[4, 4](t), \nDifferential(t)(u[5, 4](t)) ~ 1.0 + 160.0u[2, 4](t) + 160.0u[4, 4](t) - 320.0u[5, 4](t) + 160.0u[5, 3](t) + 160.0u[5, 5](t) - 320.0u[5, 4](t) + (u[5, 4](t)^2)*v[5, 4](t) - 4.4u[5, 4](t), \nDifferential(t)(u[2, 5](t)) ~ 1.0 + 160.0u[2, 2](t) + 160.0u[2, 4](t) - 320.0u[2, 5](t) + 160.0u[3, 5](t) + 160.0u[5, 5](t) - 320.0u[2, 5](t) + (u[2, 5](t)^2)*v[2, 5](t) - 4.4u[2, 5](t), \nDifferential(t)(u[3, 5](t)) ~ 1.0 + 160.0u[2, 5](t) + 160.0u[4, 5](t) - 320.0u[3, 5](t) + 160.0u[3, 2](t) + 160.0u[3, 4](t) - 320.0u[3, 5](t) + (u[3, 5](t)^2)*v[3, 5](t) - 4.4u[3, 5](t), \nDifferential(t)(u[4, 5](t)) ~ 1.0 + 160.0u[3, 5](t) + 160.0u[5, 5](t) - 320.0u[4, 5](t) + 160.0u[4, 2](t) + 160.0u[4, 4](t) - 320.0u[4, 5](t) + (u[4, 5](t)^2)*v[4, 5](t) - 4.4u[4, 5](t), \nDifferential(t)(u[5, 5](t)) ~ 1.0 + 160.0u[2, 5](t) + 160.0u[4, 5](t) - 320.0u[5, 5](t) + 160.0u[5, 2](t) + 160.0u[5, 4](t) - 320.0u[5, 5](t) + (u[5, 5](t)^2)*v[5, 5](t) - 4.4u[5, 5](t)] ","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/#Equations-for-v","page":"Generated ODE system for the Brusselator Equation","title":"Equations for v","text":"","category":"section"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"[Differential(t)(v[2, 2](t)) ~ 160.0v[2, 3](t) + 160.0v[2, 5](t) - 320.0v[2, 2](t) + 160.0v[3, 2](t) + 160.0v[5, 2](t) - 320.0v[2, 2](t) + 3.4u[2, 2](t) - (u[2, 2](t)^2)*v[2, 2](t), \nDifferential(t)(v[3, 2](t)) ~ 160.0v[2, 2](t) + 160.0v[4, 2](t) - 320.0v[3, 2](t) + 160.0v[3, 3](t) + 160.0v[3, 5](t) - 320.0v[3, 2](t) + 3.4u[3, 2](t) - (u[3, 2](t)^2)*v[3, 2](t), \nDifferential(t)(v[4, 2](t)) ~ 160.0v[3, 2](t) + 160.0v[5, 2](t) - 320.0v[4, 2](t) + 160.0v[4, 3](t) + 160.0v[4, 5](t) - 320.0v[4, 2](t) + 3.4u[4, 2](t) - (u[4, 2](t)^2)*v[4, 2](t), \nDifferential(t)(v[5, 2](t)) ~ 160.0v[5, 3](t) + 160.0v[5, 5](t) - 320.0v[5, 2](t) + 160.0v[2, 2](t) + 160.0v[4, 2](t) - 320.0v[5, 2](t) + 3.4u[5, 2](t) - (u[5, 2](t)^2)*v[5, 2](t), \nDifferential(t)(v[2, 3](t)) ~ 160.0v[2, 2](t) + 160.0v[2, 4](t) - 320.0v[2, 3](t) + 160.0v[3, 3](t) + 160.0v[5, 3](t) - 320.0v[2, 3](t) + 3.4u[2, 3](t) - (u[2, 3](t)^2)*v[2, 3](t), \nDifferential(t)(v[3, 3](t)) ~ 160.0v[3, 2](t) + 160.0v[3, 4](t) - 320.0v[3, 3](t) + 160.0v[2, 3](t) + 160.0v[4, 3](t) - 320.0v[3, 3](t) + 3.4u[3, 3](t) - (u[3, 3](t)^2)*v[3, 3](t), \nDifferential(t)(v[4, 3](t)) ~ 160.0v[3, 3](t) + 160.0v[5, 3](t) - 320.0v[4, 3](t) + 160.0v[4, 2](t) + 160.0v[4, 4](t) - 320.0v[4, 3](t) + 3.4u[4, 3](t) - (u[4, 3](t)^2)*v[4, 3](t), \nDifferential(t)(v[5, 3](t)) ~ 160.0v[2, 3](t) + 160.0v[4, 3](t) - 320.0v[5, 3](t) + 160.0v[5, 2](t) + 160.0v[5, 4](t) - 320.0v[5, 3](t) + 3.4u[5, 3](t) - (u[5, 3](t)^2)*v[5, 3](t), \nDifferential(t)(v[2, 4](t)) ~ 160.0v[2, 3](t) + 160.0v[2, 5](t) - 320.0v[2, 4](t) + 160.0v[3, 4](t) + 160.0v[5, 4](t) - 320.0v[2, 4](t) + 3.4u[2, 4](t) - (u[2, 4](t)^2)*v[2, 4](t), \nDifferential(t)(v[3, 4](t)) ~ 160.0v[2, 4](t) + 160.0v[4, 4](t) - 320.0v[3, 4](t) + 160.0v[3, 3](t) + 160.0v[3, 5](t) - 320.0v[3, 4](t) + 3.4u[3, 4](t) - (u[3, 4](t)^2)*v[3, 4](t), \nDifferential(t)(v[4, 4](t)) ~ 160.0v[3, 4](t) + 160.0v[5, 4](t) - 320.0v[4, 4](t) + 160.0v[4, 3](t) + 160.0v[4, 5](t) - 320.0v[4, 4](t) + 3.4u[4, 4](t) - (u[4, 4](t)^2)*v[4, 4](t), \nDifferential(t)(v[5, 4](t)) ~ 160.0v[2, 4](t) + 160.0v[4, 4](t) - 320.0v[5, 4](t) + 160.0v[5, 3](t) + 160.0v[5, 5](t) - 320.0v[5, 4](t) + 3.4u[5, 4](t) - (u[5, 4](t)^2)*v[5, 4](t), \nDifferential(t)(v[2, 5](t)) ~ 160.0v[2, 2](t) + 160.0v[2, 4](t) - 320.0v[2, 5](t) + 160.0v[3, 5](t) + 160.0v[5, 5](t) - 320.0v[2, 5](t) + 3.4u[2, 5](t) - (u[2, 5](t)^2)*v[2, 5](t), \nDifferential(t)(v[3, 5](t)) ~ 160.0v[2, 5](t) + 160.0v[4, 5](t) - 320.0v[3, 5](t) + 160.0v[3, 2](t) + 160.0v[3, 4](t) - 320.0v[3, 5](t) + 3.4u[3, 5](t) - (u[3, 5](t)^2)*v[3, 5](t), \nDifferential(t)(v[4, 5](t)) ~ 160.0v[4, 2](t) + 160.0v[4, 4](t) - 320.0v[4, 5](t) + 160.0v[3, 5](t) + 160.0v[5, 5](t) - 320.0v[4, 5](t) + 3.4u[4, 5](t) - (u[4, 5](t)^2)*v[4, 5](t), \nDifferential(t)(v[5, 5](t)) ~ 160.0v[2, 5](t) + 160.0v[4, 5](t) - 320.0v[5, 5](t) + 160.0v[5, 2](t) + 160.0v[5, 4](t) - 320.0v[5, 5](t) + 3.4u[5, 5](t) - (u[5, 5](t)^2)*v[5, 5](t)]","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/#Boundary-condition-Equations","page":"Generated ODE system for the Brusselator Equation","title":"Boundary condition Equations","text":"","category":"section"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"u[2, 1](t) ~ u[2, 5](t), \nu[3, 1](t) ~ u[3, 5](t), \nu[4, 1](t) ~ u[4, 5](t), \nu[5, 1](t) ~ u[5, 5](t), \nu[1, 2](t) ~ u[5, 2](t), \nu[1, 3](t) ~ u[5, 3](t), \nu[1, 4](t) ~ u[5, 4](t), \nu[1, 5](t) ~ u[5, 5](t), \nu[1, 1](t) ~ 0, # Invalid corner point set to 0\nv[2, 1](t) ~ v[2, 5](t), \nv[3, 1](t) ~ v[3, 5](t), \nv[4, 1](t) ~ v[4, 5](t), \nv[5, 1](t) ~ v[5, 5](t), \nv[1, 2](t) ~ v[5, 2](t), \nv[1, 3](t) ~ v[5, 3](t), \nv[1, 4](t) ~ v[5, 4](t), \nv[1, 5](t) ~ v[5, 5](t), \nv[1, 1](t) ~ 0] # Invalid corner point set to 0","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_ode_eqs/","page":"Generated ODE system for the Brusselator Equation","title":"Generated ODE system for the Brusselator Equation","text":"On the call to ODEProblem, this code is generated.","category":"page"},{"location":"modules/NeuralPDE/manual/training_strategies/#Training-Strategies","page":"Training Strategies","title":"Training Strategies","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"Training strategies are the choices for how the points are sampled for the definition the physics-informed loss.","category":"page"},{"location":"modules/NeuralPDE/manual/training_strategies/#Recommendations","page":"Training Strategies","title":"Recommendations","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"QuasiRandomTraining with its default LatinHyperCubeSample() is a well-rounded training strategy which can be used for most situations. It scales well for high dimensional spaces and is GPU-compatible. QuadratureTraining can lead to faster or more robust convergence with one of the H-Cubature or P-Cubature methods, but are not currently GPU compatible. For very high dimensional cases, QuadratureTraining with an adaptive Monte Carlo quadrature method, such as CubaVegas, can be beneficial for difficult or stiff problems.","category":"page"},{"location":"modules/NeuralPDE/manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining should only be used for testing purposes and should not be relied upon for real training cases. StochasticTraining achieves a lower convergence rate the quasi-Monte Carlo methods and thus QuasiRandomTraining should be preferred in most cases.","category":"page"},{"location":"modules/NeuralPDE/manual/training_strategies/#API","page":"Training Strategies","title":"API","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining\nStochasticTraining\nQuasiRandomTraining\nQuadratureTraining","category":"page"},{"location":"modules/NeuralPDE/manual/training_strategies/#NeuralPDE.GridTraining","page":"Training Strategies","title":"NeuralPDE.GridTraining","text":"GridTraining(dx)\n\nA training strategy that uses the grid points in a multidimensional grid with spacings dx. If the grid is multidimensional, then dx is expected to be an array of dx values matching the dimension of the domain, corresponding to the grid spacing in each dimension.\n\nPositional Arguments\n\ndx: the discretization of the grid.\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/training_strategies/#NeuralPDE.StochasticTraining","page":"Training Strategies","title":"NeuralPDE.StochasticTraining","text":"StochasticTraining(points; bcs_points = points)\n\nPostional Arguments\n\npoints: number of points in random select training set\n\nKeyword Arguments\n\nbcs_points: number of points in random select training set for boundry conditions  (by default, it equals points).\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/training_strategies/#NeuralPDE.QuasiRandomTraining","page":"Training Strategies","title":"NeuralPDE.QuasiRandomTraining","text":"QuasiRandomTraining(points; bcs_points = points,\n                            sampling_alg = LatinHypercubeSample(), resampling = true,\n                            minibatch = 0)\n\nA training strategy which uses quasi-Monte Carlo sampling for low discrepency sequences that accelerate the convergence in high dimensional spaces over pure random sequences.\n\nPositional Arguments\n\npoints:  the number of quasi-random points in a sample\n\nKeyword Arguments\n\nbcs_points: the number of quasi-random points in a sample for boundry conditions  (by default, it equals points),\nsampling_alg: the quasi-Monte Carlo sampling algorithm,\nresampling: if it's false - the full training set is generated in advance before training,  and at each iteration, one subset is randomly selected out of the batch.  if it's true - the training set isn't generated beforehand, and one set of quasi-random  points is generated directly at each iteration in runtime. In this case minibatch has no effect,\nminibatch: the number of subsets, if resampling == false.\n\nFor more information, see QuasiMonteCarlo.jl\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/training_strategies/#NeuralPDE.QuadratureTraining","page":"Training Strategies","title":"NeuralPDE.QuadratureTraining","text":"QuadratureTraining(; quadrature_alg = CubatureJLh(), \n                     reltol = 1e-6, abstol = 1e-3,\n                     maxiters = 1_000, batch = 100)\n\nA training strategy which treats the loss function as the integral of ||condition|| over the domain. Uses an Integrals.jl algorithm for computing the (adaptive) quadrature of this loss with respect to the chosen tolerances with a batching batch corresponding to the maximum number of points to evaluate in a given integrand call.\n\nKeyword Arguments\n\nquadrature_alg: quadrature algorithm,\nreltol: relative tolerance,\nabstol: absolute tolerance,\nmaxiters: the maximum number of iterations in quadrature algorithm,\nbatch: the preferred number of points to batch.\n\nFor more information on the argument values and algorithm choices, see  Integrals.jl.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLOperators/#SciMLOperators.jl:-The-SciML-Operators-Interface","page":"Home","title":"SciMLOperators.jl: The SciML Operators Interface","text":"","category":"section"},{"location":"modules/SciMLOperators/","page":"Home","title":"Home","text":"Many functions, from linear solvers to differential equations, require the use of matrix-free operators in order to achieve maximum performance in many scenarios. SciMLOperators.jl defines the abstract interface for how operators in the SciML ecosystem are supposed to be defined. It gives the common set of functions and traits which solvers can rely on for properly performing their tasks. Along with that, SciMLOperators.jl provides definitions for the basic standard operators which are used in building blocks for most tasks, both simplifying the use of operators while also demonstrating to users how such operators can be built and used in practice.","category":"page"},{"location":"modules/SciMLOperators/#Why-SciMLOperators?","page":"Home","title":"Why SciMLOperators?","text":"","category":"section"},{"location":"modules/SciMLOperators/","page":"Home","title":"Home","text":"SciMLOperators.jl has the design that is required in order to be used in all scenarios of equation solvers. For example, Magnus integrators for differential equations require defining an operator u = A(t)u, while Munthe-Kaas methods require defining operators of the form u = A(u)u. Thus the operators need some form of time and state dependence which the solvers can update and query when they are non-constant (update_coefficients!). Additionally, the operators need the ability to act like \"normal\" functions for equation solvers. For example, if A(u,p,t) has the same operation as update_coefficients(A,u,p,t); A*u, then A can be used in any place where a differential equation definition f(u,p,t) is used without requring the user or solver to do any extra work. ","category":"page"},{"location":"modules/SciMLOperators/","page":"Home","title":"Home","text":"Thus while previous good efforts for matrix-free operators have existed in the Julia ecosystem,  such as LinearMaps.jl, those operator interfaces lack these aspects in order to actually be fully seamless with downstream equation solvers. This necessitates the definition and use of an extended operator interface with all of these properties, hence the AbstractSciMLOperator interface.","category":"page"},{"location":"modules/SciMLOperators/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/SciMLOperators/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums (look for the modelingtoolkit tag\nSee also SciML Community page","category":"page"},{"location":"modules/Surrogates/polychaos/#Polynomial-chaos-surrogate","page":"Polynomial Chaos","title":"Polynomial chaos surrogate","text":"","category":"section"},{"location":"modules/Surrogates/polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"note: Note\nThis surrogate requires the 'SurrogatesPolyChaos' module which can be added by inputting \"]add SurrogatesPolyChaos\" from the Julia command line. ","category":"page"},{"location":"modules/Surrogates/polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We can create a surrogate using a polynomial expansion, with a different polynomial basis depending on the distribution of the data we are trying to fit. Under the hood, PolyChaos.jl has been used. It is possible to specify a type of polynomial for each dimension of the problem.","category":"page"},{"location":"modules/Surrogates/polychaos/#Sampling","page":"Polynomial Chaos","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"We choose to sample f in 25 points between 0 and 10 using the sample function. The sampling points are chosen using a Low Discrepancy, this can be done by passing LowDiscrepancySample() to the sample function.","category":"page"},{"location":"modules/Surrogates/polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"using Surrogates\nusing SurrogatesPolyChaos\nusing Plots\ndefault()\n\nn = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,LowDiscrepancySample(2))\nf = x -> log(x)*x + sin(x)\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/polychaos/#Building-a-Surrogate","page":"Polynomial Chaos","title":"Building a Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/polychaos/","page":"Polynomial Chaos","title":"Polynomial Chaos","text":"poly1 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound)\npoly2 = PolynomialChaosSurrogate(x,y,lower_bound,upper_bound, op = SurrogatesPolyChaos.GaussOrthoPoly(5))\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly1, label=\"First polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(poly2, label=\"Second polynomial\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/#Programmatic-Construction-of-Symbolic-Reaction-Systems","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"While the DSL provides a simple interface for creating ReactionSystems, it can often be convenient to build or augment a ReactionSystem programmatically. In this tutorial we show how to build the repressilator model of the Using Catalyst tutorial directly using symbolic variables, and then summarize the basic API functionality for accessing information stored within ReactionSystems.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/#Directly-Building-the-Repressilator-with-ReactionSystems","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Directly Building the Repressilator with ReactionSystems","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"We first load Catalyst","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"using Catalyst","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"and then define symbolic variables for each parameter and species in the system (the latter corresponding to a variable or state in ModelingToolkit terminology)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@parameters α K n δ γ β μ\n@variables t m₁(t) m₂(t) m₃(t) P₁(t) P₂(t) P₃(t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@parameters α, K, n, δ, γ, β, μ;\n@variables t, m₁(t), m₂(t), m₃(t), P₁(t), P₂(t), P₃(t);","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Note, each species is declared as a variable that is a function of time!","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Next we specify the chemical reactions that comprise the system using Catalyst Reactions","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rxs = [Reaction(hillr(P₃,α,K,n), nothing, [m₁]),\n       Reaction(hillr(P₁,α,K,n), nothing, [m₂]),\n       Reaction(hillr(P₂,α,K,n), nothing, [m₃]),\n       Reaction(δ, [m₁], nothing),\n       Reaction(γ, nothing, [m₁]),\n       Reaction(δ, [m₂], nothing),\n       Reaction(γ, nothing, [m₂]),\n       Reaction(δ, [m₃], nothing),\n       Reaction(γ, nothing, [m₃]),\n       Reaction(β, [m₁], [m₁,P₁]),\n       Reaction(β, [m₂], [m₂,P₂]),\n       Reaction(β, [m₃], [m₃,P₃]),\n       Reaction(μ, [P₁], nothing),\n       Reaction(μ, [P₂], nothing),\n       Reaction(μ, [P₃], nothing)]","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rxs = [Reaction(hillr(P₃,α,K,n), nothing, [m₁]),\n       Reaction(hillr(P₁,α,K,n), nothing, [m₂]),\n       Reaction(hillr(P₂,α,K,n), nothing, [m₃]),\n       Reaction(δ, [m₁], nothing),\n       Reaction(γ, nothing, [m₁]),\n       Reaction(δ, [m₂], nothing),\n       Reaction(γ, nothing, [m₂]),\n       Reaction(δ, [m₃], nothing),\n       Reaction(γ, nothing, [m₃]),\n       Reaction(β, [m₁], [m₁,P₁]),\n       Reaction(β, [m₂], [m₂,P₂]),\n       Reaction(β, [m₃], [m₃,P₃]),\n       Reaction(μ, [P₁], nothing),\n       Reaction(μ, [P₂], nothing),\n       Reaction(μ, [P₃], nothing)]","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Here we use nothing where the DSL used varnothing. Finally, we are ready to construct our ReactionSystem as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@named repressilator = ReactionSystem(rxs, t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@named repressilator = ReactionSystem(rxs, t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Notice, the model is named repressilator. A name must always be specified when directly constructing a ReactionSystem (the DSL will auto-generate one if left out). Using @named when constructing a ReactionSystem causes the name of the system to be the same as the name of the variable storing the system. Alternatively, one can use the name=:repressilator keyword argument to the ReactionSystem constructor.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"We can check that this is the same model as the one we defined via the DSL as follows (this requires that we use the same names for rates, species and the system)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"repressilator2 = @reaction_network repressilator begin\n    hillr(P₃,α,K,n), ∅ --> m₁\n    hillr(P₁,α,K,n), ∅ --> m₂\n    hillr(P₂,α,K,n), ∅ --> m₃\n    (δ,γ), m₁ <--> ∅\n    (δ,γ), m₂ <--> ∅\n    (δ,γ), m₃ <--> ∅\n    β, m₁ --> m₁ + P₁\n    β, m₂ --> m₂ + P₂\n    β, m₃ --> m₃ + P₃\n    μ, P₁ --> ∅\n    μ, P₂ --> ∅\n    μ, P₃ --> ∅\nend α K n δ γ β μ\nrepressilator == repressilator2","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"For more options in building ReactionSystems, see the ReactionSystem API docs.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/#More-General-Reactions","page":"Programmatic Construction of Symbolic Reaction Systems","title":"More General Reactions","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"In the example above all the specified Reactions were first or zero order. The three-argument form of Reaction implicitly assumes all species have a stoichiometric coefficient of one, i.e. for substrates [S₁,...,Sₘ] and products [P₁,...,Pₙ] it has the possible forms","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"# rate, S₁ + ... + Sₘ --> P₁ + ... + Pₙ\nReaction(rate, [S₁,...,Sₘ], [P₁,...,Pₙ])\n\n# rate, S₁ + ... + Sₘ --> ∅\nReaction(rate, [S₁,...,Sₘ], nothing)\n\n# rate, ∅ --> P₁ + ... + Pₙ\nReaction(rate, nothing, [P₁,...,Pₙ])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"To allow for other stoichiometric coefficients we also provide a five argument form","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"# rate, α₁*S₁ + ... + αₘ*Sₘ --> β₁*P₁ + ... + βₙ*Pₙ\nReaction(rate, [S₁,...,Sₘ], [P₁,...,Pₙ], [α₁,...,αₘ], [β₁,...,βₙ])\n\n# rate, α₁*S₁ + ... + αₘ*Sₘ --> ∅\nReaction(rate, [S₁,...,Sₘ], nothing, [α₁,...,αₘ], nothing)\n\n# rate, ∅ --> β₁*P₁ + ... + βₙ*Pₙ\nReaction(rate, nothing, [P₁,...,Pₙ], nothing, [β₁,...,βₙ])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Finally, we note that the rate constant, rate above, does not need to be a constant or fixed function, but can be a general symbolic expression:","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@parameters α, β\n@variables t, A(t), B(t)\nrx = Reaction(α+β*t*A, [A], [B])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"See the FAQs for info on using general user-specified functions for the rate constant.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/#@reaction-macro-for-constructing-Reactions","page":"Programmatic Construction of Symbolic Reaction Systems","title":"@reaction macro for constructing Reactions","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"In some cases one wants to build reactions incrementally, as in the repressilator example, but it would be nice to still have a short hand as in the @reaction_network DSL. In this case one can construct individual reactions using the @reaction macro.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"For example, the repressilator reactions could also have been constructed like","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@variables t P₁(t) P₂(t) P₃(t)\nrxs = [(@reaction hillr($P₃,α,K,n), ∅ --> m₁),\n       (@reaction hillr($P₁,α,K,n), ∅ --> m₂),\n       (@reaction hillr($P₂,α,K,n), ∅ --> m₃),\n       (@reaction δ, m₁ --> ∅),\n       (@reaction γ, ∅ --> m₁),\n       (@reaction δ, m₂ --> ∅),\n       (@reaction γ, ∅ --> m₂),\n       (@reaction δ, m₃ --> ∅),\n       (@reaction γ, ∅ --> m₃),\n       (@reaction β, m₁ --> m₁ + P₁),\n       (@reaction β, m₂ --> m₂ + P₂),\n       (@reaction β, m₃ --> m₃ + P₃),\n       (@reaction μ, P₁ --> ∅),\n       (@reaction μ, P₂ --> ∅),\n       (@reaction μ, P₃ --> ∅)]\n@named repressilator = ReactionSystem(rxs, t)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Note, there are a few differences when using the @reaction macro to specify one reaction versus using the full @reaction_network macro to create a ReactionSystem. First, only one reaction (i.e. a single forward arrow type) can be used, i.e. reversible arrows like <--> will not work (since they define more than one reaction). Second, the @reaction macro must try to infer which symbols are species versus parameters, and uses the heuristic that anything appearing in the rate expression is a parameter. Coefficients in the reaction part are also inferred as parameters, while rightmost symbols (i.e. substrates and products) are inferred as species. As such, the following are equivalent","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx = @reaction hillr(P,α,K,n), A --> B","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"is equivalent to","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"@parameters P α K n\n@variables t A(t) B(t)\nrx = Reaction(hillr(P,α,K,n), [A], [B])","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Here (P,α,K,n) are parameters and (A,B) are species.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"This behavior is the reason that in the repressilator example above we pre-declared (P₁(t),P₂(t),P₃(t)) as variables, and then used them via interpolating their values into the rate law expressions using $ in the macro. This ensured they were properly treated as species and not parameters. See the @reaction macro docstring for more information.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/#Basic-Querying-of-ReactionSystems","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Basic Querying of ReactionSystems","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"The Catalyst.jl API provides a large variety of functionality for querying properties of a reaction network. Here we go over a few of the most useful basic functions. Given the repressillator defined above we have that","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"species(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"parameters(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"reactions(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"We can test if a Reaction is mass action, i.e. the rate does not depend on t or other species, as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"# Catalyst.hillr(P₃(t), α, K, n), ∅ --> m₁\nrx1 = reactions(repressilator)[1]\nismassaction(rx1,repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"while","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"# δ, m₁ --> ∅\nrx2 = reactions(repressilator)[4]\nismassaction(rx2,repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Similarly, we can determine which species a reaction's rate law will depend on like","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rn = @reaction_network begin\n       k*W, 2X + 3Y --> 5Z + W\n     end k\ndependents(reactions(rn)[1], rn)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Basic stoichiometry matrices can be obtained from a ReactionSystem as","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"substoichmat(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"prodstoichmat(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"netstoichmat(repressilator)","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Here the (ij) entry gives the corresponding stoichiometric coefficient of species i for reaction j.","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"Finally, we can directly access fields of individual reactions like","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.rate","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.substrates","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.products","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.substoich","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.prodstoich","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"rx1.netstoich","category":"page"},{"location":"modules/Catalyst/tutorials/reaction_systems/","page":"Programmatic Construction of Symbolic Reaction Systems","title":"Programmatic Construction of Symbolic Reaction Systems","text":"See the Catalyst.jl API for much more detail on the various querying and analysis functions provided by Catalyst.","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/#eFAST-Method","page":"eFAST Method","title":"eFAST Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"struct eFAST <: GSAMethod\n    num_harmonics::Int\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"The eFAST object has num_harmonics as the only field, which is the number of harmonics to sum in the Fourier series decomposition, this defaults to 4.","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/#eFAST-Method-Details","page":"eFAST Method","title":"eFAST Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"eFAST offers a robust, especially at low sample size, and computationally efficient procedure to get the first and total order indices as discussed in Sobol. It utilizes monodimensional Fourier decomposition along a curve exploring the parameter space. The curve is defined by a set of parametric equations,","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"x_i(s) = G_i(sin ω_is)  i=12  n","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"where s is a scalar variable varying over the range -  s  +, G_i are transformation functions and ω_i  i=12n is a set of different (angular) frequencies, to be properly selected, associated with each factor. For more details on the transformation used and other implementation details you can go through  A. Saltelli et al..","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/#API","page":"eFAST Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"function gsa(f, method::eFAST, p_range::AbstractVector; n::Int=1000, batch=false, distributed::Val{SHARED_ARRAY} = Val(false), kwargs...) where {SHARED_ARRAY}","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/#Example","page":"eFAST Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"Below we show use of eFAST on the Ishigami function.","category":"page"},{"location":"modules/GlobalSensitivity/methods/efast/","page":"eFAST Method","title":"eFAST Method","text":"using GlobalSensitivity, QuasiMonteCarlo\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nres1 = gsa(ishi,eFAST(),[[lb[i],ub[i]] for i in 1:4],n=15000)\n\n##with batching\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\n\nres2 = gsa(ishi_batch,eFAST(),[[lb[i],ub[i]] for i in 1:4],n=15000,batch=true)\n","category":"page"},{"location":"modules/DiffEqDocs/features/noise_process/#noise_process","page":"Noise Processes","title":"Noise Processes","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Noise processes are essential in continuous stochastic modeling. The NoiseProcess types are distributionally-exact, meaning they are not solutions of stochastic differential equations and instead are directly generated according to their analytical distributions. These processes are used as the noise term in the SDE and RODE solvers. Additionally, the noise processes themselves can be simulated and solved using the DiffEq common interface (including the Monte Carlo interface).","category":"page"},{"location":"modules/DiffEqDocs/features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"For more details, see DiffEqNoiseProcess.jl","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Data-Parallel-Multithreaded,-Distributed,-and-Multi-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqFlux.jl allows for data-parallel batching optimally on one computer, across an entire compute cluster, and batching along GPUs. This can be done by parallelizing within an ODE solve or between the ODE solves. The automatic differentiation tooling is compatible with the parallelism. The following examples demonstrate training over a few different modes of parallelism. These examples are not exhaustive.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Within-ODE-Multithreaded-and-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Within-ODE Multithreaded and GPU Batching","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"We end by noting that there is an alternative way of batching which can be more efficient in some cases like neural ODEs. With a neural networks, columns are treated independently (by the properties of matrix multiplication). Thus for example, with Chain we can define an ODE:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using Lux, DiffEqFlux, DifferentialEquations, Random\nrng = Random.default_rng()\n\ndudt = Lux.Chain(Lux.Dense(2,50,tanh),Lux.Dense(50,2))\np,st = Lux.setup(rng, dudt)\nf(u,p,t) = dudt(u,p,st)[1]","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"and we can solve this ODE where the initial condition is a vector:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"u0 = Float32[2.; 0.]\nprob = ODEProblem(f,u0,(0f0,1f0),p)\nsolve(prob,Tsit5())","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"or we can solve this ODE where the initial condition is a matrix, where each column is an independent system:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"u0 = Float32.([0 1 2\n               0 0 0])\nprob = ODEProblem(f,u0,(0f0,1f0),p)\nsolve(prob,Tsit5())","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"On the CPU this will multithread across the system (due to BLAS) and on GPUs this will parallelize the operations across the GPU. To GPU this, you'd simply move the parameters and the initial condition to the GPU:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"xs = Float32.([0 1 2\n               0 0 0])\nprob = ODEProblem(f,gpu(u0),(0f0,1f0),gpu(p))\nsolve(prob,Tsit5())","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"This method of parallelism is optimal if all of the operations are linear algebra operations such as a neural ODE. Thus this method of parallelism is demonstrated in the MNIST tutorial.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"However, this method of parallelism has many limitations. First of all, the ODE function is required to be written in a way that is independent across the columns. Not all ODEs are written like this, so one needs to be careful. But additionally, this method is ineffective if the ODE function has many serial operations, like u[1]*u[2] - u[3]. In such a case, this indexing behavior will dominate the runtime and cause the parallelism to sometimes even be detrimental.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Out-of-ODE-Parallelism","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Out of ODE Parallelism","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Instead of parallelizing within an ODE solve, one can parallelize the solves to the ODE itself. While this will be less effective on very large ODEs, like big neural ODE image classifiers, this method be effective even if the ODE is small or the f function is not well-parallelized. This kind of parallelism is done via the DifferentialEquations.jl ensemble interface. The following examples showcase multithreaded, cluster, and (multi)GPU parallelism through this interface.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Multithreaded-Batching-At-a-Glance","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multithreaded Batching At a Glance","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"The following is a full copy-paste example for the multithreading. Distributed and GPU minibatching are described below.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using DifferentialEquations, Optimization, OptimizationOptimJL, OptimizationFlux\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem((u, p, t) -> 1.01u .* p, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\n# loss function\nloss_serial(θ)   = sum(abs2,1.0.-Array(model1(θ,EnsembleSerial())))\nloss_threaded(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleThreads())))\n\ncallback = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nl1 = loss_serial(θ)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_serial(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\n\nres_serial = Optimization.solve(optprob, opt; callback = callback, maxiters=100)\n\noptf2 = Optimization.OptimizationFunction((x,p)->loss_threaded(x), adtype)\noptprob2 = Optimization.OptimizationProblem(optf2, θ)\n\nres_threads = Optimization.solve(optprob2, opt; callback = callback, maxiters=100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Multithreaded-Batching-In-Depth","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multithreaded Batching In-Depth","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"In order to make use of the ensemble interface, we need to build an EnsembleProblem. The prob_func is the function for determining the different DEProblems to solve. This is the place where we can randomly sample initial conditions or pull initial conditions from an array of batches in order to perform our study. To do this, we first define a prototype DEProblem. Here we use the following ODEProblem as our base:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"prob = ODEProblem((u, p, t) -> 1.01u .* p, [θ[1]], (0.0, 1.0), [θ[2]])","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"In the prob_func we define how to build a new problem based on the base problem. In this case, we want to change u0 by a constant, i.e. 0.5 .+ i/100 .* prob.u0 for different trajectories labelled by i. Thus we use the remake function from the problem interface to do so:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"function prob_func(prob, i, repeat)\n  remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"We now build the EnsembleProblem with this basis:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Now to solve an ensemble problem, we need to choose an ensembling algorithm and choose the number of trajectories to solve. Here let's solve this in serial with 100 trajectories. Note that i will thus run from 1:100.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleSerial(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"and thus running in multithreading would be:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleThreads(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"This whole mechanism is differentiable, so we then put it in a training loop and it soars. Note that you need to make sure that Julia's multithreading is enabled, which you can do via:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Threads.nthreads()","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Distributed-Batching-Across-a-Cluster","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Distributed Batching Across a Cluster","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Changing to distributed computing is very simple as well. The setup is all the same, except you utilize EnsembleDistributed as the ensembler:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"sim = solve(ensemble_prob, Tsit5(), EnsembleDistributed(), saveat = 0.1, trajectories = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"Note that for this to work you need to ensure that your processes are already started. For more information on setting up processes and utilizing a compute cluster, see the official distributed documentation. The key feature to recognize is that, due to the message passing required for cluster compute, one needs to ensure that all of the required functions are defined on the worker processes. The following is a full example of a distributed batching setup:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using Distributed\naddprocs(4)\n\n@everywhere begin\n  using DifferentialEquations, Optimization, OptimizationOptimJL\n  function f(u,p,t)\n    1.01u .* p\n  end\nend\n\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem(f, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\ncallback = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nloss_distributed(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleDistributed())))\nl1 = loss_distributed(θ)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_distributed(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\n\nres_distributed = Optimization.solve(optprob, opt; callback = callback, maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"And note that only addprocs(4) needs to be changed in order to make this demo run across a cluster. For more information on adding processes to a cluster, check out ClusterManagers.jl.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Minibatching-Across-GPUs-with-DiffEqGPU","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Minibatching Across GPUs with DiffEqGPU","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqGPU.jl allows for generating code parallelizes an ensemble on generated CUDA kernels. This method is efficient for sufficiently small (<100 ODE) problems where the significant computational cost is due to the large number of batch trajectories that need to be solved. This kernel-building process adds a few restrictions to the function, such as requiring it has no boundschecking or allocations. The following is an example of minibatch ensemble parallelism across a GPU:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"using DifferentialEquations, Optimization, OptimizationOptimJL\nfunction f(du,u,p,t)\n  @inbounds begin\n    du[1] = 1.01 * u[1] * p[1] * p[2]\n  end\nend\n\npa = [1.0]\nu0 = [3.0]\nθ = [u0;pa]\n\nfunction model1(θ,ensemble)\n  prob = ODEProblem(f, [θ[1]], (0.0, 1.0), [θ[2]])\n\n  function prob_func(prob, i, repeat)\n    remake(prob, u0 = 0.5 .+ i/100 .* prob.u0)\n  end\n\n  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)\n  sim = solve(ensemble_prob, Tsit5(), ensemble, saveat = 0.1, trajectories = 100)\nend\n\ncallback = function (θ,l) # callback function to observe training\n  @show l\n  false\nend\n\nopt = ADAM(0.1)\nloss_gpu(θ) = sum(abs2,1.0.-Array(model1(θ,EnsembleGPUArray())))\nl1 = loss_gpu(θ)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_gpu(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, θ)\n\nres_gpu = Optimization.solve(optprob, opt; callback = callback, maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/#Multi-GPU-Batching","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Multi-GPU Batching","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/data_parallel/","page":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","title":"Data-Parallel Multithreaded, Distributed, and Multi-GPU Batching","text":"DiffEqGPU supports batching across multiple GPUs. See its README for details on setting it up.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/#Exposing-More-Parallelism-By-Tearing-Algebraic-Equations-in-ODESystems","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Sometimes it can be very non-trivial to parallelize a system. In this tutorial we will demonstrate how to make use of structural_simplify to expose more parallelism in the solution process and parallelize the resulting simulation.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/#The-Component-Library","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"The Component Library","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"The following tutorial will use the following set of components describing electrical circuits:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"using ModelingToolkit, OrdinaryDiffEq\n\n# Basic electric components\n@variables t\nconst D = Differential(t)\n@connector function Pin(;name)\n    @variables v(t)=1.0 i(t)=1.0 [connect = Flow]\n    ODESystem(Equation[], t, [v, i], [], name=name)\nend\n\nfunction Ground(;name)\n    @named g = Pin()\n    eqs = [g.v ~ 0]\n    compose(ODESystem(eqs, t, [], [], name=name), g)\nend\n\nfunction ConstantVoltage(;name, V = 1.0)\n    val = V\n    @named p = Pin()\n    @named n = Pin()\n    @parameters V=V\n    eqs = [\n           V ~ p.v - n.v\n           0 ~ p.i + n.i\n          ]\n    compose(ODESystem(eqs, t, [], [V], name=name), p, n)\nend\n\n@connector function HeatPort(;name)\n    @variables T(t)=293.15 Q_flow(t)=0.0 [connect = Flow]\n    ODESystem(Equation[], t, [T, Q_flow], [], name=name)\nend\n\nfunction HeatingResistor(;name, R=1.0, TAmbient=293.15, alpha=1.0)\n    @named p = Pin()\n    @named n = Pin()\n    @named h = HeatPort()\n    @variables v(t) RTherm(t)\n    @parameters R=R TAmbient=TAmbient alpha=alpha\n    eqs = [\n           RTherm ~ R*(1 + alpha*(h.T - TAmbient))\n           v ~ p.i * RTherm\n           h.Q_flow ~ -v * p.i # -LossPower\n           v ~ p.v - n.v\n           0 ~ p.i + n.i\n          ]\n    compose(ODESystem(\n        eqs, t, [v, RTherm], [R, TAmbient, alpha],\n        name=name,\n    ), p, n, h)\nend\n\nfunction HeatCapacitor(;name, rho=8050, V=1, cp=460, TAmbient=293.15)\n    @parameters rho=rho V=V cp=cp\n    C = rho*V*cp\n    @named h = HeatPort()\n    eqs = [\n           D(h.T) ~ h.Q_flow / C\n          ]\n    compose(ODESystem(\n        eqs, t, [], [rho, V, cp],\n        name=name,\n    ), h)\nend\n\nfunction Capacitor(;name, C = 1.0)\n    @named p = Pin()\n    @named n = Pin()\n    @variables v(t)=0.0\n    @parameters C=C\n    eqs = [\n           v ~ p.v - n.v\n           0 ~ p.i + n.i\n           D(v) ~ p.i / C\n          ]\n    compose(ODESystem(\n        eqs, t, [v], [C],\n        name=name\n    ), p, n)\nend\n\nfunction parallel_rc_model(i; name, source, ground, R, C)\n    resistor = HeatingResistor(name=Symbol(:resistor, i), R=R)\n    capacitor = Capacitor(name=Symbol(:capacitor, i), C=C)\n    heat_capacitor = HeatCapacitor(name=Symbol(:heat_capacitor, i))\n\n    rc_eqs = [\n              connect(source.p, resistor.p)\n              connect(resistor.n, capacitor.p)\n              connect(capacitor.n, source.n, ground.g)\n              connect(resistor.h, heat_capacitor.h)\n             ]\n\n    compose(ODESystem(rc_eqs, t, name=Symbol(name, i)),\n            [resistor, capacitor, source, ground, heat_capacitor])\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/#The-Model","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"The Model","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Assuming that the components are defined, our model is 50 resistors and capacitors connected in parallel. Thus following the acausal components tutorial, we can connect a bunch of RC components as follows:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"V = 2.0\n@named source = ConstantVoltage(V=V)\n@named ground = Ground()\nN = 50\nRs = 10 .^range(0, stop=-4, length=N)\nCs = 10 .^range(-3, stop=0, length=N)\nrc_systems = map(1:N) do i\n    parallel_rc_model(i; name=:rc, source=source, ground=ground, R=Rs[i], C=Cs[i])\nend;\n@variables E(t)=0.0\neqs = [\n       D(E) ~ sum(((i, sys),)->getproperty(sys, Symbol(:resistor, i)).h.Q_flow, enumerate(rc_systems))\n      ]\n@named _big_rc = ODESystem(eqs, t, [E], [])\n@named big_rc = compose(_big_rc, rc_systems)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Now let's say we want to expose a bit more parallelism via running tearing. How do we do that?","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"sys = structural_simplify(big_rc)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Done, that's it. There's no more to it.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/#What-Happened?","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"What Happened?","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"Yes, that's a good question! Let's investigate a little bit more what had happened. If you look at the system we defined:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"length(equations(big_rc))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"You see it started as a massive 1051 set of equations. However, after eliminating redundancies we arrive at 151 equations:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"equations(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"That's not all though. In addition, the tearing process has turned the sets of nonlinear equations into separate blocks and constructed a DAG for the dependencies between the blocks. We can use the bipartite graph functionality to dig in and investigate what this means:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"using ModelingToolkit.BipartiteGraphs\nts = TearingState(expand_connections(big_rc))\ninc_org = BipartiteGraphs.incidence_matrix(ts.structure.graph)\nblt_org = StructuralTransformations.sorted_incidence_matrix(ts, only_algeqs=true, only_algvars=true)\nblt_reduced = StructuralTransformations.sorted_incidence_matrix(ModelingToolkit.get_tearing_state(sys), only_algeqs=true, only_algvars=true)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"(Image: )","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"The figure on the left is the original incidence matrix of the algebraic equations. Notice that the original formulation of the model has dependencies between different equations, and so the full set of equations must be solved together. That exposes no parallelism. However, the Block Lower Triangular (BLT) transformation exposes independent blocks. This is then further improved by the tearing process, which removes 90% of the equations and transforms the nonlinear equations into 50 independent blocks which can now all be solved in parallel. The conclusion is that, your attempts to parallelize are neigh: performing parallelism after structural simplification greatly improves the problem that can be parallelized, so this is better than trying to do it by hand.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/tearing_parallelism/","page":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","title":"Exposing More Parallelism By Tearing Algebraic Equations in ODESystems","text":"After performing this, you can construct the ODEProblem/ODAEProblem and set parallel_form to use the exposed parallelism in multithreaded function constructions, but this showcases why structural_simplify is so important to that process.","category":"page"},{"location":"modules/Surrogates/cantilever/#Cantilever-Beam-Function","page":"Cantilever beam","title":"Cantilever Beam Function","text":"","category":"section"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"The Cantilever Beam function is defined as: f(wt) = frac4L^3Ewt*sqrt (fracYt^2)^2 + (fracXw^2)^2  With parameters L,E,X and Y given.","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"function f(x)\n    t = x[1]\n    w = x[2]\n    L = 100.0\n    E = 2.770674127819261e7\n    X = 530.8038576066307\n    Y = 997.8714938733949\n    return (4*L^3)/(E*w*t)*sqrt( (Y/t^2)^2 + (X/w^2)^2)\nend","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Let's plot it:","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"n = 100\nlb = [1.0,1.0]\nub = [8.0,8.0]\nxys = sample(n,lb,ub,SobolSample());\nzs = f.(xys);\nx, y = 0:8, 0:8\np1 = surface(x, y, (x1,x2) -> f((x1,x2)))\nxs = [xy[1] for xy in xys]\nys = [xy[2] for xy in xys]\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> f((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Fitting different Surrogates:","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"mypoly = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nloba = PolynomialChaosSurrogate(xys, zs,  lb, ub)\nrad = RadialBasis(xys,zs,lb,ub)","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"Plotting:","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> mypoly([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Polynomial expansion\")","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> loba([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Lobachevsky\")","category":"page"},{"location":"modules/Surrogates/cantilever/","page":"Cantilever beam","title":"Cantilever beam","text":"p1 = surface(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> rad([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Inverse distance surrogate\")","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#optim","page":"Optim.jl","title":"Optim.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim is Julia package implementing various algorithm to perform univariate and multivariate optimization.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Installation:-OptimizationOptimJL.jl","page":"Optim.jl","title":"Installation: OptimizationOptimJL.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"To use this package, install the OptimizationOptimJL package:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"import Pkg; Pkg.add(\"OptimizationOptimJL\")","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Methods","page":"Optim.jl","title":"Methods","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl algorithms can be one of the following:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.NelderMead()\nOptim.SimulatedAnnealing()\nOptim.ParticleSwarm()\nOptim.ConjugateGradient()\nOptim.GradientDescent()\nOptim.BFGS()\nOptim.LBFGS()\nOptim.NGMRES()\nOptim.OACCEL()\nOptim.NewtonTrustRegion()\nOptim.Newton()\nOptim.KrylovTrustRegion()\nOptim.ParticleSwarm()\nOptim.SAMIN()","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Each optimizer also takes special arguments which are outlined in the sections below.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The following special keyword arguments which are not covered by the common solve arguments can be used with Optim.jl optimizers:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"x_tol: Absolute tolerance in changes of the input vector x, in infinity norm. Defaults to 0.0.\ng_tol: Absolute tolerance in the gradient, in infinity norm. Defaults to 1e-8. For gradient free methods, this will control the main convergence tolerance, which is solver specific.\nf_calls_limit: A soft upper limit on the number of objective calls. Defaults to 0 (unlimited).\ng_calls_limit: A soft upper limit on the number of gradient calls. Defaults to 0 (unlimited).\nh_calls_limit: A soft upper limit on the number of Hessian calls. Defaults to 0 (unlimited).\nallow_f_increases: Allow steps that increase the objective value. Defaults to false. Note that, when setting this to true, the last iterate will be returned as the minimizer even if the objective increased.\nstore_trace: Should a trace of the optimization algorithm's state be stored? Defaults to false.\nshow_trace: Should a trace of the optimization algorithm's state be shown on stdout? Defaults to false.\nextended_trace: Save additional information. Solver dependent. Defaults to false.\ntrace_simplex: Include the full simplex in the trace for NelderMead. Defaults to false.\nshow_every: Trace output is printed every show_everyth iteration.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"For a more extensive documentation of all the algorithms and options please consult the  Documentation","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Local-Optimizer","page":"Optim.jl","title":"Local Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/#Local-Constraint","page":"Optim.jl","title":"Local Constraint","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl implements the following local constraint algorithms:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.IPNewton()\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\nμ0 specifies the initial barrier penalty coefficient as either a number or :auto\nshow_linesearch is an option to turn on linesearch verbosity.\nDefaults:\nlinesearch::Function = Optim.backtrack_constrained_grad\nμ0::Union{Symbol,Number} = :auto\nshow_linesearch::Bool = false","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.IPNewton() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\ncons= (x,p) -> [x[1]^2 + x[2]^2]\nx0 = zeros(2)\np  = [1.0,100.0]\nprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= cons)\nprob = Optimization.OptimizationProblem(prob, x0, p, lcons = [-5.0], ucons = [10.0])\nsol = solve(prob, IPNewton())","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Derivative-Free","page":"Optim.jl","title":"Derivative-Free","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Derivative-free optimizers are optimizers that can be used even in cases where no derivatives or automatic differentiation is specified. While they tend to be less efficient than derivative-based optimizers, they can be easily applied to cases where defining derivatives is difficult. Note that while these methods do not support general constraints, all support bounds constraints via lb and ub in the Optimization.OptimizationProblem.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl implements the following derivative-free algorithms:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.NelderMead(): Nelder-Mead optimizer\nsolve(problem, NelderMead(parameters, initial_simplex))\nparameters = AdaptiveParameters() or parameters = FixedParameters()\ninitial_simplex = AffineSimplexer()\nDefaults:\nparameters = AdaptiveParameters()\ninitial_simplex = AffineSimplexer()\nOptim.SimulatedAnnealing(): Simulated Annealing\nsolve(problem, SimulatedAnnealing(neighbor, T, p))\nneighbor is a mutating function of the current and proposed x\nT is a function of the current iteration that returns a temperature\np is a function of the current temperature\nDefaults:\nneighbor = default_neighbor!\nT = default_temperature\np = kirkpatrick\nOptim.ParticleSwarm()","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.NelderMead() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0,100.0]\nprob = Optimization.OptimizationProblem(rosenbrock, x0, p)\nsol = solve(prob, Optim.NelderMead())","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Gradient-Based","page":"Optim.jl","title":"Gradient-Based","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Gradient-based optimizers are optimizers which utilise the gradient information based on derivatives defined or automatic differentiation.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl implements the following gradient-based algorithms:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.ConjugateGradient(): Conjugate Gradient Descent\nsolve(problem, ConjugateGradient(alphaguess, linesearch, eta, P, precondprep))\nalphaguess computes the initial step length (for more information, consult this source and this example)\navailable initial step length procedures:\nInitialPrevious\nInitialStatic\nInitialHagerZhang\nInitialQuadratic\nInitialConstantChange\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\neta determines the next step direction\nP is an optional preconditioner (for more information, see this source)\nprecondpred is used to update P as the state variable x changes\nDefaults:\nalphaguess = LineSearches.InitialHagerZhang()\nlinesearch = LineSearches.HagerZhang()\neta = 0.4\nP = nothing\nprecondprep = (P, x) -> nothing\nOptim.GradientDescent(): Gradient Descent (a quasi-Newton solver)\nsolve(problem, GradientDescent(alphaguess, linesearch, P, precondprep))\nalphaguess computes the initial step length (for more information, consult this source and this example)\navailable initial step length procedures:\nInitialPrevious\nInitialStatic\nInitialHagerZhang\nInitialQuadratic\nInitialConstantChange\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\nP is an optional preconditioner (for more information, see this source)\nprecondpred is used to update P as the state variable x changes\nDefaults:\nalphaguess = LineSearches.InitialPrevious()\nlinesearch = LineSearches.HagerZhang()\nP = nothing\nprecondprep = (P, x) -> nothing\nOptim.BFGS(): Broyden-Fletcher-Goldfarb-Shanno algorithm\nsolve(problem, BFGS(alpaguess, linesearch, initial_invH, initial_stepnorm, manifold))\nalphaguess computes the initial step length (for more information, consult this source and this example)\navailable initial step length procedures:\nInitialPrevious\nInitialStatic\nInitialHagerZhang\nInitialQuadratic\nInitialConstantChange\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\ninitial_invH specifies an optional initial matrix\ninitial_stepnorm determines that initial_invH is an identity matrix scaled by the value of initial_stepnorm multiplied by the sup-norm of the gradient at the initial point\nmanifold specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult this source)\navailable manifolds:\nFlat\nSphere\nStiefel\nmeta-manifolds:\nPowerManifold\nProductManifold\ncustom manifolds\nDefaults:\nalphaguess = LineSearches.InitialStatic()\nlinesearch = LineSearches.HagerZhang()\ninitial_invH = nothing\ninitial_stepnorm = nothing\nmanifold = Flat()\nOptim.LBFGS(): Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm\nm is the number of history points\nalphaguess computes the initial step length (for more information, consult this source and this example)\navailable initial step length procedures:\nInitialPrevious\nInitialStatic\nInitialHagerZhang\nInitialQuadratic\nInitialConstantChange\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\nP is an optional preconditioner (for more information, see this source)\nprecondpred is used to update P as the state variable x changes\nmanifold specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult this source)\navailable manifolds:\nFlat\nSphere\nStiefel\nmeta-manifolds:\nPowerManifold\nProductManifold\ncustom manifolds\nscaleinvH0: whether to scale the initial Hessian approximation\nDefaults:\nm = 10\nalphaguess = LineSearches.InitialStatic()\nlinesearch = LineSearches.HagerZhang()\nP = nothing\nprecondprep = (P, x) -> nothing\nmanifold = Flat()\nscaleinvH0::Bool = true && (typeof(P) <: Nothing)\nOptim.NGMRES()\nOptim.OACCEL()","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.LD_LBFGS() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0,100.0]\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(optprob, x0, p, lb=[-1.0, -1.0], ub=[0.8, 0.8])\nsol = solve(prob, NLopt.LD_LBFGS())","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Hessian-Based-Second-Order","page":"Optim.jl","title":"Hessian-Based Second Order","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Hessian-based optimization methods are second order optimization methods which use the direct computation of the Hessian. These can converge faster but require fast and accurate methods for calulating the Hessian in order to be appropriate.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl implements the following hessian-based algorithms:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.NewtonTrustRegion(): Newton Trust Region method\ninitial_delta: The starting trust region radius\ndelta_hat: The largest allowable trust region radius\neta: When rho is at least eta, accept the step.\nrho_lower: When rho is less than rho_lower, shrink the trust region.\nrho_upper: When rho is greater than rhoupper, grow the trust region (though no greater than deltahat).\nDefaults:\ninitial_delta = 1.0\ndelta_hat = 100.0\neta = 0.1\nrho_lower = 0.25\nrho_upper = 0.75\nOptim.Newton(): Newton's method with line search\nalphaguess computes the initial step length (for more information, consult this source and this example)\navailable initial step length procedures:\nInitialPrevious\nInitialStatic\nInitialHagerZhang\nInitialQuadratic\nInitialConstantChange\nlinesearch specifies the line search algorithm (for more information, consult this source and this example)\navailable line search algorithms:\nHaegerZhang\nMoreThuente\nBackTracking\nStrongWolfe\nStatic\nDefaults:\nalphaguess = LineSearches.InitialStatic()\nlinesearch = LineSearches.HagerZhang()","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.Newton() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0,100.0]\nf = OptimizationFunction(rosenbrock,ModelingToolkit.AutoModelingToolkit(),x0,p,grad=true,hess=true)\nprob = Optimization.OptimizationProblem(f,x0,p)\nsol = solve(prob,Optim.Newton())","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Hessian-Free-Second-Order","page":"Optim.jl","title":"Hessian-Free Second Order","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Hessian-free methods are methods which perform second order optimization by direct computation of Hessian-vector products (Hv) without requiring the construction of the full Hessian. As such, these methods can perform well for large second order optimization problems, but can require special case when considering conditioning of the Hessian.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.jl implements the following hessian-free algorithms:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.KrylovTrustRegion(): A Newton-Krylov method with Trust Regions\ninitial_delta: The starting trust region radius\ndelta_hat: The largest allowable trust region radius\neta: When rho is at least eta, accept the step.\nrho_lower: When rho is less than rho_lower, shrink the trust region.\nrho_upper: When rho is greater than rhoupper, grow the trust region (though no greater than deltahat).\nDefaults:\ninitial_delta = 1.0\ndelta_hat = 100.0\neta = 0.1\nrho_lower = 0.25\nrho_upper = 0.75","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.KrylovTrustRegion() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\ncons= (x,p) -> [x[1]^2 + x[2]^2]\nx0 = zeros(2)\np  = [1.0,100.0]\noptprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= cons)\nprob = Optimization.OptimizationProblem(optprob, x0, p)\nsol = solve(prob, Optim.KrylovTrustRegion())","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#Global-Optimizer","page":"Optim.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/#Without-Constraint-Equations","page":"Optim.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The following method in Optim is performing global optimization on problems without constraint equations. It works both with and without lower and upper constraints set by lb and ub in the Optimization.OptimizationProblem.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.ParticleSwarm(): Particle Swarm Optimization\nsolve(problem, ParticleSwarm(lower, upper, n_particles))\nlower/upper are vectors of lower/upper bounds respectively\nn_particles is the number of particles in the swarm\ndefaults to: lower = [], upper = [], n_particles = 0","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.ParticleSwarm() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0,100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb=[-1.0, -1.0], ub=[1.0, 1.0])\nsol = solve(prob, Optim.ParticleSwarm(lower=prob.lb, upper= prob.ub, n_particles=100))","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/#With-Constraint-Equations","page":"Optim.jl","title":"With Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The following method in Optim is performing global optimization on problems with constraint equations.","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"Optim.SAMIN(): Simulated Annealing with bounds\nsolve(problem, SAMIN(nt, ns, rt, neps, f_tol, x_tol, coverage_ok, verbosity))\nDefaults:\nnt = 5\nns = 5\nrt = 0.9\nneps = 5\nf_tol = 1e-12\nx_tol = 1e-6\ncoverage_ok = false\nverbosity = 0","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"The Rosenbrock function can optimized using the Optim.SAMIN() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/optim/","page":"Optim.jl","title":"Optim.jl","text":"rosenbrock(x, p) =  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0,100.0]\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, p, lb=[-1.0, -1.0], ub=[1.0, 1.0])\nsol = solve(prob, Optim.SAMIN())","category":"page"},{"location":"highlevels/modeling_tools/#SciML-Modeling-Libraries","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"While in theory one can build perfect code for all models from scratch, in practice  many scientists and engineers need or want some help! The SciML modeling tools provide a higher level interface over the equation solvers which helps the translation from good models to good simulations in a way that abstracts away the mathematical and computational details without giving up performance.","category":"page"},{"location":"highlevels/modeling_tools/#ModelingToolkit.jl:-Acausal-Symbolic-Modeling","page":"SciML Modeling Libraries","title":"ModelingToolkit.jl: Acausal Symbolic Modeling","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"Acausal modeling is an extension of causal modeling that is more composable and allows for more code reuse. Build a model of an electric engine, then build a model of a battery, and now declare connections by stating \"the voltage at the engine equals the voltage at the connector of the battery\", and generate the composed model. The tool for this is ModelingToolkit.jl. ModelingToolkit.jl is a sophisticated symbolic modeling library which allows for specifying these types of large-scale differential equation models in a simple way, abstracting away the computational details. However, its symbolic analysis allows for generating much more performant code for differential-algebraic equations than most users could ever write by hand, with its structural_simplify automatically correcting the model to improve parallelism, numerical stability, and automatically remove variables which it can show are redundant.","category":"page"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"ModelingToolkit.jl is the base of the SciML symbolic modeling ecosystem, defining the AbstractSystem types, such as ODESystem, SDESystem, OptimizationSystem, PDESystem, and more, which are then used by all of the other modeling tools. As such, when using other modeling tools like Catalyst.jl, the reference for all of the things that can be done with the symbolic representation is simply ModelingToolkit.jl.","category":"page"},{"location":"highlevels/modeling_tools/#ModelingToolkitStandardLibrary.jl:-A-Standard-Library-for-ModelingToolkit","page":"SciML Modeling Libraries","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"Given the composable nature of acausal modeling systems, it's helpful to not have to define every component from scratch and instead build off a common base of standard components. ModelingToolkitStandardLibrary.jl is that library. It provides components for standard models to start building everything from circuits and engines to robots.","category":"page"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"(Image: )","category":"page"},{"location":"highlevels/modeling_tools/#Catalyst.jl:-Chemical-Reaction-Networks-(CRN),-Systems-Biology,-and-Quantiative-Systems-Pharmacology-(QSP)-Modeling","page":"SciML Modeling Libraries","title":"Catalyst.jl: Chemical Reaction Networks (CRN), Systems Biology, and Quantiative Systems Pharmacology (QSP) Modeling","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"Catalyst.jl is a modeling interface for efficient simulation  of chemical master equation representations chemical reaction networks and other systems models.  It uses a highly intuitive chemical reaction syntax interface which generates all of the extra  functionality necessary for the fastest use with DiffEqJump.jl and DifferentialEquations.jl. Its  ReactionSystem type is a programmable extension of the ModelingToolkit AbstractSystem interface,  meaning that complex reaction systems can be generated through code.","category":"page"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"For an overview of the library, see  Modeling Biochemical Systems with Catalyst.jl - Samuel Isaacson","category":"page"},{"location":"highlevels/modeling_tools/#NBodySimulator.jl:-A-differentiable-simulator-for-N-body-problems,-including-astrophysical-and-molecular-dynamics","page":"SciML Modeling Libraries","title":"NBodySimulator.jl: A differentiable simulator for N-body problems, including astrophysical and molecular dynamics","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"NBodySimulator.jl is differentiable simulator for N-body problems,  including astrophysical and molecular dynamics. It uses the DifferentialEquations.jl solvers, allowing for one to choose between a large variety of symplectic integration schemes. It implements many of the thermostats required for doing standard molecular dynamics approximations.","category":"page"},{"location":"highlevels/modeling_tools/#ParameterizedFunctions.jl:-Simple-Differential-Equation-Definitions-Made-Easy","page":"SciML Modeling Libraries","title":"ParameterizedFunctions.jl: Simple Differential Equation Definitions Made Easy","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"(Image: )","category":"page"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"This image that went viral is actually runnable code from ParameterizedFunctions.jl. Define equations and models using a very simple high level syntax and let the code generation tools build symbolic fast Jacobian, gradient, etc. functions for you.","category":"page"},{"location":"highlevels/modeling_tools/#Model-Import-Libraries","page":"SciML Modeling Libraries","title":"Model Import Libraries","text":"","category":"section"},{"location":"highlevels/modeling_tools/#SBMLToolkit.jl:-SBML-Import","page":"SciML Modeling Libraries","title":"SBMLToolkit.jl: SBML Import","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"SBMLToolkit.jl is a library for reading  SBML files into the standard formats for Catalyst.jl and ModelingToolkit.jl. There are more than one thousand biological models available in the the BioModels Repository.","category":"page"},{"location":"highlevels/modeling_tools/#CellMLToolkit.jl:-CellML-Import","page":"SciML Modeling Libraries","title":"CellMLToolkit.jl: CellML Import","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"CellMLToolkit.jl is a library for reading  CellML files into the standard formats for ModelingToolkit.jl. There are several hundred biological models available in the the CellML Model Repository.","category":"page"},{"location":"highlevels/modeling_tools/#ReactionNetworkImporters.jl:-BioNetGen-Import","page":"SciML Modeling Libraries","title":"ReactionNetworkImporters.jl: BioNetGen Import","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"ReactionNetworkImporters.jl is a library  for reading BioNetGen files into the standard formats for Catalyst.jl and ModelingToolkit.jl.","category":"page"},{"location":"highlevels/modeling_tools/#Third-Party-Tools-of-Note","page":"SciML Modeling Libraries","title":"Third Party Tools of Note","text":"","category":"section"},{"location":"highlevels/modeling_tools/#ReactionMechanismSimulator.jl:-Simulation-and-Analysis-of-Large-Chemical-Reaction-Systems","page":"SciML Modeling Libraries","title":"ReactionMechanismSimulator.jl: Simulation and Analysis of Large Chemical Reaction Systems","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"ReactionMechanismSimulator.jl is a tool for simulating and analyzing large chemical reaction mechanisms. It interfaces with the ReactionMechanismGenerator suite for automatically constructing reaction pathways from chemical components to quickly build realistic models of chemical systems.","category":"page"},{"location":"highlevels/modeling_tools/#MomentClosure.jl:-Automated-Generation-of-Moment-Closure-Equations","page":"SciML Modeling Libraries","title":"MomentClosure.jl: Automated Generation of Moment Closure Equations","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"MomentClosure.jl is a library for generating the moment closure equations for a given chemical master equation or stochastic differential equation. Thus instead of solving a stochastic model thousands of times to find the mean and variance, this library can generate the deterministic equations for how the mean and variance evolve in order to be solved in a single run. MomentClosure.jl uses Catalyst ReactionSystem and ModelingToolkit SDESystem types as the input for its symbolic generation processes.","category":"page"},{"location":"highlevels/modeling_tools/#FiniteStateProjection.jl:-Direct-Solution-of-Chemical-Master-Equations","page":"SciML Modeling Libraries","title":"FiniteStateProjection.jl: Direct Solution of Chemical Master Equations","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"FiniteStateProjection.jl is a library for finite state projection direct solving of the chemical master equation. It automatically converts the Catayst ReactionSystem definitions into ModelingToolkit ODESystem representations for the evolution of probability distributions to allow for directly solving the weak form of the stochastic model.","category":"page"},{"location":"highlevels/modeling_tools/#AlgebraicPetri.jl:-Applied-Category-Theory-of-Modeling","page":"SciML Modeling Libraries","title":"AlgebraicPetri.jl: Applied Category Theory of Modeling","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"AlgebraicPetri.jl is a library for automating the intuitive generation of dynamical models using a Category theory based approach.","category":"page"},{"location":"highlevels/modeling_tools/#Agents.jl:-Agent-Based-Modeling-Framework-in-Julia","page":"SciML Modeling Libraries","title":"Agents.jl: Agent-Based Modeling Framework in Julia","text":"","category":"section"},{"location":"highlevels/modeling_tools/","page":"SciML Modeling Libraries","title":"SciML Modeling Libraries","text":"If one wants to do agent-based modeling in Julia,  Agents.jl is the go-to library. It's fast and flexible, making it a solid foundation for any agent-based model.","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#Discrete-Solvers","page":"Discrete Solvers","title":"Discrete Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#DiscreteProblems","page":"Discrete Solvers","title":"DiscreteProblems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"solve(prob::DiscreteProblem,alg;kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Solves the discrete function map defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#Recommended-Methods","page":"Discrete Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"The implementation for solving discrete equations is the FunctionMap algorithm in OrdinaryDiffEq.jl. It allows the full common interface (including events/callbacks) to solve function maps, along with everything else like plot recipes, while completely ignoring the ODE functionality related to continuous equations (except for a tiny bit of initialization). However, the SimpleFunctionMap from SimpleDiffEq.jl can be more efficient if the mapping function is sufficiently cheap, but it doesn't have all of the extras like callbacks and saving support (but does have an integrator interface).","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#Full-List-of-Methods","page":"Discrete Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#OrdinaryDiffEq.jl","page":"Discrete Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"FunctionMap: A basic function map which implements the full common interface.","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"OrdinaryDiffEq.jl also contains the FunctionMap algorithm which lets you  It has a piecewise constant interpolation and allows for all of the  callback/event handling capabilities (of course, with rootfind=false. If a  ContinuousCallback is given, it's always assumed rootfind=false).","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"The constructor is:","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"FunctionMap()\nFunctionMap{scale_by_time}()","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Every step is the update","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"u_n+1 = f(t_n+1u_n)","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"If in addition scale_by_time is marked true (default is false),  then every step is the update:","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"u_n+1 = u_n + dtf(t_n+1u_n)","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Notice that this is the same as updates from the Euler method, except in this case we assume that its a discrete change and thus the interpolation is piecewise constant.","category":"page"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/#SimpleDiffEq.jl","page":"Discrete Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"SimpleFunctionMap: A barebones implementation of a function map. Is optimally-efficient and has an integrator interface version, but does not support callbacks or saving controls.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/#sdae_solve","page":"SDAE Solvers","title":"SDAE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/#Recommended-Methods","page":"SDAE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"The recommendations for SDAEs are the same recommended implicit SDE methods for stiff equations when the SDAE is specified in mass matrix form.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/#Mass-Matrix-Form","page":"SDAE Solvers","title":"Mass Matrix Form","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal,Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/#Notes","page":"SDAE Solvers","title":"Notes","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"modules/DiffEqDocs/solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take much more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/#Boundary-Value-Problems","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"This tutorial will introduce you to the functionality for solving BVPs. Other introductions can be found by checking out SciMLTutorials.jl. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"In this example we will solve the ODE that satisfies the boundary condition in the form of","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"beginaligned\nfracdudt = f(t u) \ng(u) = vec0\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/#Example-1:-Simple-Pendulum","page":"Boundary Value Problems","title":"Example 1: Simple Pendulum","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The concrete example that we are solving is the simple pendulum ddotu+fracgLsin(u)=0 on the time interval tin0fracpi2. First, we need to define the ODE","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"using BoundaryValueDiffEq\nconst g = 9.81\nL = 1.0\ntspan = (0.0,pi/2)\nfunction simplependulum!(du,u,p,t)\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(g/L)*sin(θ)\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/#Boundary-Condition","page":"Boundary Value Problems","title":"Boundary Condition","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"There are two problem types available:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"A problem type for general boundary conditions BVProblem ( including conditions that may be anywhere/ everywhere on the integration interval ).\nA problem type for boundaries that are specified at the beginning and the end of the integration interval TwoPointBVProblem","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/#BVProblem","page":"Boundary Value Problems","title":"BVProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The boundary conditions are specified by a function that calculates the residual in-place from the problem solution, such that the residual is vec0 when the boundary condition is satisfied.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"function bc1!(residual, u, p, t)\n    residual[1] = u[end÷2][1] + pi/2 # the solution at the middle of the time span should be -pi/2\n    residual[2] = u[end][1] - pi/2 # the solution at the end of the time span should be pi/2\nend\nbvp1 = BVProblem(simplependulum!, bc1!, [pi/2,pi/2], tspan)\nsol1 = solve(bvp1, GeneralMIRK4(), dt=0.05)\nplot(sol1)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot1)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The third argument of BVProblem  is the initial guess of the solution, which is constant in this example. <!– add examples of more general initial conditions –> We need to use GeneralMIRK4 or Shooting methods to solve BVProblem. GeneralMIRK4 is a collocation method, whereas Shooting treats the problem as an IVP and varies the initial conditions until the boundary conditions are met. If you can have a good initial guess, Shooting method works very well.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"using OrdinaryDiffEq\nu₀_2 = [-1.6, -1.7] # the initial guess\nfunction bc3!(residual, sol, p, t)\n    residual[1] = sol(pi/4)[1] + pi/2 # use the interpolation here, since indexing will be wrong for adaptive methods\n    residual[2] = sol(pi/2)[1] - pi/2\nend\nbvp3 = BVProblem(simplependulum!, bc3!, u₀_2, tspan)\nsol3 = solve(bvp3, Shooting(Vern7()))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The initial guess can also be supplied via a function of t or a previous solution type, this is espacially handy for parameter analysis. We changed u to sol to emphasize the fact that in this case the boundary condition can be written on the solution object. Thus all of the features on the solution type such as interpolations are available when using the Shooting method (i.e. you can have a boundary condition saying that the maximum over the interval is 1 using an optimization function on the continuous output). Note that user has to import the IVP solver before it can be used. Any common interface ODE solver is acceptable.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"plot(sol3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/#TwoPointBVProblem","page":"Boundary Value Problems","title":"TwoPointBVProblem","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"Defining a similar problem as TwoPointBVProblem is shown in the following example. At the moment MIRK4 is the only solver for TwoPointBVProblems.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"function bc2!(residual, u, p, t) # u[1] is the beginning of the time span, and u[end] is the ending\n    residual[1] = u[1][1] + pi/2 # the solution at the beginning of the time span should be -pi/2\n    residual[2] = u[end][1] - pi/2 # the solution at the end of the time span should be pi/2\nend\nbvp2 = TwoPointBVProblem(simplependulum!, bc2!, [pi/2,pi/2], tspan)\nsol2 = solve(bvp2, MIRK4(), dt=0.05) # we need to use the MIRK4 solver for TwoPointBVProblem\nplot(sol2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"Note that u is a tuple of ( u[1], u[end] ) just like t is ( t[1], t[end] ) and p holds the parameters of the given problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot2)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/#brusselator","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/#Using-the-Brusselator-PDE-as-an-example","page":"Tutorial","title":"Using the Brusselator PDE as an example","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"The Brusselator PDE is defined as follows:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"beginalign\nfracpartial upartial t = 1 + u^2v - 44u + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2) + f(x y t)\nfracpartial vpartial t = 34u - u^2v + alpha(fracpartial^2 vpartial x^2 + fracpartial^2 vpartial y^2)\nendalign","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"where","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"f(x y t) = begincases\n5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11 \n0  quad textelse\nendcases","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"and the initial conditions are","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"beginalign\nu(x y 0) = 22cdot (y(1-y))^32 \nv(x y 0) = 27cdot (x(1-x))^32\nendalign","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"with the periodic boundary condition","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"beginalign\nu(x+1yt) = u(xyt) \nu(xy+1t) = u(xyt)\nendalign","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"on a timespan of t in 0115.","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/#Solving-with-MethodOfLines","page":"Tutorial","title":"Solving with MethodOfLines","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"With ModelingToolkit.jl, we first symbolicaly define the system, see also the docs for PDESystem:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"using ModelingToolkit, MethodOfLines, OrdinaryDiffEq, DomainSets\n\n\n@parameters x y t\n@variables u(..) v(..)\nDt = Differential(t)\nDx = Differential(x)\nDy = Differential(y)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n∇²(u) = Dxx(u) + Dyy(u)\n\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\n\nx_min = y_min = t_min = 0.0\nx_max = y_max = 1.0\nt_max = 11.5\n\nα = 10.\n\nu0(x,y,t) = 22(y*(1-y))^(3/2)\nv0(x,y,t) = 27(x*(1-x))^(3/2)\n\neq = [Dt(u(x,y,t)) ~ 1. + v(x,y,t)*u(x,y,t)^2 - 4.4*u(x,y,t) + α*∇²(u(x,y,t)) + brusselator_f(x, y, t),\n       Dt(v(x,y,t)) ~ 3.4*u(x,y,t) - v(x,y,t)*u(x,y,t)^2 + α*∇²(v(x,y,t))]\n\ndomains = [x ∈ Interval(x_min, x_max),\n              y ∈ Interval(y_min, y_max),\n              t ∈ Interval(t_min, t_max)]\n\n# Periodic BCs\nbcs = [u(x,y,0) ~ u0(x,y,0),\n       u(0,y,t) ~ u(1,y,t),\n       u(x,0,t) ~ u(x,1,t),\n\n       v(x,y,0) ~ v0(x,y,0),\n       v(0,y,t) ~ v(1,y,t),\n       v(x,0,t) ~ v(x,1,t)] \n\n@named pdesys = PDESystem(eq,bcs,domains,[x,y,t],[u(x,y,t),v(x,y,t)])","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"For a list of limitations constraining which systems will work, see here","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/#Method-of-lines-discretization","page":"Tutorial","title":"Method of lines discretization","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"Then, we create the discretization, leaving the time dimension undiscretized by supplying t as an argument. Optionally, all dimensions can be discretized in this step, just remove the argument t and supply t=>dt in the dxs. See here for more information on the MOLFiniteDifference constructor arguments and options.","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"N = 32\n\ndx = (x_max-x_min)/N\ndy = (y_max-y_min)/N\n\norder = 2\n\ndiscretization = MOLFiniteDifference([x=>dx, y=>dy], t, approx_order=order, grid_align=center_align)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"Next, we discretize the system, converting the PDESystem in to an ODEProblem or NonlinearProblem.","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"# Convert the PDE problem into an ODE problem\nprintln(\"Discretization:\")\n@time prob = discretize(pdesys,discretization)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/#Solving-the-problem","page":"Tutorial","title":"Solving the problem","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"Now your problem can be solved with an appropriate ODE solver, or Nonlinear solver if you have not supplied a time dimension in the MOLFiniteDifference constructor. Include these solvers with using OrdinaryDiffEq or using NonlinearSolve, then call sol = solve(prob, AppropriateSolver()) or sol = NonlinearSolve.solve(prob, AppropriateSolver()). For more information on the available solvers, see the docs for DifferentialEquations.jl, NonlinearSolve.jl and SteadyStateDiffEq.jl. Tsit5() is a good first choice of solver for many problems.","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"println(\"Solve:\")\n@time sol = solve(prob, TRBDF2(), saveat=0.1)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/#Extracting-results","page":"Tutorial","title":"Extracting results","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"To retrieve your solution, for example for u, use sol[u]. To get the time axis, use sol.t.","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"Due to current limitations in the sol interface, above 1 discretized dimension the result must be manually reshaped to correctly display the result, best done with the help of the get_discrete helper function. Here is an example of how to do this:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"grid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\ndiscrete_y = grid[y]\ndiscrete_t = sol[t]\n\nsolu = [map(d -> sol[d][i], grid[u(x, y, t)]) for i in 1:length(sol[t])]\nsolv = [map(d -> sol[d][i], grid[v(x, y, t)]) for i in 1:length(sol[t])]","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"The result after plotting an animation:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"For u:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"using Plots\nanim = @animate for k in 1:length(discrete_t)\n    heatmap(solu[k][2:end, 2:end], title=\"$(discrete_t[k])\") # 2:end since end = 1, periodic condition\nend\ngif(anim, \"plots/Brusselator2Dsol_u.gif\", fps = 8)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"(Image: Brusselator2Dsol_u)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"For v:","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"anim = @animate for k in 1:length(discrete_t)\n    heatmap(solv[k][2:end, 2:end], title=\"$(discrete_t[k])\")\nend\ngif(anim, \"plots/Brusselator2Dsol_v.gif\", fps = 8)","category":"page"},{"location":"modules/MethodOfLines/tutorials/brusselator/","page":"Tutorial","title":"Tutorial","text":"(Image: Brusselator2Dsol_v)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/#optcontrol","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Here we will solve a classic optimal control problem with a universal differential equation. Let","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"x^ = u^3(t)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where we want to optimize our controller u(t) such that the following is minimized:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert x^prime(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"where i is measured on (0,8) at 0.01 intervals. To do this, we rewrite the ODE in first order form:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"beginaligned\nx^prime = v \nv^ = u^3(t) \nendaligned","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"and thus","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"L(theta) = sum_i Vert 4 - x(t_i) Vert + 2 Vert v(t_i) Vert + Vert u(t_i) Vert","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"is our loss function on the first order system. We thus choose a neural network form for u and optimize the equation with respect to this loss. Note that we will first reduce control cost (the last term) by 10x in order to bump the network out of a local minimum. This looks like:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"using Flux, DifferentialEquations, Optimization, OptimizationOptimJL, OptimizationFlux, \n      SciMLSensitivity, Zygote, Plots, Statistics, Random\n\nrng = Random.default_rng()\ntspan = (0.0f0,8.0f0)\nann = Flux.Chain(Flux.Dense(1,32,tanh), Flux.Dense(32,32,tanh), Flux.Dense(32,1))\nθ, re = Flux.destructure(ann)\nfunction dxdt_(dx,x,p,t)\n    x1, x2 = x\n    dx[1] = x[2]\n    dx[2] = re(p)([t])[1]^3\nend\nx0 = [-4f0,0f0]\nts = Float32.(collect(0.0:0.01:tspan[2]))\nprob = ODEProblem(dxdt_,x0,tspan,θ)\nsolve(prob,Vern9(),abstol=1e-10,reltol=1e-10)\n\nfunction predict_adjoint(θ)\n  Array(solve(prob,Vern9(),p=θ,saveat=ts,sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(re(θ)([t])) for t in ts])/10\nend\n\nl = loss_adjoint(θ)\ncallback = function (θ,l; doplot=false)\n  println(l)\n\n  if doplot\n    p = plot(solve(remake(prob,p=θ),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\n    plot!(p,ts,[first(re(θ)([t])) for t in ts],label=\"u(t)\",lw=3)\n    display(p)\n  end\n\n  return false\nend\n\n# Display the ODE with the current parameter values.\n\ncallback(θ,l)\n\n# Setup and run the optimization\n\nloss1 = loss_adjoint(θ)\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_adjoint(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, θ)\nres1 = Optimization.solve(optprob, ADAM(0.005), callback = callback,maxiters=100)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2,\n                              BFGS(), maxiters=100,\n                              allow_f_increases = false)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Now that the system is in a better behaved part of parameter space, we return to the original loss function to finish the optimization:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"function loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  mean(abs2,4.0 .- x[1,:]) + 2mean(abs2,x[2,:]) + mean(abs2,[first(re(θ)([t])) for t in ts])\nend\noptf3 = Optimization.OptimizationFunction((x,p)->loss_adjoint(x), adtype)\n\noptprob3 = Optimization.OptimizationProblem(optf3, res2.u)\nres3 = Optimization.solve(optprob3,\n                              BFGS(),maxiters=100,\n                              allow_f_increases = false)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"Now let's see what we received:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"l = loss_adjoint(res3.u)\ncallback(res3.u,l)\np = plot(solve(remake(prob,p=res3.u),Tsit5(),saveat=0.01),ylim=(-6,6),lw=3)\nplot!(p,ts,[first(re(res3.u)([t])) for t in ts],label=\"u(t)\",lw=3)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/optimal_control/","page":"Solving Optimal Control Problems with Universal Differential Equations","title":"Solving Optimal Control Problems with Universal Differential Equations","text":"(Image: )","category":"page"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/#Solving-KdV-Solitons-with-Upwinding-Operators","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"The KdV equation is of the form uₜ + αuuₓ + βuₓₓₓ = 0. Here we'll use α = 6, β = 1 for  simplicity of the true solution expression.","category":"page"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/#Soliton-solution-using-Upwind-Difference","page":"Solving KdV Solitons with Upwinding Operators","title":"1-Soliton solution using Upwind Difference","text":"","category":"section"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"The analytical expression for the single soliton case takes the form u(x,t) = (c/2)/cosh²(√c * ξ/2).","category":"page"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"c > 0 (wave speed) ; ξ  = x - c*t (moving coordinate)","category":"page"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"using Test\nusing DiffEqOperators, OrdinaryDiffEq, LinearAlgebra\n\n# Space domain and grids\nN = 21\nΔx = 1/(N-1)\nc = 1\nx = -10:Δx:10;\n\n# solution of the single forward moving wave\nϕ(x,t) = (1/2)*sech.((x .- t)/2).^2 \n\n# Discretizing the PDE at t = 0\nu0 = ϕ(x,0);\ndu = zeros(size(x)); \n\n# Declaring the Upwind operator with winding = -1 since the wave travels from left to right \nA = UpwindDifference{Float64}(1,3,Δx,length(x),-1);\n\n# Defining the ODE problem\nfunction KdV(du, u, p, t)\n\tbc = GeneralBC([0,1,-6*ϕ(-10,t),0,-1],[0,1,-6*ϕ(10,t),0,-1],Δx,3) \n\tmul!(du,A,bc*u)\nend\n\nsingle_solition = ODEProblem(KdV, u0, (0.,5.));\n\n# Solving the ODE problem \nsoln = solve(single_solition,Tsit5(),abstol=1e-6,reltol=1e-6);\n\n# Plotting the results, comparing Analytical and Numerical solutions \nusing Plots\nplot(ϕ(x,0), title  = \"Single forward moving wave\", yaxis=\"u(x,t)\", label = \"t = 0.0 (Analytic)\")\nplot!(ϕ(x,2), label = \"t = 2.0 (Analytic)\")\nplot!(soln(0.0), label = \"t = 0.0 (Numerical)\",ls = :dash)\nplot!(soln(2.0), label = \"t = 2.0 (Numerical)\",ls = :dash)","category":"page"},{"location":"modules/DiffEqOperators/operator_tutorials/kdv/","page":"Solving KdV Solitons with Upwinding Operators","title":"Solving KdV Solitons with Upwinding Operators","text":"(Image: solution_plot)","category":"page"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/#Global-Identifiability-Tools","page":"Global Identifiability Tools","title":"Global Identifiability Tools","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/","page":"Global Identifiability Tools","title":"Global Identifiability Tools","text":"Pages=[\"global_identifiability.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/","page":"Global Identifiability Tools","title":"Global Identifiability Tools","text":"CurrentModule=StructuralIdentifiability","category":"page"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/","page":"Global Identifiability Tools","title":"Global Identifiability Tools","text":"StructuralIdentifiability.extract_identifiable_functions\nStructuralIdentifiability.check_field_membership\nStructuralIdentifiability.find_identifiable_functions\nStructuralIdentifiability.get_degree_and_coeffsize","category":"page"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/#StructuralIdentifiability.extract_identifiable_functions","page":"Global Identifiability Tools","title":"StructuralIdentifiability.extract_identifiable_functions","text":"extract_identifiable_functions(io_equations, parameters)\n\nFor the io_equation and the list of all parameter variables, returns a set of generators of a field of all functions of parameters\n\nNote: an experimental functionality at the moment, may fail be inefficient\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/#StructuralIdentifiability.check_field_membership","page":"Global Identifiability Tools","title":"StructuralIdentifiability.check_field_membership","text":"check_field_membership(generators, rat_funcs, p, [method=:GroebnerBasis])\n\nChecks whether given rational function belogn to a given field of rational functions\n\nInputs:\n\ngenerators - a list of lists of polynomials. Each of the lists, say, [f1, ..., fn], defines generators f2/f1, ..., fn/f1. Let F be the field generated by all of them.\nrat_funcs - list rational functions\np - a real number between 0 and 1, the probability of correctness\n\nOutput: \n\na list L[i] of bools of length length(rat_funcs) such that L[i] is true iff  the i-th function belongs to F. The whole result is correct with probability at least p\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/#StructuralIdentifiability.find_identifiable_functions","page":"Global Identifiability Tools","title":"StructuralIdentifiability.find_identifiable_functions","text":"find_identifiable_functions(ode::ODE{<: MPolyElem{fmpq}}, p::Float64=0.99)\n\nInput:\n\node - ODE-system\np - probability of correctness\n\nOutput:\n\nreturns a set of generators of the field of all functions of parameters\n\nFind identifiable functions of parameters for a given ode. \n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/utils/global_identifiability/#StructuralIdentifiability.get_degree_and_coeffsize","page":"Global Identifiability Tools","title":"StructuralIdentifiability.get_degree_and_coeffsize","text":"get_degree_and_coeffsize(f)\n\nfor f being a polynomial/rational function over rationals (QQ) returns a tuple (degree, max_coef_size)\n\n\n\n\n\n","category":"function"},{"location":"modules/Surrogates/gek/#Gradient-Enhanced-Kriging","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Gradient-enhanced Kriging is an extension of kriging which supports gradient information. GEK is usually more accurate than kriging, however, it is not computationally efficient when the number of inputs, the number of sampling points, or both, are high. This is mainly due to the size of the corresponding correlation matrix that increases proportionally with both the number of inputs and the number of sampling points.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's have a look to the following function to use Gradient Enhanced Surrogate: f(x) = sin(x) + 2*x^2","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all, we will import Surrogates and Plots packages:","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/gek/#Sampling","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"We choose to sample f in 8 points between 0 to 1 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 10\r\nlower_bound = 2\r\nupper_bound = 10\r\nxs = lower_bound:0.001:upper_bound\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nf(x) = x^3 - 6x^2 + 4x + 12\r\ny1 = f.(x)\r\nder = x -> 3*x^2 - 12*x + 4\r\ny2 = der.(x)\r\ny = vcat(y1,y2)\r\nscatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/gek/#Building-a-surrogate","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"With our sampled points we can build the Gradient Enhanced Kriging surrogate using the GEK function.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"my_gek = GEK(x, y, lower_bound, upper_bound, p = 1.4);","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"scatter(x, y1, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\r\nplot!(my_gek, label=\"Surrogate function\", ribbon=p->std_error_at_point(my_gek, p), xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/gek/#Gradient-Enhanced-Kriging-Surrogate-Tutorial-(ND)","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging Surrogate Tutorial (ND)","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"First of all let's define the function we are going to build a surrogate for.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Now, let's define the function:","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"function leon(x)\r\n      x1 = x[1]\r\n      x2 = x[2]\r\n      term1 = 100*(x2 - x1^3)^2\r\n      term2 = (1 - x1)^2\r\n      y = term1 + term2\r\nend","category":"page"},{"location":"modules/Surrogates/gek/#Sampling-2","page":"Gradient Enhanced Kriging","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 10, and 0, 10 for the second dimension. We are taking 80 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"n_samples = 80\r\nlower_bound = [0, 0]\r\nupper_bound = [10, 10]\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny1 = leon.(xys);","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"x, y = 0:10, 0:10 # hide\r\np1 = surface(x, y, (x1,x2) -> leon((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, y1) # hide\r\np2 = contour(x, y, (x1,x2) -> leon((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/gek/#Building-a-surrogate-2","page":"Gradient Enhanced Kriging","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"grad1 = x1 -> 2*(300*(x[1])^5 - 300*(x[1])^2*x[2] + x[1] -1)\r\ngrad2 = x2 -> 200*(x[2] - (x[1])^3)\r\nd = 2\r\nn = 10\r\nfunction create_grads(n, d, grad1, grad2, y)\r\n      c = 0\r\n      y2 = zeros(eltype(y[1]),n*d)\r\n      for i in 1:n\r\n            y2[i + c] = grad1(x[i])\r\n            y2[i + c + 1] = grad2(x[i])\r\n            c = c + 1\r\n      end\r\n      return y2\r\nend\r\ny2 = create_grads(n, d, grad2, grad2, y)\r\ny = vcat(y1,y2)","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"my_GEK = GEK(xys, y, lower_bound, upper_bound, p=[1.9, 1.9])","category":"page"},{"location":"modules/Surrogates/gek/","page":"Gradient Enhanced Kriging","title":"Gradient Enhanced Kriging","text":"p1 = surface(x, y, (x, y) -> my_GEK([x y])) # hide\r\nscatter!(xs, ys, y1, marker_z=y1) # hide\r\np2 = contour(x, y, (x, y) -> my_GEK([x y])) # hide\r\nscatter!(xs, ys, marker_z=y1) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/contributing/#Contributions","page":"Contributing","title":"Contributions","text":"","category":"section"},{"location":"modules/Surrogates/contributing/","page":"Contributing","title":"Contributing","text":"Contributions are very welcome! There are many ways do help:","category":"page"},{"location":"modules/Surrogates/contributing/","page":"Contributing","title":"Contributing","text":"Opening/solving issues\nMaking the code more efficient\nOpening a new PR with a new Sampling technique, Surrogate or optimization method\nWriting more tutorials with your own unique use case of the library\nYour own idea!","category":"page"},{"location":"modules/Surrogates/contributing/","page":"Contributing","title":"Contributing","text":"You can also contact me on the Julia slack channel at @ludoro.","category":"page"},{"location":"modules/Surrogates/contributing/#List-of-contributors","page":"Contributing","title":"List of contributors","text":"","category":"section"},{"location":"modules/Surrogates/contributing/","page":"Contributing","title":"Contributing","text":"Ludovico Bessi (@ludoro)\nChris Rackauckas (@ChrisRackauckas)\nRohit Singh Rathaur (@RohitRathore1)\nAndrea Cognolato (@mrandri19)\nKanav Gupta (@kanav99)","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/rc_circuit/#RC-Circuit-Model","page":"RC Circuit","title":"RC Circuit Model","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/rc_circuit/","page":"RC Circuit","title":"RC Circuit","text":"This tutorial is a simplified version of the RC circuit tutorial in the ModelingToolkit.jl documentation. In that tutorial, the full RC circuit is built from scratch. Here, we will use the components of the Electrical model in the ModelingToolkit Standard Library to simply connect pre-made components and simulate the model.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/rc_circuit/","page":"RC Circuit","title":"RC Circuit","text":"using ModelingToolkit, OrdinaryDiffEq, Plots\nusing ModelingToolkitStandardLibrary.Electrical\n\nR = 1.0\nC = 1.0\nV = 1.0\n@variables t\n@named resistor = Resistor(R=R)\n@named capacitor = Capacitor(C=C)\n@named source = ConstantVoltage(V=V)\n@named ground = Ground()\n\nrc_eqs = [\n        connect(source.p, resistor.p)\n        connect(resistor.n, capacitor.p)\n        connect(capacitor.n, source.n, ground.g)\n        ]\n\n@named rc_model = ODESystem(rc_eqs, t, systems=[resistor, capacitor, source, ground])\nsys = structural_simplify(rc_model)\nprob = ODAEProblem(sys, Pair[], (0, 10.0))\nsol = solve(prob, Tsit5())\nplot(sol, vars = [capacitor.v,resistor.i],\n     title = \"RC Circuit Demonstration\",\n     labels = [\"Capacitor Voltage\" \"Resistor Current\"])\nsavefig(\"plot.png\")","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/rc_circuit/","page":"RC Circuit","title":"RC Circuit","text":"(Image: )","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/#Using-JuMP-with-DiffEqParamEstim","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"JuMP is a domain-specific modeling language for mathematical optimization embedded in Julia.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"using OrdinaryDiffEq, DiffEqParamEstim, JuMP, NLopt, Plots","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's define the Lorenz equation to use as our example","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"function g(du,u,p,t)\n  σ,ρ,β = p\n  x,y,z = u\n  du[1] = dx = σ*(y-x)\n  du[2] = dy = x*(ρ-z) - y\n  du[3] = dz = x*y - β*z\nend","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's get a solution of the system with parameter values σ=10.0 ρ=28.0 β=8/3 to use as our data. We define some convenience functions model_ode (to create an ODEProblem) and solve_model(to obtain solution of the ODEProblem) to use in a custom objective function later.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"u0 = [1.0;0.0;0.0]\nt = 0.0:0.01:1.0\ntspan = (0.0,1.0)\nmodel_ode(p_) = ODEProblem(g, u0, tspan,p_)\nsolve_model(mp_) = OrdinaryDiffEq.solve(model_ode(mp_), Tsit5(),saveat=0.01)\nmock_data = Array(solve_model([10.0,28.0,8/3]))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Now we define a custom objective function to pass for optimization to JuMP using the build_loss_objective described above provided by DiffEqParamEstim that defines an objective function for the parameter estimation problem.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"loss_objective(mp_, dat) = build_loss_objective(model_ode(mp_), Tsit5(), L2Loss(t,dat))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"We create a JuMP model, variables, set the objective function and the choice of optimization algorithm to be used in the JuMP syntax. You can read more about this in JuMP's documentation.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"juobj(args...) = loss_objective(args, mock_data)(args)\njumodel = Model()\nJuMP.register(jumodel, :juobj, 3, juobj, autodiff=true)\n@variables jumodel begin\n    σ,(start=8)\n    ρ,(start=25.0)\n    β,(start=10/3)\nend\n@NLobjective(jumodel, Min, juobj(σ, ρ, β))\nsetsolver(jumodel, NLoptSolver(algorithm=:LD_MMA))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's call the optimizer to obtain the fitted parameter values.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = JuMP.solve(jumodel)\nbest_mp = getvalue.(getindex.((jumodel,), Symbol.(jumodel.colNames)))","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"Let's compare the solution at the obtained parameter values and our data.","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"sol = OrdinaryDiffEq.solve(best_mp |> model_ode, Tsit5())\nplot(getindex.(sol.(t),1))\nscatter!(mock_data, markersize=2)","category":"page"},{"location":"modules/DiffEqParamEstim/tutorials/jump/","page":"Using JuMP with DiffEqParamEstim","title":"Using JuMP with DiffEqParamEstim","text":"(Image: jumpestimationplot)","category":"page"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#OptimizationSystem","page":"OptimizationSystem","title":"OptimizationSystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#System-Constructors","page":"OptimizationSystem","title":"System Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/","page":"OptimizationSystem","title":"OptimizationSystem","text":"OptimizationSystem","category":"page"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#ModelingToolkit.OptimizationSystem","page":"OptimizationSystem","title":"ModelingToolkit.OptimizationSystem","text":"struct OptimizationSystem <: AbstractTimeIndependentSystem\n\nA scalar equation for optimization.\n\nFields\n\nop\nVector of equations defining the system.\nstates\nUnknown variables.\nps\nParameters.\nvar_to_name\nArray variables.\nobserved\nconstraints\nname\nName: the name of the system.  These are required to have unique names.\n\nsystems\nsystems: The internal systems\n\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nExamples\n\n@variables x y z\n@parameters σ ρ β\n\nop = σ*(y-x) + x*(ρ-z)-y + x*y - β*z\n@named os = OptimizationSystem(op, [x,y,z],[σ,ρ,β])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#Composition-and-Accessor-Functions","page":"OptimizationSystem","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/","page":"OptimizationSystem","title":"OptimizationSystem","text":"get_eqs(sys) or equations(sys): The equation to be minimized.\nget_states(sys) or states(sys): The set of states for the optimization.\nget_ps(sys) or parameters(sys): The parameters for the optimization.","category":"page"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#Transformations","page":"OptimizationSystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#Analyses","page":"OptimizationSystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#Applicable-Calculation-and-Generation-Functions","page":"OptimizationSystem","title":"Applicable Calculation and Generation Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/","page":"OptimizationSystem","title":"OptimizationSystem","text":"calculate_gradient\r\ncalculate_hessian\r\ngenerate_gradient\r\ngenerate_hessian\r\nhessian_sparsity","category":"page"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#Problem-Constructors","page":"OptimizationSystem","title":"Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/","page":"OptimizationSystem","title":"OptimizationSystem","text":"OptimizationProblem","category":"page"},{"location":"modules/ModelingToolkit/systems/OptimizationSystem/#SciMLBase.OptimizationProblem","page":"OptimizationSystem","title":"SciMLBase.OptimizationProblem","text":"Defines a optimization problem. Documentation Page: https://galacticoptim.sciml.ai/dev/API/optimization_problem/\n\nMathematical Specification of a Optimization Problem\n\nTo define an Optimization Problem, you simply need to give the function f which defines the cost function to minimize:\n\nmin_u f(up)\n\nu₀ is an initial guess of the minimum. f should be specified as f(u,p) and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher-dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nOptimizationProblem{iip}(f, x, p = SciMLBase.NullParameters(),;\n                        lb = nothing,\n                        ub = nothing,\n                        lcons = nothing,\n                        ucons = nothing,\n                        sense = nothing,\n                        kwargs...)\n\nisinplace optionally sets whether the function is in-place or not. This is determined automatically, but not inferred. Note that for OptimizationProblem, in-place only refers to the Jacobian and Hessian functions, and thus by default if the OptimizationFunction is not defined directly then iip = true is done by default.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nlb and ub are the upper and lower bounds for box constraints on the optimization. They should be an AbstractArray matching the geometry of u, where (lb[I],ub[I]) is the box constraint (lower and upper bounds) for u[I].\n\nlcons and ucons are the upper and lower bounds for equality constraints on the optimization. They should be an AbstractArray matching the geometry of u, where (lcons[I],ucons[I]) is the constraint (lower and upper bounds) for cons[I].\n\nIf f is a standard Julia function, it is automatically converted into an OptimizationFunction with NoAD(), i.e., no automatic generation of the derivative functions.\n\nAny extra keyword arguments are captured to be sent to the optimizers.\n\nFields\n\nf: the function in the problem.\nu0: the initial guess for the optima.\np: the parameters for the problem. Defaults to NullParameters.\nlb: the lower bounds for the optimization of u.\nub: the upper bounds for the optimization of u.\nlcons: the vector of lower bounds for the constraints passed to OptimizationFunction.   Defaults to nothing, implying no lower bounds for the constraints (i.e. the constraint bound is -Inf)\nucons: the vector of upper bounds for the constraints passed to OptimizationFunction.   Defaults to nothing, implying no upper bounds for the constraints (i.e. the constraint bound is Inf)\nsense: the objective sense, can take MaxSense or MinSense from Optimization.jl.\nkwargs: the keyword arguments passed on to the solvers.\n\nInequality and Equality Constraints\n\nBoth inequality and equality constraints are defined by the f.cons function in the OptimizationFunction description of the problem structure. This f.cons is given as a function f.cons(u,p) which computes the value of the constraints at u. For example, take f.cons(u,p) = u[1] - u[2]. With these definitions, lcons and ucons define the bounds on the constraint that the solvers try to satisfy. If lcons and ucons are nothing, then there are no constraints bounds, meaning that the constraint is satisfied when -Inf < f.cons < Inf (which of course is always!). If lcons[i] = ucons[i] = 0, then the constraint is satisfied when f.cons(u,p)[i] = 0, and so this implies the equality constraint u1 = u2 If lconsi = uconsi = a thenu[1] - u[2] = a`` is the equality constraint. \n\nInequality constraints are then given by making lcons[i] != ucons[i]. For example, lcons[i] = -Inf and ucons[i] = 0 would imply the inequality constraint u1 = u2 since any f.cons[i] <= 0 satisfies the constraint. Similarly, lcons[i] = -1 and ucons[i] = 1 would imply that -1 <= f.cons[i] <= 1 is required or -1 = u1 - u2 = 1.\n\nNote that these vectors must be sized to match the number of constraints, with one set of conditions for each constraint.\n\n\n\n","category":"type"},{"location":"modules/ParameterizedFunctions/ode_def/#The-ode_def-macro","page":"The ode_def macro","title":"The ode_def macro","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/ode_def/","page":"The ode_def macro","title":"The ode_def macro","text":"ParameterizedFunctions.@ode_def\nParameterizedFunctions.@ode_def_bare\nParameterizedFunctions.@ode_def_all","category":"page"},{"location":"modules/ParameterizedFunctions/ode_def/#ParameterizedFunctions.@ode_def","page":"The ode_def macro","title":"ParameterizedFunctions.@ode_def","text":"@ode_def name begin\n    differential equation\nend parameters :: ODEFunction\n\nDefinition of the Domain-Specific Language (DSL)\n\nA helper macro is provided to make it easier to define a ParameterizedFunction, and it will symbolically compute a bunch of extra functions to make the differential equation solvers run faster. For example, to define the previous LotkaVolterra, you can use the following command:\n\nf = @ode_def LotkaVolterra begin\n    dx = a*x - b*x*y\n    dy = -c*y + d*x*y\nend a b c d\n\nor you can define it anonymously:\n\nf = @ode_def begin\n    dx = a*x - b*x*y\n    dy = -c*y + d*x*y\nend a b c d\n\n@ode_def uses ModelingToolkit.jl internally and returns an ODEFunction with the extra definitions (Jacobian, parameter Jacobian, etc.) defined through the MTK symbolic tools.\n\n\n\n\n\n","category":"macro"},{"location":"modules/ParameterizedFunctions/ode_def/#ParameterizedFunctions.@ode_def_bare","page":"The ode_def macro","title":"ParameterizedFunctions.@ode_def_bare","text":"@ode_def_bare name begin\n    differential equation\nend parameters :: ODEFunction\n\nLike @ode_def but the opts options are set so that no symbolic functions are generated. See the @ode_def docstring for more details.\n\n\n\n\n\n","category":"macro"},{"location":"modules/ParameterizedFunctions/ode_def/#ParameterizedFunctions.@ode_def_all","page":"The ode_def macro","title":"ParameterizedFunctions.@ode_def_all","text":"@ode_def_all name begin\n    differential equation\nend parameters :: ODEFunction\n\nLike @ode_def but the opts options are set so that all possible symbolic functions are generated. See the @ode_def docstring for more details.\n\n\n\n\n\n","category":"macro"},{"location":"modules/ParameterizedFunctions/ode_def/#Internal-API","page":"The ode_def macro","title":"Internal API","text":"","category":"section"},{"location":"modules/ParameterizedFunctions/ode_def/","page":"The ode_def macro","title":"The ode_def macro","text":"ParameterizedFunctions.ode_def_opts","category":"page"},{"location":"modules/ParameterizedFunctions/ode_def/#ParameterizedFunctions.ode_def_opts","page":"The ode_def macro","title":"ParameterizedFunctions.ode_def_opts","text":"ode_def_opts(name::Symbol,opts::Dict{Symbol,Bool},curmod,ex::Expr,params...;depvar=:t)\n\nThe core internal. Users should only interact with this through the @ode_def_* macros.\n\nOptions are self-explanatory by name mapping to ODEFunction:\n\nbuild_tgrad\nbuild_jac\nbuild_expjac\nbuild_invjac\nbuild_invW\nbuildinvWt\nbuild_hes\nbuild_invhes\nbuild_dpfuncs\n\ndepvar sets the symbol for the dependent variable.\n\nExample:\n\nopts = Dict{Symbol,Bool}(\n      :build_tgrad => true,\n      :build_jac => true,\n      :build_expjac => false,\n      :build_invjac => true,\n      :build_invW => true,\n      :build_invW_t => true,\n      :build_hes => false,\n      :build_invhes => false,\n      :build_dpfuncs => true)\n\n\n\n\n\n","category":"function"},{"location":"modules/NeuralOperators/introduction/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"modules/NeuralOperators/introduction/","page":"Introduction","title":"Introduction","text":"Neural operator is a novel deep learning architecture. It learns a operator, which is a mapping between infinite-dimensional function spaces. It can be used to resolve partial differential equations (PDE). Instead of solving by time-consuming finite element method, a PDE problem can be resolved by training a neural network to learn an operator mapping from infinite-dimensional space (u t) to infinite-dimensional space f(u t). Neural operator learns a continuous function between two continuous function spaces. The kernel can be trained on different geometry, including regular Euclidean space or a graph topology.","category":"page"},{"location":"modules/NeuralOperators/introduction/#[Fourier-Neural-Operators](https://github.com/SciML/NeuralOperators.jl/blob/main/src/FNO/FNO.jl)","page":"Introduction","title":"Fourier Neural Operators","text":"","category":"section"},{"location":"modules/NeuralOperators/introduction/","page":"Introduction","title":"Introduction","text":"Fourier neural operator (FNO) learns a neural operator with Dirichlet kernel to form a Fourier transformation. It performs Fourier transformation across infinite-dimensional function spaces and learns better than neural operator.","category":"page"},{"location":"modules/NeuralOperators/introduction/#[Markov-Neural-Operators](https://github.com/SciML/NeuralOperators.jl/blob/main/src/FNO/FNO.jl)","page":"Introduction","title":"Markov Neural Operators","text":"","category":"section"},{"location":"modules/NeuralOperators/introduction/","page":"Introduction","title":"Introduction","text":"Markov neural operator (MNO) learns a neural operator with Fourier operators. With only one time step information of learning, it can predict the following few steps with low loss by linking the operators into a Markov chain.","category":"page"},{"location":"modules/NeuralOperators/introduction/#[Deep-Operator-Network](https://github.com/SciML/NeuralOperators.jl/blob/main/src/DeepONet/DeepONet.jl)","page":"Introduction","title":"Deep Operator Network","text":"","category":"section"},{"location":"modules/NeuralOperators/introduction/","page":"Introduction","title":"Introduction","text":"Deep operator network (DeepONet) learns a neural operator with the help of two sub-neural network structures described as the branch and the trunk network. The branch network is fed the initial conditions data, whereas the trunk is fed with the locations where the target(output) is evaluated from the corresponding initial conditions. It is important that the output size of the branch and trunk subnets is same so that a dot product can be performed between them.","category":"page"},{"location":"modules/NeuralOperators/introduction/#[Nonlinear-Manifold-Decoders-for-Operator-Learning](https://github.com/SciML/NeuralOperators.jl/blob/main/src/NOMAD/NOMAD.jl)","page":"Introduction","title":"Nonlinear Manifold Decoders for Operator Learning","text":"","category":"section"},{"location":"modules/NeuralOperators/introduction/","page":"Introduction","title":"Introduction","text":"Nonlinear Manifold Decoders for Operator Learning (NOMAD) learns a neural operator with a nonlinear decoder parameterized by a deep neural network which jointly takes output of approximator and the locations as parameters. The approximator network is fed with the initial conditions data. The output-of-approximator and the locations are then passed to a decoder neural network to get the target (output). It is important that the input size of the decoder subnet is sum of size of the output-of-approximator and number of locations.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Enforcing-Physical-Constraints-via-Universal-Differential-Algebraic-Equations","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"As shown in the stiff ODE tutorial, differential-algebraic equations (DAEs) can be used to impose physical constraints. One way to define a DAE is through an ODE with a singular mass matrix. For example, if we make Mu' = f(u) where the last row of M is all zeros, then we have a constraint defined by the right hand side. Using NeuralODEMM, we can use this to define a neural ODE where the sum of all 3 terms must add to one. An example of this is as follows:","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using Lux, DiffEqFlux, Optimization, OptimizationOptimJL, DifferentialEquations, Plots\n\nusing Random\nrng = Random.default_rng()\n\nfunction f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend\n\nu₀ = [1.0, 0, 0]\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\np = [0.04, 3e7, 1e4]\n\nstiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)\n\nnn_dudt2 = Lux.Chain(Lux.Dense(3, 64, tanh),\n                 Lux.Dense(64, 2))\n\npinit, st = Lux.setup(rng, nn_dudt2)\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(autodiff=false), saveat = 0.1)\nmodel_stiff_ndae(u₀, Lux.ComponentArray(pinit), st)\n\nfunction predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p, st)[1]\nend\n\nfunction loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, Array(sol_stiff) .- pred)\n    return loss, pred\nend\n\n# callback = function (p, l, pred) #callback function to observe training\n#   display(l)\n#   return false\n# end\n\nl1 = first(loss_stiff_ndae(Lux.ComponentArray(pinit)))\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_stiff_ndae(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\nresult_stiff = Optimization.solve(optprob, BFGS(), maxiters=100)","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Step-by-Step-Description","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Step-by-Step Description","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Load-Packages","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Load Packages","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"using Lux, DiffEqFlux, Optimization, OptimizationOptimJL, DifferentialEquations, Plots\n\nusing Random\nrng = Random.default_rng()","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Differential-Equation","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Differential Equation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"First, we define our differential equations as a highly stiff problem which makes the fitting difficult.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function f!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁*y₁ + k₃*y₂*y₃\n    du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\n    du[3] =  y₁ + y₂ + y₃ - 1\n    return nothing\nend","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Parameters","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Parameters","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = [1.0, 0, 0]\n\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\n\ntspan = (0.0,1.0)\n\np = [0.04, 3e7, 1e4]","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"u₀ = Initial Conditions\nM = Semi-explicit Mass Matrix (last row is the constraint equation and are therefore","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"all zeros)","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"tspan = Time span over which to evaluate\np = parameters k1, k2 and k3 of the differential equation above","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#ODE-Function,-Problem-and-Solution","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"ODE Function, Problem and Solution","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We define and solve our ODE problem to generate the \"labeled\" data which will be used to train our Neural Network.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"stiff_func = ODEFunction(f!, mass_matrix = M)\nprob_stiff = ODEProblem(stiff_func, u₀, tspan, p)\nsol_stiff = solve(prob_stiff, Rodas5(), saveat = 0.1)","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a DAE we need to make sure to use a compatible solver. Rodas5 works well for this example.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Neural-Network-Layers","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Neural Network Layers","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Next, we create our layers using Lux.Chain. We use this instead of Flux.Chain because it is more suited to SciML applications (similarly for Lux.Dense). The input to our network will be the initial conditions fed in as u₀.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"nn_dudt2 = Lux.Chain(Lux.Dense(3, 64, tanh),\n                 Lux.Dense(64, 2))\n\npinit, st = Lux.setup(rng, nn_dudt2)\n\nmodel_stiff_ndae = NeuralODEMM(nn_dudt2, (u, p, t) -> [u[1] + u[2] + u[3] - 1],\n                               tspan, M, Rodas5(autodiff=false), saveat = 0.1)\nmodel_stiff_ndae(u₀, Lux.ComponentArray(pinit), st)","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Because this is a stiff problem, we have manually imposed that sum constraint via (u,p,t) -> [u[1] + u[2] + u[3] - 1], making the fitting easier.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Prediction-Function","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Prediction Function","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"For simplicity, we define a wrapper function that only takes in the model's parameters to make predictions.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function predict_stiff_ndae(p)\n    return model_stiff_ndae(u₀, p, st)[1]\nend","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Train-Parameters","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train Parameters","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Training our network requires a loss function, an optimizer and a callback function to display the progress.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Loss","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Loss","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"We first make our predictions based on the current parameters, then calculate the loss from these predictions. In this case, we use least squares as our loss.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"function loss_stiff_ndae(p)\n    pred = predict_stiff_ndae(p)\n    loss = sum(abs2, sol_stiff .- pred)\n    return loss, pred\nend\n\nl1 = first(loss_stiff_ndae(Lux.ComponentArray(pinit)))","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Notice that we are feeding the parameters of model_stiff_ndae to the loss_stiff_ndae function. model_stiff_node.p are the weights of our NN and is of size 386 (4 * 64 + 65 * 2) including the biases.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Optimizer","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Optimizer","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The optimizer is BFGS(see below).","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Callback","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Callback","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"The callback function displays the loss during training.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"callback = function (p, l, pred) #callback function to observe training\n  display(l)\n  return false\nend","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/#Train","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Train","text":"","category":"section"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"Finally, training with Optimization.solve by passing: loss function, model parameters, optimizer, callback and maximum iteration.","category":"page"},{"location":"modules/SciMLSensitivity/dae_fitting/physical_constraints/","page":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","title":"Enforcing Physical Constraints via Universal Differential-Algebraic Equations","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_stiff_ndae(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(pinit))\nresult_stiff = Optimization.solve(optprob, BFGS(), maxiters=100)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/#Optimization-of-Stochastic-Differential-Equations","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Here we demonstrate sensealg = ForwardDiffSensitivity() (provided by SciMLSensitivity.jl) for forward-mode automatic differentiation of a small stochastic differential equation. For large parameter equations, like neural stochastic differential equations, you should use reverse-mode automatic differentiation. However, forward-mode can be more efficient for low numbers of parameters (<100). (Note: the default is reverse-mode AD which is more suitable for things like neural SDEs!)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/#Example-1:-Fitting-Data-with-SDEs-via-Method-of-Moments-and-Parallelism","page":"Optimization of Stochastic Differential Equations","title":"Example 1: Fitting Data with SDEs via Method of Moments and Parallelism","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's do the most common scenario: fitting data. Let's say our ecological system is a stochastic process. Each time we solve this equation we get a different solution, so we need a sensible data source.","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using DifferentialEquations, SciMLSensitivity, Plots\n\nfunction lotka_volterra!(du,u,p,t)\n  x,y = u\n  α,β,γ,δ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = δ*x*y - γ*y\nend\nu0 = [1.0,1.0]\ntspan = (0.0,10.0)\n\nfunction multiplicative_noise!(du,u,p,t)\n  x,y = u\n  du[1] = p[5]*x\n  du[2] = p[6]*y\nend\np = [1.5,1.0,3.0,1.0,0.3,0.3]\n\nprob = SDEProblem(lotka_volterra!,multiplicative_noise!,u0,tspan,p)\nsol = solve(prob)\nplot(sol)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's assume that we are observing the seasonal behavior of this system and have 10,000 years of data, corresponding to 10,000 observations of this timeseries. We can utilize this to get the seasonal means and variances. To simulate that scenario, we will generate 10,000 trajectories from the SDE to build our dataset:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using Statistics\nensembleprob = EnsembleProblem(prob)\n@time sol = solve(ensembleprob,SOSRI(),saveat=0.1,trajectories=10_000)\ntruemean = mean(sol,dims=3)[:,:]\ntruevar  = var(sol,dims=3)[:,:]","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"From here, we wish to utilize the method of moments to fit the SDE's parameters. Thus our loss function will be to solve the SDE a bunch of times and compute moment equations and use these as our loss against the original series. We then plot the evolution of the means and variances to verify the fit. For example:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"function loss(p)\n  tmp_prob = remake(prob,p=p)\n  ensembleprob = EnsembleProblem(tmp_prob)\n  tmp_sol = solve(ensembleprob,SOSRI(),saveat=0.1,trajectories=1000)\n  arrsol = Array(tmp_sol)\n  sum(abs2,truemean - mean(arrsol,dims=3)) + 0.1sum(abs2,truevar - var(arrsol,dims=3)),arrsol\nend\n\nfunction cb2(p,l,arrsol)\n  @show p,l\n  means = mean(arrsol,dims=3)[:,:]\n  vars = var(arrsol,dims=3)[:,:]\n  p1 = plot(sol[1].t,means',lw=5)\n  scatter!(p1,sol[1].t,truemean')\n  p2 = plot(sol[1].t,vars',lw=5)\n  scatter!(p2,sol[1].t,truevar')\n  p = plot(p1,p2,layout = (2,1))\n  display(p)\n  false\nend","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"We can then use Optimization.solve to fit the SDE:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using Optimization, Zygote, OptimizationFlux\npinit = [1.2,0.8,2.5,0.8,0.1,0.1]\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, pinit)\n@time res = Optimization.solve(optprob,ADAM(0.05),callback=cb2,maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Notice that both the parameters of the deterministic drift equations and the stochastic portion (the diffusion equation) are fit through this process! Also notice that the final fit of the moment equations is close:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"The time for the full fitting process was:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"250.654845 seconds (4.69 G allocations: 104.868 GiB, 11.87% gc time)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"approximately 4 minutes.","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/#Example-2:-Fitting-SDEs-via-Bayesian-Quasi-Likelihood-Approaches","page":"Optimization of Stochastic Differential Equations","title":"Example 2: Fitting SDEs via Bayesian Quasi-Likelihood Approaches","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"An inference method which can be much more efficient in many cases is the quasi-likelihood approach. This approach matches the random likelihood of the SDE output with the random sampling of a Bayesian inference problem to more efficiently directly estimate the posterior distribution. For more information, please see the Turing.jl Bayesian Differential Equations tutorial","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/#Example-3:-Controlling-SDEs-to-an-objective","page":"Optimization of Stochastic Differential Equations","title":"Example 3: Controlling SDEs to an objective","text":"","category":"section"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"In this example, we will find the parameters of the SDE that force the solution to be close to the constant 1.","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"using DifferentialEquations, DiffEqFlux, Optimization, OptimizationFlux, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\nfunction lotka_volterra_noise!(du, u, p, t)\n  du[1] = 0.1u[1]\n  du[2] = 0.1u[2]\nend\n\nu0 = [1.0,1.0]\ntspan = (0.0, 10.0)\np = [2.2, 1.0, 2.0, 0.4]\nprob_sde = SDEProblem(lotka_volterra!, lotka_volterra_noise!, u0, tspan)\n\n\nfunction predict_sde(p)\n  return Array(solve(prob_sde, SOSRI(), p=p,\n               sensealg = ForwardDiffSensitivity(), saveat = 0.1))\nend\n\nloss_sde(p) = sum(abs2, x-1 for x in predict_sde(p))","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"For this training process, because the loss function is stochastic, we will use the ADAM optimizer from Flux.jl. The Optimization.solve function is the same as before. However, to speed up the training process, we will use a global counter so that way we only plot the current results every 10 iterations. This looks like:","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"callback = function (p, l)\n  display(l)\n  remade_solution = solve(remake(prob_sde, p = p), SOSRI(), saveat = 0.1)\n  plt = plot(remade_solution, ylim = (0, 6))\n  display(plt)\n  return false\nend","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"Let's optimize","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_sde(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, p)\nresult_sde = Optimization.solve(optprob, ADAM(0.1),\n                                    callback = callback, maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/sde_fitting/optimization_sde/","page":"Optimization of Stochastic Differential Equations","title":"Optimization of Stochastic Differential Equations","text":"(Image: )","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/#Sobol-Method","page":"Sobol Method","title":"Sobol Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"struct Sobol <: GSAMethod\n    order::Vector{Int}\n    nboot::Int\n    conf_level::Float64\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"The Sobol object has as its fields the order of the indices to be estimated. ","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"order - the order of the indices to calculate. Defaults to [0,1], which means the Total and First order indices. Passing 2 enables calculation of the Second order indices as well.","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"For confidence interval calculation nboot should be specified for the number (>0) of bootstrap runs  and conf_level for the confidence level, the default for which is 0.95.","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/#Sobol-Method-Details","page":"Sobol Method","title":"Sobol Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"Sobol is a variance-based method and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter's sensitivities but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":" Y = f_0+ sum_i=1^d f_i(X_i)+ sum_i  j^d f_ij(X_iX_j)  + f_12d(X_1X_2X_d)","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":" Var(Y) = sum_i=1^d V_i + sum_i  j^d V_ij +  + V_12d","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"The Sobol Indices are \"order\"ed, the first order indices given by S_i = fracV_iVar(Y) the contribution to the output variance of the main effect of X_i, therefore it measures the effect of varying X_i alone, but averaged over variations in other input parameters. It is standardised by the total variance to provide a fractional contribution. Higher-order interaction indices S_ij S_ijk and so on can be formed by dividing other terms in the variance decomposition by Var(Y).","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/#API","page":"Sobol Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"function gsa(f, method::Sobol, A::AbstractMatrix{TA}, B::AbstractMatrix;\n             batch=false, Ei_estimator = :Jansen1999, distributed::Val{SHARED_ARRAY} = Val(false), kwargs...) where {TA, SHARED_ARRAY}\n","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"Ei_estimator can take :Homma1996, :Sobol2007 and :Jansen1999 for which   Monte Carlo estimator is used for the Ei term. Defaults to :Jansen1999. Details for these can be found in the    corresponding papers:     - :Homma1996 - Homma, T. and Saltelli, A., 1996. Importance measures in global sensitivity analysis of nonlinear models. Reliability Engineering & System Safety, 52(1), pp.1-17.     - :Sobol2007 - I.M. Sobol, S. Tarantola, D. Gatelli, S.S. Kucherenko and W. Mauntz, 2007, Estimating the approx- imation errors when fixing unessential factors in global sensitivity analysis, Reliability Engineering and System Safety, 92, 957–960.     A. Saltelli, P. Annoni, I. Azzini, F. Campolongo, M. Ratto and S. Tarantola, 2010, Variance based sensitivity analysis of model output. Design and estimator for the total sensitivity index, Computer Physics Communications 181, 259–270.     - :Jansen1999 - M.J.W. Jansen, 1999, Analysis of variance designs for model output, Computer Physics Communi- cation, 117, 35–43.","category":"page"},{"location":"modules/GlobalSensitivity/methods/sobol/#Example","page":"Sobol Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/sobol/","page":"Sobol Method","title":"Sobol Method","text":"using GlobalSensitivity, QuasiMonteCarlo\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nn = 600000\nlb = -ones(4)*π\nub = ones(4)*π\nsampler = SobolSample()\nA,B = QuasiMonteCarlo.generate_design_matrices(n,lb,ub,sampler)\n\nres1 = gsa(ishi,Sobol(order=[0,1,2]),A,B)\n\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\n\nres2 = gsa(ishi_batch,Sobol(),A,B,batch=true)","category":"page"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#Dependency-Graphs","page":"Dependency Graphs","title":"Dependency Graphs","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#Types","page":"Dependency Graphs","title":"Types","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/","page":"Dependency Graphs","title":"Dependency Graphs","text":"BipartiteGraph","category":"page"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.BipartiteGraphs.BipartiteGraph","page":"Dependency Graphs","title":"ModelingToolkit.BipartiteGraphs.BipartiteGraph","text":"mutable struct BipartiteGraph{I<:Integer, M} <: Graphs.AbstractGraph{I<:Integer}\n\nA bipartite graph representation between two, possibly distinct, sets of vertices (source and dependencies). Maps source vertices, labelled 1:N₁, to vertices on which they depend (labelled 1:N₂).\n\nFields\n\nne\nfadjlist\nbadjlist\nmetadata\n\nExample\n\nusing ModelingToolkit\n\nne = 4\nsrcverts = 1:4\ndepverts = 1:2\n\n# six source vertices\nfadjlist = [[1],[1],[2],[2],[1],[1,2]]\n\n# two vertices they depend on\nbadjlist = [[1,2,5,6],[3,4,6]]\n\nbg = BipartiteGraph(7, fadjlist, badjlist)\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#Utility-functions-for-BiPartiteGraphs","page":"Dependency Graphs","title":"Utility functions for BiPartiteGraphs","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/","page":"Dependency Graphs","title":"Dependency Graphs","text":"Base.isequal","category":"page"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#Base.isequal","page":"Dependency Graphs","title":"Base.isequal","text":"Base.isequal(bg1::BipartiteGraph{T}, bg2::BipartiteGraph{T}) where {T<:Integer}\n\nTest whether two BipartiteGraphs are equal.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#Functions-for-calculating-dependency-graphs","page":"Dependency Graphs","title":"Functions for calculating dependency graphs","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/","page":"Dependency Graphs","title":"Dependency Graphs","text":"equation_dependencies\nasgraph\nvariable_dependencies\nasdigraph\neqeq_dependencies\nvarvar_dependencies","category":"page"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.equation_dependencies","page":"Dependency Graphs","title":"ModelingToolkit.equation_dependencies","text":"equation_dependencies(sys::AbstractSystem; variables=states(sys))\n\nGiven an AbstractSystem calculate for each equation the variables it depends on.\n\nNotes:\n\nVariables that are not in variables are filtered out.\nget_variables! is used to determine the variables within a given equation.\nreturns a Vector{Vector{Variable}}() mapping the index of an equation to the variables it depends on.\n\nExample:\n\nusing ModelingToolkit\n@parameters β γ κ η \n@variables t S(t) I(t) R(t)\n\nrate₁   = β*S*I\nrate₂   = γ*I+t\naffect₁ = [S ~ S - 1, I ~ I + 1]\naffect₂ = [I ~ I - 1, R ~ R + 1]\nj₁ = ConstantRateJump(rate₁,affect₁)\nj₂ = VariableRateJump(rate₂,affect₂)\n\n# create a JumpSystem using these jumps\n@named jumpsys = JumpSystem([j₁,j₂], t, [S,I,R], [β,γ])\n\n# dependency of each jump rate function on state variables\nequation_dependencies(jumpsys)\n\n# dependency of each jump rate function on parameters\nequation_dependencies(jumpsys, variables=parameters(jumpsys))\n\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.asgraph","page":"Dependency Graphs","title":"ModelingToolkit.asgraph","text":"asgraph(eqdeps, vtois)\n\nConvert a collection of equation dependencies, for example as returned by equation_dependencies, to a BipartiteGraph.\n\nNotes:\n\nvtois should provide a Dict like mapping from each Variable dependency in eqdeps to the integer idx of the variable to use in the graph.\n\nExample: Continuing the example started in equation_dependencies\n\ndigr = asgraph(equation_dependencies(odesys), Dict(s => i for (i,s) in enumerate(states(odesys))))\n\n\n\n\n\nasgraph(sys::AbstractSystem; variables=states(sys),\n                                      variablestoids=Dict(convert(Variable, v) => i for (i,v) in enumerate(variables)))\n\nConvert an AbstractSystem to a BipartiteGraph mapping the index of equations to the indices of variables they depend on.\n\nNotes:\n\nDefaults for kwargs creating a mapping from equations(sys) to states(sys) they depend on.\nvariables should provide the list of variables to use for generating the dependency graph.\nvariablestoids should provide Dict like mapping from a Variable to its Int index within variables.\n\nExample: Continuing the example started in equation_dependencies\n\ndigr = asgraph(odesys)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.variable_dependencies","page":"Dependency Graphs","title":"ModelingToolkit.variable_dependencies","text":"variable_dependencies(sys::AbstractSystem; variables=states(sys), variablestoids=nothing)\n\nFor each variable determine the equations that modify it and return as a BipartiteGraph.\n\nNotes:\n\nDependencies are returned as a BipartiteGraph mapping variable indices to the indices of equations that modify them.\nvariables denotes the list of variables to determine dependencies for.\nvariablestoids denotes a Dict mapping Variables to their Int index in variables.\n\nExample: Continuing the example of equation_dependencies\n\nvariable_dependencies(odesys)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.asdigraph","page":"Dependency Graphs","title":"ModelingToolkit.asdigraph","text":"asdigraph(g::BipartiteGraph, sys::AbstractSystem; variables = states(sys), equationsfirst = true)\n\nConvert a BipartiteGraph to a LightGraph.SimpleDiGraph.\n\nNotes:\n\nThe resulting SimpleDiGraph unifies the two sets of vertices (equations and then states in the case it comes from asgraph), producing one ordered set of integer vertices (SimpleDiGraph does not support two distinct collections of vertices so they must be merged).\nvariables gives the variables that g is associated with (usually the states of a system).\nequationsfirst (default is true) gives whether the BipartiteGraph gives a mapping from equations to variables they depend on (true), as calculated by asgraph, or whether it gives a mapping from variables to the equations that modify them, as calculated by variable_dependencies.\n\nExample: Continuing the example in asgraph\n\ndg = asdigraph(digr)\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.eqeq_dependencies","page":"Dependency Graphs","title":"ModelingToolkit.eqeq_dependencies","text":"eqeq_dependencies(eqdeps::BipartiteGraph{T}, vardeps::BipartiteGraph{T}) where {T <: Integer}\n\nCalculate a LightGraph.SimpleDiGraph that maps each equation to equations they depend on.\n\nNotes:\n\nThe fadjlist of the SimpleDiGraph maps from an equation to the equations that modify variables it depends on.\nThe badjlist of the SimpleDiGraph maps from an equation to equations that depend on variables it modifies.\n\nExample: Continuing the example of equation_dependencies\n\neqeqdep = eqeq_dependencies(asgraph(odesys), variable_dependencies(odesys))\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/DependencyGraphs/#ModelingToolkit.varvar_dependencies","page":"Dependency Graphs","title":"ModelingToolkit.varvar_dependencies","text":"varvar_dependencies(eqdeps::BipartiteGraph{T}, vardeps::BipartiteGraph{T}) where {T <: Integer} = eqeq_dependencies(vardeps, eqdeps)\n\nCalculate a LightGraph.SimpleDiGraph that maps each variable to variables they depend on.\n\nNotes:\n\nThe fadjlist of the SimpleDiGraph maps from a variable to the variables that depend on it.\nThe badjlist of the SimpleDiGraph maps from a variable to variables on which it depends.\n\nExample: Continuing the example of equation_dependencies\n\nvarvardep = varvar_dependencies(asgraph(odesys), variable_dependencies(odesys))\n\n\n\n\n\n","category":"function"},{"location":"modules/MuladdMacro/#MuladdMacro.jl","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"","category":"section"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"(Image: Build Status) (Image: Coverage Status) (Image: codecov.io)","category":"page"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"This package provides the @muladd macro. It automatically converts expressions with multiplications and additions or subtractions to calls with muladd which then fuse via FMA when it would increase the performance of the code. The @muladd macro can be placed on code blocks and it will automatically find the appropriate expressions and nest muladd expressions when necessary. In mixed expressions summands without multiplication  will be grouped together and evaluated first but otherwise the order of evaluation of multiplications and additions is not changed.","category":"page"},{"location":"modules/MuladdMacro/#Examples","page":"MuladdMacro.jl","title":"Examples","text":"","category":"section"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"julia> @macroexpand(@muladd k3 = f(t + c3*dt, @. uprev+dt*(a031*k1+a032*k2)))\n:(k3 = f((muladd)(c3, dt, t), (muladd).(dt, (muladd).(a032, k2, (*).(a031, k1)), uprev)))\n\njulia> @macroexpand(@muladd integrator.EEst = integrator.opts.internalnorm((update - dt*(bhat1*k1 + bhat4*k4 + bhat5*k5 + bhat6*k6 + bhat7*k7 + bhat10*k10))./ @. (integrator.opts.abstol+max(abs(uprev),abs(u))*integrator.opts.reltol)))\n:(integrator.EEst = (integrator.opts).internalnorm((muladd)(-dt, (muladd)(bhat10, k10, (muladd)(bhat7, k7, (muladd)(bhat6, k6, (muladd)(bhat5, k5, (muladd)(bhat4, k4, bhat1 * k1))))), update) ./ (muladd).(max.(abs.(uprev), abs.(u)), (integrator.opts).reltol, (integrator.opts).abstol)))","category":"page"},{"location":"modules/MuladdMacro/#Broadcasting","page":"MuladdMacro.jl","title":"Broadcasting","text":"","category":"section"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"A muladd call will be broadcasted if both the * and the + or - are broadcasted. If either one is not broadcasted, then the expression will be converted to a non-dotted muladd.","category":"page"},{"location":"modules/MuladdMacro/#Limitations","page":"MuladdMacro.jl","title":"Limitations","text":"","category":"section"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"Currently, @muladd handles only explicit calls of + and *. In particular, assignments using += or literal power such as ^2 are not supported. Thus, you need to rewrite them, e.g.","category":"page"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"julia> using MuladdMacro\n\njulia> a = 1.0; b = 2.0; c = 3.0;\n\njulia> @macroexpand @muladd a += b * c # does not work\n:(a += b * c)\n\njulia> @macroexpand @muladd a = a + b * c # good alternative\n:(a = (muladd)(b, c, a))\n\njulia> @macroexpand @muladd a + b^2 # does not work\n:(a + b ^ 2)\n\njulia> @macroexpand @muladd a + b * b # good alternative\n:((muladd)(b, b, a))","category":"page"},{"location":"modules/MuladdMacro/#Credit","page":"MuladdMacro.jl","title":"Credit","text":"","category":"section"},{"location":"modules/MuladdMacro/","page":"MuladdMacro.jl","title":"MuladdMacro.jl","text":"Most of the credit goes to @fcard and @devmotion for building the first version and greatly refining the macro. These contributions are not directly shown as this was developed in Gitter chats and in the DiffEqBase.jl repository, but these two individuals did almost all of the work.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/#rode_example","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"This tutorial will introduce you to the functionality for solving RODEs. Other introductions can be found by checking out SciMLTutorials.jl.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/#Example-1:-Scalar-RODEs","page":"Random Ordinary Differential Equations","title":"Example 1: Scalar RODEs","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"du = f(uptW)dt","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"where f(uptW)=2usin(W) and W(t) is a Wiener process (Gaussian process).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"using DifferentialEquations\nfunction f(u,p,t,W)\n  2u*sin(W)\nend\nu0 = 1.00\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan)\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: intro_rode)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"The random process defaults to a Gaussian/Wiener process, so there is nothing else required here! See the documentation on NoiseProcesses for details on how to define other noise proceses.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/#Example-2:-Systems-of-RODEs","page":"Random Ordinary Differential Equations","title":"Example 2: Systems of RODEs","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"As with the other problem types, there is an in-place version which is more efficient for systems. The signature is f(du,u,p,t,W). For example,","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"using DifferentialEquations\nfunction f(du,u,p,t,W)\n  du[1] = 2u[1]*sin(W[1] - W[2])\n  du[2] = -2u[2]*cos(W[1] + W[2])\nend\nu0 = [1.00;1.00]\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan)\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: rode_system)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"By default, the size of the noise process matches the size of u0. However, you can use the rand_prototype keyword to explicitly set the size of the random process:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"function f(du,u,p,t,W)\n  du[1] = -2W[3]*u[1]*sin(W[1] - W[2])\n  du[2] = -2u[2]*cos(W[1] + W[2])\nend\nu0 = [1.00;1.00]\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan,rand_prototype=zeros(3))\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: noise_choice)","category":"page"},{"location":"modules/StructuralIdentifiability/export/export/#Exporting-to-Other-Systems","page":"Exporting to Other Systems","title":"Exporting to Other Systems","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/export/export/","page":"Exporting to Other Systems","title":"Exporting to Other Systems","text":"Here we put some helpful utilities to export you code to other identifiability software.","category":"page"},{"location":"modules/StructuralIdentifiability/export/export/","page":"Exporting to Other Systems","title":"Exporting to Other Systems","text":"print_for_maple\nprint_for_DAISY\nprint_for_GenSSI\nprint_for_COMBOS","category":"page"},{"location":"modules/StructuralIdentifiability/export/export/#StructuralIdentifiability.print_for_maple","page":"Exporting to Other Systems","title":"StructuralIdentifiability.print_for_maple","text":"print_for_maple(ode, package)\n\nPrints the ODE in the format accepted by maple packages\n\nSIAN (https://github.com/pogudingleb/SIAN) if package=:SIAN\nDifferentialAlgebra if package=:DifferentialAlgebra\nDifferentialThomas if package=:DifferentialThomas\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/export/export/#StructuralIdentifiability.print_for_DAISY","page":"Exporting to Other Systems","title":"StructuralIdentifiability.print_for_DAISY","text":"print_for_DAISY(ode)\n\nPrints the ODE in the format accepted by DAISY (https://daisy.dei.unipd.it/)\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/export/export/#StructuralIdentifiability.print_for_GenSSI","page":"Exporting to Other Systems","title":"StructuralIdentifiability.print_for_GenSSI","text":"print_for_GenSSI(ode)\n\nPrints the ODE in the format accepted by GenSSI 2.0 (https://github.com/genssi-developer/GenSSI)\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/export/export/#StructuralIdentifiability.print_for_COMBOS","page":"Exporting to Other Systems","title":"StructuralIdentifiability.print_for_COMBOS","text":"print_for_COMBOS(ode)\n\nPrints the ODE in the format accepted by COMBOS (http://biocyb1.cs.ucla.edu/combos/)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/models/chemical_reactions/#Chemical-Reactions","page":"Chemical Reactions","title":"Chemical Reactions","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/chemical_reactions/","page":"Chemical Reactions","title":"Chemical Reactions","text":"Catalyst.jl is a domain specific  language (DSL) for the easy generation of models of chemical reaction systems,  which can be used with SciML tooling to enable high performance simulation and  analysis of chemical reaction networks. Catalyst generates ReactionSystems,  leveraging ModelingToolkit to  enable large-scale simulations through auto-vectorization and parallelism.  ReactionSystems can be used to generate ModelingToolkit-based models,  allowing the easy simulation and parameter estimation of mass action ODE  models, Chemical Langevin SDE models, stochastic chemical kinetics jump process  models, and more. These generated models can be used with  DifferentialEquations.jl solvers, but also with higher level SciML packages  (e.g. for sensitivity analysis, parameter estimation, machine learning  applications, etc). See the Catalyst.jl  documentation for more information.","category":"page"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/#BVP-Solvers","page":"BVP Solvers","title":"BVP Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/#Recomended-Methods","page":"BVP Solvers","title":"Recomended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"GeneralMIRK4 is a good well-rounded method when the problem is not too large. It uses highly stable trust region methods to solve a 4th order fully implicit Runge-Kutta scheme. As an alternative on general BVProblems, the Shooting method paired with an appropriate integrator for the IVP, such as Shooting(Tsit5()), is a flexible and efficient option. This will allow one to combine callbacks/event handling with the BVP solver, and the high order interpolations can be used to define complex boundary conditions. However, Shooting methods can in some cases be prone to sensitivity of the boundary condition.","category":"page"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"When the problem is a large two-point boundary value problem that is sensitive to the boundary conditions, MIRK4 utilizes a sparse Jacobian to greatly improve the efficiency.","category":"page"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/#Full-List-of-Methods","page":"BVP Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/#BoundaryValueDiffEq.jl","page":"BVP Solvers","title":"BoundaryValueDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"Shooting - A wrapper over initial value problem solvers.\nGeneralMIRK4 - A 4th order collocation method using an implicit Runge-Kutta tableau solved using a trust region dogleg method from NLsolve.jl.\nMIRK4 - A 4th order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian. Compatible only with two-point boundary value problems.","category":"page"},{"location":"modules/DiffEqFlux/layers/TensorLayer/#Tensor-Product-Layer","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"The following layer is a helper function for easily constructing a TensorLayer, which takes as input an array of n tensor product basis, B_1 B_2  B_n, a data point x, computes zi = Wi  B_1(x1)  B_2(x2)    B_n(xn), where W is the layer's weight, and returns [z[1], ..., z[out]].","category":"page"},{"location":"modules/DiffEqFlux/layers/TensorLayer/","page":"Tensor Product Layer","title":"Tensor Product Layer","text":"TensorLayer","category":"page"},{"location":"modules/DiffEqFlux/layers/TensorLayer/#DiffEqFlux.TensorLayer","page":"Tensor Product Layer","title":"DiffEqFlux.TensorLayer","text":"Constructs the Tensor Product Layer, which takes as input an array of n tensor product basis, [B1, B2, ..., Bn] a data point x, computes z[i] = W[i,:] ⨀ [B1(x[1]) ⨂ B2(x[2]) ⨂ ... ⨂ Bn(x[n])], where W is the layer's weight, and returns [z[1], ..., z[out]].\n\nTensorLayer(model,out,p=nothing)\n\nArguments:\n\nmodel: Array of TensorProductBasis [B1(n1), ..., Bk(nk)], where k corresponds to the dimension of the input.\nout: Dimension of the output.\np: Optional initialization of the layer's weight. Initialized to standard normal by default.\n\n\n\n\n\n","category":"type"},{"location":"modules/Surrogates/abstractgps/#Gaussian-Process-Surrogate-Tutorial","page":"Gaussian Process","title":"Gaussian Process Surrogate Tutorial","text":"","category":"section"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"note: Note\nThis surrogate requires the 'SurrogatesAbstractGPs' module which can be added by inputting \"]add SurrogatesAbstractGPs\" from the Julia command line. ","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Gaussian Process regression in Surrogates.jl is implemented as a simple wrapper around the AbstractGPs.jl package. AbstractGPs comes with a variety of covariance functions (kernels). See KernelFunctions.jl for examples.","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"tip: Tip\nThe examples below demonstrate the use of AbstractGPs with out-of-the-box settings without hyperparameter optimization (i.e. without changing parameters like lengthscale, signal variance and noise variance.) Beyond hyperparameter optimization, careful initialization of hyperparameters and priors on the parameters is required for this surrogate to work properly. For more details on how to fit GPs in practice, check out A Practical Guide to Gaussian Processes.Also see this example to understand hyperparameter optimization with AbstractGPs.","category":"page"},{"location":"modules/Surrogates/abstractgps/#D-Example","page":"Gaussian Process","title":"1D Example","text":"","category":"section"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"In the example below, the 'gp_surrogate' assignment code can be commented / uncommented to see how the different kernels influence the predictions. ","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Surrogates\nusing Plots\ndefault()\nusing AbstractGPs #required to access different types of kernels\nusing SurrogatesAbstractGPs\n\nf(x) = (6 * x - 2)^2 * sin(12 * x - 4)\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 1.0\nxs = lower_bound:0.001:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\n#gp_surrogate = AbstractGPSurrogate(x,y, gp=GP(SqExponentialKernel()), Σy=0.05) #example of Squared Exponential Kernel\n#gp_surrogate = AbstractGPSurrogate(x,y, gp=GP(MaternKernel()), Σy=0.05) #example of MaternKernel\ngp_surrogate = AbstractGPSurrogate(x,y, gp=GP(PolynomialKernel(; c=2.0, degree=5)), Σy=0.25)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-7, 17), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)\nplot!(0:0.001:1, gp_surrogate.gp_posterior; label=\"Posterior\", ribbon_scale=2)","category":"page"},{"location":"modules/Surrogates/abstractgps/#Optimization-Example","page":"Gaussian Process","title":"Optimization Example","text":"","category":"section"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"This example shows the use of AbstractGP Surrogates to find the minima of a function:","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Surrogates\nusing Plots\nusing AbstractGPs\nusing SurrogatesAbstractGPs\n\nf(x) = (x-2)^2\nn_samples = 4\nlower_bound = 0.0\nupper_bound = 4.0\nxs = lower_bound:0.1:upper_bound\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\ngp_surrogate = AbstractGPSurrogate(x,y)\n@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, gp_surrogate, SobolSample())","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Plotting the function and the sampled points: ","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"scatter(gp_surrogate.x, gp_surrogate.y, label=\"Sampled points\", ylims=(-1.0, 5.0), legend=:top)\nplot!(xs, gp_surrogate.(xs), label=\"Surrogate function\", ribbon=p->std_error_at_point(gp_surrogate, p), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/abstractgps/#ND-Example","page":"Gaussian Process","title":"ND Example","text":"","category":"section"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"using Plots\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\")\nusing Surrogates\nusing AbstractGPs\nusing SurrogatesAbstractGPs\n\n\nhypot_func = z -> 3*hypot(z...)+1\nn_samples = 50\nlower_bound = [-1.0, -1.0]\nupper_bound = [1.0, 1.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = hypot_func.(xys);\n\nx, y = -2:2, -2:2 \np1 = surface(x, y, (x1,x2) -> hypot_func((x1,x2))) \nxs = [xy[1] for xy in xys] \nys = [xy[2] for xy in xys] \nscatter!(xs, ys, zs) \np2 = contour(x, y, (x1,x2) -> hypot_func((x1,x2)))\nscatter!(xs, ys)\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"Now let's see how our surrogate performs:","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"gp_surrogate = AbstractGPSurrogate(xys, zs)\np1 = surface(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, zs, marker_z=zs)\np2 = contour(x, y, (x, y) -> gp_surrogate([x y]))\nscatter!(xs, ys, marker_z=zs)\nplot(p1, p2, title=\"Surrogate\")","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show gp_surrogate((0.2,0.2))","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show hypot_func((0.2,0.2))","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"And this is our log marginal posterior predictive probability:","category":"page"},{"location":"modules/Surrogates/abstractgps/","page":"Gaussian Process","title":"Gaussian Process","text":"@show logpdf_surrogate(gp_surrogate)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/nonlinear/#Modeling-Nonlinear-Systems","page":"Modeling Nonlinear Systems","title":"Modeling Nonlinear Systems","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/nonlinear/","page":"Modeling Nonlinear Systems","title":"Modeling Nonlinear Systems","text":"In this example we will go one step deeper and showcase the direct function generation capabilities in ModelingToolkit.jl to build nonlinear systems. Let's say we wanted to solve for the steady state of the previous ODE. This is the nonlinear system defined by where the derivatives are zero. We use (unknown) variables for our nonlinear system.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/nonlinear/","page":"Modeling Nonlinear Systems","title":"Modeling Nonlinear Systems","text":"using ModelingToolkit, NonlinearSolve\r\n\r\n@variables x y z\r\n@parameters σ ρ β\r\n\r\n# Define a nonlinear system\r\neqs = [0 ~ σ*(y-x),\r\n       0 ~ x*(ρ-z)-y,\r\n       0 ~ x*y - β*z]\r\n@named ns = NonlinearSystem(eqs, [x,y,z], [σ,ρ,β])\r\n\r\nguess = [x => 1.0,\r\n         y => 0.0,\r\n         z => 0.0]\r\n\r\nps = [\r\n      σ => 10.0\r\n      ρ => 26.0\r\n      β => 8/3\r\n      ]\r\n\r\nprob = NonlinearProblem(ns,guess,ps)\r\nsol = solve(prob,NewtonRaphson())","category":"page"},{"location":"modules/ModelingToolkit/tutorials/nonlinear/","page":"Modeling Nonlinear Systems","title":"Modeling Nonlinear Systems","text":"We can similarly ask to generate the NonlinearProblem with the analytical Jacobian function:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/nonlinear/","page":"Modeling Nonlinear Systems","title":"Modeling Nonlinear Systems","text":"prob = NonlinearProblem(ns,guess,ps,jac=true)\r\nsol = solve(prob,NewtonRaphson())","category":"page"},{"location":"modules/DiffEqDocs/models/multiscale/#Multi-Scale-Models","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/multiscale/","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"The multi-scale modeling functionality is provided by MultiScaleArrays.jl. It allows for designing a multi-scale model as an extension of an array, which in turn can be directly used in the native Julia solvers of DifferentialEquations.jl.","category":"page"},{"location":"modules/DiffEqDocs/models/multiscale/#More-Information","page":"Multi-Scale Models","title":"More Information","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/multiscale/","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"For more information, please see the MultiScaleArrays.jl README.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#Convergence-Simulations","page":"Convergence Simulations","title":"Convergence Simulations","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"The convergence simulation type is useful for deriving order of convergence estimates from a group of simulations. This object will automatically assemble error vectors into a more useful manner and provide plotting functionality. Convergence estimates are also given by pair-wise estimates.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"One can automatically have DifferentialEquations.jl perform the error analysis by passing a ConvergenceSimulation a vector of solutions, or using one of the provided test_convergence functions. These will give order of convergence estimates and provide plotting functionality. This requires that the true solution was provided in the problem definition.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"ConvergenceSimulations can either be created by passing the constructor the appropriate solution array or by using one of the provided test_convergence functions.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#The-ConvergenceSimulation-Type","page":"Convergence Simulations","title":"The ConvergenceSimulation Type","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"A type which holds the data from a convergence simulation.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#Fields","page":"Convergence Simulations","title":"Fields","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"solutions::Array{<:DESolution}: Holds all the PdeSolutions.\nerrors: Dictionary of the error calculations. Can contain:\nh1Errors: Vector of the H1 errors.\nl2Errors: Vector of the L2 errors.\nmaxErrors: Vector of the nodal maximum errors.\nnode2Errors: Vector of the nodal l2 errors.\nN: The number of simulations.\nauxdata: Auxillary data of the convergence simluation. Entries can include:\ndts: The dt's in the simulations.\ndxs: The dx's in the simulations.\nμs: The CFL μ's in the simulations.\nνs: The CFL ν's in the simulations.\n𝒪est: Dictionary of order estimates. Can contain:\nConvEst_h1: The H1 error order of convergence estimate for the convergence simulation.  Generated via log2(error[i+1]/error[i]). Thus only valid if generated by halving/doubling  the dt/dx. If alternate scaling, modify by dividing of log(base,ConvEst_h1)\nConvEst_l2: The L2 error order of convergence estimate for the convergence simulation.  Generated via log2(error[i+1]/error[i]). Thus only valid if generated by halving/doubling  the dt/dx. If alternate scaling, modify by dividing of log(base,ConvEst_l2)\nConvEst_max: The nodal maximum error order of convergence estimate for the convergence simulation.  Generated via log2(error[i+1]/error[i]). Thus only valid if generated by halving/doubling  the dt/dx. If alternate scaling, modify by dividing of log(base,ConvEst_max)\nConvEst_node2: The nodal l2 error order of convergence estimate for the convergence simulation.  Generated via log2(error[i+1]/error[i]). Thus only valid if generated by halving/doubling  the dt/dx. If alternate scaling, modify by dividing of log(base,ConvEst_node2)\nconvergence_axis: The axis along which convergence is calculated. For example, if  we calculate the dt convergence, convergence_axis is the dts used in the calculation.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#Plot-Functions","page":"Convergence Simulations","title":"Plot Functions","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"The plot functionality is provided by a Plots.jl recipe. What is plotted is a line series for each calculated error along the convergence axis. To plot a convergence simulation, simply use:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"plot(sim::ConvergenceSimulation)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"All of the functionality (keyword arguments) provided by Plots.jl are able to be used in this command. Please see the Plots.jl documentation for more information.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#ODE","page":"Convergence Simulations","title":"ODE","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"test_convergence(dts::AbstractArray,prob::AbstractODEProblem)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"Tests the order of the time convergence of the given algorithm on the given problem solved over the given dts. Keyword arguments are passed to the ODE solver.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#SDE","page":"Convergence Simulations","title":"SDE","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"test_convergence(dts::AbstractArray,prob::AbstractSDEProblem)","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"Tests the strong order time convergence of the given algorithm on the given problem solved over the given dts. Keyword arguments are passed to the ODE solver. Except:","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"numMonte: The number of simulations for each dt. Default is 10000.","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/#Order-Estimation","page":"Convergence Simulations","title":"Order Estimation","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"calc𝒪estimates(error::Vector{Number})`","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"Computes the pairwise convergence estimate for a convergence test done by halving/doubling stepsizes via","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"log2(error[i+1]/error[i])","category":"page"},{"location":"modules/DiffEqDevDocs/alg_dev/convergence/","page":"Convergence Simulations","title":"Convergence Simulations","text":"Returns the mean of the convergence estimates.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/#Parameter-Estimation-on-Highly-Stiff-Systems","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"This tutorial goes into training a model on stiff chemical reaction system data.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/#Copy-Pasteable-Code","page":"Parameter Estimation on Highly Stiff Systems","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"using DifferentialEquations, DiffEqFlux, Optimization, OptimizationOptimJL, LinearAlgebra\nusing ForwardDiff\nusing DiffEqBase: UJacobianWrapper\nusing Plots\nfunction rober(du,u,p,t)\n    y₁,y₂,y₃ = u\n    k₁,k₂,k₃ = p\n    du[1] = -k₁*y₁+k₃*y₂*y₃\n    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n    du[3] =  k₂*y₂^2\n    nothing\nend\n\np = [0.04,3e7,1e4]\nu0 = [1.0,0.0,0.0]\nprob = ODEProblem(rober,u0,(0.0,1e5),p)\nsol = solve(prob,Rosenbrock23())\nts = sol.t\nJs = map(u->I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)\n\nfunction predict_adjoint(p)\n    p = exp.(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(p)\n    prediction = predict_adjoint(p)\n    prediction = [prediction[:, i] for i in axes(prediction, 2)]\n    diff = map((J,u,data) -> J * (abs2.(u .- data)) , Js, prediction, sol.u)\n    loss = sum(abs, sum(diff)) |> sqrt\n    loss, prediction\nend\n\ncallback = function (p,l,pred) #callback function to observe training\n    println(\"Loss: $l\")\n    println(\"Parameters: $(exp.(p))\")\n    # using `remake` to re-create our `prob` with current parameters `p`\n    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |> display\n    return false # Tell it to not halt the optimization. If return true, then optimization stops\nend\n\ninitp = ones(3)\n# Display the ODE with the initial parameter values.\ncallback(initp,loss_adjoint(initp)...)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_adjoint(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, initp)\n\nres = Optimization.solve(optprob, ADAM(0.01), callback = callback, maxiters = 300)\n\noptprob2 = Optimization.OptimizationProblem(optf, res.u)\n\nres2 = Optimization.solve(optprob2, BFGS(), callback = callback, maxiters = 30, allow_f_increases=true)\nprintln(\"Ground truth: $(p)\\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Output:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Ground truth: [0.04, 3.0e7, 10000.0]\nFinal parameters: [0.040002, 3.0507e7, 10084.0]\nError: 1.69%","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/#Explanation","page":"Parameter Estimation on Highly Stiff Systems","title":"Explanation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"First, let's get a time series array from the Robertson's equation as data.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"using DifferentialEquations, DiffEqFlux, Optimization, OptimizationOptimJL, LinearAlgebra\nusing ForwardDiff\nusing DiffEqBase: UJacobianWrapper\nusing Plots\nfunction rober(du,u,p,t)\n    y₁,y₂,y₃ = u\n    k₁,k₂,k₃ = p\n    du[1] = -k₁*y₁+k₃*y₂*y₃\n    du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n    du[3] =  k₂*y₂^2\n    nothing\nend\n\np = [0.04,3e7,1e4]\nu0 = [1.0,0.0,0.0]\nprob = ODEProblem(rober,u0,(0.0,1e5),p)\nsol = solve(prob,Rosenbrock23())\nts = sol.t\nJs = map(u->I + 0.1*ForwardDiff.jacobian(UJacobianWrapper(rober, 0.0, p), u), sol.u)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Note that we also computed a shifted and scaled Jacobian along with the solution. We will use this matrix to scale the loss later.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We fit the parameters in log space, so we need to compute exp.(p) to get back the original parameters.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"function predict_adjoint(p)\n    p = exp.(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob,Rosenbrock23(autodiff=false),saveat=ts,sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(p)\n    prediction = predict_adjoint(p)\n    prediction = [prediction[:, i] for i in axes(prediction, 2)]\n    diff = map((J,u,data) -> J * (abs2.(u .- data)) , Js, prediction, sol.u)\n    loss = sum(abs, sum(diff)) |> sqrt\n    loss, prediction\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"The difference between the data and the prediction is weighted by the transformed Jacobian to do a relative scaling of the loss.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We define a callback function.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"callback = function (p,l,pred) #callback function to observe training\n    println(\"Loss: $l\")\n    println(\"Parameters: $(exp.(p))\")\n    # using `remake` to re-create our `prob` with current parameters `p`\n    plot(solve(remake(prob, p=exp.(p)), Rosenbrock23())) |> display\n    return false # Tell it to not halt the optimization. If return true, then optimization stops\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"We then use a combination of ADAM and BFGS to minimize the loss function to accelerate the optimization. The initial guess of the parameters are chosen to be [1, 1, 1.0].","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"initp = ones(3)\n# Display the ODE with the initial parameter values.\ncallback(initp,loss_adjoint(initp)...)\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_adjoint(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, initp)\nres = Optimization.solve(optprob, ADAM(0.01), callback = callback, maxiters = 300)\n\noptprob2 = Optimization.OptimizationProblem(optf, res.u)\nres2 = Optimization.solve(optprob2, BFGS(), callback = callback, maxiters = 30, allow_f_increases=true)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Finally, we can analyze the difference between the fitted parameters and the ground truth.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"println(\"Ground truth: $(p)\\nFinal parameters: $(round.(exp.(res2.u), sigdigits=5))\\nError: $(round(norm(exp.(res2.u) - p) ./ norm(p) .* 100, sigdigits=3))%\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"It gives the output","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/stiff_ode_fit/","page":"Parameter Estimation on Highly Stiff Systems","title":"Parameter Estimation on Highly Stiff Systems","text":"Ground truth: [0.04, 3.0e7, 10000.0]\nFinal parameters: [0.040002, 3.0507e7, 10084.0]\nError: 1.69%","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/#Morris-Method","page":"Morris Method","title":"Morris Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"struct Morris <: GSAMethod\n    p_steps::Array{Int,1}\n    relative_scale::Bool\n    num_trajectory::Int\n    total_num_trajectory::Int\n    len_design_mat::Int\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"Morris has the following keyword arguments:","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"p_steps - Vector of Delta for the step sizes in each direction. Required.\nrelative_scale - The elementary effects are calculated with the assumption that the parameters lie in the range [0,1] but as this is not always the case scaling is used to get more informative, scaled effects. Defaults to false.\ntotal_num_trajectory, num_trajectory - The total number of design matrices that are generated out of which num_trajectory matrices with the highest spread are used in calculation.\nlen_design_mat - The size of a design matrix.","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/#Morris-Method-Details","page":"Morris Method","title":"Morris Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"The Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"We calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"EE_i = fracf(x_1x_2x_i+ Deltax_k) - yDelta","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"These are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/#API","page":"Morris Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"function gsa(f, method::Morris, p_range::AbstractVector; batch=false, kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/#Example","page":"Morris Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"Morris method on Ishigami function ","category":"page"},{"location":"modules/GlobalSensitivity/methods/morris/","page":"Morris Method","title":"Morris Method","text":"using GlobalSensitivity\n\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nm = gsa(ishi, Morris(num_trajectory=500000), [[lb[i],ub[i]] for i in 1:4])","category":"page"},{"location":"modules/SciMLBase/fundamentals/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/SciMLBase/fundamentals/FAQ/#What-are-the-code-styling-rules-for-SciML?","page":"Frequently Asked Questions","title":"What are the code styling rules for SciML?","text":"","category":"section"},{"location":"modules/SciMLBase/fundamentals/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"All SciML libraries are supposed to follow SciMLStyle. Any deviation from that style is something to be fixed.","category":"page"},{"location":"modules/SciMLBase/fundamentals/FAQ/#Where-do-I-find-more-information-on-the-internals-of-some-packages?","page":"Frequently Asked Questions","title":"Where do I find more information on the internals of some packages?","text":"","category":"section"},{"location":"modules/SciMLBase/fundamentals/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The SciML Developer Documentation describes the internals of some of the larger solver libraries at length.","category":"page"},{"location":"modules/SciMLBase/fundamentals/FAQ/#What-are-the-community-practices-that-SciML-developers-should-use?","page":"Frequently Asked Questions","title":"What are the community practices that SciML developers should use?","text":"","category":"section"},{"location":"modules/SciMLBase/fundamentals/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"See ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","category":"page"},{"location":"modules/SciMLBase/fundamentals/FAQ/#Are-there-developer-programs-to-help-fund-parties-interested-in-helping-develop-SciML?","page":"Frequently Asked Questions","title":"Are there developer programs to help fund parties interested in helping develop SciML?","text":"","category":"section"},{"location":"modules/SciMLBase/fundamentals/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes! See the SciML Developer Programs webpage.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/#Jump-Diffusion-Equations","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Jump Diffusion equations are stochastic differential equations with discontinuous jumps. These can be written as:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"du = f(upt)dt + sum_jg_j(upt)dW_j(t) + sum_ih_i(upt)dN_i(t)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"where N_i is a Poisson-counter which denotes jumps of size h_i. In this tutorial we will show how to solve problems with even more general jumps.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/#Defining-a-ConstantRateJump-Problem","page":"Jump Diffusion Equations","title":"Defining a ConstantRateJump Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"To start, let's solve an ODE with constant rate jumps. A jump is defined as being \"constant rate\" if the rate is only dependent on values from other constant rate jumps, meaning that its rate must not be coupled with time or the solution to the differential equation. However, these types of jumps are cheaper to compute.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Note: if your rate is only \"slightly\" dependent on the solution of the differential equation, then it may be okay to use a ConstantRateJump. Accuracy loss will be related to the percentage that the rate changes over the jump intervals.)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Let's solve the following problem. We will have a linear ODE with a Poisson counter of rate 2 (which is the mean and variance), where at each jump the current solution will be halved. To solve this problem, we first define the ODEProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function f(du,u,p,t)\n  du[1] = u[1]\nend\n\nprob = ODEProblem(f,[0.2],(0.0,10.0))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Notice that, even though our equation is on 1 number, we define it using the in-place array form. Variable rate jump equations will require this form. Note that for this tutorial we solve a one-dimensional problem, but the same syntax applies for solving a system of differential equations with multiple jumps.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we define our rate equation for our jump. Since it's just the constant value 2, we do:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = 2","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we define the affect! of the jump. This is the same as an affect! from a DiscreteCallback, and thus acts directly on the integrator. Therefore, to make it halve the current value of u, we do:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"affect!(integrator) = (integrator.u[1] = integrator.u[1]/2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Then we build our jump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump = ConstantRateJump(rate,affect!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Next, we extend our ODEProblem to a JumpProblem by attaching the jump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"We can now solve this extended problem using any ODE solver:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,Tsit5())\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: constant_rate_jump)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/#Variable-Rate-Jumps","page":"Jump Diffusion Equations","title":"Variable Rate Jumps","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now let's define a jump which is coupled to the differential equation. Let's let the rate be the current value of the solution, that is:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = u[1]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Using the same affect!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"affect!(integrator) = (integrator.u[1] = integrator.u[1]/2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"we build a VariableRateJump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump = VariableRateJump(rate,affect!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"To make things interesting, let's copy this jump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump2 = deepcopy(jump)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"so that way we have two independent jump processes. We now couple these jumps to the ODEProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"which we once again solve using an ODE solver:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,Tsit5())\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: variable_rate_jump)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/#Jump-Diffusion","page":"Jump Diffusion Equations","title":"Jump Diffusion","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we will finally solve the jump diffusion problem. The steps are the same as before, except we now start with a SDEProblem instead of an ODEProblem. Using the same drift function f as before, we add multiplicative noise via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function g(du,u,p,t)\n  du[1] = u[1]\nend\n\nprob = SDEProblem(f,g,[0.2],(0.0,10.0))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"and couple it to the jumps:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"We then solve it using an SDE algorithm:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,SRIW1())\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: jump_diffusion)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/#Coupling-Jump-Problems","page":"Jump Diffusion Equations","title":"Coupling Jump Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"In many applications one is interested in coupling two stochastic processes. This has applications in Monte Carlo simulations and sensitivity analysis, for example. Currently, the coupling that is implemented for jump processes is known as the split coupling. The split coupling couples two jump processes by coupling the underlying Poisson processes driving the jump components.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Suppose prob and prob_control are two problems we wish to couple. Then the coupled problem is obtained by","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"prob_coupled =  SplitCoupledJumpProblem(jump_prob,jump_prob_control,Direct(),coupling_map)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Here, coupling_map specifies which jumps to couple. If (j,i) is in coupling_map, then the ith jump in prob will be coupled to the jth jump in prob_control. Note that currently SplitCoupledJumpProblem is only implemented for constant rate jump problems.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"As an example, consider a doubly stochastic Poisson process, that is, a Poisson process whose rate is itself a stochastic process. In particular, we will take the rate to randomly switch between zero and 10 at unit rates:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = u[2]*10\naffect!(integrator) = integrator.u[1] += 1.\njump1 = ConstantRateJump(rate,affect!)\nrate(u,p,t) = u[2]\naffect!(integrator) = (integrator.u[2] -= 1.;integrator.u[3] += 1.)\njump2 = ConstantRateJump(rate,affect!)\n\nrate(u,p,t) = u[3]\naffect!(integrator) = (integrator.u[2] += 1.;integrator.u[3] -= 1.)\njump3 = ConstantRateJump(rate,affect!)\nprob = DiscreteProblem(u0,tspan)\njump_prob = JumpProblem(prob,Direct(),jump1,jump2,jump3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"The doubly stochastic poisson process has two sources of randomness: one due to the Poisson process, and another due to random evolution of the rate. This is typical of many multiscale stochastic processes appearing in applications, and it is often useful to compare such a process to one obtained by removing one source of randomness. In present context, this means looking at an ODE with constant jump rates, where the deterministic evolution between jumps is given by the expected value of the Poisson process:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function f(du,u,p,t)\n  du[1] = u[2]*10\n  du[2] = 0.\n  du[3] = 0.\nend\nprob_control = ODEProblem(f,u0,tspan)\njump_prob_control = JumpProblem(prob_control,Direct(),jump2,jump3)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Let's couple the two problems by coupling the jumps corresponding the switching of the rate:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"coupling_map = [(2,1),(3,2)]\nprob_coupled =  SplitCoupledJumpProblem(jump_prob,jump_prob_control,Direct(),coupling_map)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now prob_coupled will be dealt with like any other JumpProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(prob_coupled,Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: jump_diffusion)","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#The-AbstractSystem-Interface","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Overview","page":"The AbstractSystem Interface","title":"Overview","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"The AbstractSystem interface is the core of the system level of ModelingToolkit.jl. It establishes a common set of functionality that is used between systems representing ODEs, PDEs, SDEs and more, allowing users to have a common framework for model manipulation and compilation.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Subtypes","page":"The AbstractSystem Interface","title":"Subtypes","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"There are three immediate subtypes of AbstractSystem, classified by how many independent variables each type has:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"AbstractTimeIndependentSystem: has no independent variable (eg: NonlinearSystem)\nAbstractTimeDependentSystem: has a single independent variable (eg: ODESystem)\nAbstractMultivariateSystem: may have multiple independent variables (eg: PDESystem)","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Constructors-and-Naming","page":"The AbstractSystem Interface","title":"Constructors and Naming","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"The AbstractSystem interface has a consistent method for constructing systems. Generally it follows the order of:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Equations\nIndependent Variables\nDependent Variables (or States)\nParameters","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"All other pieces are handled via keyword arguments. AbstractSystems share the same keyword arguments, which are:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"system: This is used for specifying subsystems for hierarchical modeling with reusable components. For more information, see the components page\nDefaults: Keyword arguments like defaults are used for specifying default values which are used. If a value is not given at the SciMLProblem construction time, its numerical value will be the default.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Composition-and-Accessor-Functions","page":"The AbstractSystem Interface","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Each AbstractSystem has lists of variables in context, such as distinguishing parameters vs states. In addition, an AbstractSystem also can hold other AbstractSystem types. Direct accessing of the values, such as sys.states, gives the immediate list, while the accessor functions states(sys) gives the total set, which includes that of all systems held inside.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"The values which are common to all AbstractSystems are:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"equations(sys): All equations that define the system and its subsystems.\nstates(sys): All the states in the system and its subsystems.\nparameters(sys): All parameters of the system and its subsystems.\nnameof(sys): The name of the current-level system.\nget_eqs(sys): Equations that define the current-level system.\nget_states(sys): States that are in the current-level system.\nget_ps(sys): Parameters that are in the current-level system.\nget_systems(sys): Subsystems of the current-level system.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Optionally, a system could have:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"observed(sys): All observed equations of the system and its subsystems.\nget_observed(sys): Observed equations of the current-level system.\nget_continuous_events(sys): SymbolicContinuousCallbacks of the current-level system.\nget_defaults(sys): A Dict that maps variables into their default values.\nindependent_variables(sys): The independent variables of a system.\nget_noiseeqs(sys): Noise equations of the current-level system.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Note that if you know a system is an AbstractTimeDependentSystem you could use get_iv to get the  unique independent variable directly, rather than using independent_variables(sys)[1], which is clunky and may cause problems if sys is an AbstractMultivariateSystem because there may be more than one independent variable. AbstractTimeIndependentSystems do not have a method get_iv, and independent_variables(sys) will return a size-zero result for such. For an AbstractMultivariateSystem, get_ivs is equivalent.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"A system could also have caches:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"get_jac(sys): The Jacobian of a system.\nget_tgrad(sys): The gradient with respect to time of a system.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Transformations","page":"The AbstractSystem Interface","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Transformations are functions which send a valid AbstractSystem definition to another AbstractSystem. These are passes, like optimizations (e.g., Block-Lower Triangle transformations), or changes to the representation, which allow for alternative numerical methods to be utilized on the model (e.g., DAE index reduction).","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Analyses","page":"The AbstractSystem Interface","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Analyses are functions on a system which return information about the corresponding properties, like whether its parameters are structurally identifiable, or whether it's linear.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Function-Calculation-and-Generation","page":"The AbstractSystem Interface","title":"Function Calculation and Generation","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"The calculation and generation functions allow for calculating additional quantities to enhance the numerical methods applied to the resulting system. The calculations, like calculate_jacobian, generate ModelingToolkit IR for the Jacobian of the system, while the generations, like generate_jacobian, generate compiled output for the numerical solvers by applying build_function to the generated code. Additionally, many systems have function-type outputs, which cobble together the generation functionality for a system, for example, ODEFunction can be used to generate a DifferentialEquations-based ODEFunction with compiled version of the ODE itself, the Jacobian, the mass matrix, etc.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Below are the possible calculation and generation functions:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"calculate_tgrad\ncalculate_gradient\ncalculate_jacobian\ncalculate_factorized_W\ncalculate_hessian\ngenerate_tgrad\ngenerate_gradient\ngenerate_jacobian\ngenerate_factorized_W\ngenerate_hessian","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.calculate_tgrad","page":"The AbstractSystem Interface","title":"ModelingToolkit.calculate_tgrad","text":"calculate_tgrad(sys::AbstractTimeDependentSystem)\n\nCalculate the time gradient of a system.\n\nReturns a vector of Num instances. The result from the first call will be cached in the system object.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.calculate_gradient","page":"The AbstractSystem Interface","title":"ModelingToolkit.calculate_gradient","text":"calculate_gradient(sys::AbstractSystem)\n\nCalculate the gradient of a scalar system.\n\nReturns a vector of Num instances. The result from the first call will be cached in the system object.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.calculate_jacobian","page":"The AbstractSystem Interface","title":"ModelingToolkit.calculate_jacobian","text":"calculate_jacobian(sys::AbstractSystem)\n\nCalculate the jacobian matrix of a system.\n\nReturns a matrix of Num instances. The result from the first call will be cached in the system object.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.calculate_factorized_W","page":"The AbstractSystem Interface","title":"ModelingToolkit.calculate_factorized_W","text":"calculate_factorized_W(sys::AbstractSystem)\n\nCalculate the factorized W-matrix of a system.\n\nReturns a matrix of Num instances. The result from the first call will be cached in the system object.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.calculate_hessian","page":"The AbstractSystem Interface","title":"ModelingToolkit.calculate_hessian","text":"calculate_hessian(sys::AbstractSystem)\n\nCalculate the hessian matrix of a scalar system.\n\nReturns a matrix of Num instances. The result from the first call will be cached in the system object.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.generate_tgrad","page":"The AbstractSystem Interface","title":"ModelingToolkit.generate_tgrad","text":"generate_tgrad(sys::AbstractTimeDependentSystem, dvs = states(sys), ps = parameters(sys), expression = Val{true}; kwargs...)\n\nGenerates a function for the time gradient of a system. Extra arguments control the arguments to the internal build_function call.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.generate_gradient","page":"The AbstractSystem Interface","title":"ModelingToolkit.generate_gradient","text":"generate_gradient(sys::AbstractSystem, dvs = states(sys), ps = parameters(sys), expression = Val{true}; kwargs...)\n\nGenerates a function for the gradient of a system. Extra arguments control the arguments to the internal build_function call.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.generate_jacobian","page":"The AbstractSystem Interface","title":"ModelingToolkit.generate_jacobian","text":"generate_jacobian(sys::AbstractSystem, dvs = states(sys), ps = parameters(sys), expression = Val{true}; sparse = false, kwargs...)\n\nGenerates a function for the jacobian matrix matrix of a system. Extra arguments control the arguments to the internal build_function call.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.generate_factorized_W","page":"The AbstractSystem Interface","title":"ModelingToolkit.generate_factorized_W","text":"generate_factorized_W(sys::AbstractSystem, dvs = states(sys), ps = parameters(sys), expression = Val{true}; sparse = false, kwargs...)\n\nGenerates a function for the factorized W-matrix matrix of a system. Extra arguments control the arguments to the internal build_function call.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#ModelingToolkit.generate_hessian","page":"The AbstractSystem Interface","title":"ModelingToolkit.generate_hessian","text":"generate_hessian(sys::AbstractSystem, dvs = states(sys), ps = parameters(sys), expression = Val{true}; sparse = false, kwargs...)\n\nGenerates a function for the hessian matrix matrix of a system. Extra arguments control the arguments to the internal build_function call.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"Additionally, jacobian_sparsity(sys) and hessian_sparsity(sys) exist on the appropriate systems for fast generation of the sparsity patterns via an abstract interpretation without requiring differentiation.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Problem-Constructors","page":"The AbstractSystem Interface","title":"Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"At the end, the system types have DEProblem constructors, like ODEProblem, which allow for directly generating the problem types required for numerical methods. The first argument is always the AbstractSystem, and the proceeding arguments match the argument order of their original constructors. Whenever an array would normally be provided, such as u0 the initial condition of an ODEProblem, it is instead replaced with a variable map, i.e., an array of pairs var=>value, which allows the user to designate the values without having to know the order that ModelingToolkit is internally using.","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"For the value maps, the parameters are allowed to be functions of each other, and value maps of states can be functions of the parameters, i.e. you can do:","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"u0 = [\n  lorenz1.x => 2.0\n  lorenz2.x => lorenz1.x * lorenz1.p\n]","category":"page"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/#Default-Value-Handling","page":"The AbstractSystem Interface","title":"Default Value Handling","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/AbstractSystem/","page":"The AbstractSystem Interface","title":"The AbstractSystem Interface","text":"The AbstractSystem types allow for specifying default values, for example defaults inside of them. At problem construction time, these values are merged into the value maps, where for any repeats the value maps override the default. In addition, defaults of a higher level in the system override the defaults of a lower level in the system.","category":"page"},{"location":"modules/Surrogates/water_flow/#Water-flow-function","page":"Water Flow function","title":"Water flow function","text":"","category":"section"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"The water flow function is defined as: f(r_wrT_uH_uT_lH_lLK_w) = frac2*pi*T_u(H_u - H_l)log(fracrr_w)*1 + frac2LT_ulog(fracrr_w)*r_w^2*K_w+ fracT_uT_l ","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"It has 8 dimension.","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"using Surrogates\nusing Plots\nusing LinearAlgebra\ndefault()","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"Define the objective function:","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"function f(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r/r_w)\n    return (2*pi*T_u*(H_u - H_l))/ ( log_val*(1 + (2*L*T_u/(log_val*r_w^2*K_w)) + T_u/T_l))\nend","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"n = 180\nd = 8\nlb = [0.05,100,63070,990,63.1,700,1120,9855]\nub = [0.15,50000,115600,1110,116,820,1680,12045]\nx = sample(n,lb,ub,SobolSample())\ny = f.(x)\nn_test = 1000\nx_test = sample(n_test,lb,ub,GoldenSample());\ny_true = f.(x_test);","category":"page"},{"location":"modules/Surrogates/water_flow/","page":"Water Flow function","title":"Water Flow function","text":"my_rad = RadialBasis(x,y,lb,ub)\ny_rad = my_rad.(x_test)\nmy_poly = PolynomialChaosSurrogate(x,y,lb,ub)\ny_poli = my_poli.(x_test)\nmse_rad = norm(y_true - y_rad,2)/n_test\nmse_poli = norm(y_true - y_poli,2)/n_test\nprint(\"MSE Radial: $mse_rad\")\nprint(\"MSE Radial: $mse_poli\")","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/#MultiStartOptimization.jl","page":"MultistartOptimization.jl","title":"MultiStartOptimization.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"MultistartOptimization is a is a Julia package implementing a global optimization multistart method which performs local optimization after choosing multiple starting points.","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"MultistartOptimization requires both a global and local method to be defined. The global multistart method chooses a set of initial starting points from where local the local method starts from.","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"Currently, only one global method (TikTak) is implemented and called by MultiStartOptimization.TikTak(n) where n is the number of initial Sobol points. ","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/#Installation:-OptimizationMultiStartOptimization.jl","page":"MultistartOptimization.jl","title":"Installation: OptimizationMultiStartOptimization.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"To use this package, install the OptimizationMultiStartOptimization package:","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"import Pkg; Pkg.add(\"OptimizationMultiStartOptimization\")","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"note: Note\n","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"You also need to load the relevant subpackage for the local method of you choice, for example if you plan to use one of the NLopt.jl's optimizers, you'd install and load OptimizationNLopt as described in the NLopt.jl's section.","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/#Global-Optimizer","page":"MultistartOptimization.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/#Without-Constraint-Equations","page":"MultistartOptimization.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"The methods in MultistartOptimization is performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/#Examples","page":"MultistartOptimization.jl","title":"Examples","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"The Rosenbrock function can optimized using MultistartOptimization.TikTak() with 100 initial points and the local method NLopt.LD_LBFGS() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, MultistartOptimization.TikTak(100), NLopt.LD_LBFGS())","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"You can use any Optimization optimizers you like. The global method of the MultiStartOptimization is a positional argument and followed by the local method. This for example means we can perform a multistartoptimization with LBFGS as the optimizer using either the NLopt.jl or Optim.jl implementation as follows. Moreover, this interface allows you access and adjust all the optimizer settings as you normally would:","category":"page"},{"location":"modules/Optimization/optimization_packages/multistartoptimization/","page":"MultistartOptimization.jl","title":"MultistartOptimization.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, MultistartOptimization.TikTak(100), NLopt.LD_LBFGS())\nsol = solve(prob, MultistartOptimization.TikTak(100), LBFGS())","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#faq","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This page is a compilation of frequently asked questions and answers.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#faq_stability","page":"Frequently Asked Questions","title":"Stability and Divergence of ODE Solves","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For guidelines on debugging ODE solve issues, see PSA: How to help yourself debug differential equation solving issues.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#My-model-is-reporting-unstable-results.-What-can-I-do?","page":"Frequently Asked Questions","title":"My model is reporting unstable results. What can I do?","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First of all, don't panic. You may have experienced one of the following warnings:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.Instability detected. Aborting","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"These are all pointing to a similar behavior: for some reason or another, the ODE solve is diverging to infinity. As it diverges to infinity, the dt of the integrator will drop (trying to control the speed and error), so it will either hit the minimum dt, hit dt=NaN, or have a value in the ODE hit Inf. Whichever one occurs first will throw the respective warning.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"How to handle this? 99.99% of the time this has been debugged, it has turned out to be an error in the user's model! A missing minus sign, an incorrect term, etc. There are many other behaviors to watch out for. In some ODEs, increasing a parameter can cause a bifurcation so that the solution diverges. With u'=a*u, if a is negative then it nicely falls to zero, but if a is positive the solution quickly diverges to infinity! This means, double check your parameters are indexed correctly!","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note: if you see these warnings during a parameter estimation process, this is likely the underlying problem. Simply check sol.retcode != :Success and throw an Inf cost and most optimizers will reject steps in those parameter regimes!","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are a few other things to check as well. In many cases, the stability of an ODE solve improves as you decrease the tolerance, so you may want to try a smaller abstol and reltol. One behavior to watch out for is that if your model is a differential-algebraic equation and your DAE is of high index (say index>1), this can impact the numerical solution. In this case you may want to use the ModelingToolkit.jl index reduction tools to improve the numerical stability of a solve. In addition, if it's a highly stiff ODE/DAE that is large and you're using a matrix-free solver (such as GMRES), make sure the tolerance of the GMRES is well-tuned and an appropriate preconditioner is applied. Finally, try other solvers. They all have different stability, so try Tsit5(), Vern7(), QNDF(), Rodas5(), TRBDF2(), KenCarp4(), Sundials.CVODE_BDF(), etc. and see what works.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If none of this works out, double check that your ODE truly has the behavior that you believe it should. This is one of the most common issues: your intuition may be deceiving. For example, u' = -sqrt(u) with u(0)=1 cannot hit zero because its derivative shrinks to zero, right? Wrong! It will hit zero in a finite time, after which the solution is undefined and does not have a purely real solution. u' = u^2 - 100u will \"usually\" go to zero, but if u(0)>10 then it will go to infinity. Plot out your diverging solution and see whether the asymtopics are correct: if u[i] gets big, do you equations make u'[i] positive and growing? That's would be a problem!","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Let's say you don't believe you made an error at all and you want to file a bug report. To do so, you'll first want to prove that it's isolated to a solver. If it's a solver issue, then you shouldn't see it happen with every single solver. Do you think it's an issue with the Julia solvers? Well fortunately, DifferentialEquations.jl offers direct unmodified wrappers to almost all previously built solvers, so if you think it's a Julia issue, try running your ODE through:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Sundials.jl, a wrapper for the C++ SUNDIALS library though CVODE_Adams, CVODE_BDF, IDA, and ARKODE.\nODEInterfaceDiffEq.jl, a wrapper for the classic Hairer Fortran codes like dorpi5, dop853, radau, rodas, etc.\nLSODA.jl, a wrapper for the classic lsoda algorithm.\nMATLABDiffEq.jl, a wrapper for the MATLAB ODE solvers ode45, ode15s, etc.\nSciPyDiffEq.jl, a wrapper for SciPy's odeint (LSODA) and other methods (LSODE, etc.).\ndeSolveDiffEq.jl, a wrapper for the commonly used R library.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"And many more. Testing this is as simple as changing solve(prob,Tsit5()) to solve(prob,lsoda()), so please give this a try. If you translated your code from another language, like Python or MATLAB, use the direct wrapper to double check the steps are the same. If they are not, then your ODE is not the same, because it's using a direct call to the solvers of those packages!","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If your ODE diverges to infinity with every ODE solver ever made, the problem is most likely not the ODE solvers. Or rather, to put it in meme form:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"(Image: )","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Don't be like Patrick. If after trying these ideas your ODE solve still seems to have issues and you haven't narrowed it down, feel free to ask on the Julia Discourse to get some help diagnosing it. If you did find a solver issue, please open an issue on the Github repository.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#A-larger-maxiters-seems-to-be-needed,-but-it's-already-high?","page":"Frequently Asked Questions","title":"A larger maxiters seems to be needed, but it's already high?","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you see:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Interrupted. Larger maxiters is needed.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that it could quite possibly arise just from having a very long timespan. If you check sol.t from the returned object and it looks like it's stepping at reasonable lengths, feel free to just pass maxiters=... into solve to bump it up from the default of Int(1e5).","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"But if your maxiters is already high, then the problem is likely that your model is stiff. A stiff ODE requires very small time steps from many explicit solvers, such as Tsit5(), Vern7(), etc., and thus those methods are not appropriate for this kind of problem. You will want to change to a different method, like Rodas5(), Rosenbrock23(), TRBDF2(), KenCarp4(), or QNDF().","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?","page":"Frequently Asked Questions","title":"My ODE goes negative but should stay positive, what tools can help?","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are many tools to help! However, let's first focus on one piece first: when you say \"should\" be positive, what do you mean by \"should\"? If you mean \"mathematically you can prove that the ODE with these values and these initial conditions will have a solution that is positive for all time\" then yes, you're looking in the right place. If by \"should\" you mean \"it's a model of biochemical reactions so the concentration should always be positive\", well ask yourself first, did you write down a model where it will always be positive?","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The following set of tools are designed to accuracy enforce positivity in ODE models which mathematically should be positive in the true solution. If they encounter a model that is actually going negative, they will work really hard to get a positive but correct solution, which is impossible, so they will simply error out. This can be more subtle than you think. Solving u'=-sqrt(u) is not guaranteed to stay positive, even though the derivative goes to zero as u goes to zero (check the analytical solution if you're curious). Similarly, analyzing nonlinear models can showcase all sorts of behavior. A common cause for accidental negativity is Hill functions in systems biology models: just because derivatives go to zero doesn't mean they are going to zero fast enough to keep things positive!","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"With that in mind, let's see the options.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The simplest trick is to change the solver tolerance. Reduce abstol (and maybe reltol) a bit. That can help reduce the error and thus keep the solution positive. For some more difficult equations, changing to a stiff ODE solver like Rosenbrock23() QNDF, or TRBDF2() can be helpful.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If those don't work, call out the big guns. One of them is isoutofdomain, where you can define a boolean function which will cause step rejections whenever it is not satisfied. For example, isoutofdomain = (u,p,t)->any(x->x<0,u) will make the solver reject any step which cases any variable u to go negative. Now, using any pure-Julia solver with this option, it's impossible to get a negative in the result! One thing you may see though is:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"or","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Interrupted. Larger maxiters is needed.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What this means is that enforcing positivity is not possible. It keeps rejecting steps that go negative, reducing dt, taking another step, rejecting, reducing, repeat until dt hits dtmin or it hits maxiters. This means that even when trying to solve the problem with the most accurate infinitesimal dt, the solution still goes negative. Are you sure the true solution is supposed to be positive? If you see this, check for issues like a missing minus sign in your equations.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If that works but is a little slow, the domain handling callbacks in the callback library are designed to function similarly but in a way that gets better performance. Instead of repeating lots of steps through rejections, it interpolates back to still take a smaller step, always progressing forwards. However, this can be a bit less stable, so its applicability depends on the equation, and once again this requires that the solution is truly positive. If the true solution goes negative, it will repeatedly try interpolating backwards until it can no longer and end with a dtmin issue.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Finally, note that ODE solvers will not be more correct than tolerance, and so one should expect that if the solution is supposed to be positive but abstol=1e-12, you may end up with u[i]=-1e-12. That is okay, that is expected behavior of numerical solvers, the ODE solver is still doing its job. If this is a major issue for your application, you may want to write your model to be robust to this behavior, such as changing sqrt(u[i]) to sqrt(max(0,u[i])). You should also consider transforming your values, like solving for u^2 or exp(u) instead of u, which mathematically can only be positive. Look into using a tool like ModelingToolkit.jl for automatically transforming your equations.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#faq_performance","page":"Frequently Asked Questions","title":"Performance","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/#GPUs,-multithreading-and-distributed-computation-support","page":"Frequently Asked Questions","title":"GPUs, multithreading and distributed computation support","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes. The *DiffEq.jl libraries (OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl) are all written to be generic to the array and number types. This means they will adopt the implementation that is given by the array type. The in-place algorithms internally utilize Julia's broadcast (with some exceptions due to a Julia bug for now, see this issue) and Julia's mul! in-place matrix multiplication function. The out-of-place algorithms utilize standard arithmetical functions. Both additionally utilize the user's norm specified via the common interface options and, if a stiff solver, ForwardDiff/DiffEqDiffTools for the Jacobian calculation, and Base linear factorizations for the linear solve. For your type, you may likely need to give a better form of the norm, Jacobian, or linear solve calculations to fully utilize parallelism.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"GPUArrays.jl (CuArrays.jl), ArrayFire.jl, DistributedArrays.jl have been tested and work in various forms, where the last one is still not recommended for common use yet.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The next question is whether it matters. Generally, your system has to be large for parallelism to matter. Using a multithreaded array for broadcast we find helpful around N>1000, though the Sundials manual says N>100,000. For high order Runge-Kutta methods it's likely lower than the Sundials estimate because of more operations packed into each internal step, but as always that will need more benchmarks to be precise and will depend on the problem being solved. GPUs generally require some intensive parallel operation in the user's f function to be viable, for example a matrix multiplication for a stencil computation in a PDE. If you're simply solving some ODE element-wise on a big array it likely won't do much or it will slow things down just due to how GPUs work. DistributedArrays require parallel linear solves to really matter, and thus are only recommended when you have a problem that cannot fit into memory or are using a stiff solver with a Krylov method for the linear solves.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices","page":"Frequently Asked Questions","title":"Note About Setting Up Your Julia Installation for Speed: BLAS Choices","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Julia uses an underlying BLAS implementation for its matrix multiplications and factorizations. This library is automatically multithreaded and accelerates the internal linear algebra of DifferentialEquations.jl. However, for optimality, you should make sure that the number of BLAS threads that you are using matches the number of physical cores and not the number of logical cores. See this issue for more details.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"To check the number of BLAS threads, use:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ccall((:openblas_get_num_threads64_, Base.libblas_name), Cint, ())","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If I want to set this directly to 4 threads, I would use:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearAlgebra\nLinearAlgebra.BLAS.set_num_threads(4)","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Additionally, in some cases Intel's MKL might be a faster BLAS than the standard BLAS that ships with Julia (OpenBLAS). To switch your BLAS implementation, you can use MKL.jl which will accelerate the linear algebra routines. This is done via:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using MKL","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#My-ODE-is-solving-really-slow","page":"Frequently Asked Questions","title":"My ODE is solving really slow","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First, check for bugs. These solvers go through a ton of convergence tests and so if there's a solver issue, it's either just something to do with how numerical methods work or it's a user-error (generally the latter, though check the later part of the FAQ on normal numerical errors). User-errors in the f function causing a divergence of the solution is the most common reason for reported slow codes.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you have no bugs, great! The standard tricks for optimizing Julia code then apply. Take a look at the Optimizing DiffEq Code tutorial for some tips and pointers.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What you want to do first is make sure your function does not allocate. If your system is small (<=100 ODEs/SDEs/DDEs/DAEs?), then you should set your system up to use StaticArrays.jl. This is demonstrated in the ODE tutorial with static matrices. Static vectors/arrays are stack-allocated, and thus creating new arrays is free and the compiler doesn't have to heap-allocate any of the temporaries (that's the expensive part!). These have specialized super fast dispatches for arithmetic operations and extra things like LU-factorizations, and thus they are preferred when possible. However, they lose efficiency if they grow too large.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For anything larger, you should use the in-place syntax f(du,u,p,t) and make sure that your function doesn't allocate. Assuming you know of a u0, you should be able to do:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"du = similar(u0)\n@time f(du,u0,p,t)","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and see close to zero allocations and close to zero memory allocated. If you see more, then you might have a type-instability or have temporary arrays. To find type-instabilities, you should do:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"@code_warntype f(du,u,p,t)","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and read the printout to see if there's any types that aren't inferred by the compiler, and fix them. If you have any global variables, you should make them const. As for allocations, some common things that allocate are:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Array slicing, like u[1:5]. Instead, use @view u[1:5]\nMatrix multiplication with *. Instead of A*b, use mul!(c,A,b) for some pre-allocated cache vector c.\nNon-broadcasted expressions. Every expression on arrays should .= into another array, or it should be re-written to loop and do computations with scalar (or static array) values.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For an example of optimizing a function resulting from a PDE discretization, see this blog post.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization","page":"Frequently Asked Questions","title":"The stiff solver takes forever to take steps for my PDE discretization","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The solvers for stiff solvers require solving a nonlinear equation each step. In order to do so, they have to do a few Newton steps. By default, these methods assume that the Jacobian is dense, automatically calculate the Jacobian for you, and do a dense factorization. However, in many cases you may want to use alternatives that are more tuned for your problem.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First of all, when available, it's recommended that you pass a function for computing your Jacobian. This is discussed in the performance overloads section. Jacobians are especially helpful for Rosenbrock methods.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Secondly, if your Jacobian isn't dense, you shouldn't use a dense Jacobian! Instead, if you're using  a *DiffEq library you should specify a linear solver and/or a jac_prototype for the matrix form, and for Sundials.jl, you should change the linear_solver option. See the ODE solve Sundials portion for details on that.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Right now, QNDF is the recommended method for stiff problems with large sparse Jacobians. You should specify jac_prototype as a special matrix, such as a banded or tridiagonal matrix, if it satisfies a special structure. If you only know the Jacobian is sparse, using automated sparsity detection can help with identifying the sparsity pattern. See the stiff ODE tutorial for more details. Lastly, using LinSolveGMRES() can help if a sparsity pattern cannot be obtained but the matrix is large, or if the sparsity cannot fit into memory. Once again, a good reference for how to handle PDE discretizations can be found at this blog post.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow","page":"Frequently Asked Questions","title":"My Problem Has Discontinuities and is Unstable / Slow","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This Discourse post goes into detail for how to handle discontinuities in your ODE function and how to use that extra information to speed up the solver.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#Complicated-Models","page":"Frequently Asked Questions","title":"Complicated Models","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/#Switching-ODE-functions-in-the-middle-of-integration","page":"Frequently Asked Questions","title":"Switching ODE functions in the middle of integration","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are a few ways to do this. The simplest way is to just have a parameter to switch between the two. For example:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"function f(du,u,p,t)\n  if p == 0\n    du[1] = 2u[1]\n  else\n    du[1] = -2u[1]\n  end\n  du[2] = -u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Then in a callback you can make the affect! function modify integrator.prob.p. For example, we can make it change when u[2]<0.5 via:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"condition(t,u,integrator) = u[2] - 0.5\naffect!(integrator) = integrator.p = 1","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Then it will change betweeen the two ODE choices for du1 at that moment. Another way to do this is to make the ODE functions all be the same type via FunctionWrappers.jl, but that is unnecessary. With the way that modern processors work, there exists branch prediction and thus execution of a conditional is free if it's predictable which branch will be taken. In this case, almost every call to f takes the p==0 route until the callback, at which point it is almost always the else route. Therefore the processor will effectively get rid of the computational cost associated with this, so you're likely over-optimizing if you're going further (unless this change happens every step, but even then this is probably the cheapest part of the computation...).","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#Numerical-Error","page":"Frequently Asked Questions","title":"Numerical Error","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/#What-does-tolerance-mean-and-how-much-error-should-I-expect","page":"Frequently Asked Questions","title":"What does tolerance mean and how much error should I expect","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The most useful options are the tolerances abstol and reltol. These tell the internal adaptive time stepping engine how precise of a solution you want. Generally, reltol is the relative accuracy while abstol is the accuracy when u is near zero. These tolerances are local tolerances and thus are not global guarantees. However, a good rule of thumb is that the total solution accuracy is 1-2 digits less than the relative tolerances. Thus for the defaults abstol=1e-6 and reltol=1e-3, you can expect a global accuracy of about 1-2 digits. This is standard across the board and applies to the native Julia methods, the wrapped Fortran and C++ methods, the calls to MATLAB/Python/R, etc.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#The-solver-doesn't-obey-physical-law-X-(e.g.-conservation-of-energy)","page":"Frequently Asked Questions","title":"The solver doesn't obey physical law X (e.g. conservation of energy)","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, this is because the numerical solution of the ODE is not the exact solution. There are a few ways that you can handle this problem. One way is to get a more exact solution. Thus instead of","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"sol = solve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"use","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"sol = solve(prob,alg,abstol=1e-10,reltol=1e-10)","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Of course, there's always a tradeoff between accuracy and efficiency, so play around to find out what's right for your problem.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Another thing you can do is use a callback. There are some premade callbacks in the callback library which handle these sorts of things like projecting to manifolds and preserving positivity.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#Symplectic-integrators-don't-conserve-energy","page":"Frequently Asked Questions","title":"Symplectic integrators don't conserve energy","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, symplectic integrators do not exactly conserve energy. It is a common misconception that they do. What symplectic integrators actually do is solve for a trajectory which rests on a symplectic manifold that is perturbed from the true solution's manifold by the truncation error. This means that symplectic integrators do not experience (very much) long time drift, but their orbit is not exactly the same as the true solution in phase space and thus you will see differences in energy that tend to look periodic. There is a small drift which grows linearly and is related to floating point error, but this drift is much less than standard methods. This is why symplectic methods are recommended for long time integration.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For conserving energy, there are a few things you can do. First of all, the energy error is related to the integration error, so simply solving with higher accuracy will reduce the error. The results in the DiffEqBenchmarks show that using a DPRKN method with low tolerance can be a great choice. Another thing you can do is use the ManifoldProjection callback from the callback library.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#How-to-get-to-zero-error","page":"Frequently Asked Questions","title":"How to get to zero error","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"You can't. For floating point numbers, you shouldn't use below abstol=1e-14 and reltol=1e-14. If you need lower than that, use arbitrary precision numbers like BigFloats or ArbFloats.jl.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#Autodifferentiation-and-Dual-Numbers","page":"Frequently Asked Questions","title":"Autodifferentiation and Dual Numbers","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/#Native-Julia-solvers-compatibility-with-autodifferentiation","page":"Frequently Asked Questions","title":"Native Julia solvers compatibility with autodifferentiation","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, they are compatible with automatic differentiation! Take a look at the sensitivity analysis page for more details.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If the algorithm does not have differentiation of parameter-dependent events, then you simply need to make the initial condition have elements of Dual numbers. If the algorithm uses Dual numbers, you need to make sure that time is also given by Dual numbers.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"To show this in action, let's say we want to find the Jacobian of solution of the Lotka-Volterra equation at t=10 with respect to the parameters.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"function func(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\nfunction f(p)\n  prob = ODEProblem(func,eltype(p).([1.0,1.0]),(0.0,10.0),p)\n  # Lower tolerances to show the methods converge to the same value\n  solve(prob,Tsit5(),save_everystep=false,abstol=1e-12,reltol=1e-12)[end]\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This function takes in new parameters and spits out the solution at the end. We make the inital condition eltype(p).([1.0,1.0]) so that way it's typed to be Dual numbers whenever p is an array of Dual numbers, and we do the same for the timespan just to show what you'd do if there was parameters-dependent events. Then we can take the Jacobian via ForwardDiff.jl:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using ForwardDiff\nForwardDiff.jacobian(f,[1.5,1.0])\n\n2×2 Array{Float64,2}:\n  2.16056   0.188569\n -6.25677  -0.697978","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and compare it to Calculus.jl:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Calculus.jacobian(f,[1.5,1.0],:central)\n\n2×2 Array{Float64,2}:\n  2.16056   0.188569\n -6.25677  -0.697978","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/#I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods","page":"Frequently Asked Questions","title":"I get Dual number errors when I solve my ODE with Rosenbrock or SDIRK methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is because you're using a cache which is not compatible with autodifferentiaion via ForwardDiff.jl. For example, if we use the ODE function:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearAlgebra, OrdinaryDiffEq\nfunction foo(du, u, (A, tmp), t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), zeros(5,5)))\nsolve(prob, Rosenbrock23())","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Here we use a cached temporary array in order to avoid the allocations of matrix multiplication. When autodifferentiation occurs, the element type of u is Dual numbers, so A*u produces Dual numbers, so the error arises when it tries to write into tmp. There are two ways to avoid this. The first way, the easy way, is to just turn off autodifferentiation with the autodiff=false option in the solver. Every solver which uses autodifferentiation has this option. Thus we'd solve this with:","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"prob = ODEProblem(f,rand(4),(0.0,1.0))\nsol = solve(prob,Rosenbrock23(autodiff=false))","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and it will use a numerical differentiation fallback (DiffEqDiffTools.jl) to calculate Jacobians.","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"We could use get_tmp and dualcache functions from PreallocationTools.jl to solve this issue, e.g.,","category":"page"},{"location":"modules/DiffEqDocs/basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearAlgebra, OrdinaryDiffEq, PreallocationTools\nfunction foo(du, u, (A, tmp), t)\n    tmp = get_tmp(tmp, first(u)*t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), DiffEqBase.dualcache(zeros(5,5))))\nsolve(prob, TRBDF2()","category":"page"},{"location":"modules/DiffEqParamEstim/methods/recommended_methods/#Recommended-Methods","page":"Recommended Methods","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"The recommended method is to use build_loss_objective with the optimizer of your choice. This method can thus be paired with global optimizers from packages like BlackBoxOptim.jl or NLopt.jl which can be much less prone to finding local minima than local optimization methods. Also, it allows the user to define the cost function in the way they choose as a function loss(sol), and thus can fit using any cost function on the solution, making it applicable to fitting non-temporal data and other types of problems. Also, build_loss_objective works for all of the DEProblem types, allowing it to optimize parameters on ODEs, SDEs, DDEs, DAEs, etc.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"However, this method requires repeated solution of the differential equation. If the data is temporal data, the most efficient method is the two_stage_method which does not require repeated solutions but is not as accurate. Usage of the two_stage_method should have a post-processing step which refines using a method like build_loss_objective.","category":"page"},{"location":"modules/LinearSolve/basics/CachingAPI/#Caching-Interface-API-Functions","page":"Caching Interface API Functions","title":"Caching Interface API Functions","text":"","category":"section"},{"location":"modules/LinearSolve/basics/CachingAPI/","page":"Caching Interface API Functions","title":"Caching Interface API Functions","text":"LinearSolve.set_A\r\nLinearSolve.set_b\r\nLinearSolve.set_u\r\nLinearSolve.set_p\r\nLinearSolve.set_prec","category":"page"},{"location":"modules/LinearSolve/basics/CachingAPI/#LinearSolve.set_A","page":"Caching Interface API Functions","title":"LinearSolve.set_A","text":"set_A(cache, A)\n\n\n\n\n\n\n","category":"function"},{"location":"modules/LinearSolve/basics/CachingAPI/#LinearSolve.set_b","page":"Caching Interface API Functions","title":"LinearSolve.set_b","text":"set_b(cache, b)\n\n\n\n\n\n\n","category":"function"},{"location":"modules/LinearSolve/basics/CachingAPI/#LinearSolve.set_u","page":"Caching Interface API Functions","title":"LinearSolve.set_u","text":"set_u(cache, u)\n\n\n\n\n\n\n","category":"function"},{"location":"modules/LinearSolve/basics/CachingAPI/#LinearSolve.set_p","page":"Caching Interface API Functions","title":"LinearSolve.set_p","text":"set_p(cache, p)\n\n\n\n\n\n\n","category":"function"},{"location":"modules/LinearSolve/basics/CachingAPI/#LinearSolve.set_prec","page":"Caching Interface API Functions","title":"LinearSolve.set_prec","text":"set_prec(cache, Pl, Pr)\n\n\n\n\n\n\n","category":"function"},{"location":"modules/MethodOfLines/generated/bruss_code/#brusscode","page":"Generated Code for the Brusselator Equation","title":"Generated Code for the Brusselator Equation","text":"","category":"section"},{"location":"modules/MethodOfLines/generated/bruss_code/","page":"Generated Code for the Brusselator Equation","title":"Generated Code for the Brusselator Equation","text":"Here's the generated julia code for the Brusselator, with dx = dy = 1/4","category":"page"},{"location":"modules/MethodOfLines/generated/bruss_code/","page":"Generated Code for the Brusselator Equation","title":"Generated Code for the Brusselator Equation","text":"begin\n    var\"##f#260\" = (ModelingToolkit.ODEFunctionClosure)(function (ˍ₋arg1, ˍ₋arg2, t)\n                begin\n                    var\"u[2, 2](t)\" = @inbounds(ˍ₋arg1[1])\n                    var\"u[3, 2](t)\" = @inbounds(ˍ₋arg1[2])\n                    var\"u[4, 2](t)\" = @inbounds(ˍ₋arg1[3])\n                    var\"u[5, 2](t)\" = @inbounds(ˍ₋arg1[4])\n                    var\"u[2, 3](t)\" = @inbounds(ˍ₋arg1[5])\n                    var\"u[3, 3](t)\" = @inbounds(ˍ₋arg1[6])\n                    var\"u[4, 3](t)\" = @inbounds(ˍ₋arg1[7])\n                    var\"u[5, 3](t)\" = @inbounds(ˍ₋arg1[8])\n                    var\"u[2, 4](t)\" = @inbounds(ˍ₋arg1[9])\n                    var\"u[3, 4](t)\" = @inbounds(ˍ₋arg1[10])\n                    var\"u[4, 4](t)\" = @inbounds(ˍ₋arg1[11])\n                    var\"u[5, 4](t)\" = @inbounds(ˍ₋arg1[12])\n                    var\"u[2, 5](t)\" = @inbounds(ˍ₋arg1[13])\n                    var\"u[3, 5](t)\" = @inbounds(ˍ₋arg1[14])\n                    var\"u[4, 5](t)\" = @inbounds(ˍ₋arg1[15])\n                    var\"u[5, 5](t)\" = @inbounds(ˍ₋arg1[16])\n                    var\"v[2, 2](t)\" = @inbounds(ˍ₋arg1[17])\n                    var\"v[3, 2](t)\" = @inbounds(ˍ₋arg1[18])\n                    var\"v[4, 2](t)\" = @inbounds(ˍ₋arg1[19])\n                    var\"v[5, 2](t)\" = @inbounds(ˍ₋arg1[20])\n                    var\"v[2, 3](t)\" = @inbounds(ˍ₋arg1[21])\n                    var\"v[3, 3](t)\" = @inbounds(ˍ₋arg1[22])\n                    var\"v[4, 3](t)\" = @inbounds(ˍ₋arg1[23])\n                    var\"v[5, 3](t)\" = @inbounds(ˍ₋arg1[24])\n                    var\"v[2, 4](t)\" = @inbounds(ˍ₋arg1[25])\n                    var\"v[3, 4](t)\" = @inbounds(ˍ₋arg1[26])\n                    var\"v[4, 4](t)\" = @inbounds(ˍ₋arg1[27])\n                    var\"v[5, 4](t)\" = @inbounds(ˍ₋arg1[28])\n                    var\"v[2, 5](t)\" = @inbounds(ˍ₋arg1[29])\n                    var\"v[3, 5](t)\" = @inbounds(ˍ₋arg1[30])\n                    var\"v[4, 5](t)\" = @inbounds(ˍ₋arg1[31])\n                    var\"v[5, 5](t)\" = @inbounds(ˍ₋arg1[32])\n                    begin\n                        (SymbolicUtils.Code.create_array)(typeof(ˍ₋arg1), nothing, Val{1}(), Val{(32,)}(), (+)((+)((+)((+)((+)((+)(1.0, (*)(-644.4, var\"u[2, 2](t)\")), (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)((^)(var\"u[2, 2](t)\", 2), var\"v[2, 2](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(-644.4, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)((^)(var\"u[3, 2](t)\", 2), var\"v[3, 2](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(-644.4, var\"u[4, 2](t)\")), (*)((^)(var\"u[4, 2](t)\", 2), var\"v[4, 2](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)(-644.4, var\"u[5, 2](t)\")), (*)((^)(var\"u[5, 2](t)\", 2), var\"v[5, 2](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(-644.4, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)((^)(var\"u[2, 3](t)\", 2), var\"v[2, 3](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(-644.4, var\"u[3, 3](t)\")), (*)((^)(var\"u[3, 3](t)\", 2), var\"v[3, 3](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(-644.4, var\"u[4, 3](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)((^)(var\"u[4, 3](t)\", 2), var\"v[4, 3](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(-644.4, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)((^)(var\"u[5, 3](t)\", 2), var\"v[5, 3](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)(-644.4, var\"u[2, 4](t)\")), (*)((^)(var\"u[2, 4](t)\", 2), var\"v[2, 4](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(-644.4, var\"u[3, 4](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)((^)(var\"u[3, 4](t)\", 2), var\"v[3, 4](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(-644.4, var\"u[4, 4](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)((^)(var\"u[4, 4](t)\", 2), var\"v[4, 4](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)(-644.4, var\"u[5, 4](t)\")), (*)((^)(var\"u[5, 4](t)\", 2), var\"v[5, 4](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(160.0, var\"u[2, 4](t)\")), (*)(-644.4, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)((^)(var\"u[2, 5](t)\", 2), var\"v[2, 5](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(-644.4, var\"u[3, 5](t)\")), (*)((^)(var\"u[3, 5](t)\", 2), var\"v[3, 5](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(-644.4, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)((^)(var\"u[4, 5](t)\", 2), var\"v[4, 5](t)\")), (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)(-644.4, var\"u[5, 5](t)\")), (*)((^)(var\"u[5, 5](t)\", 2), var\"v[5, 5](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 2](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(-640.0, var\"v[2, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 2](t)\", 2)), var\"v[2, 2](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 2](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(-640.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 2](t)\", 2)), var\"v[3, 2](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 2](t)\"), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[4, 2](t)\")), (*)(160.0, var\"v[4, 5](t)\"))[Imgur](https://i.imgur.com/3kQNMI3.gifv), (*)(160.0, var\"v[5, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 2](t)\", 2)), var\"v[4, 2](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 2](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(-640.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 2](t)\", 2)), var\"v[5, 2](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 3](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(-640.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 3](t)\", 2)), var\"v[2, 3](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 3](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[3, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 3](t)\", 2)), var\"v[3, 3](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 3](t)\"), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(-640.0, var\"v[4, 3](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 3](t)\", 2)), var\"v[4, 3](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 3](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 3](t)\", 2)), var\"v[5, 3](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 4](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[2, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 4](t)\", 2)), var\"v[2, 4](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 4](t)\"), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(-640.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 4](t)\", 2)), var\"v[3, 4](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 4](t)\"), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 4](t)\", 2)), var\"v[4, 4](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 4](t)\"), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)(-640.0, var\"v[5, 4](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 4](t)\", 2)), var\"v[5, 4](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 5](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(160.0, var\"v[2, 4](t)\")), (*)(-640.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 5](t)\", 2)), var\"v[2, 5](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 5](t)\"), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(-640.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 5](t)\", 2)), var\"v[3, 5](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 5](t)\"), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)(-640.0, var\"v[4, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 5](t)\", 2)), var\"v[4, 5](t)\")), (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 5](t)\"), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 5](t)\", 2)), var\"v[5, 5](t)\")))\n                    end\n                end\n            end, function (ˍ₋out, ˍ₋arg1, ˍ₋arg2, t)\n                begin\n                    var\"u[2, 2](t)\" = @inbounds(ˍ₋arg1[1])\n                    var\"u[3, 2](t)\" = @inbounds(ˍ₋arg1[2])\n                    var\"u[4, 2](t)\" = @inbounds(ˍ₋arg1[3])\n                    var\"u[5, 2](t)\" = @inbounds(ˍ₋arg1[4])\n                    var\"u[2, 3](t)\" = @inbounds(ˍ₋arg1[5])\n                    var\"u[3, 3](t)\" = @inbounds(ˍ₋arg1[6])\n                    var\"u[4, 3](t)\" = @inbounds(ˍ₋arg1[7])\n                    var\"u[5, 3](t)\" = @inbounds(ˍ₋arg1[8])\n                    var\"u[2, 4](t)\" = @inbounds(ˍ₋arg1[9])\n                    var\"u[3, 4](t)\" = @inbounds(ˍ₋arg1[10])\n                    var\"u[4, 4](t)\" = @inbounds(ˍ₋arg1[11])\n                    var\"u[5, 4](t)\" = @inbounds(ˍ₋arg1[12])\n                    var\"u[2, 5](t)\" = @inbounds(ˍ₋arg1[13])\n                    var\"u[3, 5](t)\" = @inbounds(ˍ₋arg1[14])\n                    var\"u[4, 5](t)\" = @inbounds(ˍ₋arg1[15])\n                    var\"u[5, 5](t)\" = @inbounds(ˍ₋arg1[16])\n                    var\"v[2, 2](t)\" = @inbounds(ˍ₋arg1[17])\n                    var\"v[3, 2](t)\" = @inbounds(ˍ₋arg1[18])\n                    var\"v[4, 2](t)\" = @inbounds(ˍ₋arg1[19])\n                    var\"v[5, 2](t)\" = @inbounds(ˍ₋arg1[20])\n                    var\"v[2, 3](t)\" = @inbounds(ˍ₋arg1[21])\n                    var\"v[3, 3](t)\" = @inbounds(ˍ₋arg1[22])\n                    var\"v[4, 3](t)\" = @inbounds(ˍ₋arg1[23])\n                    var\"v[5, 3](t)\" = @inbounds(ˍ₋arg1[24])\n                    var\"v[2, 4](t)\" = @inbounds(ˍ₋arg1[25])\n                    var\"v[3, 4](t)\" = @inbounds(ˍ₋arg1[26])\n                    var\"v[4, 4](t)\" = @inbounds(ˍ₋arg1[27])\n                    var\"v[5, 4](t)\" = @inbounds(ˍ₋arg1[28])\n                    var\"v[2, 5](t)\" = @inbounds(ˍ₋arg1[29])\n                    var\"v[3, 5](t)\" = @inbounds(ˍ₋arg1[30])\n                    var\"v[4, 5](t)\" = @inbounds(ˍ₋arg1[31])\n                    var\"v[5, 5](t)\" = @inbounds(ˍ₋arg1[32])\n                    begin\n                        @inbounds begin\n                                ˍ₋out[1] = (+)((+)((+)((+)((+)((+)(1.0, (*)(-644.4, var\"u[2, 2](t)\")), (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)((^)(var\"u[2, 2](t)\", 2), var\"v[2, 2](t)\"))\n                                ˍ₋out[2] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(-644.4, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)((^)(var\"u[3, 2](t)\", 2), var\"v[3, 2](t)\"))\n                                ˍ₋out[3] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(-644.4, var\"u[4, 2](t)\")), (*)((^)(var\"u[4, 2](t)\", 2), var\"v[4, 2](t)\"))\n                                ˍ₋out[4] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)(-644.4, var\"u[5, 2](t)\")), (*)((^)(var\"u[5, 2](t)\", 2), var\"v[5, 2](t)\"))\n                                ˍ₋out[5] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(-644.4, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)((^)(var\"u[2, 3](t)\", 2), var\"v[2, 3](t)\"))\n                                ˍ₋out[6] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(-644.4, var\"u[3, 3](t)\")), (*)((^)(var\"u[3, 3](t)\", 2), var\"v[3, 3](t)\"))\n                                ˍ₋out[7] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 3](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(-644.4, var\"u[4, 3](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)((^)(var\"u[4, 3](t)\", 2), var\"v[4, 3](t)\"))\n                                ˍ₋out[8] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(-644.4, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)((^)(var\"u[5, 3](t)\", 2), var\"v[5, 3](t)\"))\n                                ˍ₋out[9] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 3](t)\")), (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)(-644.4, var\"u[2, 4](t)\")), (*)((^)(var\"u[2, 4](t)\", 2), var\"v[2, 4](t)\"))\n                                ˍ₋out[10] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[3, 3](t)\")), (*)(-644.4, var\"u[3, 4](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)((^)(var\"u[3, 4](t)\", 2), var\"v[3, 4](t)\"))\n                                ˍ₋out[11] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 3](t)\")), (*)(-644.4, var\"u[4, 4](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)((^)(var\"u[4, 4](t)\", 2), var\"v[4, 4](t)\"))\n                                ˍ₋out[12] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 4](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(160.0, var\"u[5, 3](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)(-644.4, var\"u[5, 4](t)\")), (*)((^)(var\"u[5, 4](t)\", 2), var\"v[5, 4](t)\"))\n                                ˍ₋out[13] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 2](t)\")), (*)(160.0, var\"u[2, 4](t)\")), (*)(-644.4, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)((^)(var\"u[2, 5](t)\", 2), var\"v[2, 5](t)\"))\n                                ˍ₋out[14] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[3, 2](t)\")), (*)(160.0, var\"u[3, 4](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(-644.4, var\"u[3, 5](t)\")), (*)((^)(var\"u[3, 5](t)\", 2), var\"v[3, 5](t)\"))\n                                ˍ₋out[15] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[3, 5](t)\")), (*)(160.0, var\"u[4, 2](t)\")), (*)(160.0, var\"u[4, 4](t)\")), (*)(-644.4, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 5](t)\")), (*)((^)(var\"u[4, 5](t)\", 2), var\"v[4, 5](t)\"))\n                                ˍ₋out[16] = (+)((+)((+)((+)((+)((+)(1.0, (*)(160.0, var\"u[2, 5](t)\")), (*)(160.0, var\"u[4, 5](t)\")), (*)(160.0, var\"u[5, 2](t)\")), (*)(160.0, var\"u[5, 4](t)\")), (*)(-644.4, var\"u[5, 5](t)\")), (*)((^)(var\"u[5, 5](t)\", 2), var\"v[5, 5](t)\"))\n                                ˍ₋out[17] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 2](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(-640.0, var\"v[2, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 2](t)\", 2)), var\"v[2, 2](t)\"))\n                                ˍ₋out[18] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 2](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(-640.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 2](t)\", 2)), var\"v[3, 2](t)\"))\n                                ˍ₋out[19] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 2](t)\"), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[4, 2](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 2](t)\", 2)), var\"v[4, 2](t)\"))\n                                ˍ₋out[20] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 2](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(-640.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 2](t)\", 2)), var\"v[5, 2](t)\"))\n                                ˍ₋out[21] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 3](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(-640.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 3](t)\", 2)), var\"v[2, 3](t)\"))[Imgur](https://i.imgur.com/3kQNMI3.gifv)\n                                ˍ₋out[22] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 3](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[3, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 3](t)\", 2)), var\"v[3, 3](t)\"))\n                                ˍ₋out[23] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 3](t)\"), (*)(160.0, var\"v[3, 3](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(-640.0, var\"v[4, 3](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 3](t)\", 2)), var\"v[4, 3](t)\"))\n                                ˍ₋out[24] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 3](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[5, 3](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 3](t)\", 2)), var\"v[5, 3](t)\"))\n                                ˍ₋out[25] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 4](t)\"), (*)(160.0, var\"v[2, 3](t)\")), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[2, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 4](t)\", 2)), var\"v[2, 4](t)\"))\n                                ˍ₋out[26] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 4](t)\"), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[3, 3](t)\")), (*)(-640.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 4](t)\", 2)), var\"v[3, 4](t)\"))\n                                ˍ₋out[27] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 4](t)\"), (*)(160.0, var\"v[3, 4](t)\")), (*)(160.0, var\"v[4, 3](t)\")), (*)(-640.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 4](t)\", 2)), var\"v[4, 4](t)\"))\n                                ˍ₋out[28] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 4](t)\"), (*)(160.0, var\"v[2, 4](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 3](t)\")), (*)(-640.0, var\"v[5, 4](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 4](t)\", 2)), var\"v[5, 4](t)\"))\n                                ˍ₋out[29] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[2, 5](t)\"), (*)(160.0, var\"v[2, 2](t)\")), (*)(160.0, var\"v[2, 4](t)\")), (*)(-640.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[2, 5](t)\", 2)), var\"v[2, 5](t)\"))\n                                ˍ₋out[30] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[3, 5](t)\"), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[3, 2](t)\")), (*)(160.0, var\"v[3, 4](t)\")), (*)(-640.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[3, 5](t)\", 2)), var\"v[3, 5](t)\"))\n                                ˍ₋out[31] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[4, 5](t)\"), (*)(160.0, var\"v[3, 5](t)\")), (*)(160.0, var\"v[4, 2](t)\")), (*)(160.0, var\"v[4, 4](t)\")), (*)(160.0, var\"v[5, 5](t)\")), (*)(-640.0, var\"v[4, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[4, 5](t)\", 2)), var\"v[4, 5](t)\"))\n                                ˍ₋out[32] = (+)((+)((+)((+)((+)((+)((*)(3.4, var\"u[5, 5](t)\"), (*)(160.0, var\"v[2, 5](t)\")), (*)(160.0, var\"v[4, 5](t)\")), (*)(160.0, var\"v[5, 2](t)\")), (*)(160.0, var\"v[5, 4](t)\")), (*)(-640.0, var\"v[5, 5](t)\")), (*)((*)(-1.0, (^)(var\"u[5, 5](t)\", 2)), var\"v[5, 5](t)\"))\n                                nothing\n                            end\n                    end\n                end\n            end)\n    var\"##tgrad#261\" = nothing\n    var\"##jac#262\" = nothing\n    M = UniformScaling{Bool}(true)\n    ODEFunction{true}(var\"##f#260\", jac = var\"##jac#262\", tgrad = var\"##tgrad#261\", mass_matrix = M, jac_prototype = nothing, syms = [Symbol(\"u[2, 2](t)\"), Symbol(\"u[3, 2](t)\"), Symbol(\"u[4, 2](t)\"), Symbol(\"u[5, 2](t)\"), Symbol(\"u[2, 3](t)\"), Symbol(\"u[3, 3](t)\"), Symbol(\"u[4, 3](t)\"), Symbol(\"u[5, 3](t)\"), Symbol(\"u[2, 4](t)\"), Symbol(\"u[3, 4](t)\"), Symbol(\"u[4, 4](t)\"), Symbol(\"u[5, 4](t)\"), Symbol(\"u[2, 5](t)\"), Symbol(\"u[3, 5](t)\"), Symbol(\"u[4, 5](t)\"), Symbol(\"u[5, 5](t)\"), Symbol(\"v[2, 2](t)\"), Symbol(\"v[3, 2](t)\"), Symbol(\"v[4, 2](t)\"), Symbol(\"v[5, 2](t)\"), Symbol(\"v[2, 3](t)\"), Symbol(\"v[3, 3](t)\"), Symbol(\"v[4, 3](t)\"), Symbol(\"v[5, 3](t)\"), Symbol(\"v[2, 4](t)\"), Symbol(\"v[3, 4](t)\"), Symbol(\"v[4, 4](t)\"), Symbol(\"v[5, 4](t)\"), Symbol(\"v[2, 5](t)\"), Symbol(\"v[3, 5](t)\"), Symbol(\"v[4, 5](t)\"), Symbol(\"v[5, 5](t)\")], indepsym = :t)\nend","category":"page"},{"location":"modules/NeuralPDE/manual/logging/#Logging-Utilities","page":"Logging Utilities","title":"Logging Utilities","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/logging/","page":"Logging Utilities","title":"Logging Utilities","text":"LogOptions","category":"page"},{"location":"modules/NeuralPDE/manual/logging/#NeuralPDE.LogOptions","page":"Logging Utilities","title":"NeuralPDE.LogOptions","text":"???\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/#Nonlinear-Diffusion","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"","category":"section"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"This function handles expressions of the form ðₙ(D(ðₘu)) where n,m > 0 and  D is a function of u i.e. they vary as u(x,t) and D(u). The expansion can be carried out via  general Leibniz rule.","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"A boundary condition operator bc is first operated on u resulting in a  boundary padded vector bc*u. Since D is a function of u, its discrete values  can be obtained at grid points once u has been padded.  After producing these  two functions in the grid range, we can expand the given expression via binomial expansion through the nonlinear_diffusion and  nonlinear_diffusion! functions and produce the final discretized derivatives.","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"(Image: Expressions for general Leibnuz rule with varying m)","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"The functions implicitly put the CenteredDifference operator  to use for computing derivates of various orders, e.g.  uᵏ = CenteredDifference(k,approx_order,dx,nknots)*u, helping us generate a symmetric discretization. The two functions differ in terms of memory allocation, since the non-! one will allocate memory to the output whereas the ! one can be used for non-allocating applications.","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/#Functions","page":"Nonlinear Diffusion","title":"Functions","text":"","category":"section"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"The two functions are as follows :","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"nonlinear_diffusion(second_differential_order::Int, first_differential_order::Int, approx_order::Int,\n                    p::AbstractVector{T}, q::AbstractVector{T}, dx::Union{T , AbstractVector{T} , Real},\n                    nknots::Int) where {T<:Real, N}\n\nnonlinear_diffusion!(du::AbstractVector{T}, second_differential_order::Int, first_differential_order::Int,\n                     approx_order::Int,p::AbstractVector{T}, q::AbstractVector{T},\n                     dx::Union{T , AbstractVector{T} , Real}, nknots::Int) where {T<:Real, N}","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"Arguments :","category":"page"},{"location":"modules/DiffEqOperators/nonlinear_derivatives/nonlinear_diffusion/","page":"Nonlinear Diffusion","title":"Nonlinear Diffusion","text":"du : an input AbstractVector similar to u, to store the final discretized expression.\nsecond_differential_order : the overall order of derivative on the expression.(n)\nfirst_differential_order : the inner order of derivative to discretize for u.(m)\napprox_order : the order of the discretization in terms of O(dx^order).\np : boundary padded D.\nq : boundary padded u obtained by bc*u.\ndx: spacing of the discretization. If dx is a Number, the discretization       is uniform. If dx is an array, then the discretization is non-uniform.\nnknots : the length of discretization in the direction of operator.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.jl-API","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"CurrentModule = Catalyst","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Reaction-Network-Generation-and-Representation","page":"Catalyst.jl API","title":"Reaction Network Generation and Representation","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Catalyst provides the @reaction_network macro for generating a complete network, stored as a ReactionSystem, which in turn is composed of Reactions. ReactionSystems can be converted to other ModelingToolkit.AbstractSystems, including a ModelingToolkit.ODESystem, ModelingToolkit.SDESystem, or ModelingToolkit.JumpSystem.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"An empty network can be generated using @reaction_network with no arguments (or one argument to name the system), or the make_empty_network function. These can then be extended programmatically using addspecies!, addparam!, and addreaction!.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"It is important to note for @reaction_network that any variable not declared to be a parameter after end will be treated as a chemical species of the system. i.e. in","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"rn = @reaction_network begin\n    k*X, Y --> W\nend k","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"X, Y and W will all be classified as chemical species.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"The ReactionSystem generated by the @reaction_network macro is a ModelingToolkit.AbstractSystem that symbolically represents a system of chemical reactions. In some cases it can be convenient to bypass the macro and directly generate a collection of Reactions and a corresponding ReactionSystem encapsulating them. Below we illustrate with a simple SIR example how a system can be directly constructed, and demonstrate how to then generate from the ReactionSystem and solve corresponding chemical reaction ODE models, chemical Langevin equation SDE models, and stochastic chemical kinetics jump process models.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"using Catalyst, OrdinaryDiffEq, StochasticDiffEq, JumpProcesses\n@parameters β γ t\n@variables S(t) I(t) R(t)\n\nrxs = [Reaction(β, [S,I], [I], [1,1], [2])\n       Reaction(γ, [I], [R])]\n@named rs = ReactionSystem(rxs, t)\n\nu₀map    = [S => 999.0, I => 1.0, R => 0.0]\nparammap = [β => 1/10000, γ => 0.01]\ntspan    = (0.0, 250.0)\n\n# solve as ODEs\nodesys = convert(ODESystem, rs)\noprob = ODEProblem(odesys, u₀map, tspan, parammap)\nsol = solve(oprob, Tsit5())\n\n# solve as SDEs\nsdesys = convert(SDESystem, rs)\nsprob = SDEProblem(sdesys, u₀map, tspan, parammap)\nsol = solve(sprob, EM(), dt=.01)\n\n# solve as jump process\njumpsys = convert(JumpSystem, rs)\nu₀map    = [S => 999, I => 1, R => 0]\ndprob = DiscreteProblem(jumpsys, u₀map, tspan, parammap)\njprob = JumpProblem(jumpsys, dprob, Direct())\nsol = solve(jprob, SSAStepper())","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"@reaction_network\nmake_empty_network\n@reaction\nReaction\nReactionSystem","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.@reaction_network","page":"Catalyst.jl API","title":"Catalyst.@reaction_network","text":"@reaction_network\n\nGenerates a ReactionSystem that encodes a chemical reaction network.\n\nSee The Reaction DSL documentation for details on parameters to the macro.\n\nExamples:\n\n# a basic SIR model, with name SIR\nsir_model = @reaction_network SIR begin\n    c1, s + i --> 2i\n    c2, i --> r\nend c1 c2\n\n# a basic SIR model, with random generated name\nsir_model = @reaction_network begin\n    c1, s + i --> 2i\n    c2, i --> r\nend c1 c2\n\n# an empty network with name empty\nemptyrn = @reaction_network empty\n\n# an empty network with random generated name\nemptyrn = @reaction_network\n\n\n\n\n\n","category":"macro"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.make_empty_network","page":"Catalyst.jl API","title":"Catalyst.make_empty_network","text":"make_empty_network(; iv=DEFAULT_IV, name=gensym(:ReactionSystem))\n\nConstruct an empty ReactionSystem. iv is the independent variable, usually time, and name is the name to give the ReactionSystem.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.@reaction","page":"Catalyst.jl API","title":"Catalyst.@reaction","text":"@reaction\n\nGenerates a single Reaction object.\n\nExamples:\n\nrx = @reaction k*v, A + B --> C + D\n\n# is equivalent to\n@parameters k v\n@variables t A(t) B(t) C(t) D(t)\nrx == Reaction(k*v, [A,B], [C,D])\n\nHere k and v will be parameters and A, B, C and D will be variables. Interpolation of existing parameters/variables also works\n\n@parameters k b\n@variables t A(t)\nex = k*A^2 + t\nrx = @reaction b*$ex*$A, $A --> C\n\nNotes:\n\nAny symbols arising in the rate expression that aren't interpolated are treated as parameters. In the reaction part (α*A + B --> C + D), coefficients are treated as parameters, e.g. α, and rightmost symbols as species, e.g. A,B,C,D.\nWorks with any single arrow types supported by @reaction_network.\nInterpolation of Julia variables into the macro works similar to the @reaction_network macro. See The Reaction DSL tutorial for more details.\n\n\n\n\n\n","category":"macro"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.Reaction","page":"Catalyst.jl API","title":"Catalyst.Reaction","text":"struct Reaction{S, T}\n\nOne chemical reaction.\n\nFields\n\nrate\nThe rate function (excluding mass action terms).\nsubstrates\nReaction substrates.\nproducts\nReaction products.\nsubstoich\nThe stoichiometric coefficients of the reactants.\nprodstoich\nThe stoichiometric coefficients of the products.\nnetstoich\nThe net stoichiometric coefficients of all species changed by the reaction.\nonly_use_rate\nfalse (default) if rate should be multiplied by mass action terms to give the rate law. true if rate represents the full reaction rate law.\n\nExamples\n\nusing Catalyst\n@parameters k[1:20]\n@variables t A(t) B(t) C(t) D(t)\nrxs = [Reaction(k[1], nothing, [A]),            # 0 -> A\n       Reaction(k[2], [B], nothing),            # B -> 0\n       Reaction(k[3],[A],[C]),                  # A -> C\n       Reaction(k[4], [C], [A,B]),              # C -> A + B\n       Reaction(k[5], [C], [A], [1], [2]),      # C -> A + A\n       Reaction(k[6], [A,B], [C]),              # A + B -> C\n       Reaction(k[7], [B], [A], [2], [1]),      # 2B -> A\n       Reaction(k[8], [A,B], [A,C]),            # A + B -> A + C\n       Reaction(k[9], [A,B], [C,D]),            # A + B -> C + D\n       Reaction(k[10], [A], [C,D], [2], [1,1]), # 2A -> C + D\n       Reaction(k[11], [A], [A,B], [2], [1,1]), # 2A -> A + B\n       Reaction(k[12], [A,B,C], [C,D], [1,3,4], [2, 3]),          # A+3B+4C -> 2C + 3D\n       Reaction(k[13], [A,B], nothing, [3,1], nothing),           # 3A+B -> 0\n       Reaction(k[14], nothing, [A], nothing, [2]),               # 0 -> 2A\n       Reaction(k[15]*A/(2+A), [A], nothing; only_use_rate=true), # A -> 0 with custom rate\n       Reaction(k[16], [A], [B]; only_use_rate=true),             # A -> B with custom rate.\n       Reaction(k[17]*A*exp(B), [C], [D], [2], [1]),              # 2C -> D with non constant rate.\n       Reaction(k[18]*B, nothing, [B], nothing, [2]),             # 0 -> 2B with non constant rate.\n       Reaction(k[19]*t, [A], [B]),                                # A -> B with non constant rate.\n       Reaction(k[20]*t*A, [B,C], [D],[2,1],[2])                  # 2A +B -> 2C with non constant rate.\n  ]\n\nNotes:\n\nnothing can be used to indicate a reaction that has no reactants or no products. In this case the corresponding stoichiometry vector should also be set to nothing.\nThe three-argument form assumes all reactant and product stoichiometric coefficients are one.\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.ReactionSystem","page":"Catalyst.jl API","title":"Catalyst.ReactionSystem","text":"struct ReactionSystem{U<:Union{Nothing, ModelingToolkit.AbstractSystem}, V<:Catalyst.NetworkProperties} <: AbstractTimeDependentSystem\n\nA system of chemical reactions.\n\nFields\n\neqs\nThe reactions defining the system.\niv\nIndependent variable (usually time).\nstates\nDependent (state) variables representing amount of each species. Must not contain the independent variable.\nps\nParameter variables. Must not contain the independent variable.\nvar_to_name\nMaps Symbol to corresponding variable.\nobserved\nEquations for observed variables.\nname\nThe name of the system\nsystems\nInternal sub-systems\ndefaults\nThe default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\nconnection_type\nType of the system\nconstraints\nNon-Reaction equations that further constrain the system\nnetworkproperties\nNetworkProperties object that can be filled in by API functions. INTERNAL – not considered part of the public API.\ncombinatoric_ratelaws\nSets whether to use combinatoric scalings in rate laws. true by default.\n\nExample\n\nContinuing from the example in the Reaction definition:\n\n# simple constructor that infers species and parameters\n@named rs = ReactionSystem(rxs, t)\n\n# allows specification of species and parameters\n@named rs = ReactionSystem(rxs, t, [A,B,C,D], k)\n\nKeyword Arguments:\n\nobserved::Vector{Equation}, equations specifying observed variables.\nsystems::Vector{AbstractSystems}, vector of sub-systems. Can be ReactionSystems, ODESystems, or NonlinearSystems.\nname::Symbol, the name of the system (must be provided, or @named must be used).\ndefaults::Dict, a dictionary mapping parameters to their default values and species to their default initial values.\nchecks = true, boolean for whether to check units.\nconstraints = nothing, a NonlinearSystem or ODESystem of coupled constraint equations.\nnetworkproperties = NetworkProperties(), cache for network properties calculated via API functions.\ncombinatoric_ratelaws = true, sets the default value of combinatoric_ratelaws used in calls to convert or calling various problem types with the ReactionSystem.\nbalanced_bc_check = true, sets whether to check that BC species appearing in reactions are balanced (i.e appear as both a substrate and a product with the same stoichiometry).\n\nNotes:\n\nReactionSystems currently do rudimentary unit checking, requiring that all species have the same units, and all reactions have rate laws with units of (species units) / (time units). Unit checking can be disabled by passing the keyword argument checks=false.\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit-and-Catalyst-Accessor-Functions","page":"Catalyst.jl API","title":"ModelingToolkit and Catalyst Accessor Functions","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"A ReactionSystem is an instance of a ModelingToolkit.AbstractTimeDependentSystem, and has a number of fields that can be accessed using the Catalyst API and the ModelingToolkit.jl Abstract System Interface. Below we overview these components.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"There are three basic sets of convenience accessors that will return information either from a top-level system, the top-level system and all sub-systems that are also ReactionSystems (i.e. the full reaction-network), or the top-level system, all subs-systems, and all constraint systems (i.e. the full model). To retrieve info from just a base ReactionSystem rn, ignoring sub-systems of rn, one can use the ModelingToolkit accessors (these provide direct access to the corresponding internal fields of the ReactionSystem)","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"get_states(rn) is a vector that collects all the species defined within rn.\nget_ps(rn) is a vector that collects all the parameters defined within reactions in rn.\nget_eqs(rn) is a vector that collects all the Reactions defined within rn.\nget_iv(rn) is the independent variable used in the system (usually t to represent time).\nget_systems(rn) is a vector of all sub-systems of rn.\nget_defaults(rn) is a dictionary of all the default values for parameters and species in rn.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"These are complemented by the Catalyst accessor","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Catalyst.get_constraints(sys) is the constraint system of rn. If none is defined will return nothing.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"The preceding accessors do not allocate, directly accessing internal fields of the ReactionSystem.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"To retrieve information from the full reaction network represented by a system rn, which corresponds to information within both rn and all sub-systems of type ReactionSystem, one can call:","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"species(rn) is a vector collecting all the chemical species within the system and any sub-systems that are also ReactionSystems.\nreactionparams(rn) is a vector of all the parameters within the system and any sub-systems that are also ReactionSystems. These include all parameters that appear within some Reaction.\nreactions(rn) is a vector of all the Reactions within the system and any sub-systems that are also ReactionSystems.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"These accessors will allocate unless there are no subsystems. In the latter case they are equivalent to the corresponding get_* functions.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Finally, as some sub-systems may be other system types, for example specifying algebraic constraints with a NonlinearSystem, it can also be convenient to collect all state variables (e.g. species and algebraic variables) and such. The following ModelingToolkit functions provide this information","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"ModelingToolkit.states(rn) returns all species and variables across the system, all sub-systems, and all constraint systems.\nModelingToolkit.parameters(rn) returns all parameters across the system, all sub-systems, and all constraint systems.\nModelingToolkit.equations(rn) returns all Reactions and all Equations defined across the system, all sub-systems, and all constraint systems.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"states and parameters should be assumed to always allocate, while equations will allocate unless there are no subsystems or constraint systems. In the latter case equations is equivalent to get_eqs.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Below we list the remainder of the Catalyst API accessor functions mentioned above.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Basic-System-Properties","page":"Catalyst.jl API","title":"Basic System Properties","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"See Programmatic Construction of Symbolic Reaction Systems for examples and ModelingToolkit and Catalyst Accessor Functions for more details on the basic accessor functions.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"species\nreactionparams\nreactions\nnumspecies\nnumreactions\nnumreactionparams\nspeciesmap\nparamsmap\nreactionparamsmap\nCatalyst.isconstant\nCatalyst.isbc","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.species","page":"Catalyst.jl API","title":"Catalyst.species","text":"species(network)\n\nGiven a ReactionSystem, return a vector of all species defined in the system and any subsystems that are of type ReactionSystem. To get the variables in the system and all subsystems, including non-ReactionSystem subsystems, uses states(network).\n\nNotes:\n\nIf ModelingToolkit.get_systems(network) is non-empty will allocate.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactionparams","page":"Catalyst.jl API","title":"Catalyst.reactionparams","text":"reactionparams(network)\n\nGiven a ReactionSystem, return a vector of all parameters defined within the system and any subsystems that are of type ReactionSystem. To get the parameters in the system and all subsystems, including non-ReactionSystem subsystems, use parameters(network).\n\nNotes:\n\nIf ModelingToolkit.get_systems(network) is non-empty will allocate.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactions","page":"Catalyst.jl API","title":"Catalyst.reactions","text":"reactions(network)\n\nGiven a ReactionSystem, return a vector of all Reactions in the system.\n\nNotes:\n\nIf ModelingToolkit.get_systems(network) is not empty, will allocate.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.numspecies","page":"Catalyst.jl API","title":"Catalyst.numspecies","text":"numspecies(network)\n\nReturn the total number of species within the given ReactionSystem and subsystems that are ReactionSystems.\n\nNotes\n\nIf there are no subsystems this will be fast.\nAs this calls species, it can be slow and will allocate if there are any subsystems.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.numreactions","page":"Catalyst.jl API","title":"Catalyst.numreactions","text":"numreactions(network)\n\nReturn the total number of reactions within the given ReactionSystem and subsystems that are ReactionSystems.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.numreactionparams","page":"Catalyst.jl API","title":"Catalyst.numreactionparams","text":"numreactionparams(network)\n\nReturn the total number of parameters within the given ReactionSystem and subsystems that are ReactionSystems.\n\nNotes\n\nIf there are no subsystems this will be fast.\nAs this calls reactionparams, it can be slow and will allocate if there are any subsystems.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.speciesmap","page":"Catalyst.jl API","title":"Catalyst.speciesmap","text":"speciesmap(network)\n\nGiven a ReactionSystem, return a Dictionary mapping species that participate in Reactions to their index within species(network).\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.paramsmap","page":"Catalyst.jl API","title":"Catalyst.paramsmap","text":"paramsmap(network)\n\nGiven a ReactionSystem, return a Dictionary mapping from all parameters that appear within the system to their index within parameters(network).\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactionparamsmap","page":"Catalyst.jl API","title":"Catalyst.reactionparamsmap","text":"reactionparamsmap(network)\n\nGiven a ReactionSystem, return a Dictionary mapping from parameters that appear within Reactions to their index within reactionparams(network).\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.isconstant","page":"Catalyst.jl API","title":"Catalyst.isconstant","text":"Catalyst.isconstant(s)\n\nTests if the given symbolic variable corresponds to a constant species.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.isbc","page":"Catalyst.jl API","title":"Catalyst.isbc","text":"Catalyst.isbc(s)\n\nTests if the given symbolic variable corresponds to a boundary condition species.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Basic-Reaction-Properties","page":"Catalyst.jl API","title":"Basic Reaction Properties","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"ismassaction\ndependents\ndependants\nsubstoichmat\nprodstoichmat\nnetstoichmat\nreactionrates","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.ismassaction","page":"Catalyst.jl API","title":"Catalyst.ismassaction","text":"ismassaction(rx, rs; rxvars = get_variables(rx.rate),\n                              haveivdep = any(var -> isequal(get_iv(rs),var), rxvars),\n                              stateset = Set(states(rs)))\n\nTrue if a given reaction is of mass action form, i.e. rx.rate does not depend on any chemical species that correspond to states of the system, and does not depend explicitly on the independent variable (usually time).\n\nArguments\n\nrx, the Reaction.\nrs, a ReactionSystem containing the reaction.\nOptional: rxvars, Variables which are not in rxvars are ignored as possible dependencies.\nOptional: haveivdep, true if the Reaction rate field explicitly depends on the independent variable.\nOptional: stateset, set of states which if the rxvars are within mean rx is non-mass action.\n\nNotes:\n\nNon-integer stoichiometry is treated as non-mass action. This includes symbolic variables/terms or floating point numbers for stoichiometric coefficients.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.dependents","page":"Catalyst.jl API","title":"Catalyst.dependents","text":"dependents(rx, network)\n\nGiven a Reaction and a ReactionSystem, return a vector of the non-constant species the reaction rate law depends on. e.g., for\n\nk*W, 2X + 3Y --> 5Z + W\n\nthe returned vector would be [W(t),X(t),Y(t)].\n\nNotes:\n\nAllocates\nDoes not check for dependents within any subsystems.\nConstant species are not considered dependents since they are internally treated as parameters.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.dependants","page":"Catalyst.jl API","title":"Catalyst.dependants","text":"dependents(rx, network)\n\nSee documentation for dependents.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.substoichmat","page":"Catalyst.jl API","title":"Catalyst.substoichmat","text":"substoichmat(rn; sparse=false)\n\nReturns the substrate stoichiometry matrix, S, with S_i j the stoichiometric coefficient of the ith substrate within the jth reaction.\n\nNote:\n\nSet sparse=true for a sparse matrix representation\nNote that constant species are not considered substrates, but just components that modify the associated rate law.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.prodstoichmat","page":"Catalyst.jl API","title":"Catalyst.prodstoichmat","text":"prodstoichmat(rn; sparse=false)\n\nReturns the product stoichiometry matrix, P, with P_i j the stoichiometric coefficient of the ith product within the jth reaction.\n\nNote:\n\nSet sparse=true for a sparse matrix representation\nNote that constant species are not treated as products, but just components that modify the associated rate law.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.netstoichmat","page":"Catalyst.jl API","title":"Catalyst.netstoichmat","text":"netstoichmat(rn, sparse=false)\n\nReturns the net stoichiometry matrix, N, with N_i j the net stoichiometric coefficient of the ith species within the jth reaction.\n\nNotes:\n\nSet sparse=true for a sparse matrix representation\nCaches the matrix internally within rn so subsequent calls are fast.\nNote that constant species are not treated as reactants, but just components that modify the associated rate law. As such they do not contribute to the net stoichiometry matrix.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactionrates","page":"Catalyst.jl API","title":"Catalyst.reactionrates","text":"reactionrates(network)\n\nGiven a ReactionSystem, returns a vector of the symbolic reaction rates for each reaction.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Functions-to-Extend-or-Modify-a-Network","page":"Catalyst.jl API","title":"Functions to Extend or Modify a Network","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"ReactionSystems can be programmatically extended using addspecies!, addparam!, addreaction!, @add_reactions, or composed using ModelingToolkit.extend and ModelingToolkit.compose.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"@add_reactions\naddspecies!\nreorder_states!\naddparam!\naddreaction!\nsetdefaults!\nModelingToolkit.extend\nModelingToolkit.compose\nCatalyst.flatten\nmerge!(network1::ReactionSystem, network2::ReactionSystem)","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.@add_reactions","page":"Catalyst.jl API","title":"Catalyst.@add_reactions","text":"@add_reactions\n\nAdds the reactions declared to a preexisting ReactionSystem. All parameters used in the added reactions need to be declared after the reactions.\n\nSee the Catalyst.jl for Reaction Network Modeling documentation for details on parameters to the macro.\n\n\n\n\n\n","category":"macro"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.addspecies!","page":"Catalyst.jl API","title":"Catalyst.addspecies!","text":"addspecies!(network::ReactionSystem, s::Symbolic; disablechecks=false)\n\nGiven a ReactionSystem, add the species corresponding to the variable s to the network (if it is not already defined). Returns the integer id of the species within the system.\n\nNotes:\n\ndisablechecks will disable checking for whether the passed in variable is already defined, which is useful when adding many new variables to the system. Do not disable checks unless you are sure the passed in variable is a new variable, as this will potentially leave the system in an undefined state.\n\n\n\n\n\naddspecies!(network::ReactionSystem, s::Num; disablechecks=false)\n\nGiven a ReactionSystem, add the species corresponding to the variable s to the network (if it is not already defined). Returns the integer id of the species within the system.\n\ndisablechecks will disable checking for whether the passed in variable is already defined, which is useful when adding many new variables to the system. Do not disable checks unless you are sure the passed in variable is a new variable, as this will potentially leave the system in an undefined state.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reorder_states!","page":"Catalyst.jl API","title":"Catalyst.reorder_states!","text":"reorder_states!(rn, neworder)\n\nGiven a ReactionSystem and a vector neworder, orders the states of rn accordingly to neworder.\n\nNotes:\n\nCurrently only supports ReactionSystems without constraints or subsystems.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.addparam!","page":"Catalyst.jl API","title":"Catalyst.addparam!","text":"addparam!(network::ReactionSystem, p::Symbolic; disablechecks=false)\n\nGiven a ReactionSystem, add the parameter corresponding to the variable p to the network (if it is not already defined). Returns the integer id of the parameter within the system.\n\ndisablechecks will disable checking for whether the passed in variable is already defined, which is useful when adding many new variables to the system. Do not disable checks unless you are sure the passed in variable is a new variable, as this will potentially leave the system in an undefined state.\n\n\n\n\n\naddparam!(network::ReactionSystem, p::Num; disablechecks=false)\n\nGiven a ReactionSystem, add the parameter corresponding to the variable p to the network (if it is not already defined). Returns the integer id of the parameter within the system.\n\ndisablechecks will disable checking for whether the passed in variable is already defined, which is useful when adding many new variables to the system. Do not disable checks unless you are sure the passed in variable is a new variable, as this will potentially leave the system in an undefined state.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.addreaction!","page":"Catalyst.jl API","title":"Catalyst.addreaction!","text":"addreaction!(network::ReactionSystem, rx::Reaction)\n\nAdd the passed in reaction to the ReactionSystem. Returns the integer id of rx in the list of Reactions within network.\n\nNotes:\n\nAny new species or parameters used in rx should be separately added to   network using addspecies! and addparam!.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.setdefaults!","page":"Catalyst.jl API","title":"Catalyst.setdefaults!","text":"setdefaults!(rn, newdefs)\n\nSets the default (initial) values of parameters and species in the ReactionSystem, rn.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nsetdefaults!(sir, [:S => 999.0, :I => 1.0, :R => 1.0, :β => 1e-4, :ν => .01])\n\n# or\n@parameter β ν\n@variables t S(t) I(t) R(t)\nsetdefaults!(sir, [S => 999.0, I => 1.0, R => 0.0, β => 1e-4, ν => .01])\n\ngives initial/default values to each of S, I and β\n\nNotes:\n\nCan not be used to set default values for species, variables or parameters of subsystems or constraint systems. Either set defaults for those systems directly, or flatten to collate them into one system before setting defaults.\nDefaults can be specified in any iterable container of symbols to value pairs or symbolics to value pairs.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.extend","page":"Catalyst.jl API","title":"ModelingToolkit.extend","text":"extend(sys::ModelingToolkit.AbstractSystem, basesys::ModelingToolkit.AbstractSystem; name) -> ReactionSystem\n\n\nextend the basesys with sys, the resulting system would inherit sys's name by default.\n\n\n\n\n\nModelingToolkit.extend(sys::Union{NonlinearSystem,ODESystem}, rs::ReactionSystem; name::Symbol=nameof(sys))\n\nExtends the indicated ReactionSystem with a ModelingToolkit.NonlinearSystem or ModelingToolkit.ODESystem, which will be stored internally as constraint equations.\n\nNotes:\n\nReturns a new ReactionSystem and does not modify rs.\nBy default, the new ReactionSystem will have the same name as sys.\n\n\n\n\n\nModelingToolkit.extend(sys::ReactionSystem, rs::ReactionSystem; name::Symbol=nameof(sys))\n\nExtends the indicated ReactionSystem with another ReactionSystem. Similar to calling merge! except constraint systems are allowed (and will also be merged together).\n\nNotes:\n\nReturns a new ReactionSystem and does not modify rs.\nBy default, the new ReactionSystem will have the same name as sys.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.compose","page":"Catalyst.jl API","title":"ModelingToolkit.compose","text":"compose(sys, systems; name)\n\n\ncompose multiple systems together. The resulting system would inherit the first system's name.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.flatten","page":"Catalyst.jl API","title":"ModelingToolkit.flatten","text":"Catalyst.flatten(rs::ReactionSystem)\n\nMerges all subsystems of the given ReactionSystem up into rs.\n\nNotes:\n\nReturns a new ReactionSystem that represents the flattened system.\nAll Reactions within subsystems are namespaced and merged into the list of Reactions of rs. The merged list is then available as reactions(rs) or get_eqs(rs).\nAll algebraic equations are merged into a NonlinearSystem or ODESystem stored as get_constraints(rs). If get_constraints !== nothing then the algebraic equations are merged with the current constraints in a system of the same type as the current constraints, otherwise the new constraint system is an ODESystem.\nCurrently only ReactionSystems, NonlinearSystems and ODESystems are supported as sub-systems when flattening.\nrs.networkproperties is reset upon flattening.\nThe default value of combinatoric_ratelaws will be the logical or of all ReactionSystems.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Base.merge!-Tuple{ReactionSystem, ReactionSystem}","page":"Catalyst.jl API","title":"Base.merge!","text":"merge!(network1::ReactionSystem, network2::ReactionSystem)\n\nMerge network2 into network1.\n\nNotes:\n\nDuplicate reactions between the two networks are not filtered out.\nReactions are not deepcopied to minimize allocations, so both networks will share underlying data arrays.\nSubsystems are not deepcopied between the two networks and will hence be shared.\nReturns network1.\ncombinatoric_ratelaws is the value of network1.\n\n\n\n\n\n","category":"method"},{"location":"modules/Catalyst/api/catalyst_api/#Network-Analysis-and-Representations","page":"Catalyst.jl API","title":"Network Analysis and Representations","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Note, currently API functions for network analysis and conservation law analysis do not work with constant species (currently only generated by SBMLToolkit).","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"conservationlaws\nconservedquantities\nconservedequations\nconservationlaw_constants\nReactionComplexElement\nReactionComplex\nreactioncomplexmap\nreactioncomplexes\nincidencemat\ncomplexstoichmat\ncomplexoutgoingmat\nincidencematgraph\nlinkageclasses\ndeficiency\nsubnetworks\nlinkagedeficiencies\nisreversible\nisweaklyreversible\nreset_networkproperties!","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.conservationlaws","page":"Catalyst.jl API","title":"Catalyst.conservationlaws","text":"conservationlaws(netstoichmat::AbstractMatrix)::Matrix\n\nGiven the net stoichiometry matrix of a reaction system, computes a matrix of conservation laws, each represented as a row in the output.\n\n\n\n\n\nconservationlaws(rs::ReactionSystem)\n\nReturn the conservation law matrix of the given ReactionSystem, calculating it if it is not already stored within the system, or returning an alias to it.\n\nNotes:\n\nThe first time being called it is calculated and cached in rn, subsequent calls should be fast.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.conservedquantities","page":"Catalyst.jl API","title":"Catalyst.conservedquantities","text":"conservedquantities(state, cons_laws)\n\nCompute conserved quantities for a system with the given conservation laws.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.conservedequations","page":"Catalyst.jl API","title":"Catalyst.conservedequations","text":"conservedequations(rn::ReactionSystem)\n\nCalculate symbolic equations from conservation laws, writing dependent variables as functions of independent variables and the conservation law constants.\n\nNotes:\n\nCaches the resulting equations in rn, so will be fast on subsequent calls.\n\nExamples:\n\nrn = @reaction_network begin\n    k, A + B --> C\n    k2, C --> A + B\n    end k k2\nconservedequations(rn)\n\ngives\n\n2-element Vector{Equation}:\n B(t) ~ A(t) + _ConLaw[1]\n C(t) ~ _ConLaw[2] - A(t)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.conservationlaw_constants","page":"Catalyst.jl API","title":"Catalyst.conservationlaw_constants","text":"conservationlaw_constants(rn::ReactionSystem)\n\nCalculate symbolic equations from conservation laws, writing the conservation law constants in terms of the dependent and independent variables.\n\nNotes:\n\nCaches the resulting equations in rn, so will be fast on subsequent calls.\n\nExamples:\n\nrn = @reaction_network begin\n    k, A + B --> C\n    k2, C --> A + B\n    end k k2\nconservationlaw_constants(rn)\n\ngives\n\n2-element Vector{Equation}:\n _ConLaw[1] ~ B(t) - A(t)\n _ConLaw[2] ~ A(t) + C(t)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.ReactionComplexElement","page":"Catalyst.jl API","title":"Catalyst.ReactionComplexElement","text":"struct ReactionComplexElement{T}\n\nOne reaction complex element\n\nFields\n\nspeciesid\nThe integer id of the species representing this element.\nspeciesstoich\nThe stoichiometric coefficient of this species.\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.ReactionComplex","page":"Catalyst.jl API","title":"Catalyst.ReactionComplex","text":"struct ReactionComplex{V<:Integer} <: AbstractArray{Catalyst.ReactionComplexElement{V<:Integer}, 1}\n\nOne reaction complex.\n\nFields\n\nspeciesids\nThe integer ids of all species participating in this complex.\nspeciesstoichs\nThe stoichiometric coefficients of all species participating in this complex.\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactioncomplexmap","page":"Catalyst.jl API","title":"Catalyst.reactioncomplexmap","text":"reactioncomplexmap(rn::ReactionSystem)\n\nFind each ReactionComplex within the specified system, constructing a mapping from the complex to vectors that indicate which reactions it appears in as substrates and products.\n\nNotes:\n\nEach ReactionComplex is mapped to a vector of pairs, with each pair having the form reactionidx => ± 1, where -1 indicates the complex appears as a substrate and +1 as a product in the reaction with integer label reactionidx.\nConstant species are ignored as part of a complex. i.e. if species A is constant then the reaction A + B --> C + D is considered to consist of the complexes B and C + D. Likewise A --> B would be treated as the same as 0 --> B.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reactioncomplexes","page":"Catalyst.jl API","title":"Catalyst.reactioncomplexes","text":"reactioncomplexes(network::ReactionSystem; sparse=false)\n\nCalculate the reaction complexes and complex incidence matrix for the given ReactionSystem.\n\nNotes:\n\nreturns a pair of a vector of ReactionComplexs and the complex incidence matrix.\nAn empty ReactionComplex denotes the null (∅) state (from reactions like ∅ -> A or A -> ∅).\nConstant species are ignored in generating a reaction complex. i.e. if A is constant then A –> B consists of the complexes ∅ and B.\nThe complex incidence matrix, B, is number of complexes by number of reactions with\n\nB_i j = begincases\n-1 textif the ith complex is the substrate of the jth reaction\n1 textif the ith complex is the product of the jth reaction\n0 textotherwise\nendcases\n\nSet sparse=true for a sparse matrix representation of the incidence matrix\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.incidencemat","page":"Catalyst.jl API","title":"Catalyst.incidencemat","text":"incidencemat(rn::ReactionSystem; sparse=false)\n\nCalculate the incidence matrix of rn, see reactioncomplexes.\n\nNotes:\n\nIs cached in rn so that future calls, assuming the same sparsity, will also be fast.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.complexstoichmat","page":"Catalyst.jl API","title":"Catalyst.complexstoichmat","text":"complexstoichmat(network::ReactionSystem; sparse=false)\n\nGiven a ReactionSystem and vector of reaction complexes, return a matrix with positive entries of size number of species by number of complexes, where the non-zero positive entries in the kth column denote stoichiometric coefficients of the species participating in the kth reaction complex.\n\nNotes:\n\nSet sparse=true for a sparse matrix representation\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.complexoutgoingmat","page":"Catalyst.jl API","title":"Catalyst.complexoutgoingmat","text":"complexoutgoingmat(network::ReactionSystem; sparse=false)\n\nGiven a ReactionSystem and complex incidence matrix, B, return a matrix of size num of complexes by num of reactions that identifies substrate complexes.\n\nNotes:\n\nThe complex outgoing matrix, Delta, is defined by\n\nDelta_i j = begincases\n    = 0    textif  B_i j = 1 \n    = B_i j textotherwise\nendcases\n\nSet sparse=true for a sparse matrix representation\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.incidencematgraph","page":"Catalyst.jl API","title":"Catalyst.incidencematgraph","text":"incidencematgraph(rn::ReactionSystem)\n\nConstruct a directed simple graph where nodes correspond to reaction complexes and directed edges to reactions converting between two complexes.\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\ncomplexes,incidencemat = reactioncomplexes(sir)\nincidencematgraph(sir)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.linkageclasses","page":"Catalyst.jl API","title":"Catalyst.linkageclasses","text":"linkageclasses(rn::ReactionSystem)\n\nGiven the incidence graph of a reaction network, return a vector of the connected components of the graph (i.e. sub-groups of reaction complexes that are connected in the incidence graph).\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\ncomplexes,incidencemat = reactioncomplexes(sir)\nlinkageclasses(sir)\n\ngives\n\n2-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.deficiency","page":"Catalyst.jl API","title":"Catalyst.deficiency","text":"deficiency(rn::ReactionSystem)\n\nCalculate the deficiency of a reaction network.\n\nHere the deficiency, delta, of a network with n reaction complexes, ell linkage classes and a rank s stoichiometric matrix is\n\ndelta = n - ell - s\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nrcs,incidencemat = reactioncomplexes(sir)\nδ = deficiency(sir)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.subnetworks","page":"Catalyst.jl API","title":"Catalyst.subnetworks","text":"subnetworks(rn::ReactionSystem)\n\nFind subnetworks corresponding to each linkage class of the reaction network.\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\ncomplexes,incidencemat = reactioncomplexes(sir)\nsubnetworks(sir)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.linkagedeficiencies","page":"Catalyst.jl API","title":"Catalyst.linkagedeficiencies","text":"linkagedeficiencies(network::ReactionSystem)\n\nCalculates the deficiency of each sub-reaction network within network.\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nrcs,incidencemat = reactioncomplexes(sir)\nlinkage_deficiencies = linkagedeficiencies(sir)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.isreversible","page":"Catalyst.jl API","title":"Catalyst.isreversible","text":"isreversible(rn::ReactionSystem)\n\nGiven a reaction network, returns if the network is reversible or not.\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nrcs,incidencemat = reactioncomplexes(sir)\nisreversible(sir)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.isweaklyreversible","page":"Catalyst.jl API","title":"Catalyst.isweaklyreversible","text":"isweaklyreversible(rn::ReactionSystem, subnetworks)\n\nDetermine if the reaction network with the given subnetworks is weakly reversible or not.\n\nNotes:\n\nRequires the incidencemat to already be cached in rn by a previous call to reactioncomplexes.\n\nFor example,\n\nsir = @reaction_network SIR begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nrcs,incidencemat = reactioncomplexes(sir)\nsubnets = subnetworks(rn)\nisweaklyreversible(rn, subnets)\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.reset_networkproperties!","page":"Catalyst.jl API","title":"Catalyst.reset_networkproperties!","text":"reset_networkproperties!(rn::ReactionSystem)\n\nClears the cache of various properties (like the netstoichiometry matrix). Use if such properties need to be recalculated for some reason.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Network-Comparison","page":"Catalyst.jl API","title":"Network Comparison","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"==(rn1::Reaction, rn2::Reaction)\nisequal_ignore_names\n==(rn1::ReactionSystem, rn2::ReactionSystem)","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Base.:==-Tuple{Reaction, Reaction}","page":"Catalyst.jl API","title":"Base.:==","text":"==(rx1::Reaction, rx2::Reaction)\n\nTests whether two Reactions are identical.\n\nNotes:\n\nIgnores the order in which stoichiometry components are listed.\nDoes not currently simplify rates, so a rate of A^2+2*A+1 would be   considered different than (A+1)^2.\n\n\n\n\n\n","category":"method"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.isequal_ignore_names","page":"Catalyst.jl API","title":"Catalyst.isequal_ignore_names","text":"isequal_ignore_names(rn1::ReactionSystem, rn2::ReactionSystem)\n\nTests whether the underlying species, parameters and reactions are the same in the two ReactionSystems. Ignores the names of the systems in testing equality.\n\nNotes:\n\nDoes not currently simplify rates, so a rate of A^2+2*A+1 would be   considered different than (A+1)^2.\nDoes not include defaults in determining equality.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Base.:==-Tuple{ReactionSystem, ReactionSystem}","page":"Catalyst.jl API","title":"Base.:==","text":"==(rn1::ReactionSystem, rn2::ReactionSystem)\n\nTests whether the underlying species, parameters and reactions are the same in the two ReactionSystems. Requires the systems to have the same names too.\n\nNotes:\n\nDoes not currently simplify rates, so a rate of A^2+2*A+1 would be   considered different than (A+1)^2.\nDoes not include defaults in determining equality.\n\n\n\n\n\n","category":"method"},{"location":"modules/Catalyst/api/catalyst_api/#Network-Visualization","page":"Catalyst.jl API","title":"Network Visualization","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Latexify can be used to convert networks to LaTeX mhchem equations by","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"using Latexify\nlatexify(rn)","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"If Graphviz is installed and commandline accessible, it can be used to create and save network diagrams using Graph and savegraph.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Graph\ncomplexgraph\nsavegraph","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.Graph","page":"Catalyst.jl API","title":"Catalyst.Graph","text":"Graph(rn::ReactionSystem)\n\nConverts a ReactionSystem into a Graphviz graph. Reactions correspond to small green circles, and species to blue circles.\n\nNotes:\n\nBlack arrows from species to reactions indicate reactants, and are labelled with their input stoichiometry.\nBlack arrows from reactions to species indicate products, and are labelled with their output stoichiometry.\nRed arrows from species to reactions indicate that species is used within the rate expression. For example, in the reaction k*A, B --> C, there would be a red arrow from A to the reaction node. In k*A, A+B --> C, there would be red and black arrows from A to the reaction node.\nRequires the Graphviz jll to be installed, or Graphviz to be installed and commandline accessible.\n\n\n\n\n\n","category":"type"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.complexgraph","page":"Catalyst.jl API","title":"Catalyst.complexgraph","text":"complexgraph(rn::ReactionSystem; complexdata=reactioncomplexes(rn))\n\nCreates a Graphviz graph of the ReactionComplexs in rn. Reactions correspond to arrows and reaction complexes to blue circles. \n\nNotes:\n\nBlack arrows from complexes to complexes indicate reactions whose rate is a parameter or a Number. i.e. k, A --> B.\nRed dashed arrows from complexes to complexes indicate reactions whose rate depends on species. i.e. k*C, A --> B for C a species.\nRequires the Graphviz jll to be installed, or Graphviz to be installed and commandline accessible.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.savegraph","page":"Catalyst.jl API","title":"Catalyst.savegraph","text":"savegraph(g::Graph, fname, fmt=\"png\")\n\nGiven a Graph generated by Graph, save the graph to the file with name fname and extension fmt.\n\nNotes:\n\nfmt=\"png\" is the default output format.\nRequires the Graphviz jll to be installed, or Graphviz to be installed and commandline accessible.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Rate-Laws","page":"Catalyst.jl API","title":"Rate Laws","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"As the underlying ReactionSystem is comprised of ModelingToolkit expressions, one can directly access the generated rate laws, and using ModelingToolkit tooling generate functions or Julia Exprs from them.","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"oderatelaw\njumpratelaw\nmm\nmmr\nhill\nhillr\nhillar","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.oderatelaw","page":"Catalyst.jl API","title":"Catalyst.oderatelaw","text":"oderatelaw(rx; combinatoric_ratelaw=true)\n\nGiven a Reaction, return the symbolic reaction rate law used in generated ODEs for the reaction. Note, for a reaction defined by\n\nk*X*Y, X+Z --> 2X + Y\n\nthe expression that is returned will be k*X(t)^2*Y(t)*Z(t). For a reaction of the form\n\nk, 2X+3Y --> Z\n\nthe expression that is returned will be k * (X(t)^2/2) * (Y(t)^3/6).\n\nNotes:\n\nAllocates\ncombinatoric_ratelaw=true uses factorial scaling factors in calculating the   rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S^2/2!. If   combinatoric_ratelaw=false then the ratelaw is k*S^2, i.e. the scaling   factor is ignored.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.jumpratelaw","page":"Catalyst.jl API","title":"Catalyst.jumpratelaw","text":"jumpratelaw(rx; combinatoric_ratelaw=true)\n\nGiven a Reaction, return the symbolic reaction rate law used in generated stochastic chemical kinetics model SSAs for the reaction. Note, for a reaction defined by\n\nk*X*Y, X+Z --> 2X + Y\n\nthe expression that is returned will be k*X^2*Y*Z. For a reaction of the form\n\nk, 2X+3Y --> Z\n\nthe expression that is returned will be k * binomial(X,2) * binomial(Y,3).\n\nNotes:\n\nAllocates\ncombinatoric_ratelaw=true uses binomials in calculating the rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S*(S-1)/2. If combinatoric_ratelaw=false then the ratelaw is k*S*(S-1), i.e. the rate law is not normalized by the scaling factor.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.mm","page":"Catalyst.jl API","title":"Catalyst.mm","text":"mm(X,v,K) = v*X / (X + K)\n\nA Michaelis-Menten rate function.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.mmr","page":"Catalyst.jl API","title":"Catalyst.mmr","text":"mmr(X,v,K) = v*K / (X + K)\n\nA repressive Michaelis-Menten rate function.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.hill","page":"Catalyst.jl API","title":"Catalyst.hill","text":"hill(X,v,K,n) = v*(X^n) / (X^n + K^n)\n\nA Hill rate function.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.hillr","page":"Catalyst.jl API","title":"Catalyst.hillr","text":"hillr(X,v,K,n) = v*(K^n) / (X^n + K^n)\n\nA repressive Hill rate function.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.hillar","page":"Catalyst.jl API","title":"Catalyst.hillar","text":"hillar(X,Y,v,K,n) = v*(X^n) / (X^n + Y^n + K^n)\n\nAn activation/repressing Hill rate function.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Transformations","page":"Catalyst.jl API","title":"Transformations","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"Base.convert\nModelingToolkit.structural_simplify","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Base.convert","page":"Catalyst.jl API","title":"Base.convert","text":"convert(_, f)\n\n\nConverts a NonlinearFunction into a ODEFunction.\n\n\n\n\n\nBase.convert(::Type{<:ODESystem},rs::ReactionSystem)\n\nConvert a ReactionSystem to an ModelingToolkit.ODESystem.\n\nKeyword args and default values:\n\ncombinatoric_ratelaws=true uses factorial scaling factors in calculating the rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S^2/2!. Set combinatoric_ratelaws=false for a ratelaw of k*S^2, i.e. the scaling factor is ignored. Defaults to the value given when the ReactionSystem was constructed (which itself defaults to true).\nremove_conserved=false, if set to true will calculate conservation laws of the underlying set of reactions (ignoring constraint equations), and then apply them to reduce the number of equations.\n\n\n\n\n\nBase.convert(::Type{<:NonlinearSystem},rs::ReactionSystem)\n\nConvert a ReactionSystem to an ModelingToolkit.NonlinearSystem.\n\nKeyword args and default values:\n\ncombinatoric_ratelaws=true uses factorial scaling factors in calculating the rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S^2/2!. Set combinatoric_ratelaws=false for a ratelaw of k*S^2, i.e. the scaling factor is ignored. Defaults to the value given when the ReactionSystem was constructed (which itself defaults to true).\nremove_conserved=false, if set to true will calculate conservation laws of the underlying set of reactions (ignoring constraint equations), and then apply them to reduce the number of equations.\n\n\n\n\n\nBase.convert(::Type{<:SDESystem},rs::ReactionSystem)\n\nConvert a ReactionSystem to an ModelingToolkit.SDESystem.\n\nNotes:\n\ncombinatoric_ratelaws=true uses factorial scaling factors in calculating the rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S^2/2!. Set combinatoric_ratelaws=false for a ratelaw of k*S^2, i.e. the scaling factor is ignored. Defaults to the value given when the ReactionSystem was constructed (which itself defaults to true).\nnoise_scaling=nothing::Union{Vector{Num},Num,Nothing} allows for linear scaling of the noise in the chemical Langevin equations. If nothing is given, the default value as in Gillespie 2000 is used. Alternatively, a Num can be given, this is added as a parameter to the system (at the end of the parameter array). All noise terms are linearly scaled with this value. The parameter may be one already declared in the ReactionSystem. Finally, a Vector{Num} can be provided (the length must be equal to the number of reactions). Here the noise for each reaction is scaled by the corresponding parameter in the input vector. This input may contain repeat parameters.\nremove_conserved=false, if set to true will calculate conservation laws of the underlying set of reactions (ignoring constraint equations), and then apply them to reduce the number of equations.\n\n\n\n\n\nBase.convert(::Type{<:JumpSystem},rs::ReactionSystem; combinatoric_ratelaws=true)\n\nConvert a ReactionSystem to an ModelingToolkit.JumpSystem.\n\nNotes:\n\ncombinatoric_ratelaws=true uses binomials in calculating the rate law, i.e. for 2S -> 0 at rate k the ratelaw would be k*S*(S-1)/2. If combinatoric_ratelaws=false then the ratelaw is k*S*(S-1), i.e. the rate law is not normalized by the scaling factor. Defaults to the value given when the ReactionSystem was constructed (which itself defaults to true).\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.structural_simplify","page":"Catalyst.jl API","title":"ModelingToolkit.structural_simplify","text":"structural_simplify(sys)\nstructural_simplify(sys, io; simplify, kwargs...)\n\n\nStructurally simplify algebraic equations in a system and compute the topological sort of the observed equations. When simplify=true, the simplify function will be applied during the tearing process. It also takes kwargs allow_symbolic=false and allow_parameter=true which limits the coefficient types during tearing.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Unit-Validation","page":"Catalyst.jl API","title":"Unit Validation","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"validate(rx::Reaction; info::String = \"\")\nvalidate(rs::ReactionSystem, info::String=\"\")","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.validate-Tuple{Reaction}","page":"Catalyst.jl API","title":"ModelingToolkit.validate","text":"validate(rx::Reaction; info::String = \"\")\n\nCheck that all substrates and products within the given Reaction have the same units, and that the units of the reaction's rate expression are internally consistent (i.e. if the rate involves sums, each term in the sum has the same units).\n\n\n\n\n\n","category":"method"},{"location":"modules/Catalyst/api/catalyst_api/#ModelingToolkit.validate","page":"Catalyst.jl API","title":"ModelingToolkit.validate","text":"validate(rs::ReactionSystem, info::String=\"\")\n\nCheck that all species in the ReactionSystem have the same units, and that the rate laws of all reactions reduce to units of (species units) / (time units).\n\nNotes:\n\nDoes not check subsystems too.\n\n\n\n\n\n","category":"function"},{"location":"modules/Catalyst/api/catalyst_api/#Utility-Functions","page":"Catalyst.jl API","title":"Utility Functions","text":"","category":"section"},{"location":"modules/Catalyst/api/catalyst_api/","page":"Catalyst.jl API","title":"Catalyst.jl API","text":"symmap_to_varmap","category":"page"},{"location":"modules/Catalyst/api/catalyst_api/#Catalyst.symmap_to_varmap","page":"Catalyst.jl API","title":"Catalyst.symmap_to_varmap","text":"symmap_to_varmap(sys, symmap)\n\nGiven a system and map of Symbols to values, generates a map from corresponding symbolic variables/parameters to the values that can be used to pass initial conditions and parameter mappings.\n\nFor example,\n\nsir = @reaction_network sir begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν\nsubsys = @reaction_network subsys begin\n    k, A --> B\nend k\n@named sys = compose(sir, [subsys])\n\ngives\n\nModel sys with 3 equations\nStates (5):\n  S(t)\n  I(t)\n  R(t)\n  subsys₊A(t)\n  subsys₊B(t)\nParameters (3):\n  β\n  ν\n  subsys₊k\n\nto specify initial condition and parameter mappings from symbols we can use\n\nsymmap = [:S => 1.0, :I => 1.0, :R => 1.0, :subsys₊A => 1.0, :subsys₊B => 1.0]\nu0map  = symmap_to_varmap(sys, symmap)\npmap   = symmap_to_varmap(sys, [:β => 1.0, :ν => 1.0, :subsys₊k => 1.0])\n\nu0map and pmap can then be used as input to various problem types.\n\nNotes:\n\nAny Symbol, sym, within symmap must be a valid field of sys. i.e. sys.sym must be defined.\n\n\n\n\n\n","category":"function"},{"location":"modules/Surrogates/gramacylee/#Gramacy-and-Lee-Function","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"","category":"section"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Gramacy & Lee Function is a continuous function. It is not convex. The function is defined on 1-dimensional space. It is an unimodal. The function can be defined on any input domain but it is usually evaluated on x in -05 25.","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"The Gramacy & Lee is as follows: f(x) = fracsin(10pi x)2x + (x-1)^4.","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's import these two packages Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's define our objective function:","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"function gramacylee(x)\n    term1 = sin(10*pi*x) / 2*x;\n    term2 = (x - 1)^4;\n    y = term1 + term2;\nend","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Let's sample f in 25 points between -0.5 and 2.5 using the sample function. The sampling points are chosen using a Sobol Sample, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"n = 25\nlower_bound = -0.5\nupper_bound = 2.5\nx = sample(n, lower_bound, upper_bound, SobolSample())\ny = gramacylee.(x)\nxs = lower_bound:0.001:upper_bound\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"Now, let's fit Gramacy & Lee Function with different Surrogates:","category":"page"},{"location":"modules/Surrogates/gramacylee/","page":"Gramacy & Lee Function","title":"Gramacy & Lee Function","text":"my_pol = PolynomialChaosSurrogate(x, y, lower_bound, upper_bound)\nloba_1 = LobachevskySurrogate(x, y, lower_bound, upper_bound)\nkrig = Kriging(x, y, lower_bound, upper_bound)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound), ylims=(-5, 20), legend=:top)\nplot!(xs, gramacylee.(xs), label=\"True function\", legend=:top)\nplot!(xs, my_pol.(xs), label=\"Polynomial expansion\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"highlevels/symbolic_analysis/#SciML-Symbolic-Analysis-Libraries","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/#StructuralIdentifiability.jl:-Identifiability-Analysis-Made-Simple","page":"SciML Symbolic Analysis Libraries","title":"StructuralIdentifiability.jl: Identifiability Analysis Made Simple","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"Performing parameter estimation from a data set means attempting to recover parameters like reaction rates by fitting some model to the data. But how do you know whether you have enough data to even consider getting the \"correct\" parameters back?  StructuralIdentifiability.jl allows for running a structural identifiability analysis on a given model to determine whether it's theoretically possible to recover the correct parameters. It can state whether a given type of output data can be used to globally recover the parameters (i.e. only a unique parameter set for the model produces a given output), whether the parameters are only locally identifiable (i.e. there are finitely many parameter sets which could generate the seen data), or whether it's unidentifiable (there are infinitely many parameters which generate the same output data).","category":"page"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"For more information on what StructuralIdentifiability.jl is all about, see the SciMLCon 2022 tutorial video.","category":"page"},{"location":"highlevels/symbolic_analysis/#SymbolicNumericIntegration.jl:-Symbolic-Integration-via-Numerical-Methods","page":"SciML Symbolic Analysis Libraries","title":"SymbolicNumericIntegration.jl: Symbolic Integration via Numerical Methods","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"SymbolicNumericIntegration.jl is a package computing the solution to symbolic integration problem using numerical methods (numerical integration mixed with sparse regression).","category":"page"},{"location":"highlevels/symbolic_analysis/#JuliaSymbolics","page":"SciML Symbolic Analysis Libraries","title":"JuliaSymbolics","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"JuliaSymbolics is a sister organization of SciML. It spawned out of the symbolic modeling tools being developed within SciML  (ModelingToolkit.jl) to become its own organization dedicated to building a fully-featured Julia-based Computer Algebra System (CAS). As such, the two organizations are closely aligned in terms of its developer community and many of the SciML libraries use Symbolics.jl extensively.","category":"page"},{"location":"highlevels/symbolic_analysis/#Symbolics.jl:-The-Computer-Algebra-System-(CAS)-of-the-Julia-Programming-Language","page":"SciML Symbolic Analysis Libraries","title":"Symbolics.jl: The Computer Algebra System (CAS) of the Julia Programming Language","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"Symbolics.jl is the CAS of the Julia programming language. If something needs to be done symbolically, most likely Symbolics.jl is the answer.","category":"page"},{"location":"highlevels/symbolic_analysis/#MetaTheory.jl:-E-Graphs-to-Automate-Symbolic-Transformations","page":"SciML Symbolic Analysis Libraries","title":"MetaTheory.jl: E-Graphs to Automate Symbolic Transformations","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"Metatheory.jl is a library for defining e-graph rewriters for use on the common symbolic interface. This can be used to do all sorts of analysis and code transformations, such as improving code performance, numerical stability, and more. See Automated Code Optimization with E-Graphs for more details.","category":"page"},{"location":"highlevels/symbolic_analysis/#SymbolicUtils.jl:-Define-Your-Own-Computer-Algebra-System","page":"SciML Symbolic Analysis Libraries","title":"SymbolicUtils.jl: Define Your Own Computer Algebra System","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"SymbolicUtils.jl is the underlying utility library and rule-based rewriting language on which Symbolics.jl is developed. Symbolics.jl is standardized type and rule definitions built using SymbolicUtils.jl. However, if non-standard types are required, such as symbolic computing over Fock algebras, then SymbolicUtils.jl is the library from which the new symbolic types can be implemented.","category":"page"},{"location":"highlevels/symbolic_analysis/#Third-Party-Libraries-to-Note","page":"SciML Symbolic Analysis Libraries","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/#SIAN.jl:-Structural-Identifiability-Analyzer","page":"SciML Symbolic Analysis Libraries","title":"SIAN.jl: Structural Identifiability Analyzer","text":"","category":"section"},{"location":"highlevels/symbolic_analysis/","page":"SciML Symbolic Analysis Libraries","title":"SciML Symbolic Analysis Libraries","text":"SIAN.jl is a structural identifiability analysis package which uses an entirely different algorithm from StructuralIdentifiability.jl. For information on the differences bewteen the two approaches, see  the Structural Identifiability Tools in Julia tutoral.","category":"page"},{"location":"modules/DiffEqDocs/analysis/bifurcation/#Bifurcation-Analysis","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"Bifurcation analysis on DifferentialEquations.jl types can be performed by:","category":"page"},{"location":"modules/DiffEqDocs/analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"BifurcationKit.jl (currently the most comprehensive and activately maintained package)\nBifurcations.jl\nPyDSTool.jl (no longer recommended)","category":"page"},{"location":"modules/DiffEqDocs/analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"BifurcationKit has integration with ODEProblem for some functionality (like computing periodic orbits via shooting). If oprob is an ODEProblem, one can also just pass oprob.f.f and oprob.f.jac to BifurcationKit methods as needed. Bifurcations.jl can directly generate a BifurcationProblem from an ODEProblem. ","category":"page"},{"location":"modules/ExponentialUtilities/expv/#Expv:-Matrix-Exponentials-Times-Vectors","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"","category":"section"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"The main functionality of ExponentialUtilities is the computation of matrix-phi-vector products. The phi functions are defined as","category":"page"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"ϕ_0(z) = exp(z)\nϕ_(k+1)(z) = (ϕ_k(z) - 1) / z","category":"page"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"In exponential algorithms, products in the form of ϕ_m(tA)b is frequently encountered. Instead of computing the matrix function first and then computing the matrix-vector product, the common alternative is to construct a Krylov subspace K_m(A,b) and then approximate the matrix-phi-vector product.","category":"page"},{"location":"modules/ExponentialUtilities/expv/#Support-for-matrix-free-operators","page":"Expv: Matrix Exponentials Times Vectors","title":"Support for matrix-free operators","text":"","category":"section"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"You can use any object as the \"matrix\" A as long as it implements the following linear operator interface:","category":"page"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"Base.eltype(A)\nBase.size(A, dim)\nLinearAlgebra.mul!(y, A, x) (for computing y = A * x in place).\nLinearAlgebra.opnorm(A, p=Inf). If this is not implemented or the default implementation can be slow, you can manually pass in the operator norm (a rough estimate is fine) using the keyword argument opnorm.\nLinearAlgebra.ishermitian(A). If this is not implemented or the default implementation can be slow, you can manually pass in the value using the keyword argument ishermitian.","category":"page"},{"location":"modules/ExponentialUtilities/expv/#Core-API","page":"Expv: Matrix Exponentials Times Vectors","title":"Core API","text":"","category":"section"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"expv\nphiv\nexpv!\nphiv!\nexp_timestep\nphiv_timestep\nexp_timestep!\nphiv_timestep!\nphi","category":"page"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.expv","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.expv","text":"expv(t,A,b; kwargs) -> exp(tA)b\n\nCompute the matrix-exponential-vector product using Krylov.\n\nA Krylov subspace is constructed using arnoldi and exp! is called on the Hessenberg matrix. Consult arnoldi for the values of the keyword arguments. An alternative algorithm, where an error estimate generated on-the-fly is used to terminate the Krylov iteration, can be employed by setting the kwarg mode=:error_estimate.\n\nexpv(t,Ks; cache) -> exp(tA)b\n\nCompute the expv product using a pre-constructed Krylov subspace.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.phiv","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.phiv","text":"phiv(t,A,b,k;correct,kwargs) -> [phi_0(tA)b phi_1(tA)b ... phi_k(tA)b][, errest]\n\nCompute the matrix-phi-vector products using Krylov. k >= 1.\n\nThe phi functions are defined as\n\nvarphi_0(z) = exp(z)quad varphi_k+1(z) = fracvarphi_k(z) - 1z\n\nA Krylov subspace is constructed using arnoldi and phiv_dense is called on the Hessenberg matrix. If correct=true, then phi0 through phik-1 are updated using the last Arnoldi vector v_m+1 [1]. If errest=true then an additional error estimate for the second-to-last phi is also returned. For the additional keyword arguments, consult arnoldi.\n\nphiv(t,Ks,k;correct,kwargs) -> [phi0(tA)b phi1(tA)b ... phi_k(tA)b][, errest]\n\nCompute the matrix-phi-vector products using a pre-constructed Krylov subspace.\n\n[1]: Niesen, J., & Wright, W. (2009). A Krylov subspace algorithm for evaluating\n\nthe φ-functions in exponential integrators. arXiv preprint arXiv:0907.4631. Formula (10).\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.expv!","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.expv!","text":"expv!(w,t,Ks[;cache]) -> w\n\nNon-allocating version of expv that uses precomputed Krylov subspace Ks.\n\n\n\n\n\nexpv!(w, t, A, b, Ks, cache)\n\nAlternative interface for calculating the action of exp(t*A) on the vector b, storing the result in w. The Krylov iteration is terminated when an error estimate for the matrix exponential in the generated subspace is below the requested tolerance. Ks is a KrylovSubspace and typeof(cache)<:HermitianSubspaceCache, the exact type decides which algorithm is used to compute the subspace exponential.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.phiv!","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.phiv!","text":"phiv!(w,t,Ks,k[;cache,correct,errest]) -> w[,errest]\n\nNon-allocating version of 'phiv' that uses precomputed Krylov subspace Ks.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.phiv_timestep","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.phiv_timestep","text":"phiv_timestep(ts,A,B[;adaptive,tol,kwargs...]) -> U\n\nEvaluates the linear combination of phi-vector products using time stepping\n\nu = varphi_0(tA)b_0 + tvarphi_1(tA)b_1 + cdots + t^pvarphi_p(tA)b_p\n\nts is an array of time snapshots for u, with U[:,j] ≈ u(ts[j]). ts can also be just one value, in which case only the end result is returned and U is a vector.\n\nThe time stepping formula of Niesen & Wright is used [1]. If the time step tau is not specified, it is chosen according to (17) of Neisen & Wright. If adaptive==true, the time step and Krylov subsapce size adaptation scheme of Niesen & Wright is used, the relative tolerance of which can be set using the keyword parameter tol. The delta and gamma parameter of the adaptation scheme can also be adjusted.\n\nSet verbose=true to print out the internal steps (for debugging). For the other keyword arguments, consult arnoldi and phiv, which are used internally.\n\n[1]: Niesen, J., & Wright, W. (2009). A Krylov subspace algorithm for\n\nevaluating the φ-functions in exponential integrators. arXiv preprint arXiv:0907.4631.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.phiv_timestep!","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.phiv_timestep!","text":"phiv_timestep!(U,ts,A,B[;kwargs]) -> U\n\nNon-allocating version of phiv_timestep.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#ExponentialUtilities.phi","page":"Expv: Matrix Exponentials Times Vectors","title":"ExponentialUtilities.phi","text":"phi(z,k[;cache]) -> [phi_0(z),phi_1(z),...,phi_k(z)]\n\nCompute the scalar phi functions for all orders up to k.\n\nThe phi functions are defined as\n\nvarphi_0(z) = exp(z)quad varphi_k+1(z) = fracvarphi_k(z) - 1z\n\nInstead of using the recurrence relation, which is numerically unstable, a formula given by Sidje is used (Sidje, R. B. (1998). Expokit: a software package for computing matrix exponentials. ACM Transactions on Mathematical Software (TOMS), 24(1), 130-156. Theorem 1).\n\n\n\n\n\nphi(A,k[;cache]) -> [phi_0(A),phi_1(A),...,phi_k(A)]\n\nCompute the matrix phi functions for all orders up to k. k >= 1.\n\nThe phi functions are defined as\n\nvarphi_0(z) = exp(z)quad varphi_k+1(z) = fracvarphi_k(z) - 1z\n\nCalls phiv_dense on each of the basis vectors to obtain the answer. If A is Diagonal, instead calls the scalar phi on each diagonal element and the return values are also Diagonals\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/expv/#Caches","page":"Expv: Matrix Exponentials Times Vectors","title":"Caches","text":"","category":"section"},{"location":"modules/ExponentialUtilities/expv/","page":"Expv: Matrix Exponentials Times Vectors","title":"Expv: Matrix Exponentials Times Vectors","text":"ExpvCache\nPhivCache","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/#Regression-Method","page":"Regression Method","title":"Regression Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"struct RegressionGSA <: GSAMethod\n    rank::Bool = false\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"RegressionGSA has the following keyword arguments:","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"rank: flag which determines whether to calculate the rank coefficients. Defaults to false.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"It returns a RegressionGSAResult, which contains the pearson, standard_regression, and partial_correlation coefficients, described below. If rank is true, then it also contains the ranked versions of these coefficients. Note that the ranked version of the pearson coefficient is also known as the Spearman coefficient, which is returned here as the pearson_rank coefficient.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"For multi-variable models, the coefficient for the X_i input variable relating to the Y_j output variable is given as the [i, j] entry in the corresponding returned matrix.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/#Regression-Details","page":"Regression Method","title":"Regression Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"It is possible to fit a linear model explaining the behavior of Y given the values of X, provided that the sample size n is sufficiently large (at least n > d).","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"The measures provided for this analysis by us in GlobalSensitivity.jl are","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"a) Pearson Correlation Coefficient:","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"r = fracsum_i=1^n (x_i - overlinex)(y_i - overliney)sqrtsum_i=1^n (x_i - overlinex)^2(y_i - overliney)^2","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"b) Standard Regression Coefficient (SRC):","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"SRC_j = beta_j sqrtfracVar(X_j)Var(Y)","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"where beta_j is the linear regression coefficient associated to X_j. This is also known as a sigma-normalized derivative.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"c) Partial Correlation Coefficient (PCC):","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"PCC_j = rho(X_j - hatX_-jY_j - hatY_-j)","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"where hatX_-j is the prediction of the linear model, expressing X_j with respect to the other inputs and hatY_-j is the prediction of the linear model where X_j is absent. PCC measures the sensitivity of Y to X_j when the effects of the other inputs have been canceled.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"If rank is set to true, then the rank coefficients are also calculated.","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/#API","page":"Regression Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"function gsa(f, method::RegressionGSA, p_range::AbstractVector; samples::Int = 1000, batch::Bool = false, kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/regression/#Example","page":"Regression Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/regression/","page":"Regression Method","title":"Regression Method","text":"using GlobalSensitivity\n\nfunction linear_batch(X)\n    A= 7\n    B= 0.1\n    @. A*X[1,:]+B*X[2,:]\nend\nfunction linear(X)\n    A= 7\n    B= 0.1\n    A*X[1]+B*X[2]\nend\n\np_range = [[-1, 1], [-1, 1]]\nreg = gsa(linear_batch, RegressionGSA(), p_range; batch = true)\n\nreg = gsa(linear, RegressionGSA(), p_range; batch = false)\nreg = gsa(linear, RegressionGSA(true), p_range; batch = false) #with rank coefficients","category":"page"},{"location":"modules/Surrogates/BraninFunction/#Branin-Function","page":"Branin function","title":"Branin Function","text":"","category":"section"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"The Branin Function is commonly used as a test function for metamodelling in computer experiments, especially in the context of optimization.","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"The expression of the Branin Function is given as: f(x) = (x_2 - frac514pi^2x_1^2 + frac5pix_1 - 6)^2 + 10(1-frac18pi)cos(x_1) + 10","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"where x = (x_1 x_2) with -5leq x_1 leq 10 0 leq x_2 leq 15","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"First of all we will import these two packages Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's define our objective function:","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"function branin(x)\r\n      x1 = x[1]\r\n      x2 = x[2]\r\n      b = 5.1 / (4*pi^2);\r\n      c = 5/pi;\r\n      r = 6;\r\n      a = 1;\r\n      s = 10;\r\n      t = 1 / (8*pi);\r\n      term1 = a * (x2 - b*x1^2 + c*x1 - r)^2;\r\n      term2 = s*(1-t)*cos(x1);\r\n      y = term1 + term2 + s;\r\nend","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's plot it:","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"n_samples = 80\r\nlower_bound = [-5, 0]\r\nupper_bound = [10,15]\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = branin.(xys);\r\nx, y = -5:10, 0:15\r\np1 = surface(x, y, (x1,x2) -> branin((x1,x2)))\r\nxs = [xy[1] for xy in xys]\r\nys = [xy[2] for xy in xys]\r\nscatter!(xs, ys, zs)\r\np2 = contour(x, y, (x1,x2) -> branin((x1,x2)))\r\nscatter!(xs, ys)\r\nplot(p1, p2, title=\"True function\")","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Now it's time to try fitting different surrogates and then we will plot them. We will have a look at the kriging surrogate Kriging Surrogate. :","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"kriging_surrogate = Kriging(xys, zs, lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> kriging_surrogate([x y]))\r\nscatter!(xs, ys, zs, marker_z=zs)\r\np2 = contour(x, y, (x, y) -> kriging_surrogate([x y]))\r\nscatter!(xs, ys, marker_z=zs)\r\nplot(p1, p2, title=\"Kriging Surrogate\")","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, we will have a look on Inverse Distance Surrogate:","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"InverseDistance = InverseDistanceSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> InverseDistance([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Inverse Distance Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Now, let's talk about Lobachevsky Surrogate:","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.8,2.8], n=8)","category":"page"},{"location":"modules/Surrogates/BraninFunction/","page":"Branin function","title":"Branin function","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Lobachevsky Surrogate\") # hide","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#plot","page":"Plot Functions","title":"Plot Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/#Standard-Plots-Using-the-Plot-Recipe","page":"Plot Functions","title":"Standard Plots Using the Plot Recipe","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Plotting functionality is provided by recipes to Plots.jl. To plot solutions, simply call the plot(type) after importing Plots.jl and the plotter will generate appropriate plots.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"#]add Plots # You need to install Plots.jl before your first time using it!\nusing Plots\nplot(sol) # Plots the solution","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Many of the types defined in the DiffEq universe, such as ODESolution, ConvergenceSimulation WorkPrecision, etc. have plot recipes to handle the default plotting behavior. Plots can be customized using all of the keyword arguments provided by Plots.jl. For example, we can change the plotting backend to the GR package and put a title on the plot by doing:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"gr()\nplot(sol,title=\"I Love DiffEqs!\")","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Then to save the plot, use savefig, for example:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"savefig(\"myplot.png\")","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Density","page":"Plot Functions","title":"Density","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"If the problem was solved with dense=true, then denseplot controls whether to use the dense function for generating the plot, and plotdensity is the number of evenly-spaced points (in time) to plot. For example:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,denseplot=false)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"means \"only plot the points which the solver stepped to\", while:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,plotdensity=1000)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"means to plot 1000 points using the dense function (since denseplot=true by default).","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#plot_vars","page":"Plot Functions","title":"Choosing Variables","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"In the plot command, one can choose the variables to be plotted in each plot. The master form is:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(f1,0,1), (f2,1,3), (f3,4,5)]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"which could be used to plot f1(var₀, var₁), f2(var₁, var₃), and f3(var₄, var₅), all on the same graph. (0 is considered to be time, or the independent variable). Functions f1, f2 and f3 should take in scalars and return a tuple. If no function is given, for example,","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(0,1), (1,3), (4,5)]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"this would mean \"plot var₁(t) vs t (time), var₃(var₁) vs var₁, and var₅(var₄) vs var₄ all on the same graph, putting the independent variables (t, var₁ and var₄) on the x-axis.\" While this can be used for everything, the following conveniences are provided:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Everywhere in a tuple position where we only find an integer, this variable is plotted as a function of time.  For example, the list above is equivalent to:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [1, (1,3), (4,5)]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"and","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [1, 3, 4]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is the most concise way to plot the variables 1, 3, and 4 as a function of time.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"It is possible to omit the list if only one plot is wanted: (2,3) and 4 are respectively equivalent to [(2,3)] and [(0,4)].\nA tuple containing one or several lists will be expanded by associating corresponding elements of the lists with each other:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = ([1,2,3], [4,5,6])","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is equivalent to","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(1,4), (2,5), (3,6)]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"and","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = (1, [2,3,4])","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is equivalent to","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(1,2), (1,3), (1,4)]","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Instead of using integers, one can use the symbols from a ParameterizedFunction. For example, vars=(:x,:y) will replace the symbols with the integer values for components :x and :y.\nn-dimensional groupings are allowed. For example, (1,2,3,4,5) would be a 5-dimensional plot between the associated variables.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Complex-Numbers-and-High-Dimensional-Plots","page":"Plot Functions","title":"Complex Numbers and High Dimensional Plots","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"The recipe library DimensionalPlotRecipes.jl is provided for extra functionality on high dimensional numbers (complex numbers) and other high dimensional plots. See the README for more details on the extra controls that exist.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Timespan","page":"Plot Functions","title":"Timespan","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"A plotting timespan can be chosen by the tspan argument in plot. For example:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,tspan=(0.0,40.0))","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"only plots between t=0.0 and t=40.0. If denseplot=true these bounds will be respected exactly. Otherwise the first point inside and last point inside the interval will be plotted, i.e. no points outside the interval will be plotted.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Example","page":"Plot Functions","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"using DifferentialEquations, Plots\nfunction lorenz(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\nu0 = [1., 5., 10.]\ntspan = (0., 100.)\np = (10.0,28.0,8/3)\nprob = ODEProblem(lorenz, u0, tspan,p)\nsol = solve(prob)\nxyzt = plot(sol, plotdensity=10000,lw=1.5)\nxy = plot(sol, plotdensity=10000, vars=(1,2))\nxz = plot(sol, plotdensity=10000, vars=(1,3))\nyz = plot(sol, plotdensity=10000, vars=(2,3))\nxyz = plot(sol, plotdensity=10000, vars=(1,2,3))\nplot(plot(xyzt,xyz),plot(xy, xz, yz, layout=(1,3),w=1), layout=(2,1))","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: lorenz_plot)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"An example using the functions:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"f(x,y,z) = (sqrt(x^2+y^2+z^2),x)\nplot(sol,vars=(f,1,2,3))","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: norm_plot)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"or the norm over time:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"f(t,x,y,z) = (t,sqrt(x^2+y^2+z^2))\nplot(sol,vars=(f,0,1,2,3))","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: normtime plot)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Animations","page":"Plot Functions","title":"Animations","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Using the iterator interface over the solutions, animations can also be generated via the animate(sol) command. One can choose the filename to save to via animate(sol,filename), while the frames per second fps and the density of steps to show every can be specified via keyword arguments. The rest of the arguments will be directly passed to the plot recipe to be handled as normal. For example, we can animate our solution with a larger line-width which saves every 4th frame via:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"#]add ImageMagick # You may need to install ImageMagick.jl before your first time using it!\n#using ImageMagick # Some installations require using ImageMagick for good animations\nanimate(sol,lw=3,every=4)","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Please see Plots.jl's documentation for more information on the available attributes.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/#Plotting-Without-the-Plot-Recipe","page":"Plot Functions","title":"Plotting Without the Plot Recipe","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"What if you don't want to use Plots.jl? Odd choice, but that's okay! If the differential equation was described by a vector of values, then the solution object acts as an AbstractMatrix sol[i,j] for the ith variable at timepoint j. You can use this to plot solutions. For example, in PyPlot, Gadfly, GR, etc., you can do the following to plot the timeseries:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol.t,sol')","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"since these plot along the columns, and sol' has the timeseries along the column. Phase plots can be done similarly, for example:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol[i,:],sol[j,:],sol[k,:])","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is a 3d phase plot between variables i, j, and k.","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Notice that this does not use the interpolation. When not using the plot recipe, the interpolation must be done manually. For example:","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"n = 101 #number of timepoints\nts = range(0, stop=1, length=n)\nplot(sol(ts,idxs=i),sol(ts,idxs=j),sol(ts,idxs=k))","category":"page"},{"location":"modules/DiffEqDocs/basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is the phase space using values 0.01 apart in time.","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/#The-Reaction-DSL-Advanced","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"This section covers some of the more advanced syntax and features for building chemical reaction network models (still not very complicated!).","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/#User-defined-functions-in-reaction-rates","page":"The Reaction DSL - Advanced","title":"User defined functions in reaction rates","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"The reaction network DSL can \"see\" user defined functions that work with ModelingToolkit. E.g., this is should work","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"myHill(x) = 2.0*x^3/(x^3+1.5^3)\nrn = @reaction_network begin\n  myHill(X), ∅ → X\nend","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"In some cases, it may be necessary or desirable to register functions with Symbolics.jl before their use in Catalyst, see the discussion here.","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/#Ignoring-mass-action-kinetics","page":"The Reaction DSL - Advanced","title":"Ignoring mass action kinetics","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"While generally one wants the reaction rate to use the law of mass action, so the reaction","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"rn = @reaction_network begin\n  k, X → ∅\nend k","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"occurs at the rate dXdt = -kX, it is possible to ignore this by using any of the following non-filled arrows when declaring the reaction: ⇐, ⟽, ⇒, ⟾, ⇔, ⟺. This means that the reaction","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"rn = @reaction_network begin\n  k, X ⇒ ∅\nend k","category":"page"},{"location":"modules/Catalyst/tutorials/advanced/","page":"The Reaction DSL - Advanced","title":"The Reaction DSL - Advanced","text":"will occur at rate dXdt = -k (which might become a problem since X will be degraded at a constant rate even when very small or equal to 0).","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/#Simultaneous-Fitting-of-Multiple-Neural-Networks","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"In many cases users are interested in fitting multiple neural networks or parameters simultaneously. This tutorial addresses how to perform this kind of study.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"The following is a fully working demo on the Fitzhugh-Nagumo ODE:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"using Lux, DiffEqFlux, Optimization, OptimizationOptimJL, DifferentialEquations, Random\n\nrng = Random.default_rng()\nRandom.seed!(rng,1)\n\nfunction fitz(du,u,p,t)\n  v,w = u\n  a,b,τinv,l = p\n  du[1] = v - v^3/3 -w + l\n  du[2] = τinv*(v +  a - b*w)\nend\n\np_ = Float32[0.7,0.8,1/12.5,0.5]\nu0 = [1f0;1f0]\ntspan = (0f0,10f0)\nprob = ODEProblem(fitz,u0,tspan,p_)\nsol = solve(prob, Tsit5(), saveat = 0.5 )\n\n# Ideal data\nX = Array(sol)\nXₙ = X + Float32(1e-3)*randn(eltype(X), size(X))  #noisy data\n\n# For xz term\nNN_1 = Lux.Chain(Lux.Dense(2, 16, tanh), Lux.Dense(16, 1))\np1,st1 = Lux.setup(rng, NN_1)\n\n# for xy term\nNN_2 = Lux.Chain(Lux.Dense(3, 16, tanh), Lux.Dense(16, 1))\np2, st2 = Lux.setup(rng, NN_2)\nscaling_factor = 1f0\n\np1 = Lux.ComponentArray(p1)\np2 = Lux.ComponentArray(p2)\n\np = Lux.ComponentArray{eltype(p1)}()\np = Lux.ComponentArray(p;p1)\np = Lux.ComponentArray(p;p2)\np = Lux.ComponentArray(p;scaling_factor)\n\nfunction dudt_(u,p,t)\n    v,w = u\n    z1 = NN_1([v,w], p.p1, st1)[1]\n    z2 = NN_2([v,w,t], p.p2, st2)[1]\n    [z1[1],p.scaling_factor*z2[1]]\nend\nprob_nn = ODEProblem(dudt_,u0, tspan, p)\nsol_nn = solve(prob_nn, Tsit5(),saveat = sol.t)\n\nfunction predict(θ)\n    Array(solve(prob_nn, Vern7(), p=θ, saveat = sol.t,\n                         abstol=1e-6, reltol=1e-6,\n                         sensealg = InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\n# No regularisation right now\nfunction loss(θ)\n    pred = predict(θ)\n    sum(abs2, Xₙ .- pred), pred\nend\nloss(p)\nconst losses = []\ncallback(θ,l,pred) = begin\n    push!(losses, l)\n    if length(losses)%50==0\n        println(losses[end])\n    end\n    false\nend\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, p)\nres1_uode = Optimization.solve(optprob, ADAM(0.01), callback=callback, maxiters = 500)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1_uode.u)\nres2_uode = Optimization.solve(optprob2, BFGS(), maxiters = 10000, callback = callback)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"The key is that Optimization.solve acts on a single parameter vector p. Thus what we do here is concatenate all of the parameters into a single ComponentVector p and then train on this parameter vector. Whenever we need to evaluate the neural networks, we dereference the vector and grab the key that corresponds to the neural network. For example, the p1 portion is p.p1, which is why the first neural network's evolution is written like NN_1([v,w], p.p1).","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/multiple_nn/","page":"Simultaneous Fitting of Multiple Neural Networks","title":"Simultaneous Fitting of Multiple Neural Networks","text":"This method is flexible to use with many optimizers and in fairly optimized ways. We can also see with the scaling_factor that we can grab parameters directly out of the vector and use them as needed.","category":"page"},{"location":"modules/SciMLSensitivity/#SciMLSensitivity:-Automatic-Differentiation-and-Adjoints-for-(Differential)-Equation-Solvers","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"SciMLSensitivity.jl is the automatic differentiation and adjoints system for the SciML ecosystem. Also known as local sensitivity analysis, these methods allow for calculation of fast derivatives of SciML problem types which are commonly used to analyze model sensitivities, callibrate models to data, train neural ODEs, perform automated model discovery via universal differential equations, and more. SciMLSensitivity.jl is a high level interface that pulls together all of the tools with heuristics and helper functions to make solving inverse problems and inferring models as easy as possible without losing efficiency.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Thus, what SciMLSensitivity.jl provides is:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Automatic differentiation overloads for improving the performance and flexibility of AD calls over solve.\nA bunch of tutorials, documentation, and test cases for this combination with parameter estimation (data fitting / model calibration), neural network  libraries and GPUs.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"note: Note\nThis documentation assumes familiarity with the solver packages for the respective problem types. If one is not familiar with the solver packages, please consult the documentation for pieces like DifferentialEquations.jl,  NonlinearSolve.jl,  LinearSolve.jl, etc. first.","category":"page"},{"location":"modules/SciMLSensitivity/#High-Level-Interface:-sensealg","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"High Level Interface: sensealg","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"The highest level interface is provided by the function solve:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"solve(prob,args...;sensealg=InterpolatingAdjoint(),\n      checkpoints=sol.t,kwargs...)","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"solve is fully compatible with automatic differentiation libraries like:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Zygote.jl\nReverseDiff.jl\nTracker.jl\nForwardDiff.jl","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"and will automatically replace any calculations of the solution's derivative with a fast method. The keyword argument sensealg controls the dispatch to the AbstractSensitivityAlgorithm used for the sensitivity calculation. Note that solve in an AD context does not allow higher order interpolations unless sensealg=DiffEqBase.SensitivityADPassThrough() is used, i.e. going back to the AD mechanism.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"note: Note\nThe behavior of ForwardDiff.jl is different from the other automatic differentiation libraries mentioned above. The sensealg keyword is ignored. Instead, the differential equations are solved using Dual numbers for u0 and p. If only p is perturbed in the sensitivity analysis, but not u0, the state is still implemented as a Dual number. ForwardDiff.jl will thus not dispatch into continuous forward nor adjoint sensitivity analysis even if a sensealg is provided.","category":"page"},{"location":"modules/SciMLSensitivity/#Equation-Scope","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"Equation Scope","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"SciMLSensitivity.jl supports all of the equation types of the  SciML Common Interface, extending the problem types by adding overloads for automatic differentiation to improve the performance and flexibility of the differentiation system. This includes:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Linear systems (LinearProblem)\nDirect methods for dense and sparse\nIterative solvers with preconditioning\nNonlinear Systems (NonlinearProblem)\nSystems of nonlinear equations\nScalar bracketing systems\nIntegrals (quadrature) (QuadratureProblem)\nDifferential Equations\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (DEProblems with callbacks)\nOptimization (OptimizationProblem)\nNonlinear (constrained) optimization\n(Stochastic/Delay/Differential-Algebraic) Partial Differential Equations (PDESystem)\nFinite difference and finite volume methods\nInterfaces to finite element methods\nPhysics-Informed Neural Networks (PINNs)\nIntegro-Differential Equations\nFractional Differential Equations","category":"page"},{"location":"modules/SciMLSensitivity/#SciMLSensitivity-and-Universal-Differential-Equations","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity and Universal Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"SciMLSensitivity is for universal differential equations, where these can include delays, physical constraints, stochasticity, events, and all other kinds of interesting behavior that shows up in scientific simulations. Neural networks can be all or part of the model. They can be around the differential equation, in the cost function, or inside of the differential equation. Neural networks representing unknown portions of the model or functions can go anywhere you have uncertainty in the form of the scientific simulator. Forward sensitivity and adjoint equations are automatically generated with checkpointing and stabilization to ensure it works for large stiff equations, while specializations on static objects allows for high efficiency on small equations. For an overview of the topic with applications, consult the paper Universal Differential Equations for Scientific Machine Learning.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"You can efficiently use the package for:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Parameter estimation of scientific models (ODEs, SDEs, DDEs, DAEs, etc.)\nNeural ODEs, Neural SDE, Neural DAEs, Neural DDEs, etc.\nNonlinear optimal control, including training neural controllers\n(Stiff) universal ordinary differential equations (universal ODEs)\nUniversal stochastic differential equations (universal SDEs)\nUniversal delay differential equations (universal DDEs)\nUniversal partial differential equations (universal PDEs)\nUniversal jump stochastic differential equations (universal jump diffusions)\nHybrid universal differential equations (universal DEs with event handling)","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"with high order, adaptive, implicit, GPU-accelerated, Newton-Krylov, etc. methods. For examples, please refer to the DiffEqFlux release blog post (which we try to keep updated for changes to the libraries). Additional demonstrations, like neural PDEs and neural jump SDEs, can be found at this blog post (among many others!). All of these features are only part of the advantage, as this library routinely benchmarks orders of magnitude faster than competing libraries like torchdiffeq. Use with GPUs is highly optimized by recompiling the solvers to GPUs to remove all CPU-GPU data transfers, while use with CPUs uses specialized kernels for accelerating differential equation solves.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Many different training techniques are supported by this package, including:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Optimize-then-discretize (backsolve adjoints, checkpointed adjoints, quadrature adjoints)\nDiscretize-then-optimize (forward and reverse mode discrete sensitivity analysis)\nThis is a generalization of ANODE and ANODEv2 to all DifferentialEquations.jl ODE solvers\nHybrid approaches (adaptive time stepping + AD for adaptive discretize-then-optimize)\nO(1) memory backprop of ODEs via BacksolveAdjoint, and Virtual Brownian Trees for O(1) backprop of SDEs\nContinuous adjoints for integral loss functions\nProbabilistic programming and variational inference on ODEs/SDEs/DAEs/DDEs/hybrid equations etc. is provided by integration with Turing.jl and Gen.jl. Reproduce variational loss functions by plugging composible libraries together.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"all while mixing forward mode and reverse mode approaches as appropriate for the most speed. For more details on the adjoint sensitivity analysis methods for computing fast gradients, see the adjoints details page.","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"With this package, you can explore various ways to integrate the two methodologies:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Neural networks can be defined where the “activations” are nonlinear functions described by differential equations\nNeural networks can be defined where some layers are ODE solves\nODEs can be defined where some terms are neural networks\nCost functions on ODEs can define neural networks","category":"page"},{"location":"modules/SciMLSensitivity/#Note-on-Modularity-and-Composability-with-Solvers","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"Note on Modularity and Composability with Solvers","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Note that SciMLSensitivity.jl purely built on composable and modular infrastructure.  SciMLSensitivity provides high level helper functions and documentation for the user, but the code generation stack is modular and composes in many different ways. For example, one can use and swap out the ODE solver between any common interface compatible library, like:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"Sundials.jl\nOrdinaryDiffEq.jl\nLSODA.jl\nIRKGaussLegendre.jl\nSciPyDiffEq.jl\n... etc. many other choices!","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"In addition, due to the composability of the system, none of the components are directly tied to the Flux.jl machine learning framework. For example, you can use SciMLSensitivity.jl to generate TensorFlow graphs and train the neural network with TensorFlow.jl, use PyTorch arrays via Torch.jl, and more all with single line code changes by utilizing the underlying code generation. The tutorials shown here are thus mostly a guide on how to use the ecosystem as a whole, only showing a small snippet of the possible ways to compose the thousands of differentiable libraries together! Swap out ODEs for SDEs, DDEs, DAEs, etc., put quadrature libraries or  Tullio.jl in the loss function, the world is your  oyster!","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"As a proof of composability, note that the implementation of Bayesian neural ODEs required zero code changes to the library, and instead just relied on the composability with other Julia packages.","category":"page"},{"location":"modules/SciMLSensitivity/#Citation","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"Citation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"If you use SciMLSensitivity.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"modules/SciMLSensitivity/","page":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","title":"SciMLSensitivity.jl: Automatic Differentiation and Adjoints for (Differential) Equation Solvers","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"modules/StructuralIdentifiability/utils/util/#Other-Helpful-Functions","page":"Other Utilities","title":"Other Helpful Functions","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/util/","page":"Other Utilities","title":"Other Utilities","text":"Pages=[\"util.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/util/","page":"Other Utilities","title":"Other Utilities","text":"Modules = [StructuralIdentifiability]\nPages   = [\"util.jl\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.decompose_derivative-Tuple{String, Array{String}}","page":"Other Utilities","title":"StructuralIdentifiability.decompose_derivative","text":"decompose_derivative(varname, prefixes)\n\nDetermines if it is possible to represent the varname as a_number where a is an element of prefixes\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.eval_at_dict-Union{Tuple{P}, Tuple{P, Dict{P, <:AbstractAlgebra.RingElem}}} where P<:AbstractAlgebra.MPolyElem","page":"Other Utilities","title":"StructuralIdentifiability.eval_at_dict","text":"eval_at_dict(f, d)\n\nEvaluates a polynomial/rational function on a dictionary of type var => val and missing values are replaced with zeroes\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.extract_coefficients-Union{Tuple{P}, Tuple{P, Vector{P}}} where P<:AbstractAlgebra.MPolyElem","page":"Other Utilities","title":"StructuralIdentifiability.extract_coefficients","text":"extract_coefficients(poly, variables)\n\nIntput:\n\npoly - multivariate polynomial\nvariables - a list of variables from the generators of the ring of p\n\nOutput:\n\ndictionary with keys being tuples of length lenght(variables) and values being polynomials in the variables other than those which are the coefficients at the corresponding monomials (in a smaller polynomial ring)\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.make_substitution-Union{Tuple{P}, NTuple{4, P}} where P<:AbstractAlgebra.MPolyElem","page":"Other Utilities","title":"StructuralIdentifiability.make_substitution","text":"make_substitution(f, var_sub, val_numer, val_denom)\n\nSubstitute a variable in a polynomial with an expression\n\nInput:\n\nf - the polynomial\nvar_sub - the variable to be substituted\nvar_numer - numerator of the substitution expression\nvar_denom - denominator of the substitution expression\n\nOutput:\n\npolynomial - result of substitution\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.parent_ring_change-Tuple{AbstractAlgebra.MPolyElem, AbstractAlgebra.MPolyRing}","page":"Other Utilities","title":"StructuralIdentifiability.parent_ring_change","text":"parent_ring_change(poly, new_ring)\n\nConverts a polynomial to a different polynomial ring Input\n\npoly - a polynomial to be converted\nnew_ring - a polynomial ring such that every variable name appearing in poly appears among the generators\n\nOutput:\n\na polynomial in new_ring \"equal\" to poly\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.switch_ring-Tuple{AbstractAlgebra.MPolyElem, AbstractAlgebra.MPolyRing}","page":"Other Utilities","title":"StructuralIdentifiability.switch_ring","text":"switch_ring(v, ring)\n\nFor a variable v, returns a variable in ring with the same name\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/util/#StructuralIdentifiability.uncertain_factorization-Tuple{AbstractAlgebra.MPolyElem{Nemo.fmpq}}","page":"Other Utilities","title":"StructuralIdentifiability.uncertain_factorization","text":"uncertain_factorization(f)\n\nInput:\n\nf - polynomial with rational coefficients\n\nOutput: \n\nlist of pairs (div, certainty) where\ndiv's are divisors of f such that f is their product with certain powers\nif certainty is true, div is Q-irreducible\n\n\n\n\n\n","category":"method"},{"location":"modules/MethodOfLines/#index","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl","text":"","category":"section"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"MethodOfLines.jl is a package for automated finite difference discretization of symbolicaly-defined PDEs in N dimensions.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"It uses symbolic expressions for systems of partial differential equations as defined with ModelingToolkit.jl, and Interval from DomainSets.jl to define the space(time) over which the simulation runs.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"The package's handling is quite general, it is recommended to try out your system of equations and post an issue if you run in to trouble. If you want to solve it, we want to support it.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"Issues with questions on usage are also welcome as they help us improve the docs.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"See here for a full tutorial, involving the Brusselator equation.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"Allowable terms in the system include, but are not limited to","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"Advection\nDiffusion\nReaction\nNonlinear Diffusion\nSpherical laplacian\nAny Julia function of the symbolic parameters/dependant variables and other parameters in the environment that's defined on the whole domain. Note that more complicated functions may require registration with @register, see the ModelingToolkit.jl docs.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"Boundary conditions include, but are not limited to:","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"Dirichlet\nNeumann (can also include time derivative)\nRobin (can also include time derivative)\nPeriodic\nAny function, subject to the assumptions below","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"At the moment the centered difference, upwind difference, nonlinear laplacian and spherical laplacian schemes are implemented. If you know of a scheme with better stability or accuracy in any specific case, please post an issue with a link to a paper.","category":"page"},{"location":"modules/MethodOfLines/#limitations","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"Known Limitations","text":"","category":"section"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"At the moment the package is able to discretize almost any system, with some assumptions listed below","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"That the grid is cartesian.\nThat the equation is first order in time.\nBoundary conditions in time are supplied as initial conditions, not at the end of the simulation interal. If your system requires a final condition, please use a change of variables to rectify this. This is unlikely to change due to upstream constraints.\nIntergral equations are not supported.\nThat dependant variables always have the same argument signature, except in BCs.\nThat periodic boundary conditions are of the simple form u(t, x_min) ~ u(t, x_max), or the same with lhs and rhs reversed. Note that this generalises to higher dimensions.\nThat boundary conditions do not contain references to derivatives which are not in the direction of the boundary, except in time.\nThat initial conditions are of the form u(...) ~ ..., and don't reference the initial time derivative.\nThat simple derivative terms are purely of a dependant variable, for example Dx(u(t,x,y)) is allowed but Dx(u(t,x,y)*v(t,x,y)), Dx(u(t,x)+1) or Dx(f(u(t,x))) are not. As a workaround please expand such terms with the product rule and use the linearity of the derivative operator, or define a new auxiliary dependant variable by adding an equation for it like eqs = [Differential(x)(w(t,x))~ ... , w(t,x) ~ v(t,x)*u(t,x)], along with appropriate BCs/ICs. An exception to this is if the differential is a nonlinear or spherical laplacian, in which case only the innermost argument should be wrapped.\nThat odd order derivatives do not multiply or divide each other. A workaround is to wrap all but one derivative per term in an auxiliary variable, such as dxu(x, t) ~ Differential(x)(u(x, t)).","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"The performance hit from auxiliary variables should be negligable due to a structural simplification step.","category":"page"},{"location":"modules/MethodOfLines/","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","text":"If any of these limitations are a problem for you please post an issue and we will prioritize removing them. If you discover a limitation that isn't listed here, pleae post an issue with example code.","category":"page"},{"location":"modules/MethodOfLines/#If-you-have-any-usage-questions-or-feature-requests,-please-post-an-issue","page":"MethodOfLines.jl: Automated Finite Difference for Phyiscs-Informed Learning","title":"If you have any usage questions or feature requests, please post an issue","text":"","category":"section"},{"location":"modules/Surrogates/secondorderpoly/#Second-order-polynomial-tutorial","page":"SecondOrderPolynomial","title":"Second order polynomial tutorial","text":"","category":"section"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The square polynomial model can be expressed by: y = Xβ + ϵ Where X is the matrix of the linear model augmented by adding 2d columns, containing pair by pair product of variables and variables squared.","category":"page"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/secondorderpoly/#Sampling","page":"SecondOrderPolynomial","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"f = x -> 3*sin(x) + 10/x\nlb = 3.0\nub = 6.0\nn = 10\nx = sample(n,lb,ub,LowDiscrepancySample(2))\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub))\nplot!(f, label=\"True function\", xlims=(lb, ub))","category":"page"},{"location":"modules/Surrogates/secondorderpoly/#Building-the-surrogate","page":"SecondOrderPolynomial","title":"Building the surrogate","text":"","category":"section"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"sec = SecondOrderPolynomialSurrogate(x, y, lb, ub)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lb, ub))\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"modules/Surrogates/secondorderpoly/#Optimizing","page":"SecondOrderPolynomial","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"@show surrogate_optimize(f, SRBF(), lb, ub, sec, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lb, ub))\nplot!(sec, label=\"Surrogate function\",  xlims=(lb, ub))","category":"page"},{"location":"modules/Surrogates/secondorderpoly/","page":"SecondOrderPolynomial","title":"SecondOrderPolynomial","text":"The optimization method successfully found the minima.","category":"page"},{"location":"modules/Integrals/#Integrals.jl:-Unified-Integral-Approximation-Interface","page":"Home","title":"Integrals.jl: Unified Integral Approximation Interface","text":"","category":"section"},{"location":"modules/Integrals/","page":"Home","title":"Home","text":"Integrals.jl is a unified interface for the numerical approximation of integrals (quadrature) in Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping.","category":"page"},{"location":"modules/Integrals/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/Integrals/","page":"Home","title":"Home","text":"To install Integrals.jl, use the Julia package manager:","category":"page"},{"location":"modules/Integrals/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"Integrals\")","category":"page"},{"location":"modules/Integrals/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/Integrals/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/#Local-Identifiability-Tools","page":"Local Identifiability Tools","title":"Local Identifiability Tools","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/","page":"Local Identifiability Tools","title":"Local Identifiability Tools","text":"Pages=[\"local_identifiability.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/","page":"Local Identifiability Tools","title":"Local Identifiability Tools","text":"CurrentModule=StructuralIdentifiability","category":"page"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/","page":"Local Identifiability Tools","title":"Local Identifiability Tools","text":"StructuralIdentifiability.differentiate_solution\nStructuralIdentifiability.differentiate_output","category":"page"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/#StructuralIdentifiability.differentiate_solution","page":"Local Identifiability Tools","title":"StructuralIdentifiability.differentiate_solution","text":"differentiate_solution(ode, params, ic, inputs, prec)\n\nInput: \n\nthe same as for power_series_solutions\n\nOutput: \n\na tuple consisting of the power series solution and a dictionary of the form (u, v) => power series, where u is a state variable  v is a state or parameter, and the power series is the partial derivative of the function u w.r.t. v evaluated at the solution\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/utils/local_identifiability/#StructuralIdentifiability.differentiate_output","page":"Local Identifiability Tools","title":"StructuralIdentifiability.differentiate_output","text":"differentiate_output(ode, params, ic, inputs, prec)\n\nSimilar to differentiate_solution but computes partial derivatives of a prescribed outputs returns a dictionary of the form y_function => Dict(var => dy/dvar) where dy/dvar is the derivative of y_function with respect to var.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/#Custom-Component","page":"Custom Components","title":"Custom Component","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"In this tutorial the creation of a custom component is demonstrated via the Chua's circuit. The circuit is a simple circuit that shows chaotic behaviour.  Except for a non-linear resistor every other component already is part of ModelingToolkitStandardLibrary.Electrical.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"First we need to make some imports.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"using ModelingToolkit\nusing ModelingToolkitStandardLibrary.Electrical\nusing ModelingToolkitStandardLibrary.Electrical: OnePort\nusing OrdinaryDiffEq\nusing IfElse: ifelse\nusing Plots","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/#Custom-Component-2","page":"Custom Components","title":"Custom Component","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"Now the custom component can be defined. The Modelica implementation of the NonlinearResistor looks as follows:","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"model NonlinearResistor \"Chua's resistor\"\n  extends Interfaces.OnePort;\n\n  parameter SI.Conductance Ga \"conductance in inner voltage range\";\n  parameter SI.Conductance Gb \"conductance in outer voltage range\";\n  parameter SI.Voltage Ve \"inner voltage range limit\";\nequation \n  i = if (v < -Ve) then Gb*(v + Ve) - Ga*Ve else if (v > Ve) then Gb*(v - Ve) + Ga*Ve else Ga*v;\nend NonlinearResistor;","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"this can almost be directly translate it to the syntax of ModelingToolkit.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"@parameters t\n\nfunction NonlinearResistor(;name, Ga, Gb, Ve)\n    @named oneport = OnePort()\n    @unpack v, i = oneport\n    pars = @parameters Ga=Ga Gb=Gb Ve=Ve\n    eqs = [\n        i ~ ifelse(v < -Ve, \n                Gb*(v + Ve) - Ga*Ve, \n                ifelse(v > Ve, \n                    Gb*(v - Ve) + Ga*Ve, \n                    Ga*v,\n                ),\n            )\n    ]\n    extend(ODESystem(eqs, t, [], pars; name=name), oneport)\nend","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/#Explanation","page":"Custom Components","title":"Explanation","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"All components in ModelingToolkit are created via a function that serves as the constructor and returns some form of system, in this case a ODESystem. Since the non-linear resistor is essentially a standard electrical component with two ports, we can extend from the OnePort component of the library.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"@named oneport = OnePort()","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"This creates a OnePort with the name = :oneport. For easier notation we can unpack the states of the component","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"@unpack v, i = oneport","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"It might be a good idea to create parameters for the constants of the NonlinearResistor.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"pars = @parameters Ga=Ga Gb=Gb Ve=Ve","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"The syntax looks funny but it simply creates symbolic parameters with the name Ga where it's default value is set from the function's argument Ga. While this is not strictly necessary it allows the user to remake the problem easily with different parameters or allow for auto-tuning or parameter optimization without having to do all costly steps that may be involved with building and simplifying a model. The non-linear (in this case piece-wise constant) equation for the current can be implemented using IfElse.ifelse. Finally, the created oneport component is extended with the created equations and parameters. In this case no extra state variables are added, hence an empty vector is supplied. The independent variable t needs to be supplied as second argument.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"extend(ODESystem(eqs, t, [], pars; name=name), oneport)","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/#Building-the-Model","page":"Custom Components","title":"Building the Model","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"The final model can now be created with the components from the library and the new custom component.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"@named L = Inductor(L=18)\n@named Ro = Resistor(R=12.5e-3)\n@named G = Conductor(G=0.565)\n@named C1 = Capacitor(C=10, v_start=4)\n@named C2 = Capacitor(C=100)\n@named Nr = NonlinearResistor(\n    Ga = -0.757576,\n    Gb = -0.409091,\n    Ve=1)\n@named Gnd = Ground()\n\nconnections = [\n    connect(L.p, G.p)\n    connect(G.n, Nr.p)\n    connect(Nr.n, Gnd.g)\n    connect(C1.p, G.n)\n    connect(L.n, Ro.p)\n    connect(G.p, C2.p)\n    connect(C1.n, Gnd.g)\n    connect(C2.n, Gnd.g)\n    connect(Ro.n, Gnd.g)\n]\n\n@named model = ODESystem(connections, t, systems=[L, Ro, G, C1, C2, Nr])","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/#Simulating-the-Model","page":"Custom Components","title":"Simulating the Model","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"Now the model can be simulated. First structural_simplify is called on the model and a ODEProblem is build from the result. Since the initial voltage of the first capacitor was already specified via v_start, no initial condition is given and an empty pair is supplied.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"sys = structural_simplify(model)\nprob = ODEProblem(sys, Pair[], (0, 5e4), saveat=0.01)\nsol = solve(prob, Rodas4())\n\nPlots.plot(sol[C1.v], sol[C2.v], title=\"Chaotic Attractor\", label=\"\", ylabel=\"C1 Voltage in V\", xlabel=\"C2 Voltage in V\")\nPlots.savefig(\"chua_phase_plane.png\")\n\nPlots.plot(sol; vars=[C1.v, C2.v, L.i], labels=[\"C1 Voltage in V\" \"C1 Voltage in V\" \"Inductor Current in A\"])\nPlots.savefig(\"chua.png\")","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"(Image: Time series plot of C1.v, C2.v and L.i)","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/tutorials/custom_component/","page":"Custom Components","title":"Custom Components","text":"(Image: Phase plane plot of C1.v and C2.v)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/#Alternative-Objective-Functions","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"These are objective functions made to be used with special fitting packages.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/#LeastSquaresOptim.jl-objective","page":"Alternative Objective Functions","title":"LeastSquaresOptim.jl objective","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"build_lsoptim_objective builds an objective function to be used with LeastSquaresOptim.jl.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"build_lsoptim_objective(prob,tspan,t,data;\n                        prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p),\n                        kwargs...)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"The arguments are the same as build_loss_objective.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/#lm_fit","page":"Alternative Objective Functions","title":"lm_fit","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"lm_fit is a function for fitting the parameters of an ODE using the Levenberg-Marquardt algorithm. This algorithm is really bad and thus not recommended since, for example, the Optim.jl algorithms on an L2 loss are more performant and robust. However, this is provided for completeness as most other differential equation libraries use an LM-based algorithm, so this allows one to test the increased effectiveness of not using LM.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"lm_fit(prob::DEProblem,tspan,t,data,p0;\n       prob_generator = (prob,p) -> remake(prob,u0=convert.(eltype(p),prob.u0),p=p),\n       kwargs...)","category":"page"},{"location":"modules/DiffEqParamEstim/methods/alternative_objectives/","page":"Alternative Objective Functions","title":"Alternative Objective Functions","text":"The arguments are similar to before, but with p0 being the initial conditions for the parameters and the kwargs as the args passed to the LsqFit curve_fit function (which is used for the LM solver). This returns the fitted parameters.","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/#Nonlinear-elliptic-system-of-PDEs","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"We can also solve nonlinear systems such as the system of nonlinear elliptic PDEs","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nfracpartial^2upartial x^2 + fracpartial^2upartial y^2 = uf(fracuw) + fracuwh(fracuw) \nfracpartial^2wpartial x^2 + fracpartial^2wpartial y^2 = wg(fracuw) + h(fracuw) \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where f, g, h are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nu(0y) = y + 1 \nw(1 y) = cosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nw(x0) = cosh(sqrtf(k)) + sinh(sqrtf(k)) \nw(0y) = k(y + 1) \nu(1 y) = kcosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nu(x0) = kcosh(sqrtf(k)) + sinh(sqrtf(k)) \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k).","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"This is done using a derivative neural network approximation.","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, y\nDx = Differential(x)\nDy = Differential(y)\n@variables Dxu(..), Dyu(..), Dxw(..), Dyw(..)\n@variables u(..), w(..)\n\n\n# Arbitrary functions\nf(x) = sin(x)\ng(x) = cos(x)\nh(x) = x\nroot(x) = f(x) - g(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Bisection())                            # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nθ(x, y) = (cosh(sqrt(f(k)) * x) + sinh(sqrt(f(k)) * x)) * (y + 1)   # Analytical solution to Helmholtz equation\nw_analytic(x, y) = θ(x, y) - h(k) / f(k)\nu_analytic(x, y) = k * w_analytic(x, y)\n\n# Nonlinear Steady-State Systems of Two Reaction-Diffusion Equations with 3 arbitrary function f, g, h\neqs_ = [Dx(Dxu(x, y)) + Dy(Dyu(x, y)) ~ u(x, y) * f(u(x, y) / w(x, y)) + u(x, y) / w(x, y) * h(u(x, y) / w(x, y)),\n       Dx(Dxw(x, y)) + Dy(Dyw(x, y)) ~ w(x, y) * g(u(x, y) / w(x, y)) + h(u(x, y) / w(x, y))]\n\n# Boundary conditions\nbcs_ = [u(0, y) ~ u_analytic(0, y),\n       u(1, y) ~ u_analytic(1, y),\n       u(x, 0) ~ u_analytic(x, 0),\n       w(0, y) ~ w_analytic(0, y),\n       w(1, y) ~ w_analytic(1, y),\n       w(x, 0) ~ w_analytic(x, 0)]\n\nder_ = [Dy(u(x, y)) ~ Dyu(x, y),\n       Dy(w(x, y)) ~ Dyw(x, y),\n       Dx(u(x, y)) ~ Dxu(x, y),\n       Dx(w(x, y)) ~ Dxw(x, y)]\n\nbcs__ = [bcs_;der_]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:6] # 1:number of @variables\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\nvars = [u(x,y),w(x,y),Dxu(x,y),Dyu(x,y),Dxw(x,y),Dyw(x,y)]\n@named pdesystem  = PDESystem(eqs_, bcs__, domains, [x,y], vars)\nprob = NeuralPDE.discretize(pdesystem , discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem , discretization)\n\nstrategy = NeuralPDE.QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:6]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[7:end]\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=5000)\n\nphi = discretization.phi\n\n# Analysis\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u,:w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:2]\n\nanalytic_sol_func(x,y) = [u_analytic(x, y), w_analytic(x, y)]\nu_real  = [[analytic_sol_func(x, y)[i] for x in xs for y in ys] for i in 1:2]\nu_predict  = [[phi[i]([x,y], minimizers_[i])[1] for x in xs for y in ys] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(xs, ys, u_real[i], linetype=:contourf, title=\"u$i, analytic\");\n    p2 = plot(xs, ys, u_predict[i], linetype=:contourf, title=\"predict\");\n    p3 = plot(xs, ys, diff_u[i], linetype=:contourf, title=\"error\");\n    plot(p1, p2, p3)\n    savefig(\"non_linear_elliptic_sol_u$i\")\nend","category":"page"},{"location":"modules/NeuralPDE/examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"(Image: non_linear_elliptic_sol_u1) (Image: non_linear_elliptic_sol_u2)","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Hamiltonian Neural Networks introduced in [1] allow models to \"learn and respect exact conservation laws in an unsupervised manner\". In this example, we will train a model to learn the Hamiltonian for a 1D Spring mass system. This system is described by the equation:","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"mddot x + kx = 0","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Now we make some simplifying assumptions, and assign m = 1 and k = 1. Analytically solving this equation, we get x = sin(t). Hence, q = sin(t), and p = cos(t). Using these solutions we generate our dataset and fit the NeuralHamiltonianDE to learn the dynamics of this system.","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Flux, DiffEqFlux, DifferentialEquations, Statistics, Plots, ReverseDiff\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader((data, target); batchsize=256, shuffle=true)\n\nhnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()\n\nmodel = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Step-by-Step-Explanation","page":"Hamiltonian Neural Network","title":"Step by Step Explanation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Data-Generation","page":"Hamiltonian Neural Network","title":"Data Generation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"The HNN predicts the gradients (dot q dot p) given (q p). Hence, we generate the pairs (q p) using the equations given at the top. Additionally to supervise the training we also generate the gradients. Next we use use Flux DataLoader for automatically batching our dataset.","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using Flux, DiffEqFlux, DifferentialEquations, Statistics, Plots, ReverseDiff\n\nt = range(0.0f0, 1.0f0, length = 1024)\nπ_32 = Float32(π)\nq_t = reshape(sin.(2π_32 * t), 1, :)\np_t = reshape(cos.(2π_32 * t), 1, :)\ndqdt = 2π_32 .* p_t\ndpdt = -2π_32 .* q_t\n\ndata = cat(q_t, p_t, dims = 1)\ntarget = cat(dqdt, dpdt, dims = 1)\ndataloader = Flux.Data.DataLoader((data, target); batchsize=256, shuffle=true)","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Training-the-HamiltonianNN","page":"Hamiltonian Neural Network","title":"Training the HamiltonianNN","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We parameterize the HamiltonianNN with a small MultiLayered Perceptron (HNN also works with the Fast* Layers provided in DiffEqFlux). HNNs are trained by optimizing the gradients of the Neural Network. Zygote currently doesn't support nesting itself, so we will be using ReverseDiff in the training loop to compute the gradients of the HNN Layer for Optimization.","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"hnn = HamiltonianNN(\n    Chain(Dense(2, 64, relu), Dense(64, 1))\n)\n\np = hnn.p\n\nopt = ADAM(0.01)\n\nloss(x, y, p) = mean((hnn(x, p) .- y) .^ 2)\n\ncallback() = println(\"Loss Neural Hamiltonian DE = $(loss(data, target, p))\")\n\nepochs = 500\nfor epoch in 1:epochs\n    for (x, y) in dataloader\n        gs = ReverseDiff.gradient(p -> loss(x, y, p), p)\n        Flux.Optimise.update!(opt, p, gs)\n    end\n    if epoch % 100 == 1\n        callback()\n    end\nend\ncallback()","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Solving-the-ODE-using-trained-HNN","page":"Hamiltonian Neural Network","title":"Solving the ODE using trained HNN","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In order to visualize the learned trajectories, we need to solve the ODE. We will use the NeuralHamiltonianDE layer which is essentially a wrapper over HamiltonianNN layer and solves the ODE.","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"model = NeuralHamiltonianDE(\n    hnn, (0.0f0, 1.0f0),\n    Tsit5(), save_everystep = false,\n    save_start = true, saveat = t\n)\n\npred = Array(model(data[:, 1]))\nplot(data[1, :], data[2, :], lw=4, label=\"Original\")\nplot!(pred[1, :], pred[2, :], lw=4, label=\"Predicted\")\nxlabel!(\"Position (q)\")\nylabel!(\"Momentum (p)\")","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"(Image: HNN Prediction)","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#Expected-Output","page":"Hamiltonian Neural Network","title":"Expected Output","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Loss Neural Hamiltonian DE = 18.768814\nLoss Neural Hamiltonian DE = 0.022630047\nLoss Neural Hamiltonian DE = 0.015060622\nLoss Neural Hamiltonian DE = 0.013170851\nLoss Neural Hamiltonian DE = 0.011898238\nLoss Neural Hamiltonian DE = 0.009806873","category":"page"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/#References","page":"Hamiltonian Neural Network","title":"References","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/hamiltonian_nn/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_adjoints/#Newton-and-Hessian-Free-Newton-Krylov-with-Second-Order-Adjoint-Sensitivity-Analysis","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"In many cases it may be more optimal or more stable to fit using second order Newton-based optimization techniques. Since SciMLSensitivity.jl provides second order sensitivity analysis for fast Hessians and Hessian-vector products (via forward-over-reverse), we can utilize these in our neural/universal differential equation training processes.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"sciml_train is setup to automatically use second order sensitivity analysis methods if a second order optimizer is requested via Optim.jl. Thus Newton and NewtonTrustRegion optimizers will use a second order Hessian-based optimization, while KrylovTrustRegion will utilize a Krylov-based method with Hessian-vector products (never forming the Hessian) for large parameter optimizations.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"using Flux, DiffEqFlux, Optimization, OptimizationFlux, DifferentialEquations, Plots, Random\n\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = Flux.Chain(x -> x.^3,\n                   Flux.Dense(2, 50, tanh),\n                   Flux.Dense(50, 2))\nprob_neuralode = NeuralODE(dudt2, tspan, Tsit5(), saveat = tsteps)\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, ode_data .- pred)\n    return loss, pred\nend\n\n# Callback function to observe training\nlist_plots = []\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global list_plots, iter\n\n  if iter == 0\n    list_plots = []\n  end\n  iter += 1\n\n  display(l)\n\n  # plot current prediction against data\n  plt = scatter(tsteps, ode_data[1,:], label = \"data\")\n  scatter!(plt, tsteps, pred[1,:], label = \"prediction\")\n  push!(list_plots, plt)\n  if doplot\n    display(plot(plt))\n  end\n\n  return l < 0.01\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss_neuralode(x), adtype)\n\noptprob1 = Optimization.OptimizationProblem(optf, prob_neuralode.p)\npstart = Optimization.solve(optprob1, ADAM(0.01), callback=callback, maxiters = 100).u\n\noptprob2 = Optimization.OptimizationProblem(optf, pstart)\npmin = Optimization.solve(optprob2, NewtonTrustRegion(), callback=callback, maxiters = 200)\npmin = Optimization.solve(optprob2, Optim.KrylovTrustRegion(), callback=callback, maxiters = 200)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/second_order_adjoints/","page":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","title":"Newton and Hessian-Free Newton-Krylov with Second Order Adjoint Sensitivity Analysis","text":"Note that we do not demonstrate Newton() because we have not found a single case where it is competitive with the other two methods. KrylovTrustRegion() is generally the fastest due to its use of Hessian-vector products.","category":"page"},{"location":"modules/DataInterpolations/#DataInterpolations.jl","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"","category":"section"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"(Image: CI) (Image: codecov)","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"DataInterpolations.jl is a library for performing interpolations of one-dimensional data. By \"data interpolations\" we mean techniques for interpolating possibly noisy data, and thus some methods are mixtures of regressions with interpolations (i.e. do not hit the data points exactly, smoothing out the lines). This library can be used to fill in intermediate data points in applications like timeseries data.","category":"page"},{"location":"modules/DataInterpolations/#Tutorial-/-Demonstration","page":"DataInterpolations.jl","title":"Tutorial / Demonstration","text":"","category":"section"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"A tutorial is included and can be found at this page. To run the tutorial yourself locally, use the following Weave commands:","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"using Weave, DataInterpolations\nweave(joinpath(dirname(pathof(DataInterpolations)), \"../example\", \"DataInterpolations.jmd\"), out_path=:doc)","category":"page"},{"location":"modules/DataInterpolations/#API","page":"DataInterpolations.jl","title":"API","text":"","category":"section"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"All interpolation objects act as functions. Thus for example, using an interpolation looks like:","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"u = rand(5)\nt = 0:4\ninterp = LinearInterpolation(u,t)\ninterp(3.5) # Gives the linear interpolation value at t=3.5","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"The indexing retreives the underlying values:","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"interp[4] # Gives the 4th value of u","category":"page"},{"location":"modules/DataInterpolations/#Available-Interpolations","page":"DataInterpolations.jl","title":"Available Interpolations","text":"","category":"section"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"In all cases, u an AbstractVector of values and t is an AbstractVector of timepoints corresponding to (u,t) pairs.","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"ConstantInterpolation(u,t) - A piecewise constant interpolation.\nLinearInterpolation(u,t) - A linear interpolation.\nQuadraticInterpolation(u,t) - A quadratic interpolation.\nLagrangeInterpolation(u,t,n) - A Lagrange interpolation of order n.\nQuadraticSpline(u,t) - A quadratic spline interpolation.\nCubicSpline(u,t) - A cubic spline interpolation.\nBSplineInterpolation(u,t,d,pVec,knotVec) - An interpolation B-spline. This is a B-spline which hits each of the data points. The argument choices are: \t- d - degree of B-spline   \t- pVec - Symbol to Parameters Vector, pVec = :Uniform for uniform spaced parameters and pVec = :ArcLen for parameters generated by chord length method.   \t- knotVec - Symbol to Knot Vector, knotVec = :Uniform for uniform knot vector, knotVec = :Average for average spaced knot vector.\nBSplineApprox(u,t,d,h,pVec,knotVec) - A regression B-spline which smooths the fitting curve. The argument choices are the same as the BSplineInterpolation, with the additional parameter h<length(t) which is the number of control points to use, with smaller h indicating more smoothing.\nCurvefit(u,t,m,p,alg) - An interpolation which is done by fitting a user-given functional form m(t,p) where p is the vector of parameters. The user's input p is a an initial value for a least-square fitting, alg is the algorithm choice to use for optimize the cost function (sum of squared deviations) via Optim.jl and optimal ps are used in the interpolation.","category":"page"},{"location":"modules/DataInterpolations/#Plotting","page":"DataInterpolations.jl","title":"Plotting","text":"","category":"section"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"DataInterpolations.jl is tied into the Plots.jl ecosystem, by way of RecipesBase.   Any interpolation can be plotted using the plot command (or any other), since they have type recipes associated with them.","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"For convenience, and to allow keyword arguments to propagate properly, DataInterpolations.jl also defines several series types, corresponding to different interpolations.","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"The series types defined are:","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":":linear_interp\n:quadratic_interp\n:lagrange_interp\n:quadratic_spline\n:cubic_spline","category":"page"},{"location":"modules/DataInterpolations/","page":"DataInterpolations.jl","title":"DataInterpolations.jl","text":"By and large, these accept the same keywords as their function counterparts.","category":"page"},{"location":"modules/RecursiveArrayTools/array_types/#Recursive-Array-Types","page":"Recursive Array Types","title":"Recursive Array Types","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/array_types/","page":"Recursive Array Types","title":"Recursive Array Types","text":"The Recursive Array types are types which implement an AbstractArray interface so that recursive arrays can be handled with standard array functionality. For example, wrapped arrays will automatically do things like recurse broadcast, define optimized mapping and iteration functions, and more.","category":"page"},{"location":"modules/RecursiveArrayTools/array_types/#Abstract-Types","page":"Recursive Array Types","title":"Abstract Types","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/array_types/#Concrete-Types","page":"Recursive Array Types","title":"Concrete Types","text":"","category":"section"},{"location":"modules/RecursiveArrayTools/array_types/","page":"Recursive Array Types","title":"Recursive Array Types","text":"VectorOfArray\nDiffEqArray\nArrayPartition","category":"page"},{"location":"modules/RecursiveArrayTools/array_types/#RecursiveArrayTools.VectorOfArray","page":"Recursive Array Types","title":"RecursiveArrayTools.VectorOfArray","text":"VectorOfArray(u::AbstractVector)\n\nA VectorOfArray is an array which has the underlying data structure Vector{AbstractArray{T}} (but, hopefully, concretely typed!). This wrapper over such data structures allows one to lazily act like it's a higher-dimensional vector, and easily convert to different forms. The indexing structure is:\n\nA[i] # Returns the ith array in the vector of arrays\nA[j,i] # Returns the jth component in the ith array\nA[j1,...,jN,i] # Returns the (j1,...,jN) component of the ith array\n\nwhich presents itself as a column-major matrix with the columns being the arrays from the vector. The AbstractArray interface is implemented, giving access to copy, push, append!, etc. functions, which act appropriately. Points to note are:\n\nThe length is the number of vectors, or length(A.u) where u is the vector of arrays.\nIteration follows the linear index and goes over the vectors\n\nAdditionally, the convert(Array,VA::AbstractVectorOfArray) function is provided, which transforms the VectorOfArray into a matrix/tensor. Also, vecarr_to_vectors(VA::AbstractVectorOfArray) returns a vector of the series for each component, that is, A[i,:] for each i. A plot recipe is provided, which plots the A[i,:] series.\n\n\n\n\n\n","category":"type"},{"location":"modules/RecursiveArrayTools/array_types/#RecursiveArrayTools.DiffEqArray","page":"Recursive Array Types","title":"RecursiveArrayTools.DiffEqArray","text":"DiffEqArray(u::AbstractVector,t::AbstractVector)\n\nThis is a VectorOfArray, which stores A.t that matches A.u. This will plot (A.t[i],A[i,:]). The function tuples(diffeq_arr) returns tuples of (t,u).\n\nTo construct a DiffEqArray\n\nt = 0.0:0.1:10.0\nf(t) = t - 1\nf2(t) = t^2\nvals = [[f(tval) f2(tval)] for tval in t]\nA = DiffEqArray(vals, t)\nA[1,:]  # all time periods for f(t)\nA.t\n\n\n\n\n\n","category":"type"},{"location":"modules/RecursiveArrayTools/array_types/#RecursiveArrayTools.ArrayPartition","page":"Recursive Array Types","title":"RecursiveArrayTools.ArrayPartition","text":"ArrayPartition(x::AbstractArray...)\n\nAn ArrayPartition A is an array, which is made up of different arrays A.x. These index like a single array, but each subarray may have a different type. However, broadcast is overloaded to loop in an efficient manner, meaning that A .+= 2.+B is type-stable in its computations, even if A.x[i] and A.x[j] do not match types. A full array interface is included for completeness, which allows this array type to be used in place of a standard array where such a type stable broadcast may be needed. One example is in heterogeneous differential equations for DifferentialEquations.jl.\n\nAn ArrayPartition acts like a single array. A[i] indexes through the first array, then the second, etc., all linearly. But A.x is where the arrays are stored. Thus, for:\n\nusing RecursiveArrayTools\nA = ArrayPartition(y,z)\n\nwe would have A.x[1]==y and A.x[2]==z. Broadcasting like f.(A) is efficient.\n\n\n\n\n\n","category":"type"},{"location":"modules/LinearSolve/advanced/custom/#Passing-in-a-Custom-Linear-Solver","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"","category":"section"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"Julia users are building a wide variety of applications in the SciML ecosystem, often requiring problem-specific handling of their linear solves. As existing solvers in LinearSolve.jl may not be optimally suited for novel applications, it is essential for the linear solve interface to be easily extendable by users. To that end, the linear solve algorithm LinearSolveFunction() accepts a user-defined function for handling the solve. A user can pass in their custom linear solve function, say my_linsolve, to LinearSolveFunction(). A contrived example of solving a linear system with a custom solver is below.","category":"page"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"using LinearSolve, LinearAlgebra\n\nfunction my_linsolve(A,b,u,p,newA,Pl,Pr,solverdata;verbose=true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u = A \\ b\n    return u\nend\n\nprob = LinearProblem(Diagonal(rand(4)), rand(4))\nalg  = LinearSolveFunction(my_linsolve)\nsol  = solve(prob, alg)","category":"page"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The inputs to the function are as follows:","category":"page"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"A, the linear operator\nb, the right-hand-side\nu, the solution initialized as zero(b),\np, a set of parameters\nnewA, a Bool which is true if A has been modified since last solve\nPl, left-preconditioner\nPr, right-preconditioner\nsolverdata, solver cache set to nothing if solver hasn't been initialized\nkwargs, standard SciML keyword arguments such as verbose, maxiters, abstol, reltol","category":"page"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"The function my_linsolve must accept the above specified arguments, and return the solution, u. As memory for u is already allocated, the user may choose to modify u in place as follows:","category":"page"},{"location":"modules/LinearSolve/advanced/custom/","page":"Passing in a Custom Linear Solver","title":"Passing in a Custom Linear Solver","text":"function my_linsolve!(A,b,u,p,newA,Pl,Pr,solverdata;verbose=true, kwargs...)\n    if verbose == true\n        println(\"solving Ax=b\")\n    end\n    u .= A \\ b # in place\n    return u\nend\n\nalg  = LinearSolveFunction(my_linsolve!)\nsol  = solve(prob, alg)","category":"page"},{"location":"modules/Integrals/basics/FAQ/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"modules/Integrals/basics/FAQ/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Ask more questions.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/#paremeterized_functions","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/#Installation","page":"ParameterizedFunctions","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install ParameterizedFunctions.jl:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"]add ParameterizedFunctions\nusing ParameterizedFunctions","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/#Function-Definition-Macros","page":"ParameterizedFunctions","title":"Function Definition Macros","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"DifferentialEquations.jl provides a set of macros for more easily and legibly defining your differential equations. It exploits the standard notation for mathematically writing differential equations and the notation for \"punching differential equations into the computer\"; effectively doing the translation step for you. This is best shown by an example. Say we want to solve the ROBER model. Using the @ode_def macro from ParameterizedFunctions.jl, we can do this by writing:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"using ParameterizedFunctions\nf = @ode_def begin\n  dy₁ = -k₁*y₁+k₃*y₂*y₃\n  dy₂ =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  dy₃ =  k₂*y₂^2\nend k₁ k₂ k₃","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"This looks just like pseudocode! The macro will expand this to the \"standard form\", i.e. the ugly computer form:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"function f(du,u,p,t)\n  du[1] = -p[1]*u[1] + p[3]*u[2]*u[3]\n  du[2] = p[1]*u[1] - p[2]*u[2]^2 - p[3]*u[2]*u[3]\n  du[3] = p[2]*u[2]^2\nend","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Note that one doesn't need to use numbered variables: DifferentialEquations.jl will number the variables for you. For example, the following defines the function for the Lotka-Volterra model, with full Unicode support to boot:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"f = @ode_def begin\n  d🐁  = α*🐁  - β*🐁*🐈\n  d🐈 = -γ*🐈 + δ*🐁*🐈\nend α β γ δ","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/#Limitations","page":"ParameterizedFunctions","title":"Limitations","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"The macro is a Domain-Specific Language (DSL) and thus has different internal semantics than standard Julia functions. In particular:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Control sequences and conditionals (while, for, if) will not work in the macro.\nIntermediate calculations (lines that don't start with d_) are incompatible with the Jacobian etc. calculations.\nThe macro has to use t for the independent variable.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/#Extra-Optimizations","page":"ParameterizedFunctions","title":"Extra Optimizations","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Because the ParameterizedFunction defined by the macro holds the definition at a symbolic level, optimizations are provided by SymEngine. Using the symbolic calculator, in-place functions for many things such as Jacobians, Hessians, etc. are symbolically pre-computed. In addition, functions for the inverse Jacobian, Hessian, etc. are also pre-computed. In addition, parameter gradients and Jacobians are also used.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Normally these will be computed fast enough that the user doesn't have to worry. However, in some cases you may want to restrict the number of functions (or get rid of a warning). For more information, please see the ParameterizedFunctions.jl documentation.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#jump_solve","page":"Jump solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"solve(prob::JumpProblem,alg;kwargs)","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#Recommended-Methods","page":"Jump solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"A JumpProblem(prob,aggregator,jumps...) comes in two forms. The first major form is if it does not have a RegularJump. In this case, it can be solved with any integrator on  prob. However, in the case of a pure JumpProblem (a JumpProblem over a  DiscreteProblem), there are special algorithms available.  The SSAStepper() is an efficient streamlined algorithm for running the  aggregator version of the SSA for pure ConstantRateJump and/or MassActionJump problems. However, it is not compatible with event handling. If events are necessary, then FunctionMap does well.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"If there is a RegularJump, then specific methods must be used. The current recommended method is TauLeaping if you need adaptivity, events, etc. If you just need the most barebones fixed time step leaping method, then SimpleTauLeaping can have performance benefits.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#Special-Methods-for-Pure-Jump-Problems","page":"Jump solvers","title":"Special Methods for Pure Jump Problems","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"If you are using jumps with a differential equation, use the same methods as in the case of the differential equation solving. However, the following algorithms are optimized for pure jump problems.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#JumpProcesses.jl","page":"Jump solvers","title":"JumpProcesses.jl","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"SSAStepper: a stepping algorithm for pure ConstantRateJump and/or MassActionJump JumpProblems. Supports handling of DiscreteCallback and saving controls like saveat.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#RegularJump-Compatible-Methods","page":"Jump solvers","title":"RegularJump Compatible Methods","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/#StochasticDiffEq.jl","page":"Jump solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"These methods support mixing with event handling, other jump types, and all of the features of the normal differential equation solvers.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"TauLeaping: an adaptive tau-leaping algorithm with post-leap estimates.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#JumpProcesses.jl-2","page":"Jump solvers","title":"JumpProcesses.jl","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"SimpleTauLeaping: a tau-leaping algorithm for pure RegularJump JumpProblems. Requires a choice of dt.\nRegularSSA: a version of SSA for pure RegularJump JumpProblems.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#Regular-Jump-Diffusion-Compatible-Methods","page":"Jump solvers","title":"Regular Jump Diffusion Compatible Methods","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"Regular jump diffusions are JumpProblems where the internal problem is an SDEProblem and the jump process has designed a regular jump.","category":"page"},{"location":"modules/JumpProcesses/jump_solve/#StochasticDiffEq.jl-2","page":"Jump solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/JumpProcesses/jump_solve/","page":"Jump solvers","title":"Jump solvers","text":"EM: Explicit Euler-Maruyama.\nImplicitEM: Implicit Euler-Maruyama. See the SDE solvers page for more details.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Linear-Surrogate","page":"Linear","title":"Linear Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Linear Surrogate is a linear approach to modeling the relationship between a scalar response or dependent variable and one or more explanatory variables. We will use Linear Surrogate to optimize following function:","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":".","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"First of all we have to import these two packages: Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"using Surrogates\r\nusing Plots\r\ndefault()","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Sampling","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"We choose to sample f in 20 points between 0 and 10 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"f(x) = sin(x) + log(x)\r\nn_samples = 20\r\nlower_bound = 5.2\r\nupper_bound = 12.5\r\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\ny = f.(x)\r\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Building-a-Surrogate","page":"Linear","title":"Building a Surrogate","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"With our sampled points we can build the Linear Surrogate using the LinearSurrogate function.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"We can simply calculate linear_surrogate for any value.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"my_linear_surr_1D = LinearSurrogate(x, y, lower_bound, upper_bound)\r\nadd_point!(my_linear_surr_1D,4.0,7.2)\r\nadd_point!(my_linear_surr_1D,[5.0,6.0],[8.3,9.7])\r\nval = my_linear_surr_1D(5.0)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Now, we will simply plot linear_surrogate:","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Optimizing","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, my_linear_surr_1D, SobolSample())\r\nscatter(x, y, label=\"Sampled points\")\r\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\r\nplot!(my_linear_surr_1D, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Linear-Surrogate-tutorial-(ND)","page":"Linear","title":"Linear Surrogate tutorial (ND)","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"First of all we will define the Egg Holder function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"using Plots # hide\r\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\r\nusing Surrogates # hide\r\n\r\nfunction egg(x)\r\n    x1=x[1]\r\n    x2=x[2]\r\n    term1 = -(x2+47) * sin(sqrt(abs(x2+x1/2+47)));\r\n    term2 = -x1 * sin(sqrt(abs(x1-(x2+47))));\r\n    y = term1 + term2;\r\nend","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Sampling-2","page":"Linear","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds -10, 5, and 0, 15 for the second dimension. We are taking 50 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"n_samples = 50\r\nlower_bound = [-10.0, 0.0]\r\nupper_bound = [5.0, 15.0]\r\n\r\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\r\nzs = egg.(xys);","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"x, y = -10:5, 0:15 # hide\r\np1 = surface(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nscatter!(xs, ys, zs) # hide\r\np2 = contour(x, y, (x1,x2) -> egg((x1,x2))) # hide\r\nscatter!(xs, ys) # hide\r\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Building-a-surrogate","page":"Linear","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"my_linear_ND = LinearSurrogate(xys, zs,  lower_bound, upper_bound)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/#Optimizing-2","page":"Linear","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"surrogate_optimize(egg, SRBF(), lower_bound, upper_bound, my_linear_ND, SobolSample(), maxiters=10)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"size(xys)","category":"page"},{"location":"modules/Surrogates/LinearSurrogate/","page":"Linear","title":"Linear","text":"p1 = surface(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nxs = [xy[1] for xy in xys] # hide\r\nys = [xy[2] for xy in xys] # hide\r\nzs = egg.(xys) # hide\r\nscatter!(xs, ys, zs, marker_z=zs) # hide\r\np2 = contour(x, y, (x, y) -> my_linear_ND([x y])) # hide\r\nscatter!(xs, ys, marker_z=zs) # hide\r\nplot(p1, p2) # hide","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_jacobian_product/#Vector-Jacobian-Product-Operators","page":"Vector-Jacobian Product Operators","title":"Vector-Jacobian Product Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/vector_jacobian_product/","page":"Vector-Jacobian Product Operators","title":"Vector-Jacobian Product Operators","text":"VecJacOperator{T}(f,u::AbstractArray,p=nothing,t::Union{Nothing,Number}=nothing;autodiff=true,ishermitian=false,opnorm=true)","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_jacobian_product/","page":"Vector-Jacobian Product Operators","title":"Vector-Jacobian Product Operators","text":"The VecJacOperator is a linear operator J'*v where J acts like df/du for some function f(u,p,t). For in-place operations mul!(w,J,v), f is an in-place function f(du,u,p,t).","category":"page"},{"location":"modules/DiffEqOperators/operators/vector_jacobian_product/","page":"Vector-Jacobian Product Operators","title":"Vector-Jacobian Product Operators","text":"note: Note\nThis operator is available when Zygote is imported.","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/#ODE-with-a-3rd-Order-Derivative","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"Let's consider the ODE with a 3rd-order derivative:","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"beginalign*\n^3_x u(x) = cos(pi x)  \nu(0) = 0  \nu(1) = cos(pi)  \n_x u(0) = 1  \nx in 0 1  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We will use physics-informed neural networks.","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimJL, OptimizationOptimisers\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x\n@variables u(..)\n\nDxxx = Differential(x)^3\nDx = Differential(x)\n# ODE\neq = Dxxx(u(x)) ~ cos(pi*x)\n\n# Initial and boundary conditions\nbcs = [u(0.) ~ 0.0,\n       u(1.) ~ cos(pi),\n       Dx(u(1.)) ~ 1.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0)]\n\n# Neural network\nchain = Lux.Chain(Dense(1,8,Lux.σ),Dense(8,1))\n\ndiscretization = PhysicsInformedNN(chain, QuasiRandomTraining(20))\n@named pde_system = PDESystem(eq,bcs,domains,[x],[u(x)])\nprob = discretize(pde_system,discretization)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.01); callback = callback, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We can plot the predicted solution of the ODE and its analytical solution.","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using Plots\n\nanalytic_sol_func(x) = (π*x*(-x+(π^2)*(2*x-3)+1)-sin(π*x))/(π^3)\n\ndx = 0.05\nxs = [infimum(d.domain):dx/10:supremum(d.domain) for d in domains][1]\nu_real  = [analytic_sol_func(x) for x in xs]\nu_predict  = [first(phi(x,res.u)) for x in xs]\n\nx_plot = collect(xs)\nplot(x_plot ,u_real,title = \"real\")\nplot!(x_plot ,u_predict,title = \"predict\")","category":"page"},{"location":"modules/NeuralPDE/examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"(Image: hodeplot)","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLProblems","page":"SciMLProblems","title":"SciMLProblems","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"The cornerstone of the SciML common interface is the problem type definition. These definitions are the encoding of mathematical problems into a numerically computable form.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#Note-About-Symbolics-and-ModelingToolkit","page":"SciMLProblems","title":"Note About Symbolics and ModelingToolkit","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"The symbolic analog to the problem interface is the ModelingToolkit AbstractSystem. For example, ODESystem is the symbolic analog to ODEProblem. Each of these system types have a method for constructing the associated problem and function types.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#Definition-of-the-AbstractSciMLProblem-Interface","page":"SciMLProblems","title":"Definition of the AbstractSciMLProblem Interface","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"The following standard principles should be adhered to across all AbstractSciMLProblem instantiations.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#In-place-Specification","page":"SciMLProblems","title":"In-place Specification","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"Each AbstractSciMLProblem type can be called with an \"is inplace\" (iip) choice. For example:","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"ODEProblem(f,u0,tspan,p)\nODEProblem{iip}(f,u0,tspan,p)","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"which is a boolean for whether the function is in the inplace form (mutating to change the first value). This is automatically determined using the methods table but note that for full type-inferrability of the AbstractSciMLProblem this iip-ness should be specified.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"Additionally, the functions are fully specialized to reduce the runtimes. If one would instead like to not specialize on the functions to reduce compile time, then one can set recompile to false.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#Default-Parameters","page":"SciMLProblems","title":"Default Parameters","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"By default, AbstractSciMLProblem types use the SciMLBase.NullParameters() singleton to define the absence of parameters by default. The reason is because this throws an informative error if the parameter is used or accessed within the user's function, for example, p[1] will throw an informative error about forgetting to pass parameters.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#Keyword-Argument-Splatting","page":"SciMLProblems","title":"Keyword Argument Splatting","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"All AbstractSciMLProblem types allow for passing keyword arguments that would get forwarded to the solver. The reason for this is that in many cases, like in EnsembleProblem usage, a AbstractSciMLProblem might be associated with some solver configuration, such as a callback or tolerance. Thus, for flexibility the extra keyword arguments to the AbstractSciMLProblem are carried to the solver.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#problem_type","page":"SciMLProblems","title":"problem_type","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"AbstractSciMLProblem types include a non-public API definition of problem_type which holds a trait type corresponding to the way the AbstractSciMLProblem was constructed. For example, if a SecondOrderODEProblem constructor is used, the returned problem is simply a ODEProblem for interopability with any ODEProblem algorithm. However, in this case the problem_type will be populated with the SecondOrderODEProblem type, indicating the original definition and extra structure.","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#Remake","page":"SciMLProblems","title":"Remake","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"remake","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.remake","page":"SciMLProblems","title":"SciMLBase.remake","text":"remake(thing; <keyword arguments>)\n\nRe-construct thing with new field values specified by the keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Problems/#Problem-Traits","page":"SciMLProblems","title":"Problem Traits","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"SciMLBase.isinplace(prob::SciMLBase.AbstractDEProblem)\nSciMLBase.is_diagonal_noise","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.isinplace-Tuple{SciMLBase.AbstractDEProblem}","page":"SciMLProblems","title":"SciMLBase.isinplace","text":"isinplace(prob::AbstractSciMLProblem)\n\nDetermine whether the function of the given problem operates in place or not.\n\n\n\n\n\n","category":"method"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.is_diagonal_noise","page":"SciMLProblems","title":"SciMLBase.is_diagonal_noise","text":"is_diagonal_noise(prob::AbstractSciMLProblem)\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLBase/interfaces/Problems/#AbstractSciMLProblem-API","page":"SciMLProblems","title":"AbstractSciMLProblem API","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/#Abstract-SciMLProblems","page":"SciMLProblems","title":"Abstract SciMLProblems","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Problems/","page":"SciMLProblems","title":"SciMLProblems","text":"SciMLBase.AbstractSciMLProblem\nSciMLBase.AbstractDEProblem\nSciMLBase.AbstractLinearProblem\nSciMLBase.AbstractNonlinearProblem\nSciMLBase.AbstractQuadratureProblem\nSciMLBase.AbstractOptimizationProblem\nSciMLBase.AbstractNoiseProblem\nSciMLBase.AbstractODEProblem\nSciMLBase.AbstractDiscreteProblem\nSciMLBase.AbstractAnalyticalProblem\nSciMLBase.AbstractRODEProblem\nSciMLBase.AbstractSDEProblem\nSciMLBase.AbstractDAEProblem\nSciMLBase.AbstractDDEProblem\nSciMLBase.AbstractConstantLagDDEProblem\nSciMLBase.AbstractSecondOrderODEProblem\nSciMLBase.AbstractBVProblem\nSciMLBase.AbstractJumpProblem\nSciMLBase.AbstractSDDEProblem\nSciMLBase.AbstractConstantLagSDDEProblem\nSciMLBase.AbstractPDEProblem","category":"page"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractSciMLProblem","page":"SciMLProblems","title":"SciMLBase.AbstractSciMLProblem","text":"abstract type AbstractSciMLProblem\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractDEProblem","text":"abstract type AbstractDEProblem <: SciMLBase.AbstractSciMLProblem\n\nBase type for all DifferentialEquations.jl problems. Concrete subtypes of AbstractDEProblem contain the necessary information to fully define a differential equation of the corresponding type.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractLinearProblem","page":"SciMLProblems","title":"SciMLBase.AbstractLinearProblem","text":"abstract type AbstractLinearProblem{bType, isinplace} <: SciMLBase.AbstractSciMLProblem\n\nBase for types which define linear systems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractNonlinearProblem","page":"SciMLProblems","title":"SciMLBase.AbstractNonlinearProblem","text":"abstract type AbstractNonlinearProblem{uType, isinplace} <: SciMLBase.AbstractDEProblem\n\nBase for types which define nonlinear solve problems (f(u)=0).\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractOptimizationProblem","page":"SciMLProblems","title":"SciMLBase.AbstractOptimizationProblem","text":"abstract type AbstractOptimizationProblem{isinplace} <: SciMLBase.AbstractSciMLProblem\n\nBase for types which define equations for optimization.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractNoiseProblem","page":"SciMLProblems","title":"SciMLBase.AbstractNoiseProblem","text":"abstract type AbstractNoiseProblem <: SciMLBase.AbstractDEProblem\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractODEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractODEProblem","text":"abstract type AbstractODEProblem{uType, tType, isinplace} <: SciMLBase.AbstractDEProblem\n\nBase for types which define ODE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractDiscreteProblem","page":"SciMLProblems","title":"SciMLBase.AbstractDiscreteProblem","text":"abstract type AbstractDiscreteProblem{uType, tType, isinplace} <: SciMLBase.AbstractODEProblem{uType, tType, isinplace}\n\nBase for types which define discrete problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractAnalyticalProblem","page":"SciMLProblems","title":"SciMLBase.AbstractAnalyticalProblem","text":"abstract type AbstractAnalyticalProblem{uType, tType, isinplace} <: SciMLBase.AbstractODEProblem{uType, tType, isinplace}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractRODEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractRODEProblem","text":"abstract type AbstractRODEProblem{uType, tType, isinplace, ND} <: SciMLBase.AbstractDEProblem\n\nBase for types which define RODE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractSDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractSDEProblem","text":"abstract type AbstractSDEProblem{uType, tType, isinplace, ND} <: SciMLBase.AbstractRODEProblem{uType, tType, isinplace, ND}\n\nBase for types which define SDE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractDAEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractDAEProblem","text":"abstract type AbstractDAEProblem{uType, duType, tType, isinplace} <: SciMLBase.AbstractDEProblem\n\nBase for types which define DAE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractDDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractDDEProblem","text":"abstract type AbstractDDEProblem{uType, tType, lType, isinplace} <: SciMLBase.AbstractDEProblem\n\nBase for types which define DDE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractConstantLagDDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractConstantLagDDEProblem","text":"abstract type AbstractConstantLagDDEProblem{uType, tType, lType, isinplace} <: SciMLBase.AbstractDDEProblem{uType, tType, lType, isinplace}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractSecondOrderODEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractSecondOrderODEProblem","text":"abstract type AbstractSecondOrderODEProblem{uType, tType, isinplace} <: SciMLBase.AbstractODEProblem{uType, tType, isinplace}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractBVProblem","page":"SciMLProblems","title":"SciMLBase.AbstractBVProblem","text":"abstract type AbstractBVProblem{uType, tType, isinplace} <: SciMLBase.AbstractODEProblem{uType, tType, isinplace}\n\nBase for types which define BVP problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractJumpProblem","page":"SciMLProblems","title":"SciMLBase.AbstractJumpProblem","text":"abstract type AbstractJumpProblem{P, J} <: SciMLBase.AbstractDEProblem\n\nBase for types which define jump problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractSDDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractSDDEProblem","text":"abstract type AbstractSDDEProblem{uType, tType, lType, isinplace, ND} <: SciMLBase.AbstractDEProblem\n\nBase for types which define SDDE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractConstantLagSDDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractConstantLagSDDEProblem","text":"abstract type AbstractConstantLagSDDEProblem{uType, tType, lType, isinplace, ND} <: SciMLBase.AbstractSDDEProblem{uType, tType, lType, isinplace, ND}\n\n\n\n\n\n","category":"type"},{"location":"modules/SciMLBase/interfaces/Problems/#SciMLBase.AbstractPDEProblem","page":"SciMLProblems","title":"SciMLBase.AbstractPDEProblem","text":"abstract type AbstractPDEProblem <: SciMLBase.AbstractDEProblem\n\nBase for types which define PDE problems.\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/tutorials/optimization/#Modeling-Optimization-Problems","page":"Modeling Optimization Problems","title":"Modeling Optimization Problems","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/optimization/","page":"Modeling Optimization Problems","title":"Modeling Optimization Problems","text":"using ModelingToolkit, Optimization, OptimizationOptimJL\n\n@variables x y\n@parameters a b\nloss = (a - x)^2 + b * (y - x^2)^2\n@named sys = OptimizationSystem(loss,[x,y],[a,b])\n\nu0 = [\n    x=>1.0\n    y=>2.0\n]\np = [\n    a => 6.0\n    b => 7.0\n]\n\nprob = OptimizationProblem(sys,u0,p,grad=true,hess=true)\nsolve(prob,Newton())","category":"page"},{"location":"modules/ModelingToolkit/tutorials/optimization/","page":"Modeling Optimization Problems","title":"Modeling Optimization Problems","text":"Needs more text but it's super cool and auto-parallelizes and sparsifies too. Plus you can hierarchically nest systems to have it generate huge optimization problems.","category":"page"},{"location":"modules/DiffEqDevDocs/#SciML-Scientific-Machine-Learning-Developer-Documentation","page":"Home","title":"SciML Scientific Machine Learning Developer Documentation","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"This is the developer documentation and Contributor's Guide for the SciML ecosystem. It explains the common interface and some the package internals to help developers contribute.","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"If you have any questions, or just want to chat about solvers/using the package, please feel free to use the Gitter channel. For bug reports, feature requests, etc., please submit an issue.","category":"page"},{"location":"modules/DiffEqDevDocs/#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"The SciML ecosystem is built around the common interface. The common interface is a type-based interface where users define problems as a type, and solvers plug into the ecosystem by defining an algorithm to give a new dispatch to","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"__solve(prob,alg;kwargs...)\n__init(prob,alg;kwargs...)","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"There is a top level solve and init function which is in DiffEqBase.jl that handles distribution and function input (along with extra warnings) before sending the problems to the packages.","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"There is then an ecosystem of add-on components which use the common solver interface to add analysis tools for differential equations.","category":"page"},{"location":"modules/DiffEqDevDocs/#Contributing-to-the-Ecosystem","page":"Home","title":"Contributing to the Ecosystem","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"There are many ways to help the ecosystem. One way you can contribute is to give pull requests (PRs) to existing packages. Another way to contribute is to add your own package to the ecosystem. Adding your own package to the ecosystem allows you to keep executive control and licensing over your methods, but allows users of DifferentialEquations.jl to use your methods via the common interface, and makes your package compatible with the add-on tools (sensitivity analysis, parameter estimation, etc). Note that, in order for the method to be used as a default, one is required to move their package to the SciML organization so that way common maintenance (such as fixing deprication warnings, updating tests to newer versions, and emergency fixes / disabling) can be allowed by SciML members. However, the lead developer of the package maintains administrative control, and thus any change to the core algorithms by other SciML members will only be given through PRs.","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"Even if you don't have the time to contribute new solver algorithms or add-on tools, there's always ways to help! Improved plot recipes and new series recipes are always nice to add more default plots. It is always helpful to have benchmarks between different algorithms to see \"which is best\". Adding examples IJulia notebooks to DiffEqTutorials.jl is a good way to share knowledge about DifferentialEquations.jl. Also, please feel free to comb through the solvers and look for ways to make them more efficient. Lastly, the documentation could always use improvements. If you have any questions on how to help, just ask them in the Gitter!","category":"page"},{"location":"modules/DiffEqDevDocs/#Code-of-Conduct","page":"Home","title":"Code of Conduct","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"All contributors must adhere to the NumFOCUS Code of Conduct.  Treat everyone with respect. Failure to comply will result in individuals being banned from the community. ","category":"page"},{"location":"modules/DiffEqDevDocs/#Contributor-Guide","page":"Home","title":"Contributor Guide","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"Pages = [\n  \"contributing/ecosystem_overview.md\",\n  \"contributing/adding_packages.md\",\n  \"contributing/adding_algorithms.md\",\n  \"contributing/defining_problems.md\",\n  \"contributing/diffeq_internals.md\",\n  \"contributing/type_traits.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDevDocs/#Algorithm-Development-Tools","page":"Home","title":"Algorithm Development Tools","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"The following algorithm development tools are provided by DiffEqDevTools.jl","category":"page"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"Pages = [\n  \"alg_dev/test_problems.md\",\n  \"alg_dev/convergence.md\",\n  \"alg_dev/benchmarks.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/DiffEqDevDocs/#Internal-Documentation","page":"Home","title":"Internal Documentation","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/","page":"Home","title":"Home","text":"Pages = [\n  \"internals/fem_tools.md\",\n  \"internals/notes_on_algorithms.md\",\n  \"internals/tableaus.md\"\n]\nDepth = 2","category":"page"},{"location":"modules/JumpProcesses/api/#JumpProcesses.jl-API","page":"API","title":"JumpProcesses.jl API","text":"","category":"section"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"CurrentModule = JumpProcesses","category":"page"},{"location":"modules/JumpProcesses/api/#Core-Types","page":"API","title":"Core Types","text":"","category":"section"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"JumpProblem\nSSAStepper\nreset_aggregated_jumps!","category":"page"},{"location":"modules/JumpProcesses/api/#JumpProcesses.JumpProblem","page":"API","title":"JumpProcesses.JumpProblem","text":"mutable struct JumpProblem{iip, P, A, C, J<:Union{Nothing, JumpProcesses.AbstractJumpAggregator}, J2, J3, J4, R} <: SciMLBase.AbstractJumpProblem{P, J<:Union{Nothing, JumpProcesses.AbstractJumpAggregator}}\n\nDefines a collection of jump processes to associate with another problem type.\n\nDocumentation Page\nTutorial Page\nFAQ Page\n\nConstructors\n\nJumpProblems can be constructed by first building another problem type to which the jumps will be associated. For example, to  simulate a collection of jump processes for which the transition rates are constant between jumps (called ConstantRateJumps or MassActionJumps), we must first construct a DiscreteProblem\n\nprob = DiscreteProblem(u0, p, tspan)\n\nwhere u0 is the initial condition, p the parameters and tspan the time span. If we wanted to have the jumps coupled with a system of ODEs, or have transition rates with explicit time dependence, we would use an ODEProblem instead that defines the ODE portion of the dynamics.\n\nGiven prob we define the jumps via\n\nJumpProblem(prob, aggregator::AbstractAggregatorAlgorithm, jumps::JumpSet ; kwargs...)\nJumpProblem(prob, aggregator::AbstractAggregatorAlgorithm, jumps...; kwargs...)\n\nHere aggregator specifies the underlying algorithm for calculating next jump times and types, for example Direct. The collection of different AbstractJump types can then be passed within a single JumpSet or as subsequent sequential arguments.\n\nFields\n\nprob\nThe type of problem to couple the jumps to. For a pure jump process use DiscreteProblem, to couple to ODEs, ODEProblem, etc.\naggregator\nThe aggregator algorithm that determines the next jump times and types for ConstantRateJumps and MassActionJumps. Examples include Direct.\ndiscrete_jump_aggregation\nThe underlying state data associated with the chosen aggregator.\njump_callback\nCallBackSet with the underlying ConstantRate and VariableRate jumps.\nvariable_jumps\nThe VariableRateJumps.\nregular_jump\nThe RegularJumps.\nmassaction_jump\nThe MassActionJumps.\nrng\nThe random number generator to use.\n\nKeyword Arguments\n\nrng, the random number generator to use. On 1.7 and up defaults to Julia's builtin generator, below 1.7 uses RandomNumbers.jl's Xorshifts.Xoroshiro128Star(rand(UInt64)).\nsave_positions=(true,true), specifies whether to save the system's state (before,after) the jump occurs.\nspatial_system, for spatial problems the underlying spatial structure.\nhopping_constants, for spatial problems the spatial transition rate coefficients.\n\nPlease see the tutorial page in the DifferentialEquations.jl docs for usage examples and commonly asked questions.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.SSAStepper","page":"API","title":"JumpProcesses.SSAStepper","text":"struct SSAStepper <: SciMLBase.AbstractDEAlgorithm\n\nHighly efficient integrator for pure jump problems that involve only ConstantRateJumps and/or MassActionJumps.\n\nNotes\n\nOnly works with JumProblems defined from DiscreteProblems.\nOnly works with collections of ConstantRateJumps and MassActionJumps.\nOnly supports DiscreteCallbacks for events.\n\nExamples\n\nSIR model:\n\nusing JumpProcesses\nβ = 0.1 / 1000.0; ν = .01;\np = (β,ν)\nrate1(u,p,t) = p[1]*u[1]*u[2]  # β*S*I\nfunction affect1!(integrator)\n  integrator.u[1] -= 1         # S -> S - 1\n  integrator.u[2] += 1         # I -> I + 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = p[2]*u[2]      # ν*I\nfunction affect2!(integrator)\n  integrator.u[2] -= 1        # I -> I - 1\n  integrator.u[3] += 1        # R -> R + 1\nend\njump2 = ConstantRateJump(rate2,affect2!)\nu₀    = [999,1,0]\ntspan = (0.0,250.0)\nprob = DiscreteProblem(u₀, tspan, p)\njump_prob = JumpProblem(prob, Direct(), jump, jump2)\nsol = solve(jump_prob, SSAStepper())\n\nsee the tutorial for details.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.reset_aggregated_jumps!","page":"API","title":"JumpProcesses.reset_aggregated_jumps!","text":"reset_aggregated_jumps!(integrator, uprev = nothing; update_jump_params=true)\n\nReset the state of jump processes and associated solvers following a change in parameters or such.\n\nNotes     - update_jump_params=true will recalculate the rates stored within any       MassActionJump that was built from the parameter vector. If the parameter       vector is unchanged this can safely be set to false to improve performance.\n\n\n\n\n\n","category":"function"},{"location":"modules/JumpProcesses/api/#Types-of-Jumps","page":"API","title":"Types of Jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"ConstantRateJump\nMassActionJump\nVariableRateJump\nJumpSet","category":"page"},{"location":"modules/JumpProcesses/api/#JumpProcesses.ConstantRateJump","page":"API","title":"JumpProcesses.ConstantRateJump","text":"struct ConstantRateJump{F1, F2} <: JumpProcesses.AbstractJump\n\nDefines a jump process with a rate (i.e. hazard, intensity, or propensity) that does not explicitly depend on time. More precisely, one where the rate function is constant between the occurrence of jumps. For detailed examples and usage information see the\n\nTutorial\n\nFields\n\nrate\nFunction rate(u,p,t) that returns the jump's current rate.\naffect!\nFunction affect(integrator) that updates the state for one occurrence of the jump.\n\nExamples\n\nSuppose u[1] gives the amount of particles and p[1] the probability per time each particle can decay away. A corresponding ConstantRateJump for this jump process is\n\nrate(u,p,t) = p[1]*u[1]\naffect!(integrator) = integrator.u[1] -= 1\ncrj = ConstantRateJump(rate, affect!)\n\nNotice, here that rate changes in time, but is constant between the occurrence of jumps (when u[1] will decrease).\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.MassActionJump","page":"API","title":"JumpProcesses.MassActionJump","text":"struct MassActionJump{T, S, U, V} <: JumpProcesses.AbstractMassActionJump\n\nOptimized representation for ConstantRateJumps that can be represented in mass action form, offering improved performance within jump algorithms compared to ConstantRateJump. For detailed examples and usage information see the\n\nMain Docs\nTutorial\n\nConstructors\n\nMassActionJump(reactant_stoich, net_stoich; scale_rates=true, param_idxs=nothing)\n\nHere reactant_stoich denotes the reactant stoichiometry for each reaction and net_stoich the net stoichiometry for each reaction.\n\nFields\n\nscaled_rates\nThe (scaled) reaction rate constants.\nreactant_stoch\nThe reactant stoichiometry vectors.\nnet_stoch\nThe net stoichiometry vectors.\nparam_mapper\nParameter mapping functor to identify reaction rate constants with parameters in p vectors.\n\nKeyword Arguments\n\nscale_rates=true, whether to rescale the reaction rate constants according to the stoichiometry.\nnocopy=false, whether the MassActionJump can alias the scaled_rates and reactant_stoch from the input. Note, if scale_rates=true this will potentially modify both of these.\nparam_idxs=nothing, indexes in the parameter vector, JumpProblem.prob.p, that correspond to each reaction's rate.\n\nSee the tutorial and main docs for details.\n\nExamples\n\nAn SIR model with S + I --> 2I at rate β as the first reaction and I --> R at rate ν as the second reaction can be encoded by\n\np        = (β=1e-4, ν=.01)\nu0       = [999, 1, 0]       # (S,I,R)\ntspan    = (0.0, 250.0)\nrateidxs = [1, 2]           # i.e. [β,ν]\nreactant_stoich =\n[\n  [1 => 1, 2 => 1],         # 1*S and 1*I\n  [2 => 1]                  # 1*I\n]\nnet_stoich =\n[\n  [1 => -1, 2 => 1],        # -1*S and 1*I\n  [2 => -1, 3 => 1]         # -1*I and 1*R\n]\nmaj = MassActionJump(reactant_stoich, net_stoich; param_idxs=rateidxs)\nprob = DiscreteProblem(u0, tspan, p)\njprob = JumpProblem(prob, Direct(), maj)\n\nNotes\n\nBy default reaction rates are rescaled when constructing the MassActionJump as explained in the main docs. Disable this with the kwarg scale_rates=false.\nAlso see the main docs for how to specify reactions with no products or no reactants.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.VariableRateJump","page":"API","title":"JumpProcesses.VariableRateJump","text":"struct VariableRateJump{R, F, I, T, T2} <: JumpProcesses.AbstractJump\n\nDefines a jump process with a rate (i.e. hazard, intensity, or propensity) that may explicitly depend on time. More precisely, one where the rate function is allowed to change between the occurrence of jumps. For detailed examples and usage information see the\n\nTutorial\n\nFields\n\nrate\nFunction rate(u,p,t) that returns the jump's current rate.\naffect!\nFunction affect(integrator) that updates the state for one occurrence of the jump.\nidxs\nrootfind\ninterp_points\nsave_positions\nabstol\nreltol\n\nExamples\n\nSuppose u[1] gives the amount of particles and t*p[1] the probability per time each particle can decay away. A corresponding VariableRateJump for this jump process is\n\nrate(u,p,t) = t*p[1]*u[1]\naffect!(integrator) = integrator.u[1] -= 1\ncrj = VariableRateJump(rate, affect!)\n\nNotes\n\nVariableRateJumps result in integrators storing an effective state type that wraps the main state vector. See ExtendedJumpArray for details on using this object. Note that the presence of any VariableRateJumps will result in all ConstantRateJump, VariableRateJump and callback affect! functions receiving an integrator with integrator.u an ExtendedJumpArray.\nMust be used with ODEProblems or SDEProblems to be correctly simulated (i.e. can not currently be used with DiscreteProblems).\nSalis H., Kaznessis Y.,  Accurate hybrid stochastic simulation of a system of coupled chemical or biochemical reactions, Journal of Chemical Physics, 122 (5), DOI:10.1063/1.1835951 is used for calculating jump times with VariableRateJumps within ODE/SDE integrators.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.JumpSet","page":"API","title":"JumpProcesses.JumpSet","text":"struct JumpSet{T1, T2, T3, T4} <: JumpProcesses.AbstractJump\n\nDefines a collection of jumps that should collectively be included in a simulation.\n\nFields\n\nvariable_jumps\nCollection of VariableRateJumps\nconstant_jumps\nCollection of ConstantRateJumps\nregular_jump\nCollection of RegularJumps\nmassaction_jump\nCollection of MassActionJumps\n\nExamples\n\nHere we construct two jumps, store them in a JumpSet, and then simulate the resulting process.\n\nusing JumpProcesses, OrdinaryDiffEq\n\nrate1(u,p,t) = p[1]\naffect1!(integrator) = (integrator.u[1] += 1)\ncrj = ConstantRateJump(rate1, affect1!)\n\nrate2(u,p,t) = (t/(1+t))*p[2]*u[1]\naffect2!(integrator) = (integrator.u[1] -= 1)\nvrj = VariableRateJump(rate2, affect2!)\n\njset = JumpSet(crj, vrj)\n\nf!(du,u,p,t) = (du .= 0)\nu0 = [0.0]\np = (20.0, 2.0)\ntspan = (0.0, 200.0)\noprob = ODEProblem(f!, u0, tspan, p)\njprob = JumpProblem(oprob, Direct(), jset)\nsol = solve(jprob, Tsit5())\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#Aggregators","page":"API","title":"Aggregators","text":"","category":"section"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"Aggregators are the underlying algorithms used for sampling MassActionJumps and ConstantRateJumps.","category":"page"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"Direct\nDirectCR\nFRM\nNRM\nRDirect\nRSSA\nRSSACR\nSortingDirect","category":"page"},{"location":"modules/JumpProcesses/api/#JumpProcesses.Direct","page":"API","title":"JumpProcesses.Direct","text":"Gillespie's Direct method. ConstantRateJump rates and affects are stored in tuples. Fastest for a small (total) number of ConstantRateJumps or MassActionJumps (~10). For larger numbers of possible jumps use other methods.\n\nGillespie, Daniel T. (1976). A General Method for Numerically Simulating the Stochastic Time Evolution of Coupled Chemical Reactions. Journal of Computational Physics. 22 (4): 403–434. doi:10.1016/0021-9991(76)90041-3.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.DirectCR","page":"API","title":"JumpProcesses.DirectCR","text":"The Composition-Rejection Direct method. Performs best relative to other methods for systems with large numbers of jumps with special structure (for example a linear chain of reactions, or jumps corresponding to particles hopping on a grid or graph).\n\nA. Slepoy, A.P. Thompson and S.J. Plimpton, A constant-time kinetic Monte Carlo algorithm for simulation of large biochemical reaction networks, Journal of Chemical Physics, 128 (20), 205101 (2008). doi:10.1063/1.2919546\nS. Mauch and M. Stalzer, Efficient formulations for exact stochastic simulation of chemical systems, ACM Transactions on Computational Biology and Bioinformatics, 8 (1), 27-35 (2010). doi:10.1109/TCBB.2009.47\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.FRM","page":"API","title":"JumpProcesses.FRM","text":"Gillespie's First Reaction Method. Should not be used for practical applications due to slow performance relative to all other methods.\n\nGillespie, Daniel T. (1976). A General Method for Numerically Simulating the Stochastic Time Evolution of Coupled Chemical Reactions. Journal of Computational Physics. 22 (4): 403–434. doi:10.1016/0021-9991(76)90041-3.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.NRM","page":"API","title":"JumpProcesses.NRM","text":"The Next Reaction Method. Can significantly outperform Direct for systems with large numbers of jumps and sparse dependency graphs, but is usually slower than one of DirectCR, RSSA, or RSSACR for such systems.\n\nM. A. Gibson and J. Bruck, Efficient exact stochastic simulation of chemical systems with many species and many channels, Journal of Physical Chemistry A, 104 (9), 1876-1889 (2000). doi:10.1021/jp993732q\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.RDirect","page":"API","title":"JumpProcesses.RDirect","text":"A rejection-based direct method. \n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.RSSA","page":"API","title":"JumpProcesses.RSSA","text":"The Rejection SSA method. One of the best methods for systems with hundreds to many thousands of jumps (along with RSSACR) and sparse dependency graphs.\n\nV. H. Thanh, C. Priami and R. Zunino, Efficient rejection-based simulation of biochemical reactions with stochastic noise and delays, Journal of Chemical Physics, 141 (13), 134116 (2014). doi:10.1063/1.4896985\nV. H. Thanh, R. Zunino and C. Priami, On the rejection-based algorithm for simulation and analysis of large-scale reaction networks, Journal of Chemical Physics, 142 (24), 244106 (2015). doi:10.1063/1.4922923\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.RSSACR","page":"API","title":"JumpProcesses.RSSACR","text":"The Rejection SSA Composition-Rejection method. Often the best performer for systems with tens of thousands of jumps and sparse depedency graphs.\n\nV. H. Thanh, R. Zunino, and C. Priami, Efficient Constant-Time Complexity Algorithm for Stochastic Simulation of Large Reaction Networks, IEEE/ACM Transactions on Computational Biology and Bioinformatics, Vol. 14, No. 3, 657-667 (2017).\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.SortingDirect","page":"API","title":"JumpProcesses.SortingDirect","text":"The Sorting Direct method. Often the fastest algorithm for smaller to moderate sized systems (tens of jumps), or systems where a few jumps occur much more frequently than others.\n\nJ. M. McCollum, G. D. Peterson, C. D. Cox, M. L. Simpson and N. F. Samatova, The   sorting direct method for stochastic simulation of biochemical systems with   varying reaction execution behavior, Computational Biology and Chemistry, 30   (1), 39049 (2006). doi:10.1016/j.compbiolchem.2005.10.007\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#Private-API-Functions","page":"API","title":"Private API Functions","text":"","category":"section"},{"location":"modules/JumpProcesses/api/","page":"API","title":"API","text":"ExtendedJumpArray\nSSAIntegrator","category":"page"},{"location":"modules/JumpProcesses/api/#JumpProcesses.ExtendedJumpArray","page":"API","title":"JumpProcesses.ExtendedJumpArray","text":"struct ExtendedJumpArray{T3<:Number, T1, T<:AbstractArray{T3<:Number, T1}, T2} <: AbstractArray{T3<:Number, 1}\n\nExtended state definition used within integrators when there are VariableRateJumps in a system. For detailed examples and usage information see the\n\nTutorial\n\nFields\n\nu\nThe current state.\njump_u\nThe current rate (i.e. hazard, intensity, or propensity) values for the VariableRateJumps.\n\nExamples\n\nusing JumpProcesses, OrdinaryDiffEq\nf(du,u,p,t) = du .= 0\nrate(u,p,t) = (1+t)*u[1]*u[2]\n\n# suppose we wish to decrease each of the two variables by one\n# when a jump occurs\nfunction affect!(integrator)\n   # Method 1, direct indexing works like normal\n   integrator.u[1] -= 1\n   integrator.u[2] -= 1\n\n   # Method 2, if we want to broadcast or use array operations we need\n   # to access integrator.u.u which is the actual state object.\n   # So equivalently to above we could have said:\n   # integrator.u.u .-= 1\nend\n\nu0 = [10.0, 10.0]\nvrj = VariableRateJump(rate, affect!)\noprob = ODEProblem(f, u0, (0.0,2.0))\njprob = JumpProblem(oprob, Direct(), vrj)\nsol = solve(jprob,Tsit5())\n\nNotes\n\nIf ueja isa ExtendedJumpArray with ueja.u of size N and ueja.jump_u of size num_variableratejumps then\n# for 1 <= i <= N\nueja[i] == ueja.u[i]\n\n# for N < i <= (N+num_variableratejumps)\nueja[i] == ueja.jump_u[i]\nIn a system with VariableRateJumps all callback, ConstantRateJump, and VariableRateJump affect! functions will receive integrators with integrator.u an ExtendedJumpArray.\nAs such, affect! functions that wish to modify the state via vector operations should use ueja.u.u to obtain the aliased state object.\n\n\n\n\n\n","category":"type"},{"location":"modules/JumpProcesses/api/#JumpProcesses.SSAIntegrator","page":"API","title":"JumpProcesses.SSAIntegrator","text":"mutable struct SSAIntegrator{F, uType, tType, tdirType, P, S, CB, SA, OPT, TS} <: SciMLBase.DEIntegrator{SSAStepper, Nothing, uType, tType}\n\nSolution objects for pure jump problems solved via SSAStepper.\n\nFields\n\nf\nThe underlying prob.f function. Not currently used.\nu\nThe current solution values.\nt\nThe current solution time.\ntprev\nThe previous time a jump occured.\ntdir\nThe direction time is changing in (must be positive indicating time is increasing)\np\nThe current parameters.\nsol\nThe current solution object.\ni\ntstop\nThe next jump time.\ncb\nThe jump aggregator callback.\nsaveat\nTimes to save the solution at.\nsave_everystep\nWhether to save everytime a jump occurs.\nsave_end\nWhether to save at the final step.\ncur_saveat\nIndex of the next saveat time.\nopts\nTuple storing callbacks.\ntstops\nUser supplied times to step to, useful with callbacks.\ntstops_idx\nu_modified\nkeep_stepping\n\n\n\n\n\n","category":"type"},{"location":"modules/LinearSolve/basics/Preconditioners/#prec","page":"Preconditioners","title":"Preconditioners","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Many linear solvers can be accelerated by using what is known as a preconditioner, an approximation to the matrix inverse action which is cheap to evaluate. These can improve the numerical conditioning of the solver process and in turn improve the performance. LinearSolve.jl provides an interface for the definition of preconditioners which works with the wrapped packages.","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/#Using-Preconditioners","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/#Mathematical-Definition","page":"Preconditioners","title":"Mathematical Definition","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Preconditioners are specified in the keyword arguments to init or solve: Pl for left and Pr for right preconditioner, respectively. The right preconditioner, P_r transforms the linear system Au = b into the form:","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"AP_r^-1(P_r u) = AP_r^-1y = b","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"which is solved for y, and then P_r u = y is solved for u. The left preconditioner, P_l, transforms the linear system into the form:","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l^-1(Au - b) = 0","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"A two-sided preconditioned system is of the form:","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"P_l A P_r^-1 (P_r u) = P_l b","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"By default, if no preconditioner is given the preconditioner is assumed to be the identity I.","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/#Using-Preconditioners-2","page":"Preconditioners","title":"Using Preconditioners","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"In the following, we will use the DiagonalPreconditioner to define a two-sided preconditioned system which first divides by some random numbers and then multiplies by the same values. This is commonly used in the case where if, instead of random, s is an approximation to the eigenvalues of a system.","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"using LinearSolve, LinearAlgebra\r\ns = rand(n)\r\nPl = Diagonal(s)\r\n\r\nA = rand(n,n)\r\nb = rand(n)\r\n\r\nprob = LinearProblem(A,b)\r\nsol = solve(prob,IterativeSolvers_GMRES(),Pl=Pl)","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/#Preconditioner-Interface","page":"Preconditioners","title":"Preconditioner Interface","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"To define a new preconditioner you define a Julia type which satisfies the following interface:","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"Base.eltype(::Preconditioner) (Required only for Krylov.jl)\nLinearAlgebra.ldiv!(::AbstractVector,::Preconditioner,::AbstractVector) and LinearAlgebra.ldiv!(::Preconditioner,::AbstractVector)","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/#Curated-List-of-Pre-Defined-Preconditioners","page":"Preconditioners","title":"Curated List of Pre-Defined Preconditioners","text":"","category":"section"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"The following preconditioners match the interface of LinearSolve.jl.","category":"page"},{"location":"modules/LinearSolve/basics/Preconditioners/","page":"Preconditioners","title":"Preconditioners","text":"LinearSolve.ComposePreconditioner(prec1,prec2): composes the preconditioners to apply prec1 before prec2.\nLinearSolve.InvPreconditioner(prec): inverts mul! and ldiv! in a preconditioner definition as a lazy inverse.\nLinearAlgera.Diagonal(s::Union{Number,AbstractVector}): the lazy Diagonal matrix type of Base.LinearAlgebra. Used for efficient construction of a diagonal preconditioner.\nOther Base.LinearAlgera types: all define the full Preconditioner interface.\nIncompleteLU.ilu: an implementation of the incomplete LU-factorization preconditioner. This requires A as a SparseMatrixCSC.\nPreconditioners.CholeskyPreconditioner(A, i): An incomplete Cholesky preconditioner with cut-off level i. Requires A as a AbstractMatrix and positive semi-definite.\nAlgebraicMultiGrid: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via AlgebraicMultiGrid.aspreconditioner(AlgebraicMultiGrid.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nAlgebraicMultiGrid.ruge_stuben(A)\nAlgebraicMultiGrid.smoothed_aggregation(A)\nPyAMG: Implementations of the algebraic multigrid method. Must be converted to a preconditioner via PyAMG.aspreconditioner(PyAMG.precmethod(A)). Requires A as a AbstractMatrix. Provides the following methods:\nPyAMG.RugeStubenSolver(A)\nPyAMG.SmoothedAggregationSolver(A)\nILUZero.ILU0Precon(A::SparseMatrixCSC{T,N}, b_type = T): An incomplete LU implementation. Requires A as a SparseMatrixCSC.\nLimitedLDLFactorizations.lldl: A limited-memory LDLᵀ factorization for symmetric matrices. Requires A as a SparseMatrixCSC. Applying F = lldl(A); F.D .= abs.(F.D) before usage as a preconditioner makes the preconditioner symmetric postive definite and thus is required for Krylov methods which are specialized for symmetric linear systems.\nRandomizedPreconditioners.NystromPreconditioner A randomized sketching method for positive semidefinite matrices A. Builds a preconditioner P  A + μ*I for the system (A + μ*I)x = b","category":"page"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#Matrix-Exponentials","page":"Matrix Exponentials","title":"Matrix Exponentials","text":"","category":"section"},{"location":"modules/ExponentialUtilities/matrix_exponentials/","page":"Matrix Exponentials","title":"Matrix Exponentials","text":"exponential!\nphi","category":"page"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.exponential!","page":"Matrix Exponentials","title":"ExponentialUtilities.exponential!","text":"E=exponential!(A,[method [cache]])\n\nComputes the matrix exponential with method specified in method. The contents of A is modified allowing for less allocations. The method parameter specifies the implementation and implementation parameters, e.g. ExpMethodNative, ExpMethodDiagonalization, ExpMethodGeneric, ExpMethodHigham2005. Memory needed can be preallocated and provided in parameter cache such that the memory can recycled when calling exponential! several times. The preallocation is done with the command alloc_mem: cache=alloc_mem(A,method).\n\nExample\n\njulia> A=randn(50,50);\njulia> Acopy=B*2;\njulia> method=ExpMethodHigham2005();\njulia> cache=alloc_mem(A,method); # Main allocation done here\njulia> E1=exponential!(A,method,cache) # Very little allocation here\njulia> E2=exponential!(B,method,cache) # Very little allocation here\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#Methods","page":"Matrix Exponentials","title":"Methods","text":"","category":"section"},{"location":"modules/ExponentialUtilities/matrix_exponentials/","page":"Matrix Exponentials","title":"Matrix Exponentials","text":"ExpMethodHigham2005\nExpMethodHigham2005Base\nExpMethodGeneric\nExpMethodNative\nExpMethodDiagonalization","category":"page"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.ExpMethodHigham2005","page":"Matrix Exponentials","title":"ExponentialUtilities.ExpMethodHigham2005","text":"ExpMethodHigham2005(A::AbstractMatrix);\nExpMethodHigham2005(b::Bool=true);\n\nComputes the matrix exponential using the algorithm Higham, N. J. (2005). \"The scaling and squaring method for the matrix exponential revisited.\" SIAM J. Matrix Anal. Appl.Vol. 26, No. 4, pp. 1179–1193\" based on generated code. If a matrix is specified, balancing is determined automatically.\n\n\n\n\n\n","category":"type"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.ExpMethodHigham2005Base","page":"Matrix Exponentials","title":"ExponentialUtilities.ExpMethodHigham2005Base","text":"ExpMethodHigham2005Base()\n\nThe same as ExpMethodHigham2005 but follows Base.exp closer.\n\n\n\n\n\n","category":"type"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.ExpMethodGeneric","page":"Matrix Exponentials","title":"ExponentialUtilities.ExpMethodGeneric","text":"struct ExpMethodGeneric{T}\nExpMethodGeneric()=ExpMethodGeneric{Val{13}}();\n\nGeneric exponential implementation of the method ExpMethodHigham2005, for any exp argument x  for which the functions LinearAlgebra.opnorm, +, *, ^, and / (including addition with UniformScaling objects) are defined. The type T is used to adjust the number of terms used in the Pade approximants at compile time.\n\nSee \"The Scaling and Squaring Method for the Matrix Exponential Revisited\" by Higham, Nicholas J. in 2005 for algorithm details.\n\n\n\n\n\n","category":"type"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.ExpMethodNative","page":"Matrix Exponentials","title":"ExponentialUtilities.ExpMethodNative","text":"ExpMethodNative()\n\nMatrix exponential method corresponding to calling Base.exp.\n\n\n\n\n\n","category":"type"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#ExponentialUtilities.ExpMethodDiagonalization","page":"Matrix Exponentials","title":"ExponentialUtilities.ExpMethodDiagonalization","text":"ExpMethodDiagonalization(enforce_real=true)\n\nMatrix exponential method corresponding to the diagonalization with eigen possibly by removing imaginary part introduced by the numerical approximation.\n\n\n\n\n\n","category":"type"},{"location":"modules/ExponentialUtilities/matrix_exponentials/#Utilities","page":"Matrix Exponentials","title":"Utilities","text":"","category":"section"},{"location":"modules/ExponentialUtilities/matrix_exponentials/","page":"Matrix Exponentials","title":"Matrix Exponentials","text":"alloc_mem","category":"page"},{"location":"modules/LinearSolve/basics/LinearProblem/#Linear-Problems","page":"Linear Problems","title":"Linear Problems","text":"","category":"section"},{"location":"modules/LinearSolve/basics/LinearProblem/","page":"Linear Problems","title":"Linear Problems","text":"LinearProblem","category":"page"},{"location":"modules/LinearSolve/basics/LinearProblem/#SciMLBase.LinearProblem","page":"Linear Problems","title":"SciMLBase.LinearProblem","text":"Defines a linear system problem. Documentation Page: http://linearsolve.sciml.ai/dev/basics/LinearProblem/\n\nMathematical Specification of a Linear Problem\n\nConcrete LinearProblem\n\nTo define a LinearProblem, you simply need to give the AbstractMatrix A and an AbstractVector b which defines the linear system:\n\nAu = b\n\nMatrix-Free LinearProblem\n\nFor matrix-free versions, the specification of the problem is given by an operator A(u,p,t) which computes A*u, or in-place as A(du,u,p,t). These are specified via the AbstractSciMLOperator interface. For more details, see the SciMLBase Documentation.\n\nNote that matrix-free versions of LinearProblem definitions are not compatible with all solvers. To check a solver for compatibility, use the function xxxxx.\n\nProblem Type\n\nConstructors\n\nOptionally, an initial guess u₀ can be supplied which is used for iterative methods.\n\nLinearProblem{isinplace}(A,x,p=NullParameters();u0=nothing,kwargs...)\nLinearProblem(f::AbstractDiffEqOperator,u0,p=NullParameters();u0=nothing,kwargs...)\n\nisinplace optionally sets whether the function is in-place or not, i.e. whether the solvers are allowed to mutate. By default this is true for AbstractMatrix, and for AbstractSciMLOperators it matches the choice of the operator definition.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers.\n\nFields\n\nA: The representation of the linear operator.\nb: The right-hand side of the linear system.\np: The parameters for the problem. Defaults to NullParameters. Currently unused.\nu0: The initial condition used by iterative solvers.\nkwargs: The keyword arguments passed on to the solvers.\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/#Contextual-Variable-Types","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"ModelingToolkit.jl has a system of contextual variable types which allows for helping the system transformation machinery do complex manipulations and automatic detection. The standard variable definition in ModelingToolkit.jl is the @variable which is defined by Symbolics.jl. For example:","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"@variables x y(x)","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"This is used for the \"normal\" variable of a given system, like the states of a differential equation or objective function. All of the macros below support the same syntax as @variables.","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/#Parameters","page":"Contextual Variable Types","title":"Parameters","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"All modeling projects have some form of parameters. @parameters marks a variable as being the parameter of some system, which allows automatic detection algorithms to ignore such variables when attempting to find the states of a system.","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/#Variable-metadata-[Experimental/TODO]","page":"Contextual Variable Types","title":"Variable metadata [Experimental/TODO]","text":"","category":"section"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"In many engineering systems some variables act like \"flows\" while others do not. For example, in circuit models you have current which flows, and the related voltage which does not. Or in thermal models you have heat flows. In these cases, the connect statement enforces conservation of flow between all of the connected components.","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"For example, the following specifies that x is a 2x2 matrix of flow variables with the unit m^3/s:","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"@variables x[1:2,1:2] [connect = Flow; unit = u\"m^3/s\"]","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"ModelingToolkit defines connect, unit, noise, and description keys for the metadata. One can get and set metadata by","category":"page"},{"location":"modules/ModelingToolkit/basics/ContextualVariables/","page":"Contextual Variable Types","title":"Contextual Variable Types","text":"julia> @variables x [unit = u\"m^3/s\"];\n\njulia> hasmetadata(x, Symbolics.option_to_metadata_type(Val(:unit)))\ntrue\n\njulia> getmetadata(x, Symbolics.option_to_metadata_type(Val(:unit)))\nm³ s⁻¹\n\njulia> x = setmetadata(x, Symbolics.option_to_metadata_type(Val(:unit)), u\"m/s\")\nx\n\njulia> getmetadata(x, Symbolics.option_to_metadata_type(Val(:unit)))\nm s⁻¹","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/#Component-Based-Modeling-a-Spring-Mass-System","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"In this tutorial we will build a simple component-based model of a spring-mass system. A spring-mass system consists of one or more masses connected by springs. Hooke's law gives the force exerted by a spring when it is extended or compressed by a given distance. This specifies a differential-equation system where the acceleration of the masses is specified using the forces acting on them.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/#Copy-Paste-Example","page":"Component-Based Modeling a Spring-Mass System","title":"Copy-Paste Example","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"using ModelingToolkit, Plots, DifferentialEquations, LinearAlgebra\nusing Symbolics: scalarize\n\n@variables t\nD = Differential(t)\n\nfunction Mass(; name, m = 1.0, xy = [0., 0.], u = [0., 0.])\n    ps = @parameters m=m\n    sts = @variables pos(t)[1:2]=xy v(t)[1:2]=u\n    eqs = scalarize(D.(pos) .~ v)\n    ODESystem(eqs, t, [pos..., v...], ps; name)\nend\n\nfunction Spring(; name, k = 1e4, l = 1.)\n    ps = @parameters k=k l=l\n    @variables x(t), dir(t)[1:2]\n    ODESystem(Equation[], t, [x, dir...], ps; name)\nend\n\nfunction connect_spring(spring, a, b)\n    [\n        spring.x ~ norm(scalarize(a .- b))\n        scalarize(spring.dir .~ scalarize(a .- b))\n    ]\nend\n\nspring_force(spring) = -spring.k .* scalarize(spring.dir) .* (spring.x - spring.l)  ./ spring.x\n\nm = 1.0\nxy = [1., -1.]\nk = 1e4\nl = 1.\ncenter = [0., 0.]\ng = [0., -9.81]\n@named mass = Mass(m=m, xy=xy)\n@named spring = Spring(k=k, l=l)\n\neqs = [\n    connect_spring(spring, mass.pos, center)\n    scalarize(D.(mass.v) .~ spring_force(spring) / mass.m .+ g)\n]\n\n@named _model = ODESystem(eqs, t, [spring.x; spring.dir; mass.pos], [])\n@named model = compose(_model, mass, spring)\nsys = structural_simplify(model)\n\nprob = ODEProblem(sys, [], (0., 3.))\nsol = solve(prob, Rosenbrock23())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/#Explanation","page":"Component-Based Modeling a Spring-Mass System","title":"Explanation","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/#Building-the-components","page":"Component-Based Modeling a Spring-Mass System","title":"Building the components","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"For each component we use a Julia function that returns an ODESystem. At the top, we define the fundamental properties of a Mass: it has a mass m, a position pos and a velocity v. We also define that the velocity is the rate of change of position with respect to time.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"function Mass(; name, m = 1.0, xy = [0., 0.], u = [0., 0.])\n    ps = @parameters m=m\n    sts = @variables pos(t)[1:2]=xy v(t)[1:2]=u\n    eqs = scalarize(D.(pos) .~ v)\n    ODESystem(eqs, t, [pos..., v...], ps; name)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Note that this is an incompletely specified ODESystem. It cannot be simulated on its own since the equations for the velocity v[1:2](t) are unknown. Notice the addition of a name keyword. This allows us to generate different masses with different names. A Mass can now be constructed as:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Mass(name = :mass1)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Or using the @named helper macro","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"@named mass1 = Mass()","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Next we build the spring component. It is characterised by the spring constant k and the length l of the spring when no force is applied to it. The state of a spring is defined by its current length and direction.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"function Spring(; name, k = 1e4, l = 1.)\n    ps = @parameters k=k l=l\n    @variables x(t), dir(t)[1:2]\n    ODESystem(Equation[], t, [x, dir...], ps; name)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We now define functions that help construct the equations for a mass-spring system. First, the connect_spring function connects a spring between two positions a and b. Note that a and b can be the pos of a Mass, or just a fixed position such as [0., 0.]. In that sense, the length of the spring x is given by the length of the vector dir joining a and b.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"function connect_spring(spring, a, b)\n    [\n        spring.x ~ norm(scalarize(a .- b))\n        scalarize(spring.dir .~ scalarize(a .- b))\n    ]\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Lastly, we define the spring_force function that takes a spring and returns the force exerted by this spring.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"spring_force(spring) = -spring.k .* scalarize(spring.dir) .* (spring.x - spring.l)  ./ spring.x","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"To create our system, we will first create the components: a mass and a spring. This is done as follows:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"m = 1.0\nxy = [1., -1.]\nk = 1e4\nl = 1.\ncenter = [0., 0.]\ng = [0., -9.81]\n@named mass = Mass(m=m, xy=xy)\n@named spring = Spring(k=k, l=l)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We can now create the equations describing this system, by connecting spring to mass and a fixed point.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"eqs = [\n    connect_spring(spring, mass.pos, center)\n    scalarize(D.(mass.v) .~ spring_force(spring) / mass.m .+ g)\n]","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"Finally, we can build the model using these equations and components.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"@named _model = ODESystem(eqs, t, [spring.x; spring.dir; mass.pos], [])\n@named model = compose(_model, mass, spring)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We can take a look at the equations in the model using the equations function.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"equations(model)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"The states of this model are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"states(model)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"And the parameters of this model are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"parameters(model)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/#Simplifying-and-solving-this-system","page":"Component-Based Modeling a Spring-Mass System","title":"Simplifying and solving this system","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"This system can be solved directly as a DAE using one of the DAE solvers from DifferentialEquations.jl. However, we can symbolically simplify the system first beforehand. Running structural_simplify eliminates unnecessary variables from the model to give the leanest numerical representation of the system.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"sys = structural_simplify(model)\nequations(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We are left with only 4 equations involving 4 state variables (mass.pos[1], mass.pos[2], mass.v[1], mass.v[2]). We can solve the system by converting it to an ODEProblem. Some observed variables are not expanded by default. To view the complete equations, one can do","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"full_equations(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"This is done as follows:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"prob = ODEProblem(sys, [], (0., 3.))\nsol = solve(prob, Rosenbrock23())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"What if we want the timeseries of a different variable? That information is not lost! Instead, structural_simplify simply changes state variables into observed variables.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"observed(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"These are explicit algebraic equations which can be used to reconstruct the required variables on the fly. This leads to dramatic computational savings since implicitly solving an ODE scales as O(n^3), so fewer states are significantly better!","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We can access these variables using the solution object. For example, let's retrieve the x-position of the mass over time:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"sol[mass.pos[1]]","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"We can also plot the path of the mass:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/spring_mass/","page":"Component-Based Modeling a Spring-Mass System","title":"Component-Based Modeling a Spring-Mass System","text":"plot(sol, vars = (mass.pos[1], mass.pos[2]))","category":"page"},{"location":"modules/SciMLOperators/interface/#The-AbstractSciMLOperator-Interface","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"","category":"section"},{"location":"modules/SciMLOperators/interface/#Formal-Properties-of-SciMLOperators","page":"The AbstractSciMLOperator Interface","title":"Formal Properties of SciMLOperators","text":"","category":"section"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"These are the formal properties that an AbstractSciMLOperator should obey for it to work in the solvers.","category":"page"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"An AbstractSciMLOperator represents a linear or nonlinear operator with input/output being AbstractArrays. Specifically, a SciMLOperator, L, of size (M,N) accepts input argument u with leading length N, i.e. size(u, 1) == N, and returns an AbstractArray of the same dimension with leading length M, i.e. size(L * u, 1) == M.\nSciMLOperators can be applied to an AbstractArray via overloaded Base.*, or the in-place LinearAlgebra.mul!. Additionally, operators are allowed to be time, or parameter dependent. The state of a SciMLOperator can be updated by calling the mutating function update_coefficients!(L, u, p, t) where p representes parameters, and t, time.  Calling a SciMLOperator as L(du, u, p, t) or out-of-place L(u, p, t) will automatically update the state of L before applying it to u. L(u, p, t) is the same operation as L(u, p, t) * u.\nTo support the update functionality, we have lazily implemented a comprehensive operator algebra. That means a user can add, subtract, scale, compose and invert SciMLOperators, and the state of the resultant operator would be updated as expected upon calling L(du, u, p, t) or L(u, p, t) so long as an update function is provided for the component operators.","category":"page"},{"location":"modules/SciMLOperators/interface/#AbstractSciMLOperator-Interface-Description","page":"The AbstractSciMLOperator Interface","title":"AbstractSciMLOperator Interface Description","text":"","category":"section"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"AbstractSciMLLinearOperator <: AbstractSciMLOperator\nAbstractSciMLScalarOperator <: AbstractSciMLLinearOperator\nisconstant(A) trait for whether the operator is constant or not.\nOptional: exp(A). Required for simple exponential integration.\nOptional: expv(A,u,t) = exp(t*A)*u and expv!(v,A::AbstractSciMLOperator,u,t) Required for sparse-saving exponential integration.\nOptional: factorizations. ldiv!, factorize et. al. This is only required for algorithms which use the factorization of the operator (Crank-Nicolson), and only for when the default linear solve is used.","category":"page"},{"location":"modules/SciMLOperators/interface/#Note-About-Affine-Operators","page":"The AbstractSciMLOperator Interface","title":"Note About Affine Operators","text":"","category":"section"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"Affine operators are operators which have the action Q*x = A*x + b. These operators have no matrix representation, since if there was it would be a linear operator instead of an  affine operator. You can only represent an affine operator as a linear operator in a  dimension of one larger via the operation: [A b] * [u;1], so it would require something modified  to the input as well. As such, affine operators are a distinct generalization of linear operators.","category":"page"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"While it this seems like it might doom the idea of using matrix-free affine operators, it turns out  that affine operators can be used in all cases where matrix-free linear solvers are used due to an easy genearlization of the standard convergence proofs. If Q is the affine operator  Q(x) = Ax + b, then solving Qx = c is equivalent to solving Ax + b = c or Ax = c-b.  If you know do this same \"plug-and-chug\" handling of the affine operator in into the GMRES/CG/etc.  convergence proofs, move the affine part to the rhs residual, and show it converges to solving  Ax = c-b, and thus GMRES/CG/etc. solves Q(x) = c for an affine operator properly. ","category":"page"},{"location":"modules/SciMLOperators/interface/","page":"The AbstractSciMLOperator Interface","title":"The AbstractSciMLOperator Interface","text":"That same trick then can be used pretty much anywhere you would've had a linear operator to extend  the proof to affine operators, so then exp(A*t)*v operations via Krylov methods work for A being  affine as well, and all sorts of things. Thus affine operators have no matrix representation but they  are still compatible with essentially any Krylov method which would otherwise be compatible with matrix-free representations, hence their support in the SciMLOperators interface.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/#Strategies-to-Avoid-Local-Minima","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Local minima can be an issue with fitting neural differential equations. However, there are many strategies to avoid local minima:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Insert stochasticity into the loss function through minibatching\nWeigh the loss function to allow for fitting earlier portions first\nChanging the optimizers to allow_f_increases\nIteratively grow the fit\nTraining the initial conditions and the parameters to start","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/#allow_f_increasestrue","page":"Strategies to Avoid Local Minima","title":"allow_f_increases=true","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"With Optim.jl optimizers, you can set allow_f_increases=true in order to let increases in the loss function not cause an automatic halt of the optimization process. Using a method like BFGS or NewtonTrustRegion is not guaranteed to have monotonic convergence and so this can stop early exits which can result in local minima. This looks like:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"pmin = Optimization.solve(optprob, NewtonTrustRegion(), callback=callback,\n                              maxiters = 200, allow_f_increases = true)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/#Iterative-Growing-Of-Fits-to-Reduce-Probability-of-Bad-Local-Minima","page":"Strategies to Avoid Local Minima","title":"Iterative Growing Of Fits to Reduce Probability of Bad Local Minima","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"In this example we will show how to use strategy (4) in order to increase the robustness of the fit. Let's start with the same neural ODE example we've used before except with one small twist: we wish to find the neural ODE that fits on (0,5.0). Naively, we use the same training strategy as before:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"using Lux, DiffEqFlux, DifferentialEquations, Optimization, OptimizationOptimJL, Plots, Random\n\nrng = Random.default_rng()\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = Float32[-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = Lux.Chain(ActivationFunction(x -> x.^3),\n                  Lux.Dense(2, 16, tanh),\n                  Lux.Dense(16, 2))\n\npinit, st = Lux.setup(rng, dudt2)\npinit = Lux.ComponentArray(pinit)\nprob_neuralode = NeuralODE(dudt2, tspan, Vern7(), saveat = tsteps, abstol=1e-6, reltol=1e-6)\n\nfunction predict_neuralode(p)\n  Array(prob_neuralode(u0, p, st)[1])\nend\n\nfunction loss_neuralode(p)\n    pred = predict_neuralode(p)\n    loss = sum(abs2, (ode_data[:,1:size(pred,2)] .- pred))\n    return loss, pred\nend\n\niter = 0\ncallback = function (p, l, pred; doplot = false)\n  global iter\n  iter += 1\n\n  println(l)\n  if doplot\n    # plot current prediction against data\n    plt = scatter(tsteps[1:size(pred,2)], ode_data[1,1:size(pred,2)], label = \"data\")\n    scatter!(plt, tsteps[1:size(pred,2)], pred[1,:], label = \"prediction\")\n    display(plot(plt))\n  end\n\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_neuralode(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, pinit)\nresult_neuralode = Optimization.solve(optprob,\n                                      ADAM(0.05), callback = callback,\n                                      maxiters = 300)\n\ncallback(result_neuralode.u,loss_neuralode(result_neuralode.u)...;doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"However, we've now fallen into a trap of a local minima. If the optimizer changes the parameters so it dips early, it will increase the loss because there will be more error in the later parts of the time series. Thus it tends to just stay flat and never fit perfectly. This thus suggests strategies (2) and (3): do not allow the later parts of the time series to influence the fit until the later stages. Strategy (3) seems to be more robust, so this is what will be demonstrated.","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Let's start by reducing the timespan to (0,1.5):","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0f0,1.5f0), Tsit5(), saveat = tsteps[tsteps .<= 1.5])\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_neuralode(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, pinit)\nresult_neuralode2 = Optimization.solve(optprob,\n                                      ADAM(0.05), callback = callback,\n                                      maxiters = 300)\n\ncallback(result_neuralode2.u,loss_neuralode(result_neuralode2.u)...;doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"This fits beautifully. Now let's grow the timespan and utilize the parameters from our (0,1.5) fit as the initial condition to our next fit:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0f0,3.0f0), Tsit5(), saveat = tsteps[tsteps .<= 3.0])\n\noptprob = Optimization.OptimizationProblem(optf, result_neuralode.u)\nresult_neuralode3 = Optimization.solve(optprob,\n                                        ADAM(0.05), maxiters = 300,\n                                        callback = callback)\ncallback(result_neuralode3.u,loss_neuralode(result_neuralode3.u)...;doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"Once again a great fit. Now we utilize these parameters as the initial condition to the full fit:","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"prob_neuralode = NeuralODE(dudt2, (0.0f0,5.0f0), Tsit5(), saveat = tsteps)\noptprob = Optimization.OptimizationProblem(optf, result_neuralode3.u)\nresult_neuralode4 = Optimization.solve(optprob,\n                                      ADAM(0.01), maxiters = 300,\n                                      callback = callback)\ncallback(result_neuralode4.u,loss_neuralode(result_neuralode4.u)...;doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"(Image: )","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/#Training-both-the-initial-conditions-and-the-parameters-to-start","page":"Strategies to Avoid Local Minima","title":"Training both the initial conditions and the parameters to start","text":"","category":"section"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"In this example we will show how to use strategy (5) in order to accomplish the same goal, except rather than growing the trajectory iteratively, we can train on the whole trajectory. We do this by allowing the neural ODE to learn both the initial conditions and parameters to start, and then reset the initial conditions back and train only the parameters. Note: this strategy is demonstrated for the (0, 5) time span and (0, 10), any longer and more iterations will be required. Alternatively, one could use a mix of (4) and (5), or breaking up the trajectory into chunks and just (5).","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"\nusing Flux, Plots, DifferentialEquations, SciMLSensitivity\n\n\n#Starting example with tspan (0, 5)\nu0 = Float32[2.0; 0.0]\ndatasize = 30\ntspan = (0.0f0, 5.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nfunction trueODEfunc(du, u, p, t)\n    true_A = [-0.1 2.0; -2.0 -0.1]\n    du .= ((u.^3)'true_A)'\nend\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\n#Using flux here to easily demonstrate the idea, but this can be done with Optimization.solve!\ndudt2 = Chain(Dense(2,16, tanh),\n             Dense(16,2))\n\n\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\nprob = ODEProblem(dudt,u0,tspan)\n\nfunction predict_n_ode()\n    Array(solve(prob,u0=u0,p=p, saveat=tsteps))\nend\n\nfunction loss_n_ode()\n      pred = predict_n_ode()\n      sqnorm(x) = sum(abs2, x)\n      loss = sum(abs2,ode_data .- pred)\n      loss\nend\n\nfunction callback(;doplot=true) #callback function to observe training\n    pred = predict_n_ode()\n    display(sum(abs2,ode_data .- pred))\n    if doplot\n      # plot current prediction against data\n      pl = plot(tsteps,ode_data[1,:],label=\"data\")\n      plot!(pl,tsteps,pred[1,:],label=\"prediction\")\n      display(plot(pl))\n    end\n    return false\nend\npredict_n_ode()\nloss_n_ode()\ncallback()\n\ndata = Iterators.repeated((), 1000)\n\n#Specify to flux to include both the initial conditions (IC) and parameters of the NODE to train\nFlux.train!(loss_n_ode, Flux.params(u0, p), data,\n                    Flux.Optimise.ADAM(0.05), cb = callback)\n\n#Here we reset the IC back to the original and train only the NODE parameters\nu0 = Float32[2.0; 0.0]\nFlux.train!(loss_n_ode, Flux.params(p), data,\n            Flux.Optimise.ADAM(0.05), cb = callback)\n\ncallback()\n\n#Now use the same technique for a longer tspan (0, 10)\ndatasize = 30\ntspan = (0.0f0, 10.0f0)\ntsteps = range(tspan[1], tspan[2], length = datasize)\n\nprob_trueode = ODEProblem(trueODEfunc, u0, tspan)\node_data = Array(solve(prob_trueode, Tsit5(), saveat = tsteps))\n\ndudt2 = Chain(Dense(2,16, tanh),\n             Dense(16,2))\n\np,re = Flux.destructure(dudt2) # use this p as the initial condition!\ndudt(u,p,t) = re(p)(u) # need to restrcture for backprop!\nprob = ODEProblem(dudt,u0,tspan)\n\n\n\ndata = Iterators.repeated((), 1500)\n\nFlux.train!(loss_n_ode, Flux.params(u0, p), data,\n                    Flux.Optimise.ADAM(0.05), cb = callback)\n\n\n\nu0 = Float32[2.0; 0.0]\nFlux.train!(loss_n_ode, Flux.params(p), data,\n            Flux.Optimise.ADAM(0.05), cb = callback)\n\ncallback()\n","category":"page"},{"location":"modules/SciMLSensitivity/training_tips/local_minima/","page":"Strategies to Avoid Local Minima","title":"Strategies to Avoid Local Minima","text":"And there we go, a set of robust strategies for fitting an equation that would otherwise get stuck in a local optima.","category":"page"},{"location":"modules/MethodOfLines/tutorials/heatss/#Steady-State-Heat-Equation-No-Time-Dependance-NonlinearProblem","page":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","title":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/heatss/","page":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","title":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","text":"Sometimes it is desirable to solve an equation that has no time evolution, such as the steady state heat equation:","category":"page"},{"location":"modules/MethodOfLines/tutorials/heatss/","page":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","title":"Steady State Heat Equation - No Time Dependance - NonlinearProblem","text":"using ModelingToolkit, MethodOfLines, DomainSets, NonlinearSolve\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ 0\n\nbcs = [u(0, y) ~ x * y,\n       u(1, y) ~ x * y,\n       u(x, 0) ~ x * y,\n       u(x, 1) ~ x * y]\n\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           y ∈ Interval(0.0, 1.0)]\n\n@named pdesys = PDESystem([eq], bcs, domains, [x, y], [u(x, y)])\n\ndx = 0.1\ndy = 0.1\n\n# Note that we pass in `nothing` for the time variable `t` here since we\n# are creating a stationary problem without a dependence on time, only space.\ndiscretization = MOLFiniteDifference([x => dx, y => dy], nothing, approx_order=2)\n\nprob = discretize(pdesys, discretization)\nsol = NonlinearSolve.solve(prob, NewtonRaphson())\n\ngrid = get_discrete(pdesys, discretization)\n\nu_sol = map(d -> sol[d], grid[u(x, y)])\n\nusing Plots\n\nheatmap(grid[x], grid[y], u_sol, xlabel=\"x values\", ylabel=\"y values\",\n        title=\"Steady State Heat Equation\")","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#ode_solve","page":"ODE Solvers","title":"ODE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"solve(prob::ODEProblem,alg;kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Solves the ODE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Recommended-Methods","page":"ODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"It is suggested that you try choosing an algorithm using the alg_hints keyword argument. However, in some cases you may want something specific, or you may just be curious. This guide is to help you choose the right algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Unknown-Stiffness-Problems","page":"ODE Solvers","title":"Unknown Stiffness Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"When the stiffness of the problem is unknown, it is recommended you use a stiffness detection and auto-switching algorithm. These methods are multi-paradigm and allow for efficient solution of both stiff and non-stiff problems. The cost for auto-switching is very minimal but the choices are restrained and so they are a good go-to method when applicable.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For default tolerances, AutoTsit5(Rosenbrock23()) is a good choice. For lower tolerances, using AutoVern7 or AutoVern9 with Rodas4, KenCarp4, or Rodas5 can all be good choices depending on the problem. For very large systems (>1000 ODEs?), consider using lsoda.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Non-Stiff-Problems","page":"ODE Solvers","title":"Non-Stiff Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For non-stiff problems, the native OrdinaryDiffEq.jl algorithms are vastly more efficient than the other choices. For most non-stiff problems, we recommend Tsit5. When more robust error control is required, BS5 is a good choice. If at moderate tolerances and the interpolation error is very important, consider the OwrenZen5 method. For fast solving at higher tolerances, we recommend BS3 (or OwrenZen3 if the interpolation error is important). For high accuracy but with the range of Float64 (~1e-8-1e-12), we recommend Vern6, Vern7, or Vern8 as efficient choices. For very small non-stiff ODEs, SimpleATsit5() is a simplified implementation of Tsit5 that can cut out extra overhead and is recommended in those scenarios.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For high accuracy non-stiff solving (BigFloat and tolerances like <1e-12), we recommend the Vern9 method. If a high-order method is needed with a high order interpolant, then you should choose Vern9 which is Order 9 with an Order 9 interpolant. If you need extremely high accuracy (<1e-30?) and do not need an interpolant, try the Feagin12 or Feagin14 methods. Note that the Feagin methods are the only high-order optimized methods which do not include a high-order interpolant (they do include a 3rd order Hermite interpolation if needed). Note that these high order RK methods are more robust than the high order Adams-Bashforth methods to discontinuities and achieve very high precision, and are much more efficient than the extrapolation methods. However, the VCABM method can be a good choice for high accuracy when the system of equations is very large (>1,000 ODEs?), the function calculation is very expensive, or the solution is very smooth.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"If strict error bounds are needed, then adaptive methods with defect controls are required. Defect controls use an error measurement on the interpolating polynomial to make the error estimate better capture the error over the full interval. For medium accuracy calculations, RK4 is a good choice.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Stiff-Problems","page":"ODE Solvers","title":"Stiff Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For stiff problems at high tolerances (>1e-2?) it is recommended that you use Rosenbrock23 or TRBDF2. These are robust to oscillations and massive stiffness, though are only efficient when low accuracy is needed. Rosenbrock23 is more efficient for small systems where re-evaluating and re-factorizing the Jacobian is not too costly, and for sufficiently large systems TRBDF2 will be more efficient. QNDF or FBDF can be the most efficient the largest systems or most expensive f.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"At medium tolerances (>1e-8?) it is recommended you use Rodas5, Rodas4P (the former is more efficient but the later is more reliable), Kvaerno5, or KenCarp4. As native DifferentialEquations.jl solvers, many Julia numeric types (such as BigFloats, ArbFloats, or DecFP) will work. When the equation is defined via the @ode_def macro, these will be the most efficient.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For faster solving at low tolerances (<1e-9) but when Vector{Float64} is used, use radau.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For asymptotically large systems of ODEs (N>1000?) where f is very costly and the complex eigenvalues are minimal (low oscillations), in that case QNDF or FBDF will be the most efficient. QNDF and FBDF will also do surprisingly well if the solution is smooth. However, this method can handle less stiffness than other methods and its Newton iterations may fail at low accuracy situations. Other choices to consider in this regime are CVODE_BDF and lsoda.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Special-Properties-of-Stiff-Integrators","page":"ODE Solvers","title":"Special Properties of Stiff Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitMidpoint is a symmetric and symplectic integrator. Trapezoid is a symmetric (almost symplectic) integrator with adaptive timestepping. ImplicitEuler is an extension to the common algorithm with adaptive timestepping and efficient quasi-Newton Jacobian re-usage which is fully strong-stability preserving (SSP) for hyperbolic PDEs.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Notice that Rodas4 loses accuracy on discretizations of nonlinear parabolic PDEs, and thus it's suggested you replace it with Rodas4P in those situations which is 3rd order. ROS3P is only third order and achieves 3rd order on such problems and can thus be more efficient in this case.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Translations-from-MATLAB/Python/R","page":"ODE Solvers","title":"Translations from MATLAB/Python/R","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For users familiar with MATLAB/Python/R, good translations of the standard library methods are as follows:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ode23 –> BS3()\node45/dopri5 –> DP5(), though in most cases Tsit5() is more efficient\node23s –> Rosenbrock23(), though in most cases Rodas4() is more efficient\node113 –> VCABM(), though in many cases Vern7() is more efficient\ndop853 –> DP8(), though in most cases Vern7() is more efficient\node15s/vode –> QNDF() or FBDF(), though in many cases Rodas4(), KenCarp4(), TRBDF2(), or RadauIIA5() are more efficient\node23t –> Trapezoid()\node23tb –> TRBDF2()\nlsoda –> lsoda(), though AutoTsit5(Rosenbrock23()) or AutoVern7(Rodas5()) may be more efficient. Note that lsoda() requires the LSODA.jl extension, which can be added via ]add LSODA; using LSODA.\node15i –> IDA() or DFBDF(), though in many cases Rodas4() can handle the DAE and is significantly more efficient.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Full-List-of-Methods","page":"ODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#OrdinaryDiffEq.jl-for-Non-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Non-Stiff Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The OrdinaryDiffEq.jl algorithms achieve the highest performance for non-stiff equations while being the most generic: accepting the most Julia-based types, allow for sophisticated event handling, etc. On stiff ODEs these algorithms again consistently among the top. OrdinaryDiffEq.jl is recommended for most ODE problems.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Explicit Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Euler- The canonical forward Euler method. Fixed timestep only.\nMidpoint - The second order midpoint method. Uses embedded Euler method for adaptivity.\nHeun - The second order Heun's method. Uses embedded Euler method for adaptivity.\nRalston - The optimized second order midpoint method. Uses embedded Euler. method for adaptivity.\nRK4 - The canonical Runge-Kutta Order 4 method. Uses a defect control for adaptive stepping using maximum error over the whole interval.\nBS3 - Bogacki-Shampine 3/2 method.\nOwrenZen3 - Owren-Zennaro optimized interpolantion 3/2 method (free 3th order interpolant).\nOwrenZen4 - Owren-Zennaro optimized interpolantion 4/3 method (free 4th order interpolant).\nOwrenZen5 - Owren-Zennaro optimized interpolantion 5/4 method (free 5th order interpolant).\nDP5 - Dormand-Prince's 5/4 Runge-Kutta method. (free 4th order interpolant).\nTsit5 - Tsitouras 5/4 Runge-Kutta method. (free 4th order interpolant).\nAnas5(w) - 4th order Runge-Kutta method designed for periodic problems. Requires a periodicity estimate w which when accurate the method becomes 5th order (and is otherwise 4th order with less error for better estimates).\nFRK65(w=0) - Zero Dissipation Runge-Kutta of 6th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nPFRK87(w=0) - Phase-fitted Runge-Kutta Runge-Kutta of 8th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nRKO65 - Tsitouras' Runge-Kutta-Oliver 6 stage 5th order method. This method is robust on problems which have a singularity at t=0.\nTanYam7 - Tanaka-Yamashita 7 Runge-Kutta method.\nDP8 - Hairer's 8/5/3 adaption of the Dormand-Prince Runge-Kutta method. (7th order interpolant).\nTsitPap8 - Tsitouras-Papakostas 8/7 Runge-Kutta method.\nFeagin10 - Feagin's 10th-order Runge-Kutta method.\nFeagin12 - Feagin's 12th-order Runge-Kutta method.\nFeagin14 - Feagin's 14th-order Runge-Kutta method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example usage:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = Tsit5()\nsolve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the following algorithms have a lazy interpolant:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"BS5 - Bogacki-Shampine 5/4 Runge-Kutta method. (lazy 5th order interpolant).\nVern6 - Verner's \"Most Efficient\" 6/5 Runge-Kutta method. (lazy 6th order interpolant).\nVern7 - Verner's \"Most Efficient\" 7/6 Runge-Kutta method. (lazy 7th order interpolant).\nVern8 - Verner's \"Most Efficient\" 8/7 Runge-Kutta method. (lazy 8th order interpolant)\nVern9 - Verner's \"Most Efficient\" 9/8 Runge-Kutta method. (lazy 9th order interpolant)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a few extra steps in order to compute the high order interpolation, but these steps are only taken when the interpolation is used. These methods when lazy assume that the parameter vector p will be unchanged between the moment of the interval solving and the interpolation. If p is changed in a ContinuousCallback, or in a DiscreteCallback and the continuous solution is used after the full solution, then set lazy=false.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"solve(prob,Vern7()) # lazy by default\nsolve(prob,Vern7(lazy=false))","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Parallel-Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Explicit Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"KuttaPRK2p5 - A 5 parallel, 2 processor explicit Runge-Kutta method of 5th order.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods utilize multithreading on the f calls to parallelize the problem. This requires that simultaneous calls to f are thread-safe.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Explicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Explicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SSPRK22 - The two-stage, second order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK33 - The three-stage, third order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK53 - The five-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 2.65, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK63 - The six-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 3.518, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK73 - The seven-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 4.2879, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK83 - The eight-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 5.107, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK432 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant).\nSSPRK43 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant). The main method is the same as SSPRK432, but the embedded method has a larger stability region.\nSSPRK932 - A  3/2 adaptive strong stability preserving (SSP) method with nine stages (SSP coefficient 6, free 3rd order Hermite interpolant).\nSSPRK54 - The five-stage, fourth order strong stability preserving (SSP) method of Spiteri and Ruuth (SSP coefficient 1.508, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK104 - The ten-stage, fourth order strong stability preserving method of Ketcheson (SSP coefficient 6, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS32 - 3-stage, 2nd order SSP-optimal linear multistep method. (SSP coefficent 0.5, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS43 - 4-stage, 3rd order SSP-optimal linear multistep method. (SSP coefficent 0.33, 3rd order Hermite interpolant). Fixed timestep only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The SSP coefficients of the methods can be queried as ssp_coefficient(alg). All explicit SSP methods take two optional arguments SSPXY(stage_limiter!, step_limiter!), where stage_limiter! and step_limiter are functions taking arguments of the form limiter!(u, integrator, p, t). Here, u is the new solution value (updated inplace) after an explicit Euler stage / the whole time step , integrator the ODE integrator, and t the current time. These limiters can be used to enforce physical constraints, e.g. the positivity preserving limiters of Zhang and Shu (Zhang, Xiangxiong, and Chi-Wang Shu. \"Maximum-principle-satisfying and positivity-preserving high-order schemes for conservation laws: survey and new developments.\" Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. The Royal Society, 2011.).","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Low-Storage-Methods","page":"ODE Solvers","title":"Low-Storage Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ORK256 - 5-stage, second order low-storage method for wave propogation equations. Fixed timestep only. Like SSPRK methods, ORK256 also takes optional arguments stage_limiter!, step_limiter!, where stage_limiter! and step_limiter! are functions of the form limiter!(u, integrator, p, t).\nSSPRK53_2N1 and SSPRK53_2N2 - 5-stage, third order low-storage methods with large SSP coefficients. (SSP coefficient 2.18 and 2.15, free 3rd order Hermite interpolant). Fixed timestep only.\nCarpenterKennedy2N54 - The five-stage, fourth order low-storage method of Carpenter and Kennedy (free 3rd order Hermite interpolant). Fixed timestep only. Designed for hyperbolic PDEs (stability properties). Like SSPRK methods, CarpenterKennedy2N54 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK124 - 12-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK124 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK134 - 13-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK134 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK144 - 14-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only.  Like SSPRK methods, NDBLSRK144 also takes optional arguments stage_limiter!, step_limiter!.\nCFRLDDRK64 - 6-stage, fourth order low-storage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nTSLDDRK74 - 7-stage, fourth order low-storage low-dissipation, low-dispersion scheme with maximal accuracy and stability limit along the imaginary axes. Fixed timestep only.\nDGLDDRK73_C - 7-stage, third order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK73_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_C - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK84_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_F - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when the maximum spatial step size is not constrained. Fixed timestep only. Like SSPRK methods, DGLDDRK84_F also takes optional arguments stage_limiter!, step_limiter!.\nSHLDDRK64 - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only. Like SSPRK methods, SHLDDRK64 also takes optional arguments stage_limiter!, step_limiter!.\nRK46NL - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nParsaniKetchesonDeconinck3S32 - 3-stage, second order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S82 - 8-stage, second order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S53 - 5-stage, third order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S173 - 17-stage, third order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S94 - 9-stage, fourth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S184 - 18-stage, fourth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S105 - 10-stage, fifth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S205 - 20-stage, fifth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nCKLLSRK43_2 - 4-stage, third order low-storage scheme, optimised for compressible Navier–Stokes equations..\nCKLLSRK54_3C - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4S - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4C - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4M - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3C_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3M_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3N_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4C_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4M_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4P_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3N_4R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3M_4R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK65_4M_4R - 6-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4FM_4R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK75_4M_5R - 7-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nRDPK3Sp35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"NOTE: All the 2N Methods (ORK256, CarpenterKennedy2N54, NDBLSRK124, NDBLSRK134, NDBLSRK144, DGLDDRK73_C, DGLDDRK84_C, DGLDDRK84_F and SHLDDRK64) work on the basic principle of being able to perform step S1 = S1 + F(S2) in just 2 registers. Certain optimizations have been done to achieve this theoritical limit (when alias_u0 is set) but have a limitation that du should always be on the left hand side (assignments only) in the implementation.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example - This is an invalid implementation for 2N methods:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"function f(du,u,p,t)\n  du[1] = u[1] * u[2]\n  du[2] = du[1] * u[2] # du appears on the RHS\nend","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"If you don't wish to have the optimization and have to use du on the RHS, please set the keyword argument williamson_condition to false in the algorithm (by default it is set to true). In this case 3 registers worth memory would be needed instead.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example :","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = CarpenterKennedy2N54(;williamson_condition=false)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"So the above implementation of f becomes valid.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Parallelized-Explicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Explicit Extrapolation Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The following are adaptive order, adaptive step size extrapolation methods:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AitkenNeville - Euler extrapolation using Aitken-Neville with the Romberg Sequence.\nExtrapolationMidpointDeuflhard - Midpoint extrapolation using Barycentric coordinates\nExtrapolationMidpointHairerWanner - Midpoint extrapolation using Barycentric coordinates, following Hairer's ODEX in the adaptivity behavior.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. The sequence_factor denotes which even multiple of sequence to take while evaluating internal discretisations. threading denotes whether to automatically multithread the f evaluations, allowing for a high degree of within-method parallelism. The defaults are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"max_order=10\nmin_order=1 except for ExtrapolationMidpointHairerWanner it's 2.\ninit_order=5\nthreading=true\nseqeunce_factor = 2","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the ExtrapolationMidpointDeuflhard and ExtrapolationMidpointHairerWanner methods have the additional argument:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence: the step-number sequences, also called the subdividing","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence. Possible values are :harmonic, :romberg or :bulirsch. Default  is :harmonic.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"To override, utilize the keyword arguments. For example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ExtrapolationMidpointDeuflhard(max_order=7,min_order=4,init_order=4,sequence=:bulirsch,threading=false)\nsolve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that the order that is referred to is the extrapolation order. For AitkenNeville this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Explicit-Multistep-Methods","page":"ODE Solvers","title":"Explicit Multistep Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Methods using the approximation at more than one previous mesh point to determine the approximation at the next point are called multistep methods. These methods tend to be more efficient as the size of the system or the cost of f increases.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Adams-Bashforth-Explicit-Methods","page":"ODE Solvers","title":"Adams-Bashforth Explicit Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a choice of dt.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AB3 - The 3-step third order multistep method. Ralston's Second Order Method is used to calculate starting values.\nAB4 - The 4-step fourth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nAB5 - The 5-step fifth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nABM32 - It is third order method. In ABM32, AB3 works as predictor and Adams Moulton 2-steps method works as Corrector. Ralston's Second Order Method is used to calculate starting values.\nABM43 - It is fourth order method. In ABM43, AB4 works as predictor and Adams Moulton 3-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.\nABM54 - It is fifth order method. In ABM54, AB5 works as predictor and Adams Moulton 4-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Adaptive-step-size-Adams-explicit-Methods","page":"ODE Solvers","title":"Adaptive step size Adams explicit Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"VCAB3 - The 3rd order Adams method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCAB4 - The 4th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCAB5 - The 5th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCABM3 - The 3rd order Adams-Moulton method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCABM4 - The 4th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM5 - The 5th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM - An adaptive order adaptive time Adams Moulton method. It uses an order adaptivity algorithm is derived from Shampine's DDEABM.\nAN5 - An adaptive 5th order fixed-leading coefficient Adams method in Nordsieck form.\nJVODE_Adams - An adaptive time adaptive order fixed-leading coefficient Adams method in Nordsieck form. The order adaptivity algorithm is derived from Sundials' CVODE_Adams. In development.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#OrdinaryDiffEq.jl-for-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Stiff Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#SDIRK-Methods","page":"ODE Solvers","title":"SDIRK Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitEuler - A 1st order implicit solver. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - A second order A-stable symplectic and symmetric implicit solver. Good for highly stiff equations which need symplectic integration.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.\nTRBDF2 - A second order A-B-L-S-stable one-step ESDIRK method. Includes stiffness-robust error estimates for accurate adaptive timestepping, smoothed derivatives for highly stiff and oscillatory problems.\nSDIRK2 - An A-B-L stable 2nd order SDIRK method\nKvaerno3 - An A-L stable stiffly-accurate 3rd order ESDIRK method\nKenCarp3 - An A-L stable stiffly-accurate 3rd order ESDIRK method with splitting\nCash4 - An A-L stable 4th order SDIRK method\nHairer4 - An A-L stable 4th order SDIRK method\nHairer42 - An A-L stable 4th order SDIRK method\nKvaerno4 - An A-L stable stiffly-accurate 4th order ESDIRK method\nKenCarp4 - An A-L stable stiffly-accurate 4th order ESDIRK method with splitting\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKvaerno5 - An A-L stable stiffly-accurate 5th order ESDIRK method\nKenCarp5 - An A-L stable stiffly-accurate 5th order ESDIRK method with splitting\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Fully-Implicit-Runge-Kutta-Methods-(FIRK)","page":"ODE Solvers","title":"Fully-Implicit Runge-Kutta Methods (FIRK)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"RadauIIA3 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.\nRadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Parallel-Diagonally-Implicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Diagonally Implicit Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"PDIRK44 - A 2 processor 4th order diagonally non-adaptive implicit method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods also have option nlsolve same as SDIRK methods. These methods also need f to be thread safe. It parallelises the nlsolve calls inside the method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Rosenbrock-Methods","page":"ODE Solvers","title":"Rosenbrock Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method. Currently has a Hermite interpolant because its stiff-aware 3rd order interpolant is not yet implemented.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Rosenbrock-W-Methods","page":"ODE Solvers","title":"Rosenbrock-W Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildy stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Stabilized-Explicit-Methods","page":"ODE Solvers","title":"Stabilized Explicit Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROCK2 - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nROCK4 - Fourth order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nRKC - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nSERK2 - Second order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nESERK5 - Fifth order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROCK methods offer a min_stages and max_stages functionality. SERK methods derive higher orders by Aitken-Neville algorithm. SERK2 is defaulted to Predictive control but has option of PI control.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Parallelized-Implicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Implicit Extrapolation Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The following are adaptive order, adaptive step size extrapolation methods:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitEulerExtrapolation - Extrapolation of implicit Euler method with Romberg sequence. Similar to Hairer's SEULEX.\nImplicitDeuflhardExtrapolation - Midpoint extrapolation using Barycentric coordinates\nImplicitHairerWannerExtrapolation - Midpoint extrapolation using Barycentric coordinates, following Hairer's SODEX in the adaptivity behavior.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. threading denotes whether to automatically multithread the f evaluations and J/W instantiations+factorizations, allowing for a high degree of within-method parallelism. We recommend to switch to multi-threading when the system consists of more than ~ 150 ODES. The defaults are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"max_order=10\nmin_order=1 except for ImplicitHairerWannerExtrapolation it's 2.\ninit_order=5\nthreading=false","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the ImplicitDeuflhardExtrapolation and ImplicitHairerWannerExtrapolation methods have the additional argument:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence: the step-number sequences, also called the subdividing","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence. Possible values are :harmonic, :romberg or :bulirsch. Default  is :harmonic.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"To override, utilize the keyword arguments. For example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ImplicitDeuflhardExtrapolation(max_order=7,min_order=4,init_order=4,sequence=:bulirsch)\nsolve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that the order that is referred to is the extrapolation order. For ImplicitEulerExtrapolation this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Parallelized-DIRK-Methods","page":"ODE Solvers","title":"Parallelized DIRK Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods parallelize the J/W instantiation and factorization, making them efficient on small highly stiff ODEs. Has an option threading=true to turn on/off multithreading.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"PDIRK44: a 4th order 2-processor DIRK method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Exponential-Runge-Kutta-Methods","page":"ODE Solvers","title":"Exponential Runge-Kutta Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods are all fixed timestepping only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"LawsonEuler - First order exponential Euler scheme.\nNorsettEuler - First order exponential-RK scheme. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development).\nETDRK2 - 2nd order exponential-RK scheme.\nETDRK3 - 3rd order exponential-RK scheme.\nETDRK4 - 4th order exponential-RK scheme.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The methods are intended for semilinear problems constructed by SplitODEProblem or SplitODEFunction. They can also be used for a general nonlinear problem, in which case the jacobian of the right hand side is used as the linear operator in each time step.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Except for ETD2, all methods come with these options, which can be set in the methods' constructor:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"krylov - boolean, default: false. Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems. krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm - integer, default: 30. Controls the size of Krylov subsapce.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit jacobian is not given. See Extra Options for more details.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Adaptive-Exponential-Rosenbrock-Methods","page":"ODE Solvers","title":"Adaptive Exponential Rosenbrock Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Exprb32 - 3rd order adaptive Exponential-Rosenbrock scheme.\nExprb43 - 4th order adaptive Exponential-Rosenbrock scheme.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The exponential rosenbrock methods cannot be applied to semilinear problems. Options for the solvers are the same as Exponential Runge-Kutta Methods except that Krylov approximation is always used.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Exponential-Propagation-Iterative-Runge-Kutta-Methods-(EPIRK)","page":"ODE Solvers","title":"Exponential Propagation Iterative Runge-Kutta Methods (EPIRK)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods are all fixed timestepping only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Exp4 - 4th order EPIRK scheme.\nEPIRK4s3A - 4th order EPIRK scheme with stiff order 4.\nEPIRK4s3B - 4th order EPIRK scheme with stiff order 4.\nEPIRK5P1 - 5th order EPIRK scheme.\nEPIRK5P2 - 5th order EPIRK scheme.\nEPIRK5s3 - 5th order \"horizontal\" EPIRK scheme with stiff order 5. Broken.\nEXPRB53s3- 5th order EPIRK scheme with stiff order 5.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Options:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"adaptive_krylov - boolean, default: true. Determines if the adaptive Krylov algorithm with timestepping of Neisen & Wright is used.\nm - integer, default: 30. Controls the size of Krylov subsapce, or the size for the first step if adaptive_krylov=true.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit jacobian is not given. See Extra Options for more details.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"It should be noted that many of the methods are still at an experimental stage of development, and thus should be used with caution.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Multistep-Methods","page":"ODE Solvers","title":"Multistep Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients). Similar to ode15s.\nQBDF - An adaptive order quasi-constant timestep BDF method.  \nMEBDF2 - The second order Modified Extended BDF method, which has improved stability properties over the standard BDF. Fixed timestep only.\nFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or CVODE_BDF in divided differences form.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Implicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Implicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SSPSDIRK2 - A second order A-L stable symplectic SDIRK method with the strong stability preserving (SSP) property (SSP coefficient 2). Fixed timestep only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Extra-Options","page":"ODE Solvers","title":"Extra Options","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"All of the Rosenbrock and SDIRK methods allow for specification of linsolve: the linear solver which is used. For more information on specifying the linear solver, see the manual page on solver specification.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that performance overload information (Jacobians etc.) are not used in this mode. This can control automatic differentiation of the Jacobian as well. For more information on specifying the nonlinear solver, see the manual page on solver specification.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the Rosenbrock and SDIRK methods have differentiation controls. In each of these, autodiff can be set to turn on/off autodifferentiation, and chunk_size can be used to set the chunksize of the Dual  numbers (see the documentation for ForwardDiff.jl for details). In addition, the Rosenbrock and SDIRK methods can set diff_type, which is the type of numerical differentiation that is used (when autodifferentiation is disabled). The choices are Val{:central}, Val{:forward} or Val{:complex}.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Examples:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sol = solve(prob,Rosenbrock23()) # Standard, uses autodiff\nsol = solve(prob,Rosenbrock23(chunk_size=10)) # Autodiff with chunksize of 10\nsol = solve(prob,Rosenbrock23(autodiff=false)) # Numerical differentiation with central differencing\nsol = solve(prob,Rosenbrock23(autodiff=false,diff_type=Val{:forward})) # Numerical differentiation with forward differencing","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Tableau-Method","page":"ODE Solvers","title":"Tableau Method","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, there is the tableau method:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ExplicitRK - A general Runge-Kutta solver which takes in a tableau. Can be adaptive. Tableaus are specified via the keyword argument tab=tableau. The default tableau is for Dormand-Prince 4/5. Other supplied tableaus can be found in the Supplied Tableaus section.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example usage:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ExplicitRK(tableau=constructDormandPrince())\nsolve(prob,alg)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#CompositeAlgorithm","page":"ODE Solvers","title":"CompositeAlgorithm","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"One unique feature of OrdinaryDiffEq.jl is the CompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is CompositeAlgorithm(algtup,choice_function) where algtup is a tuple of OrdinaryDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses Tsit5() but switches to Vern7() whenever dt is too small:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"choice_function(integrator) = (Int(integrator.dt<0.001) + 1)\nalg_switch = CompositeAlgorithm((Tsit5(),Vern7()),choice_function)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The choice_function takes in an integrator and thus all of the features available in the Integrator Interface can be used in the choice function.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"A helper algorithm was created for building 2-method automatic switching for stiffness detection algorithms. This is the AutoSwitch algorithm with the following options:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AutoSwitch(nonstiffalg::nAlg, stiffalg::sAlg;\n           maxstiffstep=10, maxnonstiffstep=3,\n           nonstifftol::T=9//10, stifftol::T=9//10,\n           dtfac=2.0, stiffalgfirst=false)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The nonstiffalg must have an appropriate stiffness estimate built into the method. The stiffalg can receive its estimate from the Jacobian calculation. maxstiffstep is the number of stiffness detects before switching to the stiff algorithm and maxnonstiffstep is vice versa. nonstifftol and stifftol are the tolerances associated with the stiffness comparison against the stability region. Decreasing stifftol makes switching to the non-stiff algorithm less likely. Decreasing nonstifftol makes switching to the stiff algorithm more likely. dtfac is the factor that dt is changed when switching: multiplied when going from non-stiff to stiff and divided when going stiff to non-stiff. stiffalgfirst denotes whether the first step should use the stiff algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#Pre-Built-Stiffness-Detecting-and-Auto-Switching-Algorithms","page":"ODE Solvers","title":"Pre-Built Stiffness Detecting and Auto-Switching Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a Autoalg(stiffalg) to be chosen as the method to switch to when the ODE is stiff. It can be any of the OrdinaryDiffEq.jl one-step stiff methods and has all of the arguments of the AutoSwitch algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AutoTsit5 - Tsit5 with automated switching.\nAutoDP5 - DP5 with automated switching.\nAutoVern6 - Vern6 with automated switching.\nAutoVern7 - Vern7 with automated switching.\nAutoVern8 - Vern8 with automated switching.\nAutoVern9 - Vern9 with automated switching.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"tsidas_alg = AutoTsit5(Rodas5())\nsol = solve(prob,tsidas_alg)\n\ntsidas_alg = AutoTsit5(Rodas5(),nonstifftol = 11/10)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Is the Tsit5 method with automatic switching to Rodas5.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#ode_solve_sundials","page":"ODE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add Sundials\nusing Sundials","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and \"generally works\".","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF - CVode Backward Differentiation Formula (BDF) solver.\nCVODE_Adams - CVode Adams-Moulton solver.\nARKODE - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The Sundials algorithms all come with a 3rd order Hermite polynomial interpolation. Note that the constructors for the Sundials algorithms take two main arguments:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"method - This is the method for solving the implicit equation. For BDF this defaults to :Newton while for Adams this defaults to :Functional. These choices match the recommended pairing in the Sundials.jl manual. However, note that using the :Newton method may take less iterations but requires more memory than the :Function iteration approach.\nlinear_solver - This is the linear solver which is used in the :Newton method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The choices for the linear solver are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":":Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:Diagonal - This method is specialized for diagonal Jacobians.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF() # BDF method using Newton + Dense solver\nCVODE_BDF(method=:Functional) # BDF method using Functional iterations\nCVODE_BDF(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nCVODE_BDF(linear_solver=:BCG) # Biconjugate gradient method","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The main options for ARKODE are the choice between explicit and implicit and the method order, given via:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ARKODE(Sundials.Explicit()) # Solve with explicit tableau of default order 4\nARKODE(Sundials.Implicit(),order = 3) # Solve with explicit tableau of order 3","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The order choices for explicit are 2 through 8 and for implicit 3 through 5. Specific methods can also be set through the etable and itable options for explicit and implicit tableaus respectively. The available tableaus are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"etable:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"HEUN_EULER_2_1_2: 2nd order Heun's method\nBOGACKI_SHAMPINE_4_2_3:\nARK324L2SA_ERK_4_2_3: explicit portion of Kennedy and Carpenter's 3rd order method\nZONNEVELD_5_3_4: 4th order explicit method\nARK436L2SA_ERK_6_3_4: explicit portion of Kennedy and Carpenter's 4th order method\nSAYFY_ABURUB_6_3_4: 4th order explicit method\nCASH_KARP_6_4_5: 5th order explicit method\nFEHLBERG_6_4_5: Fehlberg's classic 5th order method\nDORMAND_PRINCE_7_4_5: the classic 5th order Dormand-Prince method\nARK548L2SA_ERK_8_4_5: explicit portion of Kennedy and Carpenter's 5th order method\nVERNER_8_5_6: Verner's classic 5th order method\nFEHLBERG_13_7_8: Fehlberg's 8th order method","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"itable:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SDIRK_2_1_2: An A-B-stable 2nd order SDIRK method\nBILLINGTON_3_3_2: A second order method with a 3rd order error predictor of less stability\nTRBDF2_3_3_2: The classic TR-BDF2 method\nKVAERNO_4_2_3: an L-stable 3rd order ESDIRK method\nARK324L2SA_DIRK_4_2_3: implicit portion of Kennedy and Carpenter's 3th order method\nCASH_5_2_4: Cash's 4th order L-stable SDIRK method\nCASH_5_3_4: Cash's 2nd 4th order L-stable SDIRK method\nSDIRK_5_3_4: Hairer's 4th order SDIRK method\nKVAERNO_5_3_4: Kvaerno's 4th order ESDIRK method\nARK436L2SA_DIRK_6_3_4: implicit portion of Kennedy and Carpenter's 4th order method\nKVAERNO_7_4_5: Kvaerno's 5th order ESDIRK method\nARK548L2SA_DIRK_8_4_5: implicit portion of Kennedy and Carpenter's 5th order method","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These can be set for example via:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ARKODE(Sundials.Explicit(),etable = Sundials.DORMAND_PRINCE_7_4_5)\nARKODE(Sundials.Implicit(),itable = Sundials.KVAERNO_4_2_3)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"All of the additional options are available. The full constructor is:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF(;method=:Newton,linear_solver=:Dense,\n          jac_upper=0,jac_lower=0,\n          stored_upper = jac_upper + jac_lower,\n          non_zero=0,krylov_dim=0,\n          stability_limit_detect=false,\n          max_hnil_warns = 10,\n          max_order = 5,\n          max_error_test_failures = 7,\n          max_nonlinear_iters = 3,\n          max_convergence_failures = 10,\n          prec = nothing, prec_side = 0)\n\nCVODE_Adams(;method=:Functional,linear_solver=:None,\n            jac_upper=0,jac_lower=0,\n            stored_upper = jac_upper + jac_lower,\n            krylov_dim=0,\n            stability_limit_detect=false,\n            max_hnil_warns = 10,\n            max_order = 12,\n            max_error_test_failures = 7,\n            max_nonlinear_iters = 3,\n            max_convergence_failures = 10,\n            prec = nothing, psetup = nothing, prec_side = 0)\n\nARKODE(stiffness=Sundials.Implicit();\n      method=:Newton,linear_solver=:Dense,\n      jac_upper=0,jac_lower=0,stored_upper = jac_upper+jac_lower,\n      non_zero=0,krylov_dim=0,\n      max_hnil_warns = 10,\n      max_error_test_failures = 7,\n      max_nonlinear_iters = 3,\n      max_convergence_failures = 10,\n      predictor_method = 0,\n      nonlinear_convergence_coefficient = 0.1,\n      dense_order = 3,\n      order = 4,\n      set_optimal_params = false,\n      crdown = 0.3,\n      dgmax = 0.2,\n      rdiv = 2.3,\n      msbp = 20,\n      adaptivity_method = 0,\n      prec = nothing, psetup = nothing, prec_side = 0\n      )","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"See the CVODE manual and the ARKODE manual for details on the additional options.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that here prec is a preconditioner function prec(z,r,p,t,y,fy,gamma,delta,lr) where:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"z: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\nlr: a flag for whether lr=1 (left) or lr=2 (right) preconditioning","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"and psetup is the preconditioner setup function for pre-computing Jacobian information psetup(p, t, u, du, jok, jcurPtr, gamma). Where:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"p: the parameters\nt: the current independent variable\nu: the current state\ndu: the current f(u,p,t)\njok: a bool indicating whether the Jacobian needs to be updated\njcurPtr: a reference to an Int for whether the Jacobian was updated. jcurPtr[]=true should be set if the Jacobian was updated, and jcurPtr[]=false should be set if the Jacobian was not updated.\ngamma: the gamma of W = M - gamma*J","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"psetup is optional when prec is set.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#ODEInterface.jl","page":"ODE Solvers","title":"ODEInterface.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The ODEInterface algorithms are the classic Fortran algorithms. While the non-stiff algorithms are superseded by the more featured and higher performance Julia implementations from OrdinaryDiffEq.jl, the stiff solvers such as radau are some of the most efficient methods available (but are restricted for use on arrays of Float64).","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODEInterfaceDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ODEInterfaceDiffEq\nusing ODEInterfaceDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"dopri5 - Hairer's classic implementation of the Dormand-Prince 4/5 method.\ndop853 - Explicit Runge-Kutta 8(5,3) by Dormand-Prince.\nodex - GBS extrapolation-algorithm based on the midpoint rule.\nseulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.\nddeabm - Adams-Bashforth-Moulton Predictor-Corrector method (order between 1 and 12)\nddebdf - Backward Differentiation Formula (orders between 1 and 5)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that while the output only has a linear interpolation, a higher order interpolation is used for intermediate dense output for saveat and for event handling.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#LSODA.jl","page":"ODE Solvers","title":"LSODA.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides a wrapper to the algorithm LSODA, a well-known method which uses switching to solve both stiff and non-stiff equations.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"lsoda - The LSODA wrapper algorithm.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use LSODA.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add LSODA\nusing LSODA","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#IRKGaussLegendre.jl","page":"ODE Solvers","title":"IRKGaussLegendre.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides a specific solver, IRKGL16, which is a 16th order Symplectic Gauss-Legendre scheme. This scheme is highly efficient for precise integration of ODEs, specifically ODEs derived from Hamiltonian systems.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use IRKGaussLegendre.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add IRKGaussLegendre\nusing IRKGaussLegendre","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#SimpleDiffEq.jl","page":"ODE Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides access to simplified versions of a few ODE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to OrdinaryDiffEq.jl and are not generally faster.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SimpleTsit5 - A fixed timestep integrator form of Tsit5. Not compatible with events.\nSimpleATsit5 - An adaptive Tsit5 with an interpolation in its simplest form. Not compatible with events.\nGPUSimpleATsit5 - A version of SimpleATsit5 without the integrator interface. Only allows solve.\nSimpleRK4 - A fixed timestep barebones RK4 implementation with integrators.\nLoopRK4 - A fixed timestep barebones RK4. Not compatible with events or the integrator interface.\nGPURK4 - A fully static RK4 for specialized compilation to accelerators like GPUs and TPUs.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add SimpleDiffEq\nusing SimpleDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#ODE.jl","page":"ODE Solvers","title":"ODE.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODE.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ODE\nusing ODE","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ode23 - Bogacki-Shampine's order 2/3 Runge-Kutta  method\node45 - A Dormand-Prince order 4/5 Runge-Kutta method\node23s - A modified Rosenbrock order 2/3 method due to Shampine\node78 - A Fehlburg order 7/8 Runge-Kutta method\node4 - The classic Runge-Kutta order 4 method\node4ms - A fixed-step, fixed order Adams-Bashforth-Moulton method†\node4s - A 4th order Rosenbrock method due to Shampine","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#MATLABDiffEq.jl","page":"ODE Solvers","title":"MATLABDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use MATLABDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/MATLABDiffEq.jl\nusing MATLABDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This requires a licensed MATLAB installation. The available methods are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"MATLABDiffEq.ode23\nMATLABDiffEq.ode45\nMATLABDiffEq.ode113\nMATLABDiffEq.ode23s\nMATLABDiffEq.ode23t\nMATLABDiffEq.ode23tb\nMATLABDiffEq.ode15s\nMATLABDiffEq.ode15i","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For more information on these algorithms, see the MATLAB documentation.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#SciPyDiffEq.jl","page":"ODE Solvers","title":"SciPyDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SciPyDiffEq.jl is a wrapper over SciPy for easing the transition of new users (same exact results!) and benchmarking. This wrapper uses Julia's JIT acceleration to accelerate about 3x over SciPy+Numba, but it is still around 1000x slower than the pure-Julia methods and thus should probably be used sparingly.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SciPyDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/SciPyDiffEq.jl\nusing SciPyDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The available methods are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SciPyDiffEq.RK45\nSciPyDiffEq.RK23\nSciPyDiffEq.Radau\nSciPyDiffEq.BDF\nSciPyDiffEq.LSODA","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#deSolveDiffEq.jl","page":"ODE Solvers","title":"deSolveDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"deSolveDiffEq.jl is a wrapper over R's deSolve for easing the transition of new users (same exact results!) and benchmarking. This wrapper is around 1000x slower than the pure-Julia methods (~2x-3x overhead from directly using R) and thus should probably be used sparingly.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use deSolveDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/deSolveDiffEq.jl\nusing deSolveDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The available methods are:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"deSolveDiffEq.lsoda\ndeSolveDiffEq.lsode\ndeSolveDiffEq.lsodes\ndeSolveDiffEq.lsodar\ndeSolveDiffEq.vode\ndeSolveDiffEq.daspk\ndeSolveDiffEq.euler\ndeSolveDiffEq.rk4\ndeSolveDiffEq.ode23\ndeSolveDiffEq.ode45\ndeSolveDiffEq.radau\ndeSolveDiffEq.bdf\ndeSolveDiffEq.bdf_d\ndeSolveDiffEq.adams\ndeSolveDiffEq.impAdams\ndeSolveDiffEq.impAdams_d","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#GeometricIntegrators.jl","page":"ODE Solvers","title":"GeometricIntegrators.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/GeometricIntegratorsDiffEq.jl\nusing GeometricIntegratorsDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"GIEuler - 1st order Euler method\nGIMidpoint - 2nd order explicit midpoint method\nGIHeun2 - 2nd order Heun's method\nGIRalston2 - 2nd order Ralston's method\nGIHeun3 - 3rd order Heun's method\nGIRalston3 - 3rd order Ralston's method\nGIRunge - 3rd order Kutta's method\nGIKutta - 3rd order Kutta's method\nGIRK4 - standard 4th order Runge-Kutta\nGIRK416\nGIRK438 - 4th order Runge-Kutta, 3/8's rule\nGIImplicitEuler - 1st order implicit Euler method\nGIImplicitMidpoint - 2nd order implicit midpoint method\nGIRadauIA(s) - s-stage Radau-IA\nGIRadauIIA(s) - s-stage Radau-IA\nGILobattoIIIA(s)\nGILobattoIIIB(s)\nGILobattoIIIC(s)\nGILobattoIIIC̄(s)\nGILobattoIIID(s)\nGILobattoIIIE(s)\nGILobattoIIIF(s)\nGISRK3 - 3-stage order 4 symmetric Runge-Kutta method\nGISSPRK3 - 3rd orer explicit SSP method\n`GICrankNicholson\nGIKraaijevangerSpijker\nGIQinZhang\nGICrouzeix\nGIGLRK(s) - Gauss-Legendre Runge-Kutta method of order 2s","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that all of these methods require the user supplies dt.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#BridgeDiffEq.jl","page":"ODE Solvers","title":"BridgeDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/BridgeDiffEq.jl\nusing BridgeDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"BridgeR3 - 3rd order Ralston method\nBridgeBS3 - 3rd order Bogacki-Shampine method","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#TaylorIntegration.jl","page":"ODE Solvers","title":"TaylorIntegration.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"TaylorIntegration.jl is a pure-Julia implementation of an adaptive order Taylor series method for high accuracy integration of ODEs. These methods are optimized when the absolute tolerance is required to be very low. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use TaylorIntegration.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add TaylorIntegration\nusing TaylorIntegration","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"TaylorMethod(order) - Taylor integration method with maximal order (required)","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note: this method is much faster if you put @taylorize on your derivative function!","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#QuDiffEq.jl","page":"ODE Solvers","title":"QuDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QuDiffEq.jl is a package for solving differential equations using quantum algorithm. It makes use of the Yao framework for simulating quantum circuits.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use QuDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/QuantumBFS/QuDiffEq.jl\nusing QuDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QuLDE(k) - Algorithm based on truncated Taylor series. The method linearizes a system of non-linear differential equations and solves the resultant by means of a quantum circuit. k selects the order in the Taylor series aprroximation (for the quantum circuit).\nQuNLDE(k,ϵ)- Algorithm uses forward Euler to solve quadratc differential equations. k selects the order in the Taylor series aprroximation (for the quantum circuit). ϵ sets the precision for Hamiltonian evolution.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#NeuralPDE.jl","page":"ODE Solvers","title":"NeuralPDE.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This method trains a neural network using Flux.jl to approximate the solution of the ODE. Currently this method isn't competitive but it is a fun curiosity that will be improved with future integration with Zygote.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use NeuralPDE.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add NeuralPDE\nusing NeuralPDE","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"nnode(chain,opt=ADAM(0.1)) - Defines a neural network solver which utilizes a Flux.jl chain under the hood which must be supplied by the user. Defaults to using the ADAM optimization method, but the user can pass any Flux.jl optimizer.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#List-of-Supplied-Tableaus","page":"ODE Solvers","title":"List of Supplied Tableaus","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"A large variety of tableaus have been supplied by default, via DiffEqDevTools.jl. The list of tableaus can be found in the developer docs. To use them, note you must install the library:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add DiffEqDevTools\nusing DiffEqDevTools","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For the most useful and common algorithms, a hand-optimized version is supplied in OrdinaryDiffEq.jl which is recommended for general uses (i.e. use DP5 instead of ExplicitRK with tableau=constructDormandPrince()). However, these serve as a good method for comparing between tableaus and understanding the pros/cons of the methods. Implemented are every published tableau (that I know exists). Note that user-defined tableaus also are accepted. To see how to define a tableau, checkout the premade tableau source code. Tableau docstrings should have appropriate citations (if not, file an issue).","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Plot recipes are provided which will plot the stability region for a given tableau.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/#ProbNumDiffEq.jl","page":"ODE Solvers","title":"ProbNumDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ProbNumDiffEq.jl provides probabilistic numerical solvers for ODEs. By casting the solution of ODEs as a problem of Bayesian inference, they return a posterior probability distribution over ODE solutions and thereby provide estimates of their own numerical approximation error. The solvers have adaptive timestepping, their order can be freely specified, and the returned posterior distribution naturally enables dense output and sampling. The full documentation is available at ProbNumDiffEq.jl.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ProbNumDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ProbNumDiffEq\nusing ProbNumDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"EK1(order=3) - A semi-implicit ODE solver based on extended Kalman filtering and smoothing with first order linearization. Recommended, but requires that the Jacobian of the vector field is specified.\nEK0(order=3) - An explicit ODE solver based on extended Kalman filtering and smoothing with zeroth order linearization.","category":"page"},{"location":"modules/DiffEqDocs/solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#ssa_tutorial","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In this tutorial we will describe how to define and simulate continuous-time jump processes, also known in biological fields as stochastic chemical kinetics (i.e. Gillespie) models. It is not necessary to have read the first tutorial. We will illustrate","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The different types of jumps that can be represented in JumpProcesses and their use cases.\nHow to speed up pure-jump simulations with only ConstantRateJumps and MassActionJumps by using the SSAStepper time stepper.\nHow to define and use MassActionJumps, a more specialized type of ConstantRateJump that offers improved computational performance.\nHow to use saving controls to reduce memory use per simulation.\nHow to use VariableRateJumps and when they should be preferred over ConstantRateJumps and MassActionJumps.\nHow to create hybrid problems mixing the various jump types with ODEs or SDEs.\nHow to use RegularJumps to enable faster, but approximate, time stepping via τ-leaping methods.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial in DifferentialEquations.jl.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We begin by demonstrating how to build jump processes using JumpProcesses.jl's different jump types, which encode the rate functions (i.e. transition rates, intensities, or propensities) and state changes when a given jump occurs.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, the SIR model considered here is a type of stochastic chemical kinetics jump process model, and as such the biological modeling functionality of Catalyst.jl can be used to easily specify the model and automatically calculate inputs needed for JumpProcesses's optimized simulation algorithms. We summarize this alternative approach at the beginning for users who may be interested in modeling chemical systems, but note this tutorial is intended to explain the general jump process formulation of JumpProcesses for all users. However, for those users constructing models that can be represented as a collection of chemical reactions we strongly recommend using Catalyst, which should ensure optimal jump types are selected to represent each reaction, and necessary data structures for the simulation algorithms, such as dependency graphs, are automatically calculated.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We'll make use of the DifferentialEquations.jl meta package, which includes JumpProcesses and ODE/SDE solvers, Plots.jl, and (optionally) Catalyst.jl in this tutorial. If not already installed they can be added as follows","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Pkg\nPkg.add(\"DifferentialEquations\")\nPkg.add(\"Plots\")\nPkg.add(\"Catalyst)                # optional","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Let's now load the required packages and set some default plot settings","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DifferentialEquations, Plots, LinearAlgebra\ndefault(; lw = 2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DifferentialEquations, Plots, LinearAlgebra\ndefault(; lw = 2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Illustrative-Model:-SIR-Disease-Dynamics","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Illustrative Model: SIR Disease Dynamics","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To illustrate the jump process solvers, we will build an SIR model which matches the tutorial from Gillespie.jl. SIR stands for susceptible, infected, and recovered, and is a model of disease spread. When a susceptible person comes in contact with an infected person, the disease has a chance of infecting the susceptible person. This \"chance\" is determined by the number of susceptible persons and the number of infected persons, since in larger populations there is a greater chance that two people come into contact. Every infected person will in turn have a rate at which they recover. In our model we'll assume there are no births or deaths, and a recovered individual is protected from reinfection.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We'll begin by giving the mathematical equations for the jump processes of the number of susceptible (S(t)), number of infected (I(t)), and number of recovered (R(t)). In the next section we give a more intuitive and biological description of the model for users that are less familiar with jump processes. Let Y_i(t), i = 12, denote independent unit Poisson processes. Our basic mathematical model for the evolution of (S(t)I(t)R(t)), written using Kurtz's time-change representation, is then","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS(t) = S(0) - Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright) \nI(t) = I(0) + Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright)\n        - Y_2 left( int_0^t nu I(s^-)   ds right) \nR(t) = R(0) + Y_2 left( int_0^t nu I(s^-)   ds right)\nendaligned","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, our model involves two jump processes with rate functions, also known as intensities or propensities, given by beta S(t) I(t) and nu I(t) respectively. These give the probability per time a new infected individual is created, and the probability per time some infected individual recovers.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For those less-familiar with the time-change representation, we next give a more intuitive explanation of the model as a collection of chemical reactions, and then demonstrate how these reactions can be written in Catalyst.jl and seamlessly converted into a form that can be used with the JumpProcesses.jl solvers. Users interested in how to directly define jumps using the lower-level JumpProcesses interface can skip to Building and Simulating the Jump Process Using the JumpProcesses Low-Level Interface.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Specifying-the-SIR-Model-with-Chemical-Reactions","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Specifying the SIR Model with Chemical Reactions","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The SIR model described above involves two basic chemical reactions,","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS + I oversetbetato 2 I \nI oversetnuto R\nendaligned","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where beta and nu are the rate constants of the reactions (with units of probability per time). In a jump process (stochastic chemical kinetics) model, we keep track of the non-negative integer number of each species at each time (i.e. (S(t) I(t) R(t)) above). Each reaction has an associated rate function (i.e. intensity or propensity) giving the probability per time it can occur when the system is in state (S(t)I(t)R(t)):","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextReaction  textRate Functions \nhline\nS + I oversetbetato 2 I  beta S(t) I(t) \nI oversetnuto R  nu I(t)\nendmatrix","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beta is determined by factors like the type of the disease. It can be interpreted as the probability per time one pair of susceptible and infected people encounter each other, with the susceptible person becoming sick. The overall rate (i.e. probability per time) that some susceptible person gets sick is then given by the rate constant multiplied by the number of possible pairs of susceptible and infected people. This formulation is known as the law of mass action. Similarly, we have that each individual infected person is assumed to recover with probability per time nu, so that the probability per time some infected person becomes recovered is nu times the number of infected people, i.e. nu I(t).","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Rate functions give the probability per time for each of the two types of jumps to occur, and hence determine when the state of our system changes. To fully specify our model we also need to specify how the state changes when a jump occurs, giving what are called affect! functions in JumpProcesses. For example, when the S + I to 2 I reaction occurs and some susceptible person becomes infected, the subsequent (instantaneous) state change is that","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS to S - 1  I to I + 1\nendaligned","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Likewise, when the I to R reaction occurs so that some infected person becomes recovered the state change is","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nI to I - 1  R to R + 1\nendaligned","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In summary, our model is described by two chemical reactions, which each in turn correspond to a jump process determined by a rate function specifying how frequently jumps should occur, and an affect! function for how the state should change when that jump type occurs.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Processes-from-Catalyst-Models","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Processes from Catalyst Models","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Using Catalyst.jl we can input our full reaction network in a form that can be easily used with JumpProcesses's solvers","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Catalyst\nsir_model = @reaction_network begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To build a pure jump process model of the reaction system, where the state is  constant between jumps, we will use a DiscreteProblem. This encodes that the state only changes at the jump times. We do this by giving the constructor u₀, the initial condition, and tspan, the timespan. Here, we will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0. We use the parameters β = 0.1/1000 and ν = 0.01. Thus we build the problem via:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"p     = (:β => 0.1/1000, :ν => 0.01)\nu₀    = [:S => 999, :I => 10, :R => 0]\ntspan = (0.0, 250.0)\nprob  = DiscreteProblem(sir_model, u₀, tspan, p)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The Catalyst reaction network can be converted into various DifferentialEquations.jl problem types, including JumpProblems, ODEProblems, or SDEProblems. To turn it into a JumpProblem representing the SIR jump process model, we simply write","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(sir_model, prob, Direct())","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of reactions using Gillespie's Direct stochastic simulation algorithm (SSA), also known as Doob's method or Kinetic Monte Carlo. See Constant Rate Jump Aggregators for other supported SSAs.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the JumpProcesses solvers. Since our model is a pure jump process (no continuously-varying components), we will use SSAStepper() to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"plot(sol)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Process-Using-the-JumpProcesses-Low-Level-Interface","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Process Using the JumpProcesses Low-Level Interface","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now show how to directly use JumpProcesses's low-level interface to construct and solve our jump process model for (S(t)I(t)R(t)). Each individual jump that can occur is represented through specifying two pieces of information; a rate function (i.e. intensity or propensity) for the jump and an affect! function for the jump. The former gives the probability per time a particular jump can occur given the current state of the system, and hence determines the time at which jumps can happen. The later specifies the instantaneous change in the state of the system when the jump occurs.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In our SIR model we have two possible jumps that can occur (one for susceptibles becoming infected and one for infected becoming recovered), with the corresponding (mathematical) rates and affects given by","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextRates  textAffects\nhline\nbeta S(t) I(t)  S to S - 1 I to I + 1 \nnu I(t)  I to I - 1  R to R + 1\nendmatrix","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"JumpProcesses offers three different ways to (exactly) represent jumps: MassActionJump, ConstantRateJump, and VariableRateJump. Choosing which to use is a trade off between the desired generality of the rate and affect! functions vs. the computational performance of the resulting simulated system. In general","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Jump Type Performance Generality\nMassActionJump Fastest Restrictive rates/affects\nConstantRateJump Somewhat Slower Much more general\nVariableRateJump Slowest Completely general","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"It is recommended to try to encode jumps using the most performant option that supports the desired generality of the underlying rate and affect functions. Below we describe the different jump types, and show how the SIR model can be formulated using first ConstantRateJumps and then MassActionJumps (VariableRateJumps are considered later).","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#ConstantRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: ConstantRateJump","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump = ConstantRateJump(rate, affect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where rate is a function rate(u,p,t) and affect! is a function of the integrator affect!(integrator) (for details on the integrator, see the integrator interface docs). Here u corresponds to the current state vector of the system; for our SIR model u[1] = S(t), u[2] = I(t) and u[3] = R(t). p corresponds to the parameters of the model, just as used for passing parameters to derivative functions in ODE solvers. Thus, to define the two possible jumps for our model we take (with β = .1/1000.0 and ν = .01).","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"β = 0.1 / 1000.0\nν = .01;\np = (β, ν)\nrate1(u, p, t) = p[1] * u[1] * u[2]  # β*S*I\nfunction affect1!(integrator)\n  integrator.u[1] -= 1         # S -> S - 1\n  integrator.u[2] += 1         # I -> I + 1\n  nothing\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u, p, t) = p[2] * u[2]         # ν*I\nfunction affect2!(integrator)\n  integrator.u[2] -= 1        # I -> I - 1\n  integrator.u[3] += 1        # R -> R + 1\n  nothing\nend\njump2 = ConstantRateJump(rate2,affect2!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0 so that","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀    = [999, 10, 0]\ntspan = (0.0, 250.0)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Since we want the system state to change only at the discrete jump times, we will build a DiscreteProblem","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"prob = DiscreteProblem(u₀, tspan, p)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then use JumpProblem from JumpProcesses to augment the discrete problem with jumps and select the stochastic simulation algorithm (SSA) to use in sampling the jump processes. To create a JumpProblem we would simply do:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of jumps that occur using Gillespie's Direct stochastic simulation algorithm (SSA), also known as Doob's method or Kinetic Monte Carlo. See Constant Rate Jump Aggregators for other supported SSAs.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the JumpProcesses solvers. Since our model is a pure jump process with all rates being constant in between jumps (i.e. no continuously-varying components), we will use SSAStepper to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"plot(sol, label=[\"S(t)\" \"I(t)\" \"R(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, in systems with more than a few jumps (more than ~10), it can be advantageous to use more sophisticated SSAs than Direct. For such systems it is recommended to use SortingDirect, RSSA or RSSACR, see the list of JumpProcesses SSAs at Constant Rate Jump Aggregators.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#*Caution-about-Constant-Rate-Jumps*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Caution about Constant Rate Jumps","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"ConstantRateJumps are quite general, but they do have one restriction. They assume that the rate functions are constant at all times between two consecutive jumps of the system. i.e. any species/states or parameters that the rate function depends on must not change between the times at which two consecutive jumps occur. Such conditions are violated if one has a time dependent parameter like beta(t) or if some of the solution components, say u[2], may also evolve through a coupled ODE, SDE, or a VariableRateJump (see below for examples). For problems where the rate function may change between consecutive jumps, VariableRateJumps must be used.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Thus in the examples above,","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate1(u,p,t) = p[1]*u[1]*u[2]\nrate2(u,p,t) = p[2]*u[2]","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"both must be constant other than changes due to some other ConstantRateJump or MassActionJump (the same restriction applies to MassActionJumps). Since these rates only change when u[1] or u[2] is changed, and u[1] and u[2] only change when one of the jumps occur, this setup is valid. However, a rate of t*p[1]*u[1]*u[2] would not be valid because the rate would change during the interval, as would p[2]*u[1]*u[4] when u[4] is the solution to a continuous problem such as an ODE or SDE or can be changed via a VariableRateJump. Thus one must be careful to follow this rule when choosing rates.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In summary, if a particular jump process has a rate function that depends explicitly or implicitly on a continuously changing quantity, you need to use a VariableRateJump.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#SSAStepper","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"SSAStepper","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Any common interface algorithm can be used to perform the time-stepping since it is implemented over the callback interface. This allows for hybrid systems that mix ODEs, SDEs and jumps. In many cases we may have a pure jump system that only involves ConstantRateJumps and/or MassActionJumps (see below). When that's the case, a substantial performance benefit may be gained by using SSAStepper. Note, SSAStepper is a more limited time-stepper which only supports discrete events, and does not allow simultaneous coupled ODEs or SDEs or VariableRateJumps. It is, however, very efficient for pure jump problems involving only ConstantRateJumps and MassActionJumps.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#save_positions_docs","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Reducing Memory Use: Controlling Saving Behavior","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that jumps act via DifferentialEquations.jl's callback interface, which defaults to saving at each event. This is required in order to accurately resolve every discontinuity exactly (and this is what allows for perfectly vertical lines in plots!). However, in many cases when using jump problems you may wish to decrease the saving pressure given by large numbers of jumps. To do this, you set the save_positions keyword argument to JumpProblem. Just like for other callbacks, this is a tuple (bool1, bool2) which sets whether to save before or after a jump. If we do not want to save at every jump, we would thus pass:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2; save_positions = (false, false))","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now the saving controls associated with the integrator should specified, see the main SciML Docs for saving options. For example, we can use saveat = 10.0 to save at an evenly spaced grid:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper(); saveat = 10.0)\n\n# we plot each solution component separately since\n# the graph should no longer be a step function\nplot(sol.t, sol[1,:]; marker = :o, label=\"S(t)\", xlabel=\"t\")\nplot!(sol.t, sol[2,:]; marker = :x, label=\"I(t)\", xlabel=\"t\")\nplot!(sol.t, sol[3,:]; marker = :d, label=\"R(t)\", xlabel=\"t\")","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice that our plot (and solutions) are now defined at precisely the specified time points. It is important to note that interpolation of the solution object will no longer be exact for a pure jump process, as the solution values at jump times have not been stored. i.e for t a time we did not save at sol(t) will no longer give the exact value of the solution at t.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#MassActionJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: MassActionJump","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For ConstantRateJumps that can be represented as mass action reactions a further specialization of the jump type is possible that offers improved computational performance; MassActionJump. Suppose the system has N chemical species S_1dotsS_N. A general mass action reaction has the form","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"R_1 S_1 + R_2 S_2 + dots + R_N S_N oversetkrightarrow P_1 S_1 + P_2 S_2 + dots + P_N S_N","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where the non-negative integers (R_1dotsR_N) denote the reactant stoichiometry of the reaction, and the non-negative integers (P_1dotsP_N) the product stoichiometry. The net stoichiometry is the net change in each chemical species from the reaction occurring one time, given by mathbfnu = (P_1-R_1dotsP_N-R_N). As such, the affect! function associated with a MassActionJump simply changes the state, mathbfu(t) = (u_1(t)dotsu_N(t)), by updating","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"mathbfu(t) to mathbfu(t) + mathbfnu","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The default rate function, ρ = rate(u,p,t), is based on stochastic chemical kinetics and given by","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"ρ(mathbfu(t)) = k prod_i=1^N beginpmatrix u_i  R_i endpmatrix\n= k prod_i=1^N fracu_iR_i (u_i - R_i)\n= k prod_i=1^N fracu_i (u_i - 1) cdots (u_i - R_i + 1)R_i","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where k denotes the rate constant of the reaction (in units of per time).","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"As an example, consider again the SIR model. The species are u = (S,I,R). The first reaction has rate β, reactant stoichiometry (1, 1, 0), product stoichiometry (0, 2, 0), and net stoichiometry (-1, 1, 0). The second reaction has rate ν, reactant stoichiometry (0, 1, 0), product stoichiometry (0, 0, 1), and net stoichiometry (0, -1, 1).","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can manually encode this system as a mass action jump by specifying the indexes of the rate constants in p, the reactant stoichiometry, and the net stoichiometry. We note that the first two determine the rate function, with the latter determining the affect function.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rateidxs = [1, 2]           # i.e. [β, ν]\nreactant_stoich =\n[\n  [1 => 1, 2 => 1],         # 1*S and 1*I\n  [2 => 1]                  # 1*I\n]\nnet_stoich =\n[\n  [1 => -1, 2 => 1],        # -1*S and 1*I\n  [2 => -1, 3 => 1]         # -1*I and 1*R\n]\nmass_act_jump = MassActionJump(reactant_stoich, net_stoich; param_idxs=rateidxs)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, one typically should define one MassActionJump that encodes each possible jump that can be represented via a mass action reaction. This is in contrast to ConstantRateJumps or VariableRateJumps where separate instances are created for each distinct jump type.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Just like for ConstantRateJumps, to then simulate the system we create a JumpProblem and call solve:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump)\nsol = solve(jump_prob, SSAStepper())\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about MassActionJumps see Defining a Mass Action Jump. We note that one could include the factors of 1  R_i directly in the rate constant passed into a MassActionJump, so that the desired rate function that gets evaluated is","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"hatk prod_i=1^N u_i (u_i - 1) cdots (u_i - R_i + 1)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"with hatk = k  prod_i=1^N R_i the renormalized rate constant. Passing the keyword argument scale_rates = false will disable MassActionJumps internally rescaling the rate constant by \\prod_{i=1}^{N} R_i!.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For chemical reaction systems Catalyst.jl automatically groups reactions into their optimal jump representation.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Defining-the-Jumps-Directly:-Mixing-ConstantRateJump-and-MassActionJump","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: Mixing ConstantRateJump and MassActionJump","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Suppose we now want to add in to the SIR model another jump that can not be represented as a mass action reaction. We can create a new ConstantRateJump and simulate a hybrid system using both the MassActionJump for the two previous reactions, and the new ConstantRateJump. Let's suppose we want to let susceptible people be born with the following jump rate:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"birth_rate(u,p,t) = 10.0 * u[1] / (200.0 + u[1]) + 10.0\nfunction birth_affect!(integrator)\n  integrator.u[1] += 1\n  nothing\nend\nbirth_jump = ConstantRateJump(birth_rate, birth_affect!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then simulate the hybrid system as","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump, birth_jump)\nsol = solve(jump_prob, SSAStepper())\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#Adding-Jumps-to-a-Differential-Equation","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding Jumps to a Differential Equation","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If we instead used some form of differential equation instead of a DiscreteProblem, we would couple the jumps/reactions to the differential equation. Let's define an ODE problem, where the continuous part only acts on some new 4th component:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using OrdinaryDiffEq\nfunction f(du, u, p, t)\n  du[4] = u[2]*u[3]/100000 - u[1]*u[4]/100000\n  nothing\nend\nu₀   = [999.0, 10.0, 0.0, 100.0]\nprob = ODEProblem(f, u₀, tspan, p)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice we gave the 4th component a starting value of 100.0, and used floating point numbers for the initial condition since some solution components now evolve continuously. The same steps as above will allow us to solve this hybrid equation when using ConstantRateJumps (or MassActionJumps). For example, we can solve it using the Tsit5() method via:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)\nsol = solve(jump_prob, Tsit5())\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\" \"u₄(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#VariableRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding a VariableRateJump","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now let's consider adding a reaction whose rate changes continuously with the differential equation. To continue our example, let there be a new reaction with rate depending on u[4] of the form u_4 to u_4 + textrmI, with a rate constant of 1e-2:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate3(u, p, t) = 1e-2 * u[4]\nfunction affect3!(integrator)\n  integrator.u[2] += 1    # I -> I + 1\n  nothing\nend\njump3 = VariableRateJump(rate3, affect3!)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, since rate3 depends on a variable that evolves continuously, and hence is not constant between jumps, we must use a VariableRateJump.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Solving the equation is exactly the same:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀   = [999.0, 10.0, 0.0, 1.0]\nprob = ODEProblem(f, u₀, tspan, p)\njump_prob = JumpProblem(prob, Direct(), jump, jump2, jump3)\nsol = solve(jump_prob, Tsit5())\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\" \"u₄(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that VariableRateJumps require using a continuous problem, like an ODE/SDE/DDE/DAE problem, and using floating point initial conditions.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Lastly, we are not restricted to ODEs. For example, we can solve the same jump problem except with multiplicative noise on u[4] by using an SDEProblem instead:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using StochasticDiffEq\nfunction g(du, u, p, t)\n  du[4] = 0.1u[4]\nend\nprob = SDEProblem(f, g, [999.0, 1.0, 0.0, 1.0], (0.0, 250.0), p)\njump_prob = JumpProblem(prob, Direct(), jump, jump2, jump3)\nsol = solve(jump_prob, SRIW1())\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\" \"u₄(t)\"])","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about VariableRateJumps see Defining a Variable Rate Jump.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/#RegularJumps-and-τ-Leaping","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"RegularJumps and τ-Leaping","text":"","category":"section"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The previous parts described how to use ConstantRateJumps, MassActionJumps, and VariableRateJumps, however, in many cases one does not require the exactness of stepping to every jump time. Instead, regular jumping (i.e. τ-leaping) allows pooling jumps together, and performing larger updates in a statistically-correct but more efficient manner. The trade-off is the introduction of a time-discretization error due to the time-stepping, but one that is controlled and convergent as the time-step is reduced to zero.","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Let's see how to define the SIR model in terms of a RegularJump. We need two functions, rate and change!. rate is a vector equation which computes the rates of each jump process","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"function rate(out, u, p, t)\n    out[1] = p[1] * u[1] * u[2]   # β * S * I\n    out[2] = p[2] * u[2]          # ν * I\n    nothing\nend","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We then define a function that given a vector storing the number of times each jump occurs during a time-step, counts, calculates the change in the state, du. For the SIR example we do this by multiplying counts by a matrix that encodes the change in the state due to one occurrence of each reaction (i.e. the net stoichiometry matrix). Below c[i,j] gives the change in u[i] due to the jth jump:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"c = zeros(3, 2)\n# S + I --> I\nc[1,1] = -1    # S -> S - 1\nc[2,1] = 1     # I -> I + 1\n\n# I --> R\nc[2,2] = -1    # I -> I - 1\nc[3,2] = 1     # R -> R + 1\n\nfunction change(du, u, p, t, counts, mark)\n  mul!(du, c, counts)\n  nothing\nend","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We are now ready to create our RegularJump, passing in the rate function, change function, and the number of jumps being encoded","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rj = RegularJump(rate, change, 2)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"From there we build a JumpProblem","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀ = [1000.0, 50.0, 0.0]\nprob = DiscreteProblem(u₀, tspan, p)\njump_prob = JumpProblem(prob, Direct(), rj)","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that when a JumpProblem has a RegularJump, τ-leaping algorithms are required for simulating it. This is detailed on the jump solvers page. One such algorithm is TauLeaping from StochasticDiffEq.jl, which we use as follows:","category":"page"},{"location":"modules/JumpProcesses/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, TauLeaping(); dt=.001)\nplot(sol; label=[\"S(t)\" \"I(t)\" \"R(t)\"])","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/#The-Derivative-neural-network-approximation","page":"The Derivative Neural Network Approximation","title":"The Derivative neural network approximation","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The accuracy and stability of numerical derivative decreases with each successive order. The accuracy of the entire solution is determined by the worst accuracy of one of the variables, in our case - the highest degree of the derivative. Meanwhile the computational cost of automatic differentation for higher orders grows the power in O(n^d), making even numerical differentiation much more efficient! Given these two bad choices, there exists an alternative which can improve training speed and accuracy: using a system to represent the derivatives directly.","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/#Demonstration","page":"The Derivative Neural Network Approximation","title":"Demonstration","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"Take the PDE system:","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\n_t u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"with the initial conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"and the boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"This is the same system as the system of equations example","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The derivative neural network approximation is such an approach that using lower-order numeric derivatives and estimates higher-order derivatives with a neural network so that allows an increase in the marginal precision for all optimization. Since u3 is only in the first and second equations, that its accuracy during training is determined by the accuracy of the second numerical derivative u3(t,x) ~ (Dtt(u1(t,x)) -Dxx(u1(t,x))) / sin(pi*x).","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"We approximate the derivative of the neural network with another neural network Dt(u1(t,x)) ~ Dtu1(t,x) and train it along with other equations, and thus we avoid using the second numeric derivative Dt(Dtu1(t,x)).","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\nDt = Differential(t)\nDx = Differential(x)\n@variables u1(..), u2(..), u3(..)\n@variables Dxu1(..) Dtu1(..) Dxu2(..) Dtu2(..)\n\neqs_ = [Dt(Dtu1(t,x)) ~ Dx(Dxu1(t,x)) + u3(t,x)*sin(pi*x),\n        Dt(Dtu2(t,x)) ~ Dx(Dxu2(t,x)) + u3(t,x)*cos(pi*x),\n        exp(-t) ~ u1(t,x)*sin(pi*x) + u2(t,x)*cos(pi*x)]\n\nbcs_ = [u1(0.,x) ~ sin(pi*x),\n       u2(0.,x) ~ cos(pi*x),\n       Dt(u1(0,x)) ~ -sin(pi*x),\n       Dt(u2(0,x)) ~ -cos(pi*x),\n       #Dtu1(0,x) ~ -sin(pi*x),\n      # Dtu2(0,x) ~ -cos(pi*x),\n       u1(t,0.) ~ 0.,\n       u2(t,0.) ~ exp(-t),\n       u1(t,1.) ~ 0.,\n       u2(t,1.) ~ -exp(-t)]\n\nder_ = [Dt(u1(t,x)) ~ Dtu1(t,x),\n        Dt(u2(t,x)) ~ Dtu2(t,x),\n        Dx(u1(t,x)) ~ Dxu1(t,x),\n        Dx(u2(t,x)) ~ Dxu2(t,x)]\n\nbcs__ = [bcs_;der_]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_,n,Lux.σ),Dense(n,n,Lux.σ),Dense(n,1)) for _ in 1:7]\n\ngrid_strategy = NeuralPDE.GridTraining(0.07)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,\n                                             grid_strategy)\n\nvars = [u1(t,x),u2(t,x),u3(t,x),Dxu1(t,x),Dtu1(t,x),Dxu2(t,x),Dtu2(t,x)]\n@named pdesystem = PDESystem(eqs_,bcs__,domains,[t,x],vars)\nprob = NeuralPDE.discretize(pdesystem,discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem,discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:7]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[9:end]\n\ncallback = function (p,l)\n    println(\"loss: \", l )\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.01); callback = callback, maxiters=2000)\nprob = remake(prob,u0=res.u)\nres = Optimization.solve(prob,BFGS(); callback = callback, maxiters=10000)\n\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"And some analysis:","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using Plots\n\nts,xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:length(chain)]\n\nu1_real(t,x) = exp(-t)*sin(pi*x)\nu2_real(t,x) = exp(-t)*cos(pi*x)\nu3_real(t,x) = (1+pi^2)*exp(-t)\nDxu1_real(t,x) = pi*exp(-t)*cos(pi*x)\nDtu1_real(t,x) = -exp(-t)*sin(pi*x)\nDxu2_real(t,x) = -pi*exp(-t)*sin(pi*x)\nDtu2_real(t,x) = -exp(-t)*cos(pi*x)\nanalytic_sol_func_all(t,x) = [u1_real(t,x), u2_real(t,x), u3_real(t,x),\n                              Dxu1_real(t,x),Dtu1_real(t,x),Dxu2_real(t,x),Dtu2_real(t,x)]\n\nu_real  = [[analytic_sol_func_all(t,x)[i] for t in ts for x in xs] for i in 1:7]\nu_predict  = [[phi[i]([t,x],minimizers_[i])[1] for t in ts  for x in xs] for i in 1:7]\ndiff_u = [abs.(u_real[i] .- u_predict[i] ) for i in 1:7]\n\ntitles = [\"u1\",\"u2\",\"u3\",\"Dtu1\",\"Dtu2\",\"Dxu1\",\"Dxu2\"]\nfor i in 1:7\n    p1 = plot(ts, xs, u_real[i], linetype=:contourf,title = \"$(titles[i]), analytic\");\n    p2 = plot(ts, xs, u_predict[i], linetype=:contourf,title = \"predict\");\n    p3 = plot(ts, xs, diff_u[i],linetype=:contourf,title = \"error\");\n    plot(p1,p2,p3)\n    savefig(\"3sol_ub$i\")\nend","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u1)","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u2)","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u3)","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/#Comparison-of-the-second-numerical-derivative-and-numerical-neural-network-derivative","page":"The Derivative Neural Network Approximation","title":"Comparison of the second numerical derivative and numerical + neural network derivative","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: DDu1)","category":"page"},{"location":"modules/NeuralPDE/tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: DDu2)","category":"page"},{"location":"modules/Optimization/tutorials/minibatch/#Data-Iterators-and-Minibatching","page":"Data Iterators and Minibatching","title":"Data Iterators and Minibatching","text":"","category":"section"},{"location":"modules/Optimization/tutorials/minibatch/","page":"Data Iterators and Minibatching","title":"Data Iterators and Minibatching","text":"note: Note\nThis example uses the OptimizationOptimisers.jl package. See the  Optimisers.jl page for details on the installation and usage.","category":"page"},{"location":"modules/Optimization/tutorials/minibatch/","page":"Data Iterators and Minibatching","title":"Data Iterators and Minibatching","text":"using Flux, Optimization, OptimizationOptimisers, OrdinaryDiffEq, DiffEqSensitivity\n\nfunction newtons_cooling(du, u, p, t)\n    temp = u[1]\n    k, temp_m = p\n    du[1] = dT = -k*(temp-temp_m)\n  end\n\nfunction true_sol(du, u, p, t)\n    true_p = [log(2)/8.0, 100.0]\n    newtons_cooling(du, u, true_p, t)\nend\n\nann = Chain(FastDense(1,8,tanh), FastDense(8,1,tanh))\npp,re = Flux.destructure(ann)\n\nfunction dudt_(u,p,t)\n    re(p)(u) .* u\nend\n\ncallback = function (p,l,pred;doplot=false) #callback function to observe training\n    display(l)\n    # plot current prediction against data\n    if doplot\n      pl = scatter(t,ode_data[1,:],label=\"data\")\n      scatter!(pl,t,pred[1,:],label=\"prediction\")\n      display(plot(pl))\n    end\n    return false\nend\n\nu0 = Float32[200.0]\ndatasize = 30\ntspan = (0.0f0, 1.5f0)\n\nt = range(tspan[1], tspan[2], length=datasize)\ntrue_prob = ODEProblem(true_sol, u0, tspan)\node_data = Array(solve(true_prob, Tsit5(), saveat=t))\n\nprob = ODEProblem{false}(dudt_, u0, tspan, pp)\n\nfunction predict_adjoint(fullp, time_batch)\n    Array(solve(prob, Tsit5(), p = fullp, saveat = time_batch))\nend\n\nfunction loss_adjoint(fullp, batch, time_batch)\n    pred = predict_adjoint(fullp,time_batch)\n    sum(abs2, batch .- pred), pred\nend\n\n\nk = 10\ntrain_loader = Flux.Data.DataLoader((ode_data, t), batchsize = k)\n\nnumEpochs = 300\nl1 = loss_adjoint(pp, train_loader.data[1], train_loader.data[2])[1]\n\noptfun = OptimizationFunction((θ, p, batch, time_batch) -> loss_adjoint(θ, batch, time_batch), Optimization.AutoZygote())\noptprob = OptimizationProblem(optfun, pp)\nusing IterTools: ncycle\nres1 = Optimization.solve(optprob, Optimisers.ADAM(0.05), ncycle(train_loader, numEpochs), callback = callback)\n@test 10res1.minimum < l1","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"using PolyChaos\nk = 1\ndeg, Nrec = 2, 20\nopq = GaussOrthoPoly(deg; Nrec=Nrec, addQuadrature=true);\nshowbasis(opq; sym=\"ξ\") # works for `op` too!\nshowpoly(0:2:deg,opq)\nL = dim(opq)\nmu, sig = 0., 1.\nx = [ convert2affinePCE(mu, sig, opq); zeros(Float64,L-2) ]\nt2 = Tensor(2, opq);\nt3 = Tensor(3, opq)\ny = [ sum( x[i]*x[j]*t3.get([i-1,j-1,m-1])/t2.get([m-1,m-1])  for i=1:L, j=1:L ) for m=1:L ]\nmoms_analytic(k) = [k, sqrt(2k), sqrt(8/k)]\nfunction myskew(y)\n   e3 = sum( y[i]*y[j]*y[k]*t3.get([i-1,j-1,k-1]) for i=1:L,j=1:L,k=1:L )\n   μ = y[1]\n   σ = std(y,opq)\n   (e3-3*μ*σ^2-μ^3)/(σ^3)\nend\nprint(\"Expected value:\\t\\t$(moms_analytic(k)[1]) = $(mean(y,opq))\\n\")\nprint(\"\\t\\t\\terror = $(abs(mean(y,opq)-moms_analytic(k)[1]))\\n\")\nprint(\"Standard deviation:\\t$(moms_analytic(k)[2]) = $(std(y,opq))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[2]-std(y,opq))\\n\")\nprint(\"Skewness:\\t\\t$(moms_analytic(k)[3]) = $(myskew(y))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[3]-myskew(y))\\n\")\nusing Plots\nNsmpl = 10000\nysmpl = samplePCE(Nsmpl, y, opq)\nhistogram(ysmpl;normalize=true,xlabel=\"t\",ylabel=\"rho(t)\")\nimport SpecialFunctions: gamma\nρ(t) = 1/(sqrt(2)*gamma(0.5))*1/sqrt(t)*exp(-0.5*t)\nt = range(0.1; stop=maximum(ysmpl), length=100)\nplot!(t, ρ.(t), w=4)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/#Chi-squared-Distribution-(k1)","page":"Chi Squared, One DOF","title":"Chi-squared Distribution (k=1)","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k1/#Theory","page":"Chi Squared, One DOF","title":"Theory","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Given a standard random variable X sim mathcalN(01) we would like to find the random variable Y = X^2. The analytic solution is known: Y follows a chi-squared distribution with k=1 degree of freedom.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Using polynomial chaos expansion (PCE), the problem can be solved using Galerkin projection. Let phi_k _k=0^n be the monic orthogonal basis relative to the probability density of X, namely","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"f_X(x) = frac1sqrt2 pi exp left( - fracx^22 right)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Then, the PCE of X is given by","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"X = sum_k=0^n x_k phi_k","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"with","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"x_0 = 0 quad x_1 = 1 quad x_i = 0 quad forall i =2dotsn","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"To find the PCE coefficients y_k for Y = sum_k=^n y_k phi_k, we apply Galerkin projection, which leads to","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"y_m langle phi_m phi_m rangle = sum_i=0^n sum_j=0^n x_i x_j langle phi_i phi_j phi_m rangle quad forall m = 0 dots n","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Hence, knowing the scalars langle phi_m phi_m rangle, and langle phi_i phi_j phi_m rangle, the PCE coefficients y_k can be obtained immediately. From the PCE coefficients we can get the moments and compare them to the closed-form expressions.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Notice: A maximum degree of 2 suffices to get the exact solution with PCE. In other words, increasing the maximum degree to values greater than 2 introduces nothing but computational overhead (and numerical errors, possibly).","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/#Practice","page":"Chi Squared, One DOF","title":"Practice","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"First, we create a orthogonal basis relative to f_X(x) of degree at most d=2 (deg below).","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Notice that we consider a total of Nrec recursion coefficients, and that we also add a quadrature rule by setting addQuadrature = true.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"using PolyChaos\nk = 1\ndeg, Nrec = 2, 20\nopq = GaussOrthoPoly(deg; Nrec=Nrec, addQuadrature=true);","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"What are the basis polynomials?","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"showbasis(opq; sym=\"ξ\")","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Note that the command showbasis is based on the more general showpoly:","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"showpoly(0:2:deg,opq)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Next, we define the PCE for X.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"L = dim(opq)\nmu, sig = 0., 1.\nx = [ convert2affinePCE(mu, sig, opq); zeros(Float64,L-2) ]","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"With the orthogonal basis and the quadrature at hand, we can compute the tensors t2 and t3 that store the entries langle phi_m phi_m rangle, and langle phi_i phi_j phi_m rangle, respectively.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"t2 = Tensor(2, opq);\nt3 = Tensor(3, opq)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"With the tensors at hand, we can compute the Galerkin projection.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"y = [ sum( x[i]*x[j]*t3.get([i-1,j-1,m-1])/t2.get([m-1,m-1])  for i=1:L, j=1:L ) for m=1:L ]","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Let's compare the moments via PCE to the closed-form expressions.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"moms_analytic(k) = [k, sqrt(2k), sqrt(8/k)]\nfunction myskew(y)\n   e3 = sum( y[i]*y[j]*y[k]*t3.get([i-1,j-1,k-1]) for i=1:L, j=1:L, k=1:L )\n   μ = y[1]\n   σ = std(y,opq)\n   (e3-3*μ*σ^2-μ^3)/(σ^3)\nend\n\nprint(\"Expected value:\\t\\t$(moms_analytic(k)[1]) = $(mean(y,opq))\\n\")\nprint(\"\\t\\t\\terror = $(abs(mean(y,opq)-moms_analytic(k)[1]))\\n\")\nprint(\"Standard deviation:\\t$(moms_analytic(k)[2]) = $(std(y,opq))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[2]-std(y,opq))\\n\")\nprint(\"Skewness:\\t\\t$(moms_analytic(k)[3]) = $(myskew(y))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[3]-myskew(y))\\n\")\n","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"Let's plot the probability density function to compare results. We first draw samples from the measure with the help of sampleMeasure(), and then evaluate the basis at these samples and multiply times the PCE coefficients. The latter stop is done using evaluatePCE(). Finally, we compare the result agains the analytical PDF rho(t) = fracmathrme^-05tsqrt2 t  Gamma(05) of the chi-squared distribution with one degree of freedom.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k1/","page":"Chi Squared, One DOF","title":"Chi Squared, One DOF","text":"using Plots\nNsmpl = 10000\n# long way: ξ = sampleMeasure(Nsmpl,opq), ysmpl = evaluatePCE(y,ξ,opq)\nysmpl = samplePCE(Nsmpl, y, opq)\nhistogram(ysmpl; normalize=true, xlabel=\"t\", ylabel=\"rho(t)\")\n\nimport SpecialFunctions: gamma\nρ(t) = 1/(sqrt(2)*gamma(0.5))*1/sqrt(t)*exp(-0.5*t)\nt = range(0.1; stop=maximum(ysmpl), length=100)\nplot!(t, ρ.(t), w=4)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"k = 12\nusing PolyChaos\ndegree, Nrec = 2, 20\nopq = GaussOrthoPoly(degree; Nrec=Nrec, addQuadrature = true);\nmop = MultiOrthoPoly([opq for i in 1:k], degree)\nL = dim(mop)\nmu, sig = 0., 1.\nx = [ assign2multi(convert2affinePCE(mu, sig, opq),i,mop.ind) for i in 1:k ]\nt2 = Tensor(2,mop)\nt3 = Tensor(3,mop)\ny = [ sum( x[i][j1]*x[i][j2]*t3.get([j1-1,j2-1,m-1])/t2.get([m-1,m-1])  for i=1:k, j1=1:L, j2=1:L ) for m=1:L ]\nmoms_analytic(k) = [k, sqrt(2k), sqrt(8/k)]\nfunction myskew(y)\n   e3 = sum( y[i]*y[j]*y[k]*t3.get([i-1,j-1,k-1]) for i=1:L,j=1:L,k=1:L )\n   μ = y[1]\n   σ = std(y,mop)\n   (e3-3*μ*σ^2-μ^3)/(σ^3)\nend\nprint(\"Expected value:\\t\\t$(moms_analytic(k)[1]) = $(mean(y,mop))\\n\")\nprint(\"\\t\\t\\terror = $(abs(mean(y,mop)-moms_analytic(k)[1]))\\n\")\nprint(\"Standard deviation:\\t$(moms_analytic(k)[2]) = $(std(y,mop))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[2]-std(y,mop))\\n\")\nprint(\"Skewness:\\t\\t$(moms_analytic(k)[3]) = $(myskew(y))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[3]-myskew(y))\\n\")\nusing Plots\nNsmpl = 10000\nysmpl = samplePCE(Nsmpl, y, mop)\nhistogram(ysmpl;normalize=true, xlabel=\"t\",ylabel=\"rho(t)\")\nimport SpecialFunctions: gamma\nρ(t) = 1/(2^(0.5*k)*gamma(0.5*k))*t^(0.5*k-1)*exp(-0.5*t)\nt = range(0.1; stop=maximum(ysmpl), length=100)\nplot!(t,ρ.(t),w=4)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/#Chi-squared-Distribution-(k1)","page":"Chi Squared, Several DOFs","title":"Chi-squared Distribution (k1)","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k_greater1/#Theory","page":"Chi Squared, Several DOFs","title":"Theory","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Given k standard random variables X_i sim mathcalN(01) for i=1dotsk we would like to find the random variable Y = sum_i=1^k X_i^2. The analytic solution is known: Y follows a chi-squared distribution with k degrees of freedom.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Using polynomial chaos expansion (PCE), the problem can be solved using Galerkin projection. Let phi_k _k=0^n be the monic orthogonal basis relative to the probability density of X = X_1 dots X_k, namely","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"f_X(x) =  prod_i=1^k frac1sqrt2 pi  exp left( - fracx_i^22 right)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Then, the PCE of X_i is given by","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"X_i = sum_k=0^n x_ik phi_k","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"with","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"x_i0 = 0 quad x_ii+1 = 1 quad x_il = 0 quad forall l in 1dotsn setminus i+1","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"To find the PCE coefficients y_k for Y = sum_k=^n y_k phi_k, we apply Galerkin projection, which leads to","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"y_m langle phi_m phi_m rangle = sum_i=1^k sum_j_1=0^n sum_j_2=0^n x_ij_1 x_ij_2 langle phi_j_1 phi_j_2 phi_m rangle quad forall m = 0 dots n","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Hence, knowing the scalars langle phi_m phi_m rangle, and langle phi_j_1 phi_j_2 phi_m rangle, the PCE coefficients y_k can be obtained immediately. From the PCE coefficients we can get the moments and compare them to the closed-form expressions.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Notice: A maximum degree of 2 suffices to get the exact solution with PCE. In other words, increasing the maximum degree to values greater than 2 introduces nothing but computational overhead (and numerical errors, possibly).","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/#Practice","page":"Chi Squared, Several DOFs","title":"Practice","text":"","category":"section"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"First, we create a orthogonal basis relative to f_X(x) of degree at most d=2 (degree below).","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Notice that we consider a total of Nrec recursion coefficients, and that we also add a quadrature rule by setting addQuadrature = true.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"k = 12\nusing PolyChaos\ndegree, Nrec = 2, 20\nopq = GaussOrthoPoly(degree; Nrec=Nrec, addQuadrature = true);","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Now let's define a multivariate basis","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"mop = MultiOrthoPoly([opq for i in 1:k], degree)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Next, we define the PCE for all X_i with i = 1 dots k.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"L = dim(mop)\nmu, sig = 0., 1.\nx = [ assign2multi(convert2affinePCE(mu, sig, opq), i, mop.ind) for i in 1:k ]","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"With the orthogonal basis and the quadrature at hand, we can compute the tensors t2 and t3 that store the entries langle phi_m phi_m rangle, and langle phi_j_1 phi_j_2 phi_m rangle, respectively.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"t2 = Tensor(2,mop)\nt3 = Tensor(3,mop)","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"With the tensors at hand, we can compute the Galerkin projection.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Notice: there are more efficient ways to do this, but let's keep it simple.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"y = [ sum( x[i][j1]*x[i][j2]*t3.get([j1-1,j2-1,m-1])/t2.get([m-1,m-1])  for i=1:k, j1=1:L, j2=1:L ) for m=1:L ]","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Let's compare the moments via PCE to the closed-form expressions.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"moms_analytic(k) = [k, sqrt(2k), sqrt(8/k)]\nfunction myskew(y)\n   e3 = sum( y[i]*y[j]*y[k]*t3.get([i-1,j-1,k-1]) for i=1:L,j=1:L,k=1:L )\n   μ = y[1]\n   σ = std(y,mop)\n   (e3-3*μ*σ^2-μ^3)/(σ^3)\nend\n\nprint(\"Expected value:\\t\\t$(moms_analytic(k)[1]) = $(mean(y,mop))\\n\")\nprint(\"\\t\\t\\terror = $(abs(mean(y,mop)-moms_analytic(k)[1]))\\n\")\nprint(\"Standard deviation:\\t$(moms_analytic(k)[2]) = $(std(y,mop))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[2]-std(y,mop))\\n\")\nprint(\"Skewness:\\t\\t$(moms_analytic(k)[3]) = $(myskew(y))\\n\")\nprint(\"\\t\\t\\terror = $(moms_analytic(k)[3]-myskew(y))\\n\")\n","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"Let's plot the probability density function to compare results. We first draw samples from the measure with the help of sampleMeasure(), and then evaluate the basis at these samples and multiply times the PCE coefficients. The latter stop is done using evaluatePCE(). Both steps are combined in the function samplePCE(). Finally, we compare the result agains the analytical PDF rho(t) = fract^t2-1mathrme^-t22^k2  Gamma(k2) of the chi-squared distribution with one degree of freedom.","category":"page"},{"location":"modules/PolyChaos/chi_squared_k_greater1/","page":"Chi Squared, Several DOFs","title":"Chi Squared, Several DOFs","text":"using Plots\nNsmpl = 10000\n# long way: ξ = sampleMeasure(Nsmpl,mop), ysmpl = evaluatePCE(y,ξ,mop)\nysmpl = samplePCE(Nsmpl, y, mop)\nhistogram(ysmpl;normalize=true, xlabel=\"t\",ylabel=\"rho(t)\")\n\nimport SpecialFunctions: gamma\nρ(t) = 1/(2^(0.5*k)*gamma(0.5*k))*t^(0.5*k-1)*exp(-0.5*t)\nt = range(0.1; stop=maximum(ysmpl), length=100)\nplot!(t, ρ.(t), w=4)","category":"page"},{"location":"modules/DiffEqFlux/layers/HamiltonianNN/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"The following layer helps construct a neural network which allows learning dynamics and conservation laws by approximating the hamiltonian of a system.","category":"page"},{"location":"modules/DiffEqFlux/layers/HamiltonianNN/","page":"Hamiltonian Neural Network Layer","title":"Hamiltonian Neural Network Layer","text":"HamiltonianNN\nNeuralHamiltonianDE","category":"page"},{"location":"modules/DiffEqFlux/layers/HamiltonianNN/#DiffEqFlux.HamiltonianNN","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.HamiltonianNN","text":"Constructs a Hamiltonian Neural Network [1]. This neural network is useful for learning symmetries and conservation laws by supervision on the gradients of the trajectories. It takes as input a concatenated vector of length 2n containing the position (of size n) and momentum (of size n) of the particles. It then returns the time derivatives for position and momentum.\n\nnote: Note\nThis doesn't solve the Hamiltonian Problem. Use NeuralHamiltonianDE for such applications.\n\nnote: Note\nThis layer currently doesn't support GPU. The support will be added in future with some AD fixes.\n\nTo obtain the gradients to train this network, ReverseDiff.gradient is supposed to be used. This prevents the usage of DiffEqFlux.sciml_train or Flux.train. Follow this tutorial to see how to define a training loop to circumvent this issue.\n\nHamiltonianNN(model; p = nothing)\nHamiltonianNN(model::FastChain; p = initial_params(model))\n\nArguments:\n\nmodel: A Chain or FastChain neural network that returns the Hamiltonian of the          system.\np: The initial parameters of the neural network.\n\nReferences:\n\n[1] Greydanus, Samuel, Misko Dzamba, and Jason Yosinski. \"Hamiltonian Neural Networks.\" Advances in Neural Information Processing Systems 32 (2019): 15379-15389.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqFlux/layers/HamiltonianNN/#DiffEqFlux.NeuralHamiltonianDE","page":"Hamiltonian Neural Network Layer","title":"DiffEqFlux.NeuralHamiltonianDE","text":"Contructs a Neural Hamiltonian DE Layer for solving Hamiltonian Problems parameterized by a Neural Network HamiltonianNN.\n\nNeuralHamiltonianDE(model, tspan, args...; kwargs...)\n\nArguments:\n\nmodel: A Chain, FastChain or Hamiltonian Neural Network that predicts the          Hamiltonian of the system.\ntspan: The timespan to be solved on.\nkwargs: Additional arguments splatted to the ODE solver. See the           Common Solver Arguments           documentation for more details.\n\n\n\n\n\n","category":"type"},{"location":"modules/CommonSolve/#CommonSolve.jl:-The-Common-Solve-Definition-and-Interface","page":"The Common Solve Interface","title":"CommonSolve.jl: The Common Solve Definition and Interface","text":"","category":"section"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"This holds the common solve, init, and solve! commands. By using the same definition, solver libraries from other completely different ecosystems can extend the functions and thus not clash with SciML if both ecosystems export the solve command. The rules are that  you must dispatch on one of your own types. That's it. No pirates.","category":"page"},{"location":"modules/CommonSolve/#General-recommendation","page":"The Common Solve Interface","title":"General recommendation","text":"","category":"section"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"solve function has the default definition","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"solve(args...; kwargs...) = solve!(init(args...; kwargs...))","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"So, we recommend defining","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"init(::ProblemType, args...; kwargs...) :: SolverType\nsolve!(::SolverType) :: SolutionType","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"where ProblemType, SolverType, and SolutionType are the types defined in your package.","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"To avoid method ambiguity, the first argument of solve, solve!, and init must be dispatched on the type defined in your package.  For example, do not define a method such as","category":"page"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"init(::AbstractVector, ::AlgorithmType)","category":"page"},{"location":"modules/CommonSolve/#API","page":"The Common Solve Interface","title":"API","text":"","category":"section"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"init\nsolve\nsolve!","category":"page"},{"location":"modules/CommonSolve/#CommonSolve.init","page":"The Common Solve Interface","title":"CommonSolve.init","text":"iter = CommonSolve.init(args...; kwargs...)\n\nSolves an equation or other mathematical problem using the algorithm specified in the arguments. Generally, the interface is:\n\niter = CommonSolve.init(prob::ProblemType,alg::SolverType; kwargs...)::IterType\nCommonSolve.solve!(iter)::SolutionType\n\nwhere the keyword arguments are uniform across all choices of algorithms. The iter type will be different for the different problem types.\n\n\n\n\n\n","category":"function"},{"location":"modules/CommonSolve/#CommonSolve.solve!","page":"The Common Solve Interface","title":"CommonSolve.solve!","text":"CommonSolve.solve!(iter)\n\nSolves an equation or other mathematical problem using the algorithm specified in the arguments. Generally, the interface is:\n\niter = CommonSolve.init(prob::ProblemType,alg::SolverType; kwargs...)::IterType\nCommonSolve.solve!(iter)::SolutionType\n\nwhere the keyword arguments are uniform across all choices of algorithms. The iter type will be different for the different problem types.\n\n\n\n\n\n","category":"function"},{"location":"modules/CommonSolve/#Contributing","page":"The Common Solve Interface","title":"Contributing","text":"","category":"section"},{"location":"modules/CommonSolve/","page":"The Common Solve Interface","title":"The Common Solve Interface","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums (look for the modelingtoolkit tag\nSee also SciML Community page","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"note: Note\nIf you are unfamiliar with the mathematical background of orthogonal polynomials, check out this tutorial.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Type-Hierarchy","page":"Type Hierarchy","title":"Type Hierarchy","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Let's look at the types PolyChaos provides. There are four AbstractTypes: AbstractMeasure, AbstractOrthoPoly, AbstractQuad, and AbstractTensor. AbstractMeasure is the core on which AbstractOrthoPoly builds, on which AbstractQuad builds, which is then used by AbstractTensor.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#AbstractMeasure","page":"Type Hierarchy","title":"AbstractMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The type tree for AbstractMeasure looks as follows","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"julia> using AbstractTrees, PolyChaos\njulia> AbstractTrees.children(x::Type) = subtypes(x)\njulia> print_tree(AbstractMeasure)\nAbstractMeasure\n├─ AbstractCanonicalMeasure\n│  ├─ Beta01Measure\n│  ├─ GammaMeasure\n│  ├─ GaussMeasure\n│  ├─ HermiteMeasure\n│  ├─ JacobiMeasure\n│  ├─ LaguerreMeasure\n│  ├─ LegendreMeasure\n│  ├─ LogisticMeasure\n│  ├─ MeixnerPollaczekMeasure\n│  ├─ Uniform01Measure\n│  ├─ Uniform_11Measure\n│  ├─ genHermiteMeasure\n│  └─ genLaguerreMeasure\n├─ Measure\n└─ ProductMeasure","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"There are several canonical measures that PolyChaos provides, all gathered in as subtypes of AbstractCanonicalMeasure. The Measure type is a generic measure, and ProductMeasure has an obvious meaning.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"What are the relevant fields?","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Measure","page":"Type Hierarchy","title":"Measure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"It all begins with a measure, more specifically absolutely continuous measures. What are the fields of such a type Measure?","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nname::String Name of measure\nw::Function Weight function w Omega rightarrow mathbbR\ndom::Tuple{Real,Real} Domain $ \\Omega$\nsymmetric::Bool Is w symmetric relative to some m in Omega, hence w(m-x) = w(m+x) for all x in Omega?\npars::Dict Additional parameters (e.g. shape parameters for Beta distribution)","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"They are a name, a weight function w Omega rightarrow mathbbR with domain Omega (dom). If the weight function is symmetric relative to some m in Omega, the field symmetric should be set to true. Symmetry relative to m means that","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"forall x in Omega quad w(m-x) = w(m+x)","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"For example, the Gaussian probability density","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"w(x) = frac1sqrt2pi mathrme^-x^22","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"is symmetric relative to the origin m=0. If the weight function has any parameters, then they are stored in the dictionary pars. For example, the probability density of the Beta distribution on Omega = 01 has two positive shape parameters alpha beta  0","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"w(x) = frac1B(alphabeta) x^alpha-1 (1-x)^beta-1","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#ProductMeasure","page":"Type Hierarchy","title":"ProductMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"So far, everything was univariate, the weight of the measure was mapping real numbers to real numbers. PolyChaos can handle product measures too. Let's assume the weight function is a product of two independent Gaussian densities","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"w mathbbR times mathbbR rightarrow mathbbR quad w(x) = frac1sqrt2pi mathrme^x_1^22 frac1sqrt2pi mathrme^x_2^22","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The type ProductMeasure serves this purpose, with its straightforward fields","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function Weight function\nmeasures::Vector{<:AbstractMeasure} Vector of univariate measures","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Canonical-Measures","page":"Type Hierarchy","title":"Canonical Measures","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Canonical measures are special, because we know their orthogonal polynomials. That is why several canonical measures are pre-defined in PolyChaos. Some of them may require additional parameters. (alphabetical order)","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Beta01Measure","page":"Type Hierarchy","title":"Beta01Measure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function frac1B(alphabeta)  t^alpha-1 (1-t)^beta-1\ndom::Tuple{<:Real,<:Real} (0 1)\nsymmetric::Bool true if alpha = beta\nashapeParameter::Real alpha  0\nbshapeParameter::Real beta  0","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#GammaMeasure","page":"Type Hierarchy","title":"GammaMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function fracbeta^alphaGamma(alpha) t^alpha-1 exp(-beta t)\ndom::Tuple{<:Real,<:Real} (0 infty)\nsymmetric::Bool false\nshapeParameter::Real alpha  0\nrateParameter::Real 1","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#GaussMeasure","page":"Type Hierarchy","title":"GaussMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function frac1sqrt2 pi  exp left( - fract^22 right)\ndom::Tuple{<:Real,<:Real} (-infty infty)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#HermiteMeasure","page":"Type Hierarchy","title":"HermiteMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function $ \\exp\\left( - t^2 \\right)$\ndom::Tuple{<:Real,<:Real} (-infty infty)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#JacobiMeasure","page":"Type Hierarchy","title":"JacobiMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\ndom::Tuple{<:Real,<:Real} (-1 1)\nsymmetric::Bool true if alpha = beta\nashapeParameter::Real alpha  -1\nbshapeParameter::Real beta  -1","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#LaguerreMeasure","page":"Type Hierarchy","title":"LaguerreMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function exp(-t)\ndom::Tuple{<:Real,<:Real} (0 infty)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#LegendreMeasure","page":"Type Hierarchy","title":"LegendreMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function 1\ndom::Tuple{<:Real,<:Real} (-1 1)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#LogisticMeasure","page":"Type Hierarchy","title":"LogisticMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function fracexp(-t)(1+exp(-t))^2\ndom::Tuple{<:Real,<:Real} (-infty infty)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#MeixnerPollaczekMeasure","page":"Type Hierarchy","title":"MeixnerPollaczekMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function frac12 pi exp((2phi-pi)t) lvertGamma(lambda + mathrmit)rvert^2\ndom::Tuple{<:Real,<:Real} (-inftyinfty)\nsymmetric::Bool false\nλParameter::Real lambda  0\nϕParameter::Real 0  phi  pi","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Uniform01Measure","page":"Type Hierarchy","title":"Uniform01Measure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function 1\ndom::Tuple{<:Real,<:Real} (0 1)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Uniform_11Measure","page":"Type Hierarchy","title":"Uniform_11Measure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function 05\ndom::Tuple{<:Real,<:Real} (-1 1)\nsymmetric::Bool true","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#genHermiteMeasure","page":"Type Hierarchy","title":"genHermiteMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function $ \\lvert t \\rvert^{2 \\mu}\\exp \\left( - t^2 \\right)$\ndom::Tuple{<:Real,<:Real} (-infty infty)\nsymmetric::Bool true\nmuParameter::Real mu  -05","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#genLaguerreMeasure","page":"Type Hierarchy","title":"genLaguerreMeasure","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Field Meaning\nw::Function t^alphaexp(-t)\ndom::Tuple{<:Real,<:Real} (0infty)\nsymmetric::Bool false\nshapeParameter::Bool alpha-1","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#AbstractOrthoPoly","page":"Type Hierarchy","title":"AbstractOrthoPoly","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Orthogonal polynomials are at the heart of PolyChaos. The type tree for AbstractOrthoPoly looks as follows","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"julia> print_tree(AbstractOrthoPoly)\nAbstractOrthoPoly\n├─ AbstractCanonicalOrthoPoly\n│  ├─ Beta01OrthoPoly\n│  ├─ GammaOrthoPoly\n│  ├─ GaussOrthoPoly\n│  ├─ HermiteOrthoPoly\n│  ├─ JacobiOrthoPoly\n│  ├─ LaguerreOrthoPoly\n│  ├─ LegendreOrthoPoly\n│  ├─ LogisticOrthoPoly\n│  ├─ MeixnerPollaczekOrthoPoly\n│  ├─ Uniform01OrthoPoly\n│  ├─ Uniform_11OrthoPoly\n│  ├─ genHermiteOrthoPoly\n│  └─ genLaguerreOrthoPoly\n├─ MultiOrthoPoly\n└─ OrthoPoly","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"It mirrors the type tree from AbstractMeasure: there is a generica (univariate) type OrthoPoly, a multivariate extension MultiOrthoPoly for product measures, and several univariate canonical orthogonal polynomials.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#OrthoPoly","page":"Type Hierarchy","title":"OrthoPoly","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Given an absolutely continuous measure we are wondering what are the monic polynomials phi_i Omega rightarrow mathbbR that are orthogonal relative to this very measure? Mathematically this reads","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"langle phi_i phi_j rangle = int_Omega phi_i(t) phi_j(t) w(t) mathrmdt =\nbegincases\n 0  i=j \n= 0  ineq j\nendcases","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"They can be constructed using the type OrthoPoly, which has the fields","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Name Meaning\nname::String Name\ndeg::Int Maximum degree\nα::Vector{<:Real} Vector of recurrence coefficients α\nβ::Vector{<:Real} Vector of recurrence coefficients β\nmeas::AbstractMeasure Underlying measure","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The purpose of name is obvious. The integer deg stands for the maxium degree of the polynomials. Rather than storing the polynomials phi_i themselves we store the recurrence coefficients α, β that characterize the system of orthogonal polynomials. These recurrence coefficients are the single most important piece of information for the orthogonal polynomials. For several common measures, there exist analytic formulae. These are built-in to PolyChaos and should be used whenever possible.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#MultiOrthoPoly","page":"Type Hierarchy","title":"MultiOrthoPoly","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Just as we did in the univariate case, we use ProductMeasure as a building block for multivariate orthogonal polynomials. The type MultiOrthoPoly combines product measures with the respective orthogonal polynomials and their quadrature rules. Its fields are","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Name Meaning\nname::Vector{String} Vector of names\ndeg::Int Maximum degree\ndim::Int Dimension\nind::Matrix{<:Int} Array of multi-indices\nmeasure::ProductMeasure Underlying product measure","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The names of the univariate bases are stored in names; the maximum degree of the basis is deg; the overall dimension of the multivariate basis is dim; the multi-index ind maps the j-th multivariate basis to the elements of the univariate bases; the product measure is stored in meas; finally, all univariate bases are collected in uni.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#AbstractCanonicalOrthoPoly","page":"Type Hierarchy","title":"AbstractCanonicalOrthoPoly","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"These are the bread-and-butter polynomials: polynomials for which we know analytic formulae for the recursion coefficients. The following canonical orthogonal polynomials are implemented","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"julia> print_tree(AbstractCanonicalOrthoPoly)\nAbstractCanonicalOrthoPoly\n├─ Beta01OrthoPoly\n├─ GammaOrthoPoly\n├─ GaussOrthoPoly\n├─ HermiteOrthoPoly\n├─ JacobiOrthoPoly\n├─ LaguerreOrthoPoly\n├─ LegendreOrthoPoly\n├─ LogisticOrthoPoly\n├─ MeixnerPollaczekOrthoPoly\n├─ Uniform01OrthoPoly\n├─ Uniform_11OrthoPoly\n├─ genHermiteOrthoPoly\n└─ genLaguerreOrthoPoly","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Their fields follow","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Name Meaning\ndeg::Int Maximum degree\nα::Vector{<:Real} Vector of recurrence coefficients\nβ::Vector{<:Real} Vector of recurrence coefficients\nmeasure::CanonicalMeasure Underlying canonical measure\nquad::AbstractQuad Quadrature rule","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Quad","page":"Type Hierarchy","title":"Quad","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Quadrature rules are intricately related to orthogonal polynomials. An n-point quadrature rule is a pair of so-called nodes t_k and weights w_k for k=1dotsn that allow to solve integrals relative to the measure","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"int_Omega f(t) w(t) mathrmd t approx sum_k=1^n w_k f(t_k)","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"If the integrand f is polynomial, then the specific Gauss quadrature rules possess the remarkable property that an n-point quadrature rule can integrate polynomial integrands f of degree at most 2n-1 exactly; no approximation error is made.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The fields of Quad are","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Name Meaning\nname::String Name\nNquad::Int Number n of quadrature points\nnodes::Vector{<:Real} Nodes\nweights::Vector{<:Real} Weights","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"with obvious meanings.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"PolyChaos provides the type EmptyQuad that is added in case no quadrature rule is desired.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/#Tensor","page":"Type Hierarchy","title":"Tensor","text":"","category":"section"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The last type we need to address is Tensor. It is used to store the results of scalar products. Its fields are","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"Name Meaning\ndim: Dimension m of tensor langle phi_i_1 phi_i_2 cdots phi_i_m-1 phi_i_m rangle\nT::SparseVector{Float64,Int} Entries of tensor\nget::Function Function to get entries from T\nop::AbstractOrthoPoly Underlying univariate orthogonal polynomials","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"The dimension m of the tensor is the number of terms that appear in the scalar product. Let's assume we set m = 3, hence have langle phi_i phi_j phi_k rangle, then the concrete entry is obtained as Tensor.get([i,j,k]).","category":"page"},{"location":"modules/PolyChaos/type_hierarchy/","page":"Type Hierarchy","title":"Type Hierarchy","text":"This tutorial shows the above in action.","category":"page"},{"location":"modules/DiffEqDocs/features/io/#io","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"The ability to save and load solutions is important for handling large datasets and analyzing the results over multiple Julia sessions. This page explains the existing functionality for doing so.","category":"page"},{"location":"modules/DiffEqDocs/features/io/#Tabular-Data:-IterableTables","page":"I/O: Saving and Loading Solution Data","title":"Tabular Data: IterableTables","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"An interface to IterableTables.jl is provided. This IterableTables link allows you to use a solution type as the data source to convert to other tabular data formats. For example, let's solve a 4x2 system of ODEs and get the DataFrame:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq, DataFrames\nf_2dlinear = (du,u,p,t) -> du.=1.01u;\nprob = ODEProblem(f_2dlinear,rand(2,2),(0.0,1.0));\nsol1 =solve(prob,Euler();dt=1//2^(4));\ndf = DataFrame(sol1)","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"│ Row │ timestamp │ value 1  │ value 2  │ value 3  │ value 4  │\n├─────┼───────────┼──────────┼──────────┼──────────┼──────────┤\n│ 1   │ 0.0       │ 0.110435 │ 0.569561 │ 0.918336 │ 0.508044 │\n│ 2   │ 0.0625    │ 0.117406 │ 0.605515 │ 0.976306 │ 0.540114 │\n│ 3   │ 0.125     │ 0.124817 │ 0.643738 │ 1.03794  │ 0.574208 │\n│ 4   │ 0.1875    │ 0.132696 │ 0.684374 │ 1.10345  │ 0.610455 │\n│ 5   │ 0.25      │ 0.141073 │ 0.727575 │ 1.17311  │ 0.64899  │\n│ 6   │ 0.3125    │ 0.149978 │ 0.773503 │ 1.24716  │ 0.689958 │\n│ 7   │ 0.375     │ 0.159445 │ 0.822331 │ 1.32589  │ 0.733511 │\n│ 8   │ 0.4375    │ 0.16951  │ 0.87424  │ 1.40959  │ 0.779814 │\n│ 9   │ 0.5       │ 0.18021  │ 0.929427 │ 1.49857  │ 0.82904  │\n│ 10  │ 0.5625    │ 0.191586 │ 0.988097 │ 1.59316  │ 0.881373 │\n│ 11  │ 0.625     │ 0.20368  │ 1.05047  │ 1.69373  │ 0.93701  │\n│ 12  │ 0.6875    │ 0.216537 │ 1.11678  │ 1.80065  │ 0.996159 │\n│ 13  │ 0.75      │ 0.230206 │ 1.18728  │ 1.91432  │ 1.05904  │\n│ 14  │ 0.8125    │ 0.244738 │ 1.26222  │ 2.03516  │ 1.12589  │\n│ 15  │ 0.875     │ 0.260187 │ 1.3419   │ 2.16363  │ 1.19697  │\n│ 16  │ 0.9375    │ 0.276611 │ 1.42661  │ 2.30021  │ 1.27252  │\n│ 17  │ 1.0       │ 0.294072 │ 1.51667  │ 2.44541  │ 1.35285  │","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"If we set syms in the DiffEqFunction, then those names will be used:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"f = ODEFunction(f_2dlinear,syms=[:a,:b,:c,:d])\nprob = ODEProblem(f,rand(2,2),(0.0,1.0));\nsol1 =solve(prob,Euler();dt=1//2^(4));\ndf = DataFrame(sol1)","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"17×5 DataFrame\n│ Row │ timestamp │ a        │ b        │ c       │ d          │\n│     │ Float64   │ Float64  │ Float64  │ Float64 │ Float64    │\n├─────┼───────────┼──────────┼──────────┼─────────┼────────────┤\n│ 1   │ 0.0       │ 0.203202 │ 0.348326 │ 0.58971 │ 0.00606127 │\n⋮\n│ 16  │ 0.9375    │ 0.508972 │ 0.87247  │ 1.47708 │ 0.015182   │\n│ 17  │ 1.0       │ 0.541101 │ 0.927544 │ 1.57032 │ 0.0161403  │","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"Many modeling frameworks will automatically set syms for this feature. Additionally, this data can be saved to a CSV:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using CSV\nCSV.write(\"out.csv\",df)","category":"page"},{"location":"modules/DiffEqDocs/features/io/#JLD2-and-BSON.jl","page":"I/O: Saving and Loading Solution Data","title":"JLD2 and BSON.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"JLD2.jl and BSON.jl will work with the full solution type if you bring the required functions back into scope before loading. For example, if we save the solution:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq, JLD2\nf(u,p,t) = 1.01*u\nu0=1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob,Tsit5(),reltol=1e-8,abstol=1e-8)\n@save \"out.jld2\" sol","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"then we can get the full solution type back, interpolations and all, if we load the dependent functions first:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using JLD2\nusing OrdinaryDiffEq\nf(u,p,t) = 1.01*u\nJLD2.@load \"out.jld2\" sol","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"The example with BSON.jl is:","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq\nf_2dlinear = (du,u,p,t) -> du.=1.01u\nprob = ODEProblem(f_2dlinear,rand(2,2),(0.0,1.0))\nsol1 =solve(prob,Euler();dt=1//2^(4))\n\nusing BSON\nbson(\"test.bson\",Dict(:sol1=>sol1))\n\n# New session\nusing OrdinaryDiffEq, LinearAlgebra\nusing BSON\nBSON.load(\"test.bson\")","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"If you load it without the DE function then for some algorithms the interpolation may not work, and for all algorithms you'll need at least a solver package or SciMLBase.jl in scope in order for the solution interface (plot recipes, array indexing, etc.) to work. If none of these are put into scope, the solution type will still load and hold all of the values (so sol.u and sol.t will work), but none of the interface will be available.","category":"page"},{"location":"modules/DiffEqDocs/features/io/#JLD","page":"I/O: Saving and Loading Solution Data","title":"JLD","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"Don't use JLD. It's dead. Julia types can be saved via JLD.jl. However, they cannot save types which have functions, which means that the solution type is currently not compatible with JLD.","category":"page"},{"location":"modules/DiffEqDocs/features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using JLD\nJLD.save(\"out.jld\",\"sol\",sol)","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Derivative-Operators","page":"Derivative Operators","title":"Derivative Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"As shown in the figure, the operators act on a set of samples f_j = f(x_j) for a function f at a grid of points x_j. The grid has n interior points at x_j = jh for j = 1 to n, and 2 boundary points at x_0 = 0 and x_{n+1} = (n+1) h. The input to the numerical operators is a vector u = [f_1, f_2, …, f_N], and they output a vector of sampled derivatives du ≈ [f'(x_1), f'(x_2), …, f'(x_N)], or a higher-order derivative as requested.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"A numerical derivative operator D of order m can be constructed for this grid with D = CenteredDifference(1, m, h, n). The argument 1 indicates that this is the first derivative. Order m means that the operator is exact up to rounding when f is a polynomial of degree m or lower.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The derivative operator D is used along with a boundary condition operator Q to compute derivatives at the interior points of the grid. A simple boundary condition f(x_0) = f(x_n+1) = 0 is constructed with Q = Dirichlet0BC(eltype(u)).","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Given these definitions, the derivatives are calculated as if the operators D and Q were matrices, du = D*Q*u. This is an abuse of notation! The particular Q in this example is a linear operator but, in general, boundary conditions are affine operators. They have the form Q(x) = M*x + c, where M is a matrix and c is a constant vector. As a consequence, Q cannot be concretized to a matrix.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"(Image: Actions of DiffEqOperators on interior points and ghost points)","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The operator D works by interpolating a polynomial of degree m through m+1 adjacent points on the grid. Near the middle of the grid, the derivative is approximated at x_j by interpolating a polynomial of order m with x_j at its centre. To define an order-m polynomial, values are required at m+1 points. When x_j is too close to the boundary for that to fit, the polynomial is interpolated through the leftmost or rightmost m+1 points, including two “ghost” points that Q appends on the boundaries. The numerical derivatives are linear combinations of the values through which the polynomials are interpolated. The vectors of the coefficients in these linear combinations are called “stencils”. Because D takes values at the ghost points and returns values at the interior points, it is an n×(n+2) matrix.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The boundary condition operator Q acts as an (n+2)×n matrix. The output Q*u is a vector of values on the n interior and the 2 boundary points, [a, f(x_1), …, f(x_N), b]. The interior points take the values of u. The values a and b are samples at “ghost” points on the grid boundaries. As shown, these values are assigned so that an interpolated polynomial P(x) satisfies the left hand boundary condition, and Q(x) satisfies the right-hand boundary condition. The boundary conditions provided by the library are precisely those for which the values a and b are affine functions of the interior values f_j, so that Q is an affine operator.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Higher-dimensions","page":"Derivative Operators","title":"Higher dimensions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"In one dimension, u is naturally stored as a Vector, and the derivative and boundary condition operators are similar to matrices.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"In two dimensions, the values f(x_j) are naturally stored as a matrix. Taking derivatives along the downwards axis is easy, because matrices act columnwise. Horizontal derivatives can be taken by transposing the matrices. The derivative along the rightward axis is (D*F')' = F*D'. This is easy to code, but less easy to read for those who haven't seen it before.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"When a function has three or more arguments, its values are naturally stored in a higher-dimensional array. Julia's multiplication operator is only defined for Vector and Matrix, so applying an operator matrix to these arrays would require a complicated and error prone series of reshape and axis permutation functions.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Therefore the types of derivative and boundary condition operators are parameterised by the axis along which the operator acts. With derivative operators, the axis is supplied as a type parameter. The simple case CenteredDifference(…) is equivalent to CenteredDifference{1}(…), row-wise derivatives are taken by CenteredDifference{2}(…), sheet-wise by CenteredDifference{3}(…), and along the Nth axis by CenteredDifference{N}(…).","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Boundary conditions are more complicated. See @doc MultiDimBC for how they are supposed to work in multiple dimensions. They don't currently work that way.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Constructors","page":"Derivative Operators","title":"Constructors","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The constructors are as follows:","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"CenteredDifference{N}(derivative_order::Int,\n                      approximation_order::Int, dx,\n                      len::Int, coeff_func=nothing)\n\nUpwindDifference{N}(derivative_order::Int,\n                    approximation_order::Int, dx,\n                    len::Int, coeff_func=nothing; offside::Int=0)","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The arguments are:","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"N: The directional dimension of the discretization. If N is not given, it is assumed to be 1, i.e., differencing occurs along columns.\nderivative_order: the order of the derivative to discretize.\napproximation_order: the order of the discretization in terms of O(dx^order).\ndx: the spacing of the discretization. If dx is a Number, the operator is a uniform discretization. If dx is an array, then the operator is a non-uniform discretization. Its type needs to match the one from the Array to be differentiated.\nlen: the length of the discretization in the direction of the operator.\ncoeff_func: An operational argument for a coefficient function f(du,u,p,t) which sets the coefficients of the operator. If coeff_func is a Number, then the coefficients are set to be constant with that number. If coeff_func is an AbstractArray with length matching len, then the coefficients are constant but spatially dependent.\noffside: A keyword argument for UpwindDifference which sets the number of offside points against the primary wind direction allowing it to have some bias/offset. Number of points used for approximation remain same. By default its 0.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"N-dimensional derivative operators need to act against a value of at least N dimensions.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Derivative-Operator-Actions","page":"Derivative Operators","title":"Derivative Operator Actions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"These operators are lazy, meaning the memory is not allocated. Similarly, the operator actions * can be performed without ever building the operator matrices. Additionally, mul!(y,L,x) can be performed for non-allocating applications of the operator.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Concretizations","page":"Derivative Operators","title":"Concretizations","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The following concretizations are provided:","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Array\nSparseMatrixCSC\nBandedMatrix\nBlockBandedMatrix","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Additionally, the function sparse is overloaded to give the most efficient matrix type for a given operator. For one-dimensional derivatives this is a BandedMatrix, while for higher-dimensional operators this is a BlockBandedMatrix. The concretizations are made to act on vec(u).","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"A contraction operator concretizes to an ordinary matrix, no matter which dimension the contraction acts along, by doing the Kronecker product formulation. I.e., the action of the built matrix will match the action on vec(u).","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Boundary-Condition-Operators","page":"Derivative Operators","title":"Boundary Condition Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Boundary conditions are implemented through a ghost node approach. The discretized values u should be the interior of the domain so that, for the boundary value operator Q, Q*u is the discretization on the closure of the domain. By using it like this, L*Q*u is the NxN operator which satisfies the boundary conditions.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Periodic-Boundary-Conditions","page":"Derivative Operators","title":"Periodic Boundary Conditions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The constructor PeriodicBC provides the periodic boundary condition operator.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Robin-Boundary-Conditions","page":"Derivative Operators","title":"Robin Boundary Conditions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The variables in l are [αl, βl, γl], and correspond to a BC of the form al*u(0) + bl*u'(0) = cl, and similarly r for the right boundary ar*u(N) + br*u'(N) = cl.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"RobinBC(l::AbstractArray{T}, r::AbstractArray{T}, dx::AbstractArray{T}, order = one(T))","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Additionally, the following helpers exist for the Neumann u'(0) = α and Dirichlet u(0) = α cases.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"DirichletBC(αl::T, αr::T)\nDirichlet0BC(T::Type) = DirichletBC(zero(T), zero(T))","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"This fixes u = αl at the first point of the grid, and u = αr at the last point.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Neumann0BC(dx::Union{AbstractVector{T}, T}, order = 1)\nNeumannBC(α::AbstractVector{T}, dx::AbstractVector{T}, order = 1)","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#General-Boundary-Conditions","page":"Derivative Operators","title":"General Boundary Conditions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Implements a generalization of the Robin boundary condition, where α is a vector of coefficients. Represents a condition of the form α[1] + α[2]u[0] + α[3]u'[0] + α[4]u''[0]+... = 0","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"GeneralBC(αl::AbstractArray{T}, αr::AbstractArray{T}, dx::AbstractArray{T}, order = 1)","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Operator-Actions","page":"Derivative Operators","title":"Operator Actions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The boundary condition operators act lazily by appending the appropriate values to the end of the array, building the ghost-point extended version for the derivative operator to act on. This utilizes special array types to not require copying the interior data.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Concretizations-2","page":"Derivative Operators","title":"Concretizations","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The following concretizations are provided:","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Array\nSparseMatrixCSC","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Additionally, the function sparse is overloaded to give the most efficient matrix type for a given operator. For these operators it's SparseMatrixCSC. The concretizations are made to act on vec(u).","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#GhostDerivative-Operators","page":"Derivative Operators","title":"GhostDerivative Operators","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"When L is a DerivativeOperator and Q is a boundary condition operator, L*Q produces a GhostDerivative operator which is the composition of the two operations.","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/#Concretizations-3","page":"Derivative Operators","title":"Concretizations","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"The following concretizations are provided:","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Array\nSparseMatrixCSC\nBandedMatrix","category":"page"},{"location":"modules/DiffEqOperators/operators/derivative_operators/","page":"Derivative Operators","title":"Derivative Operators","text":"Additionally, the function sparse is overloaded to give the most efficient matrix type for a given operator. For these operators it's BandedMatrix unless the boundary conditions are PeriodicBC, in which case it's SparseMatrixCSC. The concretizations are made to act on vec(u).","category":"page"},{"location":"modules/NeuralPDE/examples/wave/#D-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Let's solve this 1-dimensional wave equation:","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginalign*\n^2_t u(x t) = c^2 ^2_x u(x t) quad  textsffor all  0  x  1 text and  t  0   \nu(0 t) = u(1 t) = 0 quad  textsffor all  t  0   \nu(x 0) = x (1-x)     quad  textsffor all  0  x  1   \n_t u(x 0) = 0       quad  textsffor all  0  x  1   \nendalign*","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.1 and physics-informed neural networks.","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Further, the solution of this equation with the given boundary conditions is presented.","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters t, x\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC=1\neq  = Dtt(u(t,x)) ~ C^2*Dxx(u(t,x))\n\n# Initial and boundary conditions\nbcs = [u(t,0) ~ 0.,# for all t > 0\n       u(t,1) ~ 0.,# for all t > 0\n       u(0,x) ~ x*(1. - x), #for all 0 < x < 1\n       Dt(u(0,x)) ~ 0. ] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n# Discretization\ndx = 0.1\n\n# Neural network\nchain = Lux.Chain(Dense(2,16,Lux.σ),Dense(16,16,Lux.σ),Dense(16,1))\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))\n\n@named pde_system = PDESystem(eq,bcs,domains,[t,x],[u(t,x)])\nprob = discretize(pde_system,discretization)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# optimizer\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob,opt; callback = callback, maxiters=1200)\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution in order to plot the relative error.","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using Plots\n\nts,xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nanalytic_sol_func(t,x) =  sum([(8/(k^3*pi^3)) * sin(k*pi*x)*cos(C*k*pi*t) for k in 1:2:50000])\n\nu_predict = reshape([first(phi([t,x],res.u)) for t in ts for x in xs],(length(ts),length(xs)))\nu_real = reshape([analytic_sol_func(t,x) for t in ts for x in xs], (length(ts),length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype=:contourf,title = \"analytic\");\np2 =plot(ts, xs, u_predict, linetype=:contourf,title = \"predict\");\np3 = plot(ts, xs, diff_u,linetype=:contourf,title = \"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: waveplot)","category":"page"},{"location":"modules/NeuralPDE/examples/wave/#D-Damped-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Damped Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Now let's solve the 1-dimensional wave equation with damping.","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginaligned\nfracpartial^2 u(tx)partial x^2 = frac1c^2 fracpartial^2 u(tx)partial t^2 + v fracpartial u(tx)partial t \nu(t 0) = u(t L) = 0 \nu(0 x) = x(1-x) \nu_t(0 x) = 1 - 2x \nendaligned","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.05 and physics-informed neural networks. Here we take advantage of adaptive derivative to increase accuracy.","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Plots, Printf\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..) Dxu(..) Dtu(..) O1(..) O2(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDx = Differential(x)\nDt = Differential(t)\n\n# Constants\nv = 3\nb = 2\nL = 1.0\n@assert b > 0 && b < 2π / (L * v)\n\n# 1D damped wave\neq = Dx(Dxu(t, x)) ~ 1 / v^2 * Dt(Dtu(t, x)) + b * Dtu(t, x)\n\n# Initial and boundary conditions\nbcs_ = [u(t, 0) ~ 0.,# for all t > 0\n       u(t, L) ~ 0.,# for all t > 0\n       u(0, x) ~ x * (1. - x), # for all 0 < x < 1\n       Dtu(0, x) ~ 1 - 2x # for all  0 < x < 1\n       ]\n\nep = (cbrt(eps(eltype(Float64))))^2 / 6\n\nder = [Dxu(t, x) ~ Dx(u(t, x)) + ep * O1(t, x),\n       Dtu(t, x) ~  Dt(u(t, x)) + ep * O2(t, x)]\n\nbcs = [bcs_;der]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, L),\n           x ∈ Interval(0.0, L)]\n\n# Neural network\ninn = 25\ninnd = 4\nchain = [[Lux.Chain(Dense(2, inn, Lux.tanh),\n                    Dense(inn, inn, Lux.tanh),\n                    Dense(inn, inn, Lux.tanh),\n                    Dense(inn, 1)) for _ in 1:3];\n         [Lux.Chain(Dense(2, innd, Lux.tanh), Dense(innd, 1)) for _ in 1:2];]\n\nstrategy = GridTraining(0.02)\ndiscretization = PhysicsInformedNN(chain, strategy;)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x), Dxu(t, x), Dtu(t, x), O1(t, x), O2(t, x)])\nprob = discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS();callback = callback, maxiters=2000)\nprob = remake(prob, u0=res.u)\nres = Optimization.solve(prob, BFGS();callback = callback, maxiters=2000)\n\nphi = discretization.phi[1]\n\n# Analysis\nts, xs = [infimum(d.domain):0.05:supremum(d.domain) for d in domains]\n\nμ_n(k) = (v * sqrt(4 * k^2 * π^2 - b^2 * L^2 * v^2)) / (2 * L)\nb_n(k) = 2 / L * -(L^2 * ((2 * π * L - π) * k * sin(π * k) + ((π^2 - π^2 * L) * k^2 + 2 * L) * cos(π * k) - 2 * L)) / (π^3 * k^3) # vegas((x, ϕ) -> ϕ[1] = sin(k * π * x[1]) * f(x[1])).integral[1]\na_n(k) = 2 / -(L * μ_n(k)) * (L * (((2 * π * L^2 - π * L) * b * k * sin(π * k) + ((π^2 * L - π^2 * L^2) * b * k^2 + 2 * L^2 * b) * cos(π * k) - 2 * L^2 * b) * v^2 + 4 * π * L * k * sin(π * k) + (2 * π^2 - 4 * π^2 * L) * k^2 * cos(π * k) - 2 * π^2 * k^2)) / (2 * π^3 * k^3)\n\n# Plot\nanalytic_sol_func(t,x) = sum([sin((k * π * x) / L) * exp(-v^2 * b * t / 2) * (a_n(k) * sin(μ_n(k) * t) + b_n(k) * cos(μ_n(k) * t)) for k in 1:2:100]) # TODO replace 10 with 500\nanim = @animate for t ∈ ts\n    @info \"Time $t...\"\n    sol =  [analytic_sol_func(t, x) for x in xs]\n    sol_p =  [first(phi([t,x], res.u.depvar.u)) for x in xs]\n    plot(sol, label=\"analytic\", ylims=[0, 0.1])\n    title = @sprintf(\"t = %.3f\", t)\n    plot!(sol_p, label=\"predict\", ylims=[0, 0.1], title=title)\nend\ngif(anim, \"1Dwave_damped_adaptive.gif\", fps=200)\n\n# Surface plot\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_predict = reshape([first(phi([t,x], res.u.depvar.u)) for \n                        t in ts for x in xs], (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs], (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype=:contourf, title=\"analytic\");\np2 = plot(ts, xs, u_predict, linetype=:contourf, title=\"predict\");\np3 = plot(ts, xs, diff_u, linetype=:contourf, title=\"error\");\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can see the results here:","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: Damped_wave_sol_adaptive_u)","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Plotted as a line one can see the analytical solution and the prediction here:","category":"page"},{"location":"modules/NeuralPDE/examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: 1Dwave_damped_adaptive)","category":"page"},{"location":"modules/PoissonRandom/#PoissonRandom.jl:-Fast-Poisson-Random-Numbers","page":"Home","title":"PoissonRandom.jl: Fast Poisson Random Numbers","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"PoissonRandom.jl is a component of the SciML ecosystem which allows for fast generation of Poisson random numbers.","category":"page"},{"location":"modules/PoissonRandom/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"To install ParameterizedFunctions.jl, use the Julia package manager:","category":"page"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"PoissonRandom\")","category":"page"},{"location":"modules/PoissonRandom/#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"# Simple Poisson random\npois_rand(λ)\n\n# Using another RNG\nusing RandomNumbers\nrng = Xorshifts.Xoroshiro128Plus()\npois_rand(rng,λ)","category":"page"},{"location":"modules/PoissonRandom/#Implementation","page":"Home","title":"Implementation","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"It mixes two methods. A simple count method and a method from a normal approximation. See this blog post for details.","category":"page"},{"location":"modules/PoissonRandom/#Benchmark","page":"Home","title":"Benchmark","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"using RandomNumbers, Distributions, BenchmarkTools, StaticArrays,\n      RecursiveArrayTools, Plots, PoissonRandom\nlabels = [\"count_rand\",\"ad_rand\",\"pois_rand\",\"Distributions.jl\"]\nrng = Xorshifts.Xoroshiro128Plus()\n\nfunction n_count(rng,λ,n)\n  tmp = 0\n  for i in 1:n\n    tmp += PoissonRandom.count_rand(rng,λ)\n  end\nend\n\nfunction n_pois(rng,λ,n)\n  tmp = 0\n  for i in 1:n\n    tmp += pois_rand(rng,λ)\n  end\nend\n\nfunction n_ad(rng,λ,n)\n  tmp = 0\n  for i in 1:n\n    tmp += PoissonRandom.ad_rand(rng,λ)\n  end\nend\n\nfunction n_dist(λ,n)\n  tmp = 0\n  for i in 1:n\n    tmp += rand(Poisson(λ))\n  end\nend\n\nfunction time_λ(rng,λ,n)\n  t1 = @elapsed n_count(rng,λ,n)\n  t2 = @elapsed n_ad(rng,λ,n)\n  t3 = @elapsed n_pois(rng,λ,n)\n  t4 = @elapsed n_dist(λ,n)\n  @SArray [t1,t2,t3,t4]\nend\n\n# Compile\ntime_λ(rng,5,5000000)\n# Run with a bunch of λ\ntimes = VectorOfArray([time_λ(rng,n,5000000) for n in 1:20])'\nplot(times,labels = labels, lw = 3)","category":"page"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"(Image: benchmark result)","category":"page"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"So this package ends up about 30% or so faster than Distributions.jl (the method at the far edge is λ-independent so that goes on forever).","category":"page"},{"location":"modules/PoissonRandom/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/PoissonRandom/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/#Adding-a-new-package-to-the-common-interface","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"DiffEq's distributed infrastructure allows anyone to add new packages to the common interface. This set of the documentation explains how this is done. An example package is DASRK.jl whose full common interface bindings are contained in common.jl.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/#Defining-the-types","page":"Adding a new package to the common interface","title":"Defining the types","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"You should start by defining a common supertype for all your algorithm types. DASKR has DAE algorithms, so it defines","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"abstract type DASKRDAEAlgorithm{LinearSolver} <: DiffEqBase.AbstractDAEAlgorithm end","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"that its algorithms are all AbstractDAEAlgorithms and gives them a possible type parameter as well. Then the concrete algorithms are specified. Special options (i.e. non-common interface options) for the algorithm go in this type. Here there is a choice for a linear solver internally, so we allow the user to set this:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"struct daskr{LinearSolver} <: DASKRDAEAlgorithm{LinearSolver} end\ndaskr(;linear_solver=:Dense) = daskr{linear_solver}()","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"In many (most?) cases, no extra constructor is needed since there are no extra options.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/#Defining-the-overloads","page":"Adding a new package to the common interface","title":"Defining the overloads","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"Now you need to add DiffEqBase. The package should reexport DiffEqBase.jl to make using it alone act naturally, and this is done by:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"using Reexport\n@reexport using DiffEqBase","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"now we overload __solve from DiffEqBase.jl to act on our algorithm. Here's a possible signature:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"function DiffEqBase.__solve{uType,duType,tType,isinplace,LinearSolver}(\n    prob::AbstractDAEProblem{uType,duType,tType,isinplace},\n    alg::DASKRDAEAlgorithm{LinearSolver},\n    timeseries = [], ts = [], ks = [];\n\n    verbose=true,\n    callback = nothing, abstol = 1/10^6, reltol = 1/10^3,\n    saveat = Float64[], adaptive = true, maxiters = Int(1e5),\n    timeseries_errors = true, save_everystep = isempty(saveat), dense = save_everystep,\n    save_start = true, save_timeseries = nothing,\n    userdata = nothing,\n    kwargs...)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"Basically you just dispatch on your algorithm supertype (and refine the Problem choice as well so it errors on the wrong problem types), then list the common interface options. timeseries = [], ts = [], ks = []; are for pre-allocation of arrays, and this may change in the near future but you can use this to pre-allocate the t, u, and k arrays (k being internal steps if saved).","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/#Defining-the-solution","page":"Adding a new package to the common interface","title":"Defining the solution","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"In solve you do option handling and call your solver. At the end, you return the solution via:","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"build_solution(prob,alg,ts,timeseries,\n               du = dures,\n               dense = dense,\n               timeseries_errors = timeseries_errors,\n               retcode = :Success)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/adding_packages/","page":"Adding a new package to the common interface","title":"Adding a new package to the common interface","text":"Giving du is only currently allowed for DAEs and is optional. The errors flags (timeseries_errors and dense_errors) are flags for allowing the solution to calculate errors which should be passed through the solve function if applicable. A proper retcode should be placed or it will default to :Default.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#acausal","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"In this tutorial we will build a hierarchical acausal component-based model of the RC circuit. The RC circuit is a simple example where we connect a resistor and a capacitor. Kirchoff's laws are then applied to state equalities between currents and voltages. This specifies a differential-algebraic equation (DAE) system, where the algebraic equations are given by the constraints and equalities between different component variables. We then simplify this to an ODE by eliminating the equalities before solving. Let's see this in action.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"note: Note\nThis tutorial teaches how to build the entire RC circuit from scratch. However, to simulate electrical components with more ease, check out the ModelingToolkitStandardLibrary.jl which includes a  tutorial for simulating RC circuits with pre-built components","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#Copy-Paste-Example","page":"Acausal Component-Based Modeling the RC Circuit","title":"Copy-Paste Example","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"using ModelingToolkit, Plots, DifferentialEquations\n\n@variables t\n@connector function Pin(;name)\n    sts = @variables v(t)=1.0 i(t)=1.0 [connect = Flow]\n    ODESystem(Equation[], t, sts, []; name=name)\nend\n\nfunction Ground(;name)\n    @named g = Pin()\n    eqs = [g.v ~ 0]\n    compose(ODESystem(eqs, t, [], []; name=name), g)\nend\n\nfunction OnePort(;name)\n    @named p = Pin()\n    @named n = Pin()\n    sts = @variables v(t)=1.0 i(t)=1.0\n    eqs = [\n           v ~ p.v - n.v\n           0 ~ p.i + n.i\n           i ~ p.i\n          ]\n    compose(ODESystem(eqs, t, sts, []; name=name), p, n)\nend\n\nfunction Resistor(;name, R = 1.0)\n    @named oneport = OnePort()\n    @unpack v, i = oneport\n    ps = @parameters R=R\n    eqs = [\n           v ~ i * R\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend\n\nfunction Capacitor(;name, C = 1.0)\n    @named oneport = OnePort()\n    @unpack v, i = oneport\n    ps = @parameters C=C\n    D = Differential(t)\n    eqs = [\n           D(v) ~ i / C\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend\n\nfunction ConstantVoltage(;name, V = 1.0)\n    @named oneport = OnePort()\n    @unpack v = oneport\n    ps = @parameters V=V\n    eqs = [\n           V ~ v\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend\n\nR = 1.0\nC = 1.0\nV = 1.0\n@named resistor = Resistor(R=R)\n@named capacitor = Capacitor(C=C)\n@named source = ConstantVoltage(V=V)\n@named ground = Ground()\n\nrc_eqs = [\n          connect(source.p, resistor.p)\n          connect(resistor.n, capacitor.p)\n          connect(capacitor.n, source.n)\n          connect(capacitor.n, ground.g)\n         ]\n\n@named _rc_model = ODESystem(rc_eqs, t)\n@named rc_model = compose(_rc_model,\n                          [resistor, capacitor, source, ground])\nsys = structural_simplify(rc_model)\nu0 = [\n      capacitor.v => 0.0\n     ]\nprob = ODAEProblem(sys, u0, (0, 10.0))\nsol = solve(prob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#Explanation","page":"Acausal Component-Based Modeling the RC Circuit","title":"Explanation","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"We wish to build the following RC circuit by building individual components and connecting the pins:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"(Image: )","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#Building-the-Component-Library","page":"Acausal Component-Based Modeling the RC Circuit","title":"Building the Component Library","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"For each of our components we use a Julia function which emits an ODESystem. At the top we start with defining the fundamental qualities of an electrical circuit component. At every input and output pin a circuit component has two values: the current at the pin and the voltage. Thus we define the Pin component (connector) to simply be the values there. Whenever two Pins in a circuit are connected together, the system satisfies Kirchoff's laws, i.e. that currents sum to zero and voltages across the pins are equal. [connect = Flow] informs MTK that currents ought to sum to zero, and by default, variables are equal in a connection.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"@connector function Pin(;name)\n    sts = @variables v(t)=1.0 i(t)=1.0 [connect = Flow]\n    ODESystem(Equation[], t, sts, []; name=name)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Note that this is an incompletely specified ODESystem: it cannot be simulated on its own because the equations for v(t) and i(t) are unknown. Instead this just gives a common syntax for receiving this pair with some default values. Notice that in a component we define the name as a keyword argument: this is because later we will generate different Pin objects with different names to correspond to duplicates of this topology with unique variables. One can then construct a Pin like:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Pin(name=:mypin1)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"or equivalently using the @named helper macro:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"@named mypin1 = Pin()","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Next we build our ground node. A ground node is just a pin that is connected to a constant voltage reservoir, typically taken to be V=0. Thus to define this component, we generate an ODESystem with a Pin subcomponent and specify that the voltage in such a Pin is equal to zero. This gives:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"function Ground(;name)\n    @named g = Pin()\n    eqs = [g.v ~ 0]\n    compose(ODESystem(eqs, t, [], []; name=name), g)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Next we build a OnePort: an abstraction for all simple electrical component with two pins. The voltage difference between the positive pin and the negative pin is the voltage of the component, the current between two pins must sum to zero, and the current of the component equals to the current of the positive pin.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"function OnePort(;name)\n    @named p = Pin()\n    @named n = Pin()\n    sts = @variables v(t)=1.0 i(t)=1.0\n    eqs = [\n           v ~ p.v - n.v\n           0 ~ p.i + n.i\n           i ~ p.i\n          ]\n    compose(ODESystem(eqs, t, sts, []; name=name), p, n)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Next we build a resistor. A resistor is an object that has two Pins, the positive and the negative pins, and follows Ohm's law: v = i*r. The voltage of the resistor is given as the voltage difference across the two pins while by conservation of charge we know that the current in must equal the current out, which means (no matter the direction of the current flow) the sum of the currents must be zero. This leads to our resistor equations:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"function Resistor(;name, R = 1.0)\n    @named oneport = OnePort()\n    @unpack v, i = oneport\n    ps = @parameters R=R\n    eqs = [\n           v ~ i * R\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Notice that we have created this system with a default parameter R for the resistor's resistance. By doing so, if the resistance of this resistor is not overridden by a higher level default or overridden at ODEProblem construction time, this will be the value of the resistance. Also, note the use of @unpack and extend. For the Resistor, we want to simply inherit OnePort's equations and states and extend them with a new equation. ModelingToolkit makes a new namespaced variable oneport₊v(t) when using the syntax oneport.v, and we can use @unpack avoid the namespacing.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Using our knowledge of circuits we similarly construct the Capacitor:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"function Capacitor(;name, C = 1.0)\n    @named oneport = OnePort()\n    @unpack v, i = oneport\n    ps = @parameters C=C\n    D = Differential(t)\n    eqs = [\n           D(v) ~ i / C\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Now we want to build a constant voltage electrical source term. We can think of this as similarly being a two pin object, where the object itself is kept at a constant voltage, essentially generating the electrical current. We would then model this as:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"function ConstantVoltage(;name, V = 1.0)\n    @named oneport = OnePort()\n    @unpack v = oneport\n    ps = @parameters V=V\n    eqs = [\n           V ~ v\n          ]\n    extend(ODESystem(eqs, t, [], ps; name=name), oneport)\nend","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#Connecting-and-Simulating-Our-Electrical-Circuit","page":"Acausal Component-Based Modeling the RC Circuit","title":"Connecting and Simulating Our Electrical Circuit","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Now we are ready to simulate our circuit. Let's build our four components: a resistor, capacitor, source, and ground term. For simplicity we will make all of our parameter values 1. This is done by:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"R = 1.0\nC = 1.0\nV = 1.0\n@named resistor = Resistor(R=R)\n@named capacitor = Capacitor(C=C)\n@named source = ConstantVoltage(V=V)\n@named ground = Ground()","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Finally we will connect the pieces of our circuit together. Let's connect the positive pin of the resistor to the source, the negative pin of the resistor to the capacitor, and the negative pin of the capacitor to a junction between the source and the ground. This would mean our connection equations are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"rc_eqs = [\n          connect(source.p, resistor.p)\n          connect(resistor.n, capacitor.p)\n          connect(capacitor.n, source.n)\n          connect(capacitor.n, ground.g)\n         ]","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Finally we build our four component model with these connection rules:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"@named _rc_model = ODESystem(rc_eqs, t)\n@named rc_model = compose(_rc_model,\n                          [resistor, capacitor, source, ground])","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Note that we can also specify the subsystems in a vector. This model is acasual because we have not specified anything about the causality of the model. We have simply specified what is true about each of the variables. This forms a system of differential-algebraic equations (DAEs) which define the evolution of each state of the system. The equations are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"equations(expand_connections(rc_model))","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"the states are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"states(rc_model)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"and the parameters are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"parameters(rc_model)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/#Simplifying-and-Solving-this-System","page":"Acausal Component-Based Modeling the RC Circuit","title":"Simplifying and Solving this System","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"This system could be solved directly as a DAE using one of the DAE solvers from DifferentialEquations.jl. However, let's take a second to symbolically simplify the system before doing the solve. Although we can use ODE solvers that handles mass matrices to solve the above system directly, we want to run the structural_simplify function first, as it eliminates many unnecessary variables to build the leanest numerical representation of the system. Let's see what it does here:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"sys = structural_simplify(rc_model)\nequations(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"states(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"After structural simplification we are left with a system of only two equations with two state variables. One of the equations is a differential equation while the other is an algebraic equation. We can then give the values for the initial conditions of our states and solve the system by converting it to an ODEProblem in mass matrix form and solving it with an ODEProblem mass matrix DAE solver. This is done as follows:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"u0 = [\n      capacitor.v => 0.0\n      capacitor.p.i => 0.0\n     ]\nprob = ODEProblem(sys, u0, (0, 10.0))\nsol = solve(prob, Rodas4())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Since we have run structural_simplify, MTK can numerically solve all the unreduced algebraic equations numerically using the ODAEProblem (note the letter A):","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"u0 = [\n      capacitor.v => 0.0\n     ]\nprob = ODAEProblem(sys, u0, (0, 10.0))\nsol = solve(prob, Rodas4())\nplot(sol)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"Notice that this solves the whole system by only solving for one variable!","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"However, what if we wanted to plot the timeseries of a different variable? Do not worry, that information was not thrown away! Instead, transformations like structural_simplify simply change state variables into observed variables. Let's see what our observed variables are:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"observed(sys)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"These are explicit algebraic equations which can then be used to reconstruct the required variables on the fly. This leads to dramatic computational savings because implicitly solving an ODE scales like O(n^3), so making there be as few states as possible is good!","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"The solution object can be accessed via its symbols. For example, let's retrieve the voltage of the resistor over time:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"sol[resistor.v]","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"or we can plot the timeseries of the resistor's voltage:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/acausal_components/","page":"Acausal Component-Based Modeling the RC Circuit","title":"Acausal Component-Based Modeling the RC Circuit","text":"plot(sol, vars=[resistor.v])","category":"page"},{"location":"modules/LabelledArrays/LArrays/#LArrays","page":"LArrays","title":"LArrays","text":"","category":"section"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"LArrays are fully mutable arrays with labels. There is no performance loss by using labelled indexing instead of purely numerical indexing.  Using the macro with values and labels generates the labelled array with the given values:","category":"page"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"Users interested in using labelled elements in their arrays should also  consider ComponentArrays from the  ComponentArrays.jl library. ComponentArrays are well integrated into the SciML ecosystem. ","category":"page"},{"location":"modules/LabelledArrays/LArrays/#@LArray-and-@LVector-macros","page":"LArrays","title":"@LArray and @LVector macros","text":"","category":"section"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"Macro constructors are convenient for building most LArray objects. An  @LArray may be of arbitrary dimension while an @LVector is a  one dimensional array. ","category":"page"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"@LArray\n@LVector","category":"page"},{"location":"modules/LabelledArrays/LArrays/#LabelledArrays.@LArray","page":"LArrays","title":"LabelledArrays.@LArray","text":"@LArray Eltype Size Names\n@LArray Values Names\n\nThe @LArray macro creates an LArray with names determined from the Names vector and values determined from the Values vector. Otherwise, the eltype and size are used to make an LArray with undefined values.\n\nA = @LArray [1,2,3] (:a,:b,:c)\nA.a == 1\n\nUsers can also generate a labelled array with undefined values by instead giving the dimensions. This approach is useful if the user intends to pre-allocate an  array for some later input. \n\nA = @LArray Float64 (2,2) (:a,:b,:c,:d)\nW = rand(2,2)\nA .= W\nA.d == W[2,2]\n\nUsers may also use an alternative constructor to set the Names and Values and ranges at the same time.\n\njulia> z = @LArray [1.,2.,3.] (a = 1:2, b = 2:3);\njulia> z.b\n2-element view(::Array{Float64,1}, 2:3) with eltype Float64:\n 2.0\n 3.0\n\njulia> z = @LArray [1 2; 3 4] (a = (2, :), b = 2:3);\njulia> z.a\n2-element view(::Array{Int64,2}, 2, :) with eltype Int64:\n 3\n 4\n\nThe labels of LArray and SLArray can be accessed  by function symbols, which returns a tuple of symbols.\n\n\n\n\n\n","category":"macro"},{"location":"modules/LabelledArrays/LArrays/#LabelledArrays.@LVector","page":"LArrays","title":"LabelledArrays.@LVector","text":"@LVector Type Names\n\nThe @LVector macro creates an LArray of dimension 1 with eltype and undefined values. The vector's length is equal to the number of names given.\n\nAs with an LArray, the user can initialize the vector and set its values later.\n\nA = @LVector Float64 (:a,:b,:c,:d)\nA .= rand(4)\n\nOn the other hand, users can also initialize the vector and set its values at the  same time:\n\nb = @LVector [1,2,3] (:a,:b,:c)\n\n\n\n\n\n","category":"macro"},{"location":"modules/LabelledArrays/LArrays/#LArray-and-LVector-constructors","page":"LArrays","title":"LArray and LVector constructors","text":"","category":"section"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"The original constructors for LArrays and LVectors are as  follows: ","category":"page"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"LArray\nLVector","category":"page"},{"location":"modules/LabelledArrays/LArrays/#LabelledArrays.LArray","page":"LArrays","title":"LabelledArrays.LArray","text":"LArray(::Tuple, ::NamedTuple)\nLArray(::Tuple, kwargs)\n\nThe standard constructors for LArray.\n\nFor example:\n\nLArray((2,2), (a=1, b=2, c=3, d=4))  # need to specify size\nLArray((2,2); a=1, b=2, c=3, d=4)\n\n\n\n\n\nLVector(v1::Union{SLArray,LArray}; kwargs...)\n\nCreates a copy of v1 with corresponding items in kwargs replaced.\n\nFor example:\n\nABCD = @SLArray (2,2) (:a,:b,:c,:d);\nB = ABCD(1,2,3,4);\nB2 = LArray(B; c=30 )\n\n\n\n\n\n","category":"type"},{"location":"modules/LabelledArrays/LArrays/#LabelledArrays.LVector","page":"LArrays","title":"LabelledArrays.LVector","text":"LVector(::NamedTuple)\nLVector(kwargs)\n\nThe standard constructor for LVector.\n\nFor example:\n\nLVector((a=1, b=2))\nLVector(a=1, b=2)\n\n\n\n\n\nLVector(v1::Union{SLArray,LArray}; kwargs...)\n\nCreates a 1D copy of v1 with corresponding items in kwargs replaced.\n\nFor example:\n\nz = LVector(a=1, b=2, c=3);\nz2 = LVector(z; c=30)\n\n\n\n\n\n","category":"function"},{"location":"modules/LabelledArrays/LArrays/#Manipulating-LArrays-and-LVectors","page":"LArrays","title":"Manipulating LArrays and LVectors","text":"","category":"section"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"User may want a list of the labels or keys in an LArray or LVector. The symbols(::LArray) function returns a tuple of array labels.","category":"page"},{"location":"modules/LabelledArrays/LArrays/","page":"LArrays","title":"LArrays","text":"symbols","category":"page"},{"location":"modules/LabelledArrays/LArrays/#LabelledArrays.symbols","page":"LArrays","title":"LabelledArrays.symbols","text":"symbols(::SLArray)\n\nReturns the labels of the SLArray .\n\nFor example:\n\njulia> z = SLVector(a=1, b=2, c=3)\n3-element SLArray{Tuple{3}, Int64, 1, 3, (:a, :b, :c)} with indices SOneTo(3):\n :a => 1\n :b => 2\n :c => 3\n\njulia> symbols(z)\n(:a, :b, :c)\n\n\n\n\n\nsymbols(::LArray)\n\nReturns the labels of the LArray .\n\nFor example:\n\njulia julia> z = @LVector Float64 (:a, :b, :c, :d); julia> symbols(z) (:a, :b, :c, :d)`\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Dynamical,-Hamiltonian,-and-2nd-Order-ODE-Solvers","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Dynamical ODEs, such as those arising from Hamiltonians or second order ordinary differential equations, give rise to a special structure that can be specialized on in the solver for more efficiency. These algorithms require an ODE defined in the following ways:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"DynamicalODEProblem{isinplace}(f1,f2,v0,u0,tspan,p=NullParameters();kwargs...)\nSecondOrderODEProblem{isinplace}(f,du0,u0,tspan,p=NullParameters();kwargs...)\nHamiltonianProblem{T}(H,p0,q0,tspan,p=NullParameters();kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"These correspond to partitioned equations of motion:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"fracdvdt = f_1(tu) \nfracdudt = f_2(v) ","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"The functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of t and u. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent but the kinetic energy is only dependent on v.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Note that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed below make note of this requirement with \"Requires quadratic kinetic energy\".","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Recommendations","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Recommendations","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"When energy conservation is required, use a symplectic method. Otherwise the Runge-Kutta-Nyström methods will be more efficient. Energy is mostly conserved by Runge-Kutta-Nyström methods, but is not conserved for long time integrations. Thus it is suggested that for shorter integrations you use Runge-Kutta-Nyström methods as well.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"As a go-to method for efficiency, DPRKN6 is a good choice. DPRKN12 is a good choice when high accuracy, like tol<1e-10 is necessary. However, DPRKN6 is the only Runge-Kutta-Nyström method with a higher order interpolant (all default to order 3 Hermite, whereas DPRKN6 is order 6th interpolant) and thus in cases where interpolation matters (ex: event handling) one should use DPRKN6. For very smooth problems with expensive acceleration function evaluations, IRKN4 can be a good choice as it minimizes the number of evaluations.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"For symplectic methods, higher order algorithms are the most efficient when higher accuracy is needed, and when less accuracy is needed lower order methods do better. Optimized efficiency methods take more steps and thus have more force calculations for the same order, but have smaller error. Thus the \"optimized efficiency\" algorithms are recommended if your force calculation is not too sufficiency large, while the other methods are recommend when force calculations are really large (for example, like in MD simulations VelocityVerlet is very popular since it only requires one force calculation per timestep). A good go-to method would be McAte5, and a good high order choice is KahanLi8.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Standard-ODE-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Standard ODE Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"The standard ODE integrators will work on Dynamical ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Runge-Kutta-Nyström-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Runge-Kutta-Nyström Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Nystrom4: 4th order explicit Runge-Kutta-Nyström method. Allows acceleration to depend on velocity. Fixed timestep only.\nIRKN3: 4th order explicit two-step Runge-Kutta-Nyström method. Fixed timestep only.\nIRKN4: 4th order explicit two-step Runge-Kutta-Nyström method. Can be more efficient for smooth problems. Fixed timestep only.\nERKN4: 4th order Runge-Kutta-Nyström method which is integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nERKN5: 5th order Runge-Kutta-Nyström method which is integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nNystrom4VelocityIndependent: 4th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nNystrom5VelocityIndependent: 5th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nDPRKN6: 6th order explicit adaptive Runge-Kutta-Nyström method. Free 6th order interpolant.\nDPRKN8: 8th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN12: 12th order explicit adaptive Runge-Kutta-Nyström method.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#Symplectic-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Symplectic Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Note that all symplectic integrators are fixed timestep only.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"SymplecticEuler: First order explicit symplectic integrator\nVelocityVerlet: 2nd order explicit symplectic integrator. Requires f_2(t,u) = v, i.e. a second order ODE.\nVerletLeapfrog: 2nd order explicit symplectic integrator.\nPseudoVerletLeapfrog: 2nd order explicit symplectic integrator.\nMcAte2: Optimized efficiency 2nd order explicit symplectic integrator.\nRuth3: 3rd order explicit symplectic integrator.\nMcAte3: Optimized efficiency 3rd order explicit symplectic integrator.\nCandyRoz4: 4th order explicit symplectic integrator.\nMcAte4: 4th order explicit symplectic integrator. Requires quadratic kinetic energy.\nCalvoSanz4: Optimized efficiency 4th order explicit symplectic integrator.\nMcAte42: 4th order explicit symplectic integrator. (Broken)\nMcAte5: Optimized efficiency 5th order explicit symplectic integrator. Requires quadratic kinetic energy\nYoshida6: 6th order explicit symplectic integrator.\nKahanLi6: Optimized efficiency 6th order explicit symplectic integrator.\nMcAte8: 8th order explicit symplectic integrator.\nKahanLi8: Optimized efficiency 8th order explicit symplectic integrator.\nSofSpa10: 10th order explicit symplectic integrator.","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/#GeometricIntegrators.jl","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"GeometricIntegrators.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Pkg.clone(\"https://github.com/JuliaDiffEq/GeometricIntegratorsDiffEq.jl\")\nusing GeometricIntegratorsDiffEq","category":"page"},{"location":"modules/DiffEqDocs/solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"GISymplecticEulerA - First order explicit symplectic Euler A\nGISymplecticEulerB - First order explicit symplectic Euler B\nGILobattoIIIAIIIB(n) - Nth order Gauss-Labatto-IIIA-IIIB\nGILobattoIIIBIIIA(n) - Nth order Gauss-Labatto-IIIB-IIIA","category":"page"},{"location":"modules/SciMLSensitivity/manual/nonlinear_solve_sensitivities/#sensitivity_nonlinear","page":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/nonlinear_solve_sensitivities/","page":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)","title":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)","text":"SteadyStateAdjoint","category":"page"},{"location":"modules/SciMLSensitivity/manual/nonlinear_solve_sensitivities/#SciMLSensitivity.SteadyStateAdjoint","page":"Sensitivity Algorithms for Nonlinear Problems with Automatic Differentiation (AD)","title":"SciMLSensitivity.SteadyStateAdjoint","text":"SteadyStateAdjoint{CS,AD,FDT,VJP,LS} <: AbstractAdjointSensitivityAlgorithm{CS,AD,FDT}\n\nAn implementation of the adjoint differentiation of a nonlinear solve. Uses the implicit function theorem to directly compute the derivative of the solution to f(up) = 0 with respect to p.\n\nConstructor\n\nSteadyStateAdjoint(;chunk_size = 0, autodiff = true,\n                    diff_type = Val{:central},\n                    autojacvec = autodiff, linsolve = nothing)\n\nKeyword Arguments\n\nautodiff: Use automatic differentiation for constructing the Jacobian if the Jacobian needs to be constructed.  Defaults to true.\nchunk_size: Chunk size for forward-mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\ndiff_type: The method used by FiniteDiff.jl for constructing the Jacobian if the full Jacobian is required with autodiff=false.\nautojacvec: Calculate the vector-Jacobian product (J'*v) via automatic differentiation with special seeding. The default is nothing. The total set of choices are:\nfalse: the Jacobian is constructed via FiniteDiff.jl\ntrue: the Jacobian is constructed via ForwardDiff.jl\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp.\nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function.\nlinsolve: the linear solver used in the adjoint solve. Defaults to nothing, which uses a polyalgorithm to attempt to automatically choose an efficient algorithm.\n\nFor more details on the vjp choices, please consult the sensitivity algorithms documentation page or the docstrings of the vjp types.\n\nReferences\n\nJohnson, S. G., Notes on Adjoint Methods for 18.336, Online at http://math.mit.edu/stevenj/18.336/adjoint.pdf (2007)\n\n\n\n\n\n","category":"type"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/#UnivariateMonicOrthogonalPolynomials","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Univariate monic orthogonal polynomials make up the core building block of the package. These are real polynomials  pi_k _k geq 0, which are univariate pi_k mathbbR rightarrow mathbbR and orthogonal relative to a nonnegative weight function w mathbbR rightarrow mathbbR_geq 0, and which have a leading coefficient equal to one:","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"beginaligned\npi_k(t) = t^k + a_k-1 t^k-1 + dots + a_1 t + a_0 quad forall k = 0 1 dots \nlangle pi_k pi_l rangle = int_mathbbR pi_k(t) pi_l(t) w(t) mathrmdt =\nbegincases\n0  k neq l text and kl geq 0 \n pi_k ^2  0  k = l geq 0\nendcases\nendaligned","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"These univariate monic orthogonal polynomials satisfy the paramount three-term recurrence relation","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"beginaligned\npi_k+1(t) = (t - alpha_k) pi_k(t) - beta_k pi_k-1(t) quad k= 0 1 dots \npi_o(t) = 1 \npi_-1(t) = 0\nendaligned","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Hence, every system of n univariate monic orthogonal polynomials  pi_k _k=0^n is isomorphic to its recurrence coefficients  alpha_k beta_k _k=0^n.","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/#Canonical-Orthogonal-Polynomials","page":"Univariate Monic Orthogonal Polynomials","title":"Canonical Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"The so-called classical or canonical orthogonal polynomials are polynomials named after famous mathematicians who each discovered a special family of orthogonal polynomials, for example Hermite polynomials or Jacobi polynomials. For classical orthogonal polynomials there exist closed-form expressions of–-among others–-the recurrence coefficients. Also quadrature rules for classical orthogonal polynomials are well-studied (with dedicated packages such as FastGaussQuadrature.jl. However, more often than not these classical orthogonal polynomials are neither monic nor orthogonal, hence not normalized in any sense. For example, there is a distinction between the probabilists' Hermite polynomials and the physicists' Hermite polynomials. The difference is in the weight function w(t) relative to which the polynomials are orthogonal:","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"beginaligned\ntextProbabilists w(t) = frac1sqrt2 pi  exp left( - fract^22 right) \ntextPhysicists w(t) =  exp left( - t^2 right)\nendaligned","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"To streamline the computations, all classical orthogonal polynomials are converted to monic orthogonal polynomials (for which, of course, the closed-form expressions persist). Currently, the following weight functions (hence classical orthogonal polynomials) are supported:","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Name Weight w(t) Parameters Support Classical polynomial\nhermite $ \\exp \\left( - t^2 \\right)$ - (-infty infty) Hermite\ngenhermite $ \\lvert t \\rvert^{2 \\mu}\\exp \\left( - t^2 \\right)$ mu  -frac12 (-infty infty) Generalized Hermite\nlegendre 1 - (-11) Legendre\njacobi (1-t)^alpha (1+t)^beta alpha beta  -1 (-11) Jacobi\nlaguerre exp(-t) - (0infty) Laguerre\ngenlaguerre t^alphaexp(-t) alpha-1 (0infty) Generalized Laguerre\nmeixnerpollaczek frac12 pi exp((2phi-pi)t) lvertGamma(lambda + mathrmit)rvert^2 lambda  0 0phipi (-inftyinfty) Meixner-Pollaczek","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Additionally, the following weight functions that are equivalent to probability density functions are supported:","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Name Weight w(t) Parameters Support Classical polynomial\ngaussian frac1sqrt2 pi  exp left( - fract^22 right) - (-infty infty) Probabilists' Hermite\nuniform01 1 - (01) Legendre\nbeta01 frac1B(alphabeta)  t^alpha-1 (1-t)^beta-1 alpha beta  0 (01) Jacobi\ngamma fracbeta^alphaGamma(alpha) t^alpha-1 exp(-beta t) alpha beta  0 (0infty) Laguerre\nlogistic fracexp(-t)(1+exp(-t))^2 - (-inftyinfty) -","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"To generate the orthogonal polynomials up to maximum degree deg, simply call","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> using PolyChaos\n\njulia> deg = 4\n4\n\njulia> op = GaussOrthoPoly(deg)\nGaussOrthoPoly{Array{Float64,1},GaussMeasure,Quad{Float64,Array{Float64,1}}}(4, [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 2.0, 3.0, 4.0], GaussMeasure(PolyChaos.w_gaussian, (-Inf, Inf), true), Quad{Float64,Array{Float64,1}}(\"golubwelsch\", 4, [-2.3344142183389778, -0.7419637843027257, 0.7419637843027258, 2.3344142183389778], [0.04587585476806844, 0.45412414523193134, 0.45412414523193106, 0.04587585476806852]))\n\njulia> show(op)\n\nUnivariate orthogonal polynomials\ndegree:         4\n#coeffs:        5\nα =             [0.0, 0.0, 0.0, 0.0, 0.0]\nβ =             [1.0, 1.0, 2.0, 3.0, 4.0]\n\nMeasure dλ(t)=w(t)dt\nw:      w_gaussian\ndom:    (-Inf, Inf)\nsymmetric:      true\n","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"This generates opas a GaussOrthoPoly type with the underlying Gaussian measure op.measure. The recurrence coefficients are accessible via coeffs().","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> coeffs(op)\n5×2 Array{Float64,2}:\n 0.0  1.0\n 0.0  1.0\n 0.0  2.0\n 0.0  3.0\n 0.0  4.0","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"By default, the constructor for OrthoPoly generates deg+1 recurrence coefficients. Sometimes, some other number Nrec may be required. This is why Nrec is a keyword for the constructor OrthoPoly.","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> N = 100\n100\n\njulia> opLogistic = LogisticOrthoPoly(deg; Nrec=N)\nLogisticOrthoPoly{Array{Float64,1},LogisticMeasure,Quad{Float64,Array{Float64,1}}}(4, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 3.289868133696453, 10.527578027828648, 22.841084471092515, 40.10505915363294, 62.30810859273584, 89.4476035231595, 121.52266752315666, 158.53293971318436, 200.47824915030117  …  19986.565781520196, 20433.165380253333, 20884.69978120051, 21341.168984361153, 21802.572989734712, 22268.91179732067, 22740.185407118537, 23216.393819127847, 23697.53703334815, 24183.61504977904], LogisticMeasure(PolyChaos.w_logistic, (-Inf, Inf), true), Quad{Float64,Array{Float64,1}}(\"golubwelsch\", 99, [-285.97091675697385, -266.56611354854135, -251.01698966393153, -237.53179686807928, -225.4187633699017, -214.31820469129195, -204.0126795649811, -194.35793540921836, -185.25200558110012, -176.61940782973926  …  176.61940782973895, 185.25200558110018, 194.35793540921847, 204.01267956498108, 214.31820469129212, 225.4187633699016, 237.53179686807948, 251.01698966393138, 266.56611354854135, 285.9709167569736], [1.4541663108207099e-123, 2.897917000559268e-115, 1.3858976222735606e-108, 8.826460482953542e-103, 1.4618715331286334e-97, 8.935651454381735e-93, 2.49282531464423e-88, 3.6557113389197252e-84, 3.1147999002113552e-80, 1.660700338355251e-76  …  1.6607003383554774e-76, 3.1147999002111335e-80, 3.6557113389195227e-84, 2.492825314644278e-88, 8.935651454380596e-93, 1.461871533128785e-97, 8.826460482953113e-103, 1.3858976222735651e-108, 2.8979170005595435e-115, 1.4541663108207404e-123]))\n\njulia> show(opLogistic)\n\nUnivariate orthogonal polynomials\ndegree:         4\n#coeffs:        100\nα =             [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]...\nβ =             [1.0, 3.289868133696453, 10.527578027828648, 22.841084471092515, 40.10505915363294, 62.30810859273584, 89.4476035231595]...\n\nMeasure dλ(t)=w(t)dt\nw:      w_logistic\ndom:    (-Inf, Inf)\nsymmetric:      true\n","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Let's check whether we truly have more coefficients:","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> size(coeffs(opLogistic),1) == N\ntrue","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/#Arbitrary-Weights","page":"Univariate Monic Orthogonal Polynomials","title":"Arbitrary Weights","text":"","category":"section"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"If you are given a weight function w that does not belong to the Table above, it is still possible to generate the respective univariate monic orthogonal polynomials. First, we define the measure by specifying a name, the weight, the support, symmetry, and parameters","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> supp = (-1, 1)\n(-1, 1)\n\njulia> w(t) = 1 + t\nw (generic function with 1 method)\n\njulia> my_meas = Measure(\"my_meas\", w, supp, false, Dict())\nMeasure(\"my_meas\", w, (-1.0, 1.0), false, Dict{Any,Any}())","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Notice: it is advisable to define the weight such that an error is thrown for arguments outside of the support.","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Now, we want to construct the univariate monic orthogonal polynomials up to degree deg relative to my_meas. The constructor is","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> my_op = OrthoPoly(\"my_op\", deg, my_meas; Nquad=200);\n\njulia> show(my_op)\n\nUnivariate orthogonal polynomials\ndegree:         4\n#coeffs:        5\nα =             [0.3333333333333335, 0.06666666666666644, 0.028571428571428848, 0.015873015873015657, 0.010101010101010171]\nβ =             [2.0, 0.2222222222222223, 0.23999999999999996, 0.24489795918367344, 0.2469135802469136]\n\nMeasure dλ(t)=w(t)dt\nname:   my_meas\nw:      w\ndom:    (-1.0, 1.0)\nsymmetric:      false\npars:   Dict{Any,Any}()\n","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"By default, the recurrence coefficients are computed using the Stieltjes procuedure with Clenshaw-Curtis quadrature (with Nquad nodes and weights). Hence, the choice of Nquad influences accuracy.","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/#MultivariateMonicOrthogonalPolynomials","page":"Univariate Monic Orthogonal Polynomials","title":"Multivariate Monic Orthogonal Polynomials","text":"","category":"section"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Suppose we have p systems of univariate monic orthogonal polynomials,","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":" pi_k^(1) _kgeq 0   pi_k^(2) _kgeq 0 dots  pi_k^(p) _kgeq 0","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"each system being orthogonal relative to the weights w^(1) w^(2) dots w^(p) with supports mathcalW^(1) mathcalW^(2) dots mathcalW^(p). Also, let d^(i) be the maximum degree of the i-th system of univariate orthogonal polynomials. We would like to construct a p-variate monic basis  psi_k _k geq 0 with psi mathbbR^p rightarrow mathbbR of degree at most 0 leq d leq min_i=1dotsk d^(i). Further, this basis shall be orthogonal relative to the product measure w mathcalW = mathcalW^(1) otimes mathcalW^(2) mathcalW^(1) cdots otimes mathcalW^(p) rightarrow mathbbR_geq 0 given by","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"w(t) = prod_i=1^p w^(i)(t_i)","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"hence satisfies","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"langle psi_k psi_l rangle = int_mathcalW psi_k(t) psi_l(t) w(t) mathrmd t =\nbegincases\n0  k neq l text and kl geq 0 \n psi_k ^2  0  k = l geq 0\nendcases","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"For this, there exists the composite struct MultiOrthoPoly. Let's consider an example where we mix classical orthogonal polynomials with an arbitrary weight.","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> deg = [3, 5, 6, 4]\n4-element Array{Int64,1}:\n 3\n 5\n 6\n 4\n\njulia> d = minimum(deg)\n3\n\njulia> op1 = GaussOrthoPoly(deg[1]);\n\njulia> op2 = Uniform01OrthoPoly(deg[2]);\n\njulia> op3 = Beta01OrthoPoly(deg[3], 2, 1.2);\n\njulia> ops = [op1, op2, op3, my_op];\n\njulia> mop = MultiOrthoPoly(ops, d);\n\njulia> show(mop)\n\n4-variate orthogonal polynomials\nname:           GaussOrthoPoly{Array{Float64,1},GaussMeasure,Quad{Float64,Array{Float64,1}}}\n                Uniform01OrthoPoly{Array{Float64,1},Uniform01Measure,Quad{Float64,Array{Float64,1}}}\n                Beta01OrthoPoly{Array{Float64,1},Beta01Measure,Quad{Float64,Array{Float64,1}}}\n                my_op\ndeg:            3\ndim:            35\nind:            [0, 0, 0, 0]\n                [1, 0, 0, 0]\n                [0, 1, 0, 0]\n                [0, 0, 1, 0]\n                [0, 0, 0, 1]\n                [2, 0, 0, 0]\n                [1, 1, 0, 0]\n                ...\n\nfalse\n","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"The total number of  basis polynomials is stored in the field dim. The univariate basis polynomials making up the multivariate basis are stored in the field uni. The field ind contains the multi-index, i.e. row i stores what combination of univariate polynomials makes up the i-th multivariate polynomial. For example,","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"julia> i = 11;\n\njulia> mop.ind[i+1, :]\n4-element Array{Int64,1}:\n 0\n 1\n 0\n 1","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"translates mathematically to","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"psi_11(t) = pi_0^(1)(t_1) pi_1^(2)(t_2) pi_0^(3)(t_3) pi_1^(4)(t_4)","category":"page"},{"location":"modules/PolyChaos/orthogonal_polynomials_canonical/","page":"Univariate Monic Orthogonal Polynomials","title":"Univariate Monic Orthogonal Polynomials","text":"Notice that there is an offset by one, because the basis counting starts at 0, but Julia is 1-indexed. The underlying measure of mop is now of type ProductMeasure, and stored in the field measure The weight w can be evaluated as one would expect.","category":"page"},{"location":"modules/MethodOfLines/devnotes/#Notes-for-developers","page":"Notes for Developers: Implement a Scheme","title":"Notes for developers","text":"","category":"section"},{"location":"modules/MethodOfLines/devnotes/#Getting-started","page":"Notes for Developers: Implement a Scheme","title":"Getting started","text":"","category":"section"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"First, fork the repo and clone it locally.","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"Then, type in the REPL","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"julia>] dev /path/to/your/repo\njulia>] activate MethodOfLines","category":"page"},{"location":"modules/MethodOfLines/devnotes/#Overview","page":"Notes for Developers: Implement a Scheme","title":"Overview","text":"","category":"section"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"MethodOfLines.jl makes heavy use of Symbolics.jl and SymbolicUtils.jl, especially the replacement rules from the latter.","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"Take a look at src/discretization/MOL_discretization.jl to get a high level overview of how the discretization works. A more consise description can be found here. Feel free to post an issue if you would like help understanding anything, or want to know developer opinions on the best way to go about implementing something.","category":"page"},{"location":"modules/MethodOfLines/devnotes/#Adding-new-finite-difference-schemes","page":"Notes for Developers: Implement a Scheme","title":"Adding new finite difference schemes","text":"","category":"section"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"If you know of a finite difference scheme which is better than what is currently implemented, please first post an issue with a link to a paper.","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"A replacement rule is generated for each term which has a more specific higher stability/accuracy finite difference scheme than the general central difference, which represents a base case.","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"Take a look at src/discretization/generate_finite_difference_rules.jl to see how the replacement rules are generated. Note that the order that the rules are applied is important; there may be schemes that are applied first that are special cases of more general rules, for example the sphrical laplacian is a special case of the nonlinear lalacian.","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"First terms are split, isolating particular cases. Then, rules are generated and applied. Take a look at the docs for symbolic utils to get an idea of how these work. ","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"Identify a rule which will match your case, then write a function that will handle how to apply that scheme for each index in the interior, for each combination of independant and dependant variables. ","category":"page"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"Initially, don't worry if your scheme is only implemented for specific approximation orders, it is sufficient just to warn when the requested approximation order does not match that supplied by the scheme. We can work in future pull requests to generalize the scheme to higher approximation orders, where possible.","category":"page"},{"location":"modules/MethodOfLines/devnotes/#Inspecting-generated-code","page":"Notes for Developers: Implement a Scheme","title":"Inspecting generated code","text":"","category":"section"},{"location":"modules/MethodOfLines/devnotes/","page":"Notes for Developers: Implement a Scheme","title":"Notes for Developers: Implement a Scheme","text":"To get the generated code for your system, use code = ODEFunctionExpr(prob), or MethodOfLines.generate_code(pdesys, discretization, \"my_generated_code_filename.jl\"), which will create a file called my_generated_code_filename.jl in pwd(). This can be useful to find errors in the discretization, but note that it is not recommended to use this code directly, calling solve(prob, AppropriateSolver()) will handle this for you.","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/#Physics-Informed-Machine-Learning-(PIML)-with-TensorLayer","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"In this tutorial, we show how to use the DiffEqFlux TensorLayer to solve problems in Physics Informed Machine Learning.","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Let's consider the anharmonic oscillator described by the ODE","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"x = - kx - αx³ - βx -γx³","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"To obtain the training data, we solve the equation of motion using one of the solvers in DifferentialEquations:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using DiffEqFlux, Optimization, OptimizationFlux, DifferentialEquations, LinearAlgebra\nk, α, β, γ = 1, 0.1, 0.2, 0.3\ntspan = (0.0,10.0)\n\nfunction dxdt_train(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -k*u[1] - α*u[1]^3 - β*u[2] - γ*u[2]^3\nend\n\nu0 = [1.0,0.0]\nts = collect(0.0:0.1:tspan[2])\nprob_train = ODEProblem{true}(dxdt_train,u0,tspan)\ndata_train = Array(solve(prob_train,Tsit5(),saveat=ts))","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Now, we create a TensorLayer that will be able to perform 10th order expansions in a Legendre Basis:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"A = [LegendreBasis(10), LegendreBasis(10)]\nnn = TensorLayer(A, 1)","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we also instantiate the model we are trying to learn, \"informing\" the neural about the ∝x and ∝v dependencies in the equation of motion:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"f = x -> min(30one(x),x)\n\nfunction dxdt_pred(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p[1]*u[1] - p[2]*u[2] + f(nn(u,p[3:end])[1])\nend\n\nα = zeros(102)\n\nprob_pred = ODEProblem{true}(dxdt_pred,u0,tspan)","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Note that we introduced a \"cap\" in the neural network term to avoid instabilities in the solution of the ODE. We also initialized the vector of parameters to zero in order to obtain a faster convergence for this particular example.","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"Finally, we introduce the corresponding loss function:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"function predict_adjoint(θ)\n  x = Array(solve(prob_pred,Tsit5(),p=θ,saveat=ts,\n                  sensealg=InterpolatingAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\nfunction loss_adjoint(θ)\n  x = predict_adjoint(θ)\n  loss = sum(norm.(x - data_train))\n  return loss\nend\n\niter = 0\nfunction callback(θ,l)\n  global iter\n  iter += 1\n  if iter%10 == 0\n    println(l)\n  end\n  return false\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"and we train the network using two rounds of ADAM:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p) -> loss_adjoint(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, α)\nres1 = Optimization.solve(optprob, ADAM(0.05), callback = callback, maxiters = 150)\n\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\nres2 = Optimization.solve(optprob2, ADAM(0.001), callback = callback,maxiters = 150)\nopt = res2.u","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"We plot the results and we obtain a fairly accurate learned model:","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"using Plots\ndata_pred = predict_adjoint(res1.u)\nplot(ts, data_train[1,:], label = \"X (ODE)\")\nplot!(ts, data_train[2,:], label = \"V (ODE)\")\nplot!(ts, data_pred[1,:], label = \"X (NN)\")\nplot!(ts, data_pred[2,:],label = \"V (NN)\")","category":"page"},{"location":"modules/DiffEqFlux/examples/tensor_layer/","page":"Physics-Informed Machine Learning (PIML) with TensorLayer","title":"Physics-Informed Machine Learning (PIML) with TensorLayer","text":"(Image: plot_tutorial)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Convolutional-Neural-ODE-MNIST-Classifier-on-GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Training a Convolutional Neural Net Classifier for MNIST using a neural ordinary differential equation NN-ODE on GPUs with Minibatching.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"(Step-by-step description below)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs\nusing CUDA\nCUDA.allowscalar(false)\n\nfunction loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend\n\n# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)\n\ndown = Flux.Chain(Flux.Conv((3, 3), 1=>64, relu, stride = 1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, relu, stride = 2, pad=1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Flux.Chain(Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Flux.Chain(Flux.GroupNorm(64, 64), x -> relu.(x), Flux.MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Flux.Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu\n\nfunction DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend\n\n# Build our over-all model topology\nmodel = Flux.Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nimg, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)\n\nclassify(x) = argmax.(eachcol(x))\n\nfunction accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)\n\nloss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)\n\nopt = ADAM(0.05)\niter = 0\n\ncallback() = begin\n    global iter += 1\n    # Monitor that the weights do infact update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend\n\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, cb = callback)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Step-by-Step-Description","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Step-by-Step Description","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Load-Packages","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load Packages","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using DiffEqFlux, DifferentialEquations, Printf\nusing Flux.Losses: logitcrossentropy\nusing Flux.Data: DataLoader\nusing MLDatasets\nusing MLDataUtils:  LabelEnc, convertlabel, stratifiedobs","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#GPU","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"GPU","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"A good trick used here:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"using CUDA\nCUDA.allowscalar(false)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Ensures that only optimized kernels are called when using the GPU. Additionally, the gpu function is shown as a way to translate models and data over to the GPU. Note that this function is CPU-safe, so if the GPU is disabled or unavailable, this code will fallback to the CPU.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Load-MNIST-Dataset-into-Minibatches","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Load MNIST Dataset into Minibatches","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The preprocessing is done in loadmnist where the raw MNIST data is split into features x_train and labels y_train by specifying batchsize bs. The function convertlabel will then transform the current labels (labels_raw) from numbers 0 to 9 (LabelEnc.NativeLabels(collect(0:9))) into one hot encoding (LabelEnc.OneOfK).","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Features are reshaped into format [Height, Width, Color, BatchSize] or in this case [28, 28, 1, 128] meaning that every minibatch will contain 128 images with a single color channel of 28x28 pixels. The entire dataset of 60,000 images is split into the train and test dataset, ensuring a balanced ratio of labels. These splits are then passed to Flux's DataLoader. This automatically minibatches both the images and labels. Additionally, it allows us to shuffle the train dataset in each epoch while keeping the order of the test data the same.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function loadmnist(batchsize = bs, train_split = 0.9)\n    # Use MLDataUtils LabelEnc for natural onehot conversion\n    onehot(labels_raw) = convertlabel(LabelEnc.OneOfK, labels_raw,\n                                      LabelEnc.NativeLabels(collect(0:9)))\n    # Load MNIST\n    mnist = MNIST(split = :train)\n    imgs, labels_raw = mnist.features, mnist.targets\n    # Process images into (H,W,C,BS) batches\n    x_data = Float32.(reshape(imgs, size(imgs,1), size(imgs,2), 1, size(imgs,3)))\n    y_data = onehot(labels_raw)\n    (x_train, y_train), (x_test, y_test) = stratifiedobs((x_data, y_data),\n                                                         p = train_split)\n    return (\n        # Use Flux's DataLoader to automatically minibatch and shuffle the data\n        DataLoader(gpu.(collect.((x_train, y_train))); batchsize = batchsize,\n                   shuffle = true),\n        # Don't shuffle the test data\n        DataLoader(gpu.(collect.((x_test, y_test))); batchsize = batchsize,\n                   shuffle = false)\n    )\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"and then loaded from main:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Main\nconst bs = 128\nconst train_split = 0.9\ntrain_dataloader, test_dataloader = loadmnist(bs, train_split)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Layers","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Layers","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"The Neural Network requires passing inputs sequentially through multiple layers. We use Chain which allows inputs to functions to come from previous layer and sends the outputs to the next. Four different sets of layers are used here:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down = Flux.Chain(Flux.Conv((3, 3), 1=>64, relu, stride = 1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, relu, stride = 2, pad=1), Flux.GroupNorm(64, 64),\n             Flux.Conv((4, 4), 64=>64, stride = 2, pad = 1)) |>gpu\n\ndudt = Flux.Chain(Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1),\n             Flux.Conv((3, 3), 64=>64, tanh, stride=1, pad=1)) |>gpu\n\nfc = Flux.Chain(Flux.GroupNorm(64, 64), x -> relu.(x), Flux.MeanPool((6, 6)),\n           x -> reshape(x, (64, :)), Flux.Dense(64,10)) |> gpu\n          \nnn_ode = NeuralODE(dudt, (0.f0, 1.f0), Tsit5(),\n                   save_everystep = false,\n                   reltol = 1e-3, abstol = 1e-3,\n                   save_start = false) |> gpu","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"down: This layer downsamples our images into 6 x 6 x 64 dimensional features.         It takes a 28 x 28 image, and passes it through a convolutional neural network         layer with relu activation","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn: A 2 layer Convolutional Neural Network Chain with tanh activation which is used to model       our differential equation","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"nn_ode: ODE solver layer","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"fc: The final fully connected layer which maps our learned features to the probability of       the feature vector of belonging to a particular class","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"gpu: A utility function which transfers our model to GPU, if one is available","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Array-Conversion","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Array Conversion","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"When using NeuralODE, we can use the following function as a cheap conversion of DiffEqArray from the ODE solver into a Matrix that can be used in the following layer:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function DiffEqArray_to_Array(x)\n    xarr = gpu(x)\n    return xarr[:,:,:,:,1]\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"For CPU: If this function does not automatically fallback to CPU when no GPU is present, we can change gpu(x) with Array(x).","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Build-Topology","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Build Topology","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Next we connect all layers together in a single chain:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Build our over-all model topology\nmodel = Flux.Chain(down,                 # (28, 28, 1, BS) -> (6, 6, 64, BS)\n              nn_ode,               # (6, 6, 64, BS) -> (6, 6, 64, BS, 1)\n              DiffEqArray_to_Array, # (6, 6, 64, BS, 1) -> (6, 6, 64, BS)\n              fc)                   # (6, 6, 64, BS) -> (10, BS)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"There are a few things we can do to examine the inner workings of our neural network:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"img, lab = train_dataloader.data[1][:, :, :, 1:1], train_dataloader.data[2][:, 1:1]\n\n# To understand the intermediate NN-ODE layer, we can examine it's dimensionality\nx_d = down(img)\n\n# We can see that we can compute the forward pass through the NN topology\n# featuring an NNODE layer.\nx_m = model(img)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This can also be built without the NN-ODE by replacing nn-ode with a simple nn:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# We can also build the model topology without a NN-ODE\nm_no_ode = Flux.Chain(down, nn, fc) |> gpu\n\nx_m = m_no_ode(img)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Prediction","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Prediction","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To convert the classification back into readable numbers, we use classify which returns the prediction by taking the arg max of the output for each column of the minibatch:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"classify(x) = argmax.(eachcol(x))","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Accuracy","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Accuracy","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"We then evaluate the accuracy on n_batches at a time through the entire network:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"function accuracy(model, data; n_batches = 100)\n    total_correct = 0\n    total = 0\n    for (i, (x, y)) in enumerate(data)\n        # Only evaluate accuracy for n_batches\n        i > n_batches && break\n        target_class = classify(cpu(y))\n        predicted_class = classify(cpu(model(x)))\n        total_correct += sum(target_class .== predicted_class)\n        total += length(target_class)\n    end\n    return total_correct / total\nend\n\n# burn in accuracy\naccuracy(model, train_dataloader)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Training-Parameters","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Training Parameters","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Once we have our model, we can train our neural network by backpropagation using Flux.train!. This function requires Loss, Optimizer and Callback functions.","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Loss","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Loss","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Cross Entropy is the loss function computed here which applies a Softmax operation on the final output of our model. logitcrossentropy takes in the prediction from our model model(x) and compares it to actual output y:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"loss(x, y) = logitcrossentropy(model(x), y)\n\n# burn in loss\nloss(img, lab)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Optimizer","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Optimizer","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"ADAM is specified here as our optimizer with a learning rate of 0.05:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"opt = ADAM(0.05)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#CallBack","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"CallBack","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"This callback function is used to print both the training and testing accuracy after 10 training iterations:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"callback() = begin\n    global iter += 1\n    # Monitor that the weights update\n    # Every 10 training iterations show accuracy\n    if iter % 10 == 1\n        train_accuracy = accuracy(model, train_dataloader) * 100\n        test_accuracy = accuracy(model, test_dataloader;\n                                 n_batches = length(test_dataloader)) * 100\n        @printf(\"Iter: %3d || Train Accuracy: %2.3f || Test Accuracy: %2.3f\\n\",\n                iter, train_accuracy, test_accuracy)\n    end\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Train","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Train","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"To train our model, we select the appropriate trainable parameters of our network with params. In our case, backpropagation is required for down, nn_ode and fc. Notice that the parameters for Neural ODE is given by nn_ode.p:","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"# Train the NN-ODE and monitor the loss and weights.\nFlux.train!(loss, Flux.params(down, nn_ode.p, fc), train_dataloader, opt, callback = callback)","category":"page"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/#Expected-Output","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Expected Output","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/mnist_conv_neural_ode/","page":"Convolutional Neural ODE MNIST Classifier on GPU","title":"Convolutional Neural ODE MNIST Classifier on GPU","text":"Iter:   1 || Train Accuracy: 8.453 || Test Accuracy: 8.883\nIter:  11 || Train Accuracy: 14.773 || Test Accuracy: 14.967\nIter:  21 || Train Accuracy: 24.383 || Test Accuracy: 24.433\nIter:  31 || Train Accuracy: 38.820 || Test Accuracy: 38.000\nIter:  41 || Train Accuracy: 30.852 || Test Accuracy: 31.350\nIter:  51 || Train Accuracy: 29.852 || Test Accuracy: 29.433\nIter:  61 || Train Accuracy: 45.195 || Test Accuracy: 45.217\nIter:  71 || Train Accuracy: 70.336 || Test Accuracy: 68.850\nIter:  81 || Train Accuracy: 76.250 || Test Accuracy: 75.783\nIter:  91 || Train Accuracy: 80.867 || Test Accuracy: 81.017\nIter: 101 || Train Accuracy: 86.398 || Test Accuracy: 85.317\nIter: 111 || Train Accuracy: 90.852 || Test Accuracy: 90.650\nIter: 121 || Train Accuracy: 93.477 || Test Accuracy: 92.550\nIter: 131 || Train Accuracy: 93.320 || Test Accuracy: 92.483\nIter: 141 || Train Accuracy: 94.273 || Test Accuracy: 93.567\nIter: 151 || Train Accuracy: 94.531 || Test Accuracy: 93.583\nIter: 161 || Train Accuracy: 94.992 || Test Accuracy: 94.067\nIter: 171 || Train Accuracy: 95.398 || Test Accuracy: 94.883\nIter: 181 || Train Accuracy: 96.945 || Test Accuracy: 95.633\nIter: 191 || Train Accuracy: 96.430 || Test Accuracy: 95.750\nIter: 201 || Train Accuracy: 96.859 || Test Accuracy: 95.983\nIter: 211 || Train Accuracy: 97.359 || Test Accuracy: 96.500\nIter: 221 || Train Accuracy: 96.586 || Test Accuracy: 96.133\nIter: 231 || Train Accuracy: 96.992 || Test Accuracy: 95.833\nIter: 241 || Train Accuracy: 97.148 || Test Accuracy: 95.950\nIter: 251 || Train Accuracy: 96.422 || Test Accuracy: 95.950\nIter: 261 || Train Accuracy: 96.094 || Test Accuracy: 95.633\nIter: 271 || Train Accuracy: 96.719 || Test Accuracy: 95.767\nIter: 281 || Train Accuracy: 96.719 || Test Accuracy: 96.000\nIter: 291 || Train Accuracy: 96.609 || Test Accuracy: 95.817\nIter: 301 || Train Accuracy: 96.656 || Test Accuracy: 96.033\nIter: 311 || Train Accuracy: 97.594 || Test Accuracy: 96.500\nIter: 321 || Train Accuracy: 97.633 || Test Accuracy: 97.083\nIter: 331 || Train Accuracy: 98.008 || Test Accuracy: 97.067\nIter: 341 || Train Accuracy: 98.070 || Test Accuracy: 97.150\nIter: 351 || Train Accuracy: 97.875 || Test Accuracy: 97.050\nIter: 361 || Train Accuracy: 96.922 || Test Accuracy: 96.500\nIter: 371 || Train Accuracy: 97.188 || Test Accuracy: 96.650\nIter: 381 || Train Accuracy: 97.820 || Test Accuracy: 96.783\nIter: 391 || Train Accuracy: 98.156 || Test Accuracy: 97.567\nIter: 401 || Train Accuracy: 98.250 || Test Accuracy: 97.367\nIter: 411 || Train Accuracy: 97.969 || Test Accuracy: 97.267\nIter: 421 || Train Accuracy: 96.555 || Test Accuracy: 95.667","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/#feynmankac","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"","category":"section"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The Feynman Kac formula is generally stated for terminal condition problems (see e.g. Wikipedia), where","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_t u(tx) + mu(x) nabla_x u(tx) + frac12 sigma^2(x) Delta_x u(tx) + f(x u(tx))  = 0 tag1","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"with terminal condition u(T x) = g(x), and u colon R^d to R. ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"In this case the FK formula states that for all t in (0T) it holds that","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"u(t x) = int_t^T mathbbE left f(X^x_s-t u(s X^x_s-t))ds right + mathbbE left u(0 X^x_T-t) right tag2","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"where ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"X_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and B_t is a Brownian motion.","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"(Image: Brownian motion - Wikipedia)","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Intuitively, this formula is motivated by the fact that the density of Brownian particles (motion) satisfes the diffusion equation.","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"The equivalence between the average trajectory of particles and PDEs given by the Feynman-Kac formula allows to overcome the curse of dimensionality that standard numerical methods suffer from, because the expectations can be approximated Monte Carlo integrations, which approximation error decreases as 1sqrtN and is therefore not dependent on the dimensions. On the other hand, the computational complexity of traditional deterministic techniques grows exponentially in the number of dimensions. ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/#Forward-non-linear-Feynman-Kac","page":"Feynman Kac formula","title":"Forward non-linear Feynman-Kac","text":"","category":"section"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"How to transform previous equation to an initial value problem?","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Define v(tau x) = u(T-tau x). Observe that v(0x) = u(Tx). Further observe that by the chain rule","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\npartial_tau v(tau x) = partial_tau u(T-taux)\n                        = (partial_tau (T-tau)) partial_t u(T-taux)\n                        = -partial_t u(T-tau x)\nendaligned","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"From Eq. (1) we get that ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"- partial_t u(T - taux) = mu(x) nabla_x u(T - taux) + frac12 sigma^2(x) Delta_x u(T - taux) + f(x u(T - taux))","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"Replacing  u(T-tau x) by v(tau x) we get that v satisfies","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"partial_tau v(tau x) = mu(x) nabla_x v(taux) + frac12 sigma^2(x) Delta_x v(taux) + f(x v(taux)) ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"and from Eq. (2) we obtain","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"v(tau x) = int_T-tau^T mathbbE left f(X^x_s- T + tau v(s X^x_s-T + tau))ds right + mathbbE left v(0 X^x_tau) right","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"By using the substitution rule with tau to tau -T (shifting by T) and tau to - tau (inversing), and finally inversing the integral bound we get that ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"beginaligned\nv(tau x) = int_-tau^0 mathbbE left f(X^x_s + tau v(s + T X^x_s + tau))ds right + mathbbE left v(0 X^x_tau) right\n            = - int_tau^0 mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\n            = int_0^tau mathbbE left f(X^x_tau - s v(T-s X^x_tau - s))ds right + mathbbE left v(0 X^x_tau) right\nendaligned","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"This leads to the ","category":"page"},{"location":"modules/HighDimPDE/Feynman_Kac/","page":"Feynman Kac formula","title":"Feynman Kac formula","text":"info: Non-linear Feynman Kac for initial value problems\nConsider the PDEpartial_t u(tx) = mu(t x) nabla_x u(tx) + frac12 sigma^2(t x) Delta_x u(tx) + f(x u(tx))with initial conditions u(0 x) = g(x), where u colon R^d to R.  Thenu(t x) = int_0^t mathbbE left f(X^x_t - s u(T-s X^x_t - s))ds right + mathbbE left u(0 X^x_t) right tag3with X_t^x = int_0^t mu(X_s^x)ds + int_0^tsigma(X_s^x)dB_s + x","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Neural-Graph-Differential-Equations","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"This tutorial has been adapted from here.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"In this tutorial we will use Graph Differential Equations (GDEs) to perform classification on the CORA Dataset. We shall be using the Graph Neural Networks primitives from the package GraphNeuralNetworks.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"# Load the packages\nusing GraphNeuralNetworks, DifferentialEquations\nusing DiffEqFlux: NeuralODE\nusing GraphNeuralNetworks.GNNGraphs: normalized_adjacency\nusing Lux, NNlib, Optimisers, Zygote, Random, ComponentArrays\nusing Lux: AbstractExplicitLayer, glorot_normal, zeros32\nimport Lux: initialparameters, initialstates\nusing SciMLSensitivity\nusing Statistics: mean\nusing MLDatasets: Cora\nusing CUDA\nCUDA.allowscalar(false)\ndevice = CUDA.functional() ? gpu : cpu\n\n# Download the dataset\ndataset = Cora();\n\n# Preprocess the data and compute adjacency matrix\nclasses = dataset.metadata[\"classes\"]\ng = mldataset2gnngraph(dataset) |> device\nonehotbatch(data,labels)= device(labels).==reshape(data, 1,size(data)...)\nonecold(y) =  map(argmax,eachcol(y))\nX = g.ndata.features\ny = onehotbatch(g.ndata.targets, classes) # a dense matrix is not the optimal, but we don't want to use Flux here\n\nÃ = normalized_adjacency(g, add_self_loops=true) |> device\n\n(; train_mask, val_mask, test_mask) = g.ndata\nytrain = y[:,train_mask]\n\n# Model and Data Configuration\nnin = size(X, 1)\nnhidden = 16\nnout = length(classes)\nepochs = 20\n\n# Define the graph neural network\nstruct ExplicitGCNConv{F1,F2,F3,F4} <: AbstractExplicitLayer\n    in_chs::Int\n    out_chs::Int\n    activation::F1\n    init_Ã::F2  # nomalized_adjacency matrix\n    init_weight::F3\n    init_bias::F4\nend\n\nfunction Base.show(io::IO, l::ExplicitGCNConv)\n    print(io, \"ExplicitGCNConv($(l.in_chs) => $(l.out_chs)\")\n    (l.activation == identity) || print(io, \", \", l.activation)\n    print(io, \")\")\nend\n\nfunction initialparameters(rng::AbstractRNG, d::ExplicitGCNConv)\n        return (weight=d.init_weight(rng, d.out_chs, d.in_chs),\n                bias=d.init_bias(rng, d.out_chs, 1))\nend\n\ninitialstates(rng::AbstractRNG, d::ExplicitGCNConv) = (Ã = d.init_Ã(),)\n\n\nfunction ExplicitGCNConv(Ã, ch::Pair{Int,Int}, activation = identity;\n                         init_weight=glorot_normal, init_bias=zeros32) \n    init_Ã = ()->copy(Ã)\n    return ExplicitGCNConv{typeof(activation), typeof(init_Ã), typeof(init_weight), typeof(init_bias)}(first(ch), last(ch), activation, \n                                                                                                       init_Ã, init_weight, init_bias)\nend\n\nfunction (l::ExplicitGCNConv)(x::AbstractMatrix, ps, st::NamedTuple)\n    z = ps.weight * x * st.Ã\n    return l.activation.(z .+ ps.bias), st\nend\n\n# Define the Neural GDE\nfunction diffeqsol_to_array(x::ODESolution{T, N, <:AbstractVector{<:CuArray}}) where {T, N}\n    return dropdims(gpu(x); dims=3)\nend\ndiffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3)\n\n# make NeuralODE work with Lux.Chain\n# remove this once https://github.com/SciML/DiffEqFlux.jl/issues/727 is fixed\ninitialparameters(rng::AbstractRNG, node::NeuralODE) = initialparameters(rng, node.model) \ninitialstates(rng::AbstractRNG, node::NeuralODE) = initialstates(rng, node.model)\n\ngnn = Chain(ExplicitGCNConv(Ã, nhidden => nhidden, relu),\n            ExplicitGCNConv(Ã, nhidden => nhidden, relu))\n\nnode = NeuralODE(gnn, (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                 reltol = 1e-3, abstol = 1e-3, save_start = false)                \n\nmodel = Chain(ExplicitGCNConv(Ã, nin => nhidden, relu),\n              node,\n              diffeqsol_to_array,\n              Dense(nhidden, nout))\n\n# Loss\nlogitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ); dims=1))\n\nfunction loss(x, y, mask, model, ps, st)\n    ŷ, st = model(x, ps, st)\n    return logitcrossentropy(ŷ[:,mask], y), st\nend\n\nfunction eval_loss_accuracy(X, y, mask, model, ps, st)\n    ŷ, _ = model(X, ps, st)\n    l = logitcrossentropy(ŷ[:,mask], y[:,mask])\n    acc = mean(onecold(ŷ[:,mask]) .== onecold(y[:,mask]))\n    return (loss = round(l, digits=4), acc = round(acc*100, digits=2))\nend\n\n# Training\nfunction train()\n    ## Setup model \n    rng = Random.default_rng()\n    Random.seed!(rng, 0)\n\n    ps, st = Lux.setup(rng, model)\n    ps = ComponentArray(ps) |> device\n    st = st |> device\n\n    ## Optimizer\n    opt = Optimisers.ADAM(0.01f0)\n    st_opt = Optimisers.setup(opt,ps)\n\n    ## Training Loop\n    for _ in 1:epochs\n        (l,st), back = pullback(p->loss(X, ytrain, train_mask, model, p, st), ps)\n        gs = back((one(l), nothing))[1]\n        st_opt, ps = Optimisers.update(st_opt, ps, gs)\n        @show eval_loss_accuracy(X, y, val_mask, model, ps, st)\n    end\nend\n\ntrain()","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Step-by-Step-Explanation","page":"Neural Graph Differential Equations","title":"Step by Step Explanation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Load-the-Required-Packages","page":"Neural Graph Differential Equations","title":"Load the Required Packages","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"# Load the packages\nusing GraphNeuralNetworks, DifferentialEquations\nusing DiffEqFlux: NeuralODE\nusing GraphNeuralNetworks.GNNGraphs: normalized_adjacency\nusing Lux, NNlib, Optimisers, Zygote, Random, ComponentArrays\nusing Lux: AbstractExplicitLayer, glorot_normal, zeros32\nimport Lux: initialparameters, initialstates\nusing SciMLSensitivity\nusing Statistics: mean\nusing MLDatasets: Cora\nusing CUDA\nCUDA.allowscalar(false)\ndevice = CUDA.functional() ? gpu : cpu","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Load-the-Dataset","page":"Neural Graph Differential Equations","title":"Load the Dataset","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"The dataset is available in the desired format in the MLDatasets repository. We shall download the dataset from there.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"dataset = Cora();","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Preprocessing-the-Data","page":"Neural Graph Differential Equations","title":"Preprocessing the Data","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Convert the data to GNNGraph and get the adjacency matrix from the graph g.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"classes = dataset.metadata[\"classes\"]\ng = mldataset2gnngraph(dataset) |> device\nonehotbatch(data,labels)= device(labels).==reshape(data, 1,size(data)...)\nonecold(y) =  map(argmax,eachcol(y))\nX = g.ndata.features\ny = onehotbatch(g.ndata.targets, classes) # a dense matrix is not the optimal, but we don't want to use Flux here\n\nÃ = normalized_adjacency(g, add_self_loops=true) |> device","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Training-Data","page":"Neural Graph Differential Equations","title":"Training Data","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"GNNs operate on an entire graph, so we can't do any sort of minibatching here. We predict the entire dataset but train the model in a semi-supervised learning fashion. ","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"(; train_mask, val_mask, test_mask) = g.ndata\nytrain = y[:,train_mask]","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Model-and-Data-Configuration","page":"Neural Graph Differential Equations","title":"Model and Data Configuration","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"We shall use only 16 hidden state dimensions.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"nin = size(X, 1)\nnhidden = 16\nnout = length(classes)\nepochs = 20","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Define-the-Graph-Neural-Network","page":"Neural Graph Differential Equations","title":"Define the Graph Neural Network","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Here we define a type of graph neural networks called GCNConv. We use the name ExplicitGCNConv to avoid naming conflicts with GraphNeuralNetworks. For more informations on defining a layer with Lux, please consult to the doc.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"struct ExplicitGCNConv{F1,F2,F3} <: AbstractExplicitLayer\n    Ã::AbstractMatrix  # nomalized_adjacency matrix\n    in_chs::Int\n    out_chs::Int\n    activation::F1\n    init_weight::F2\n    init_bias::F3\nend\n\nfunction Base.show(io::IO, l::ExplicitGCNConv)\n    print(io, \"ExplicitGCNConv($(l.in_chs) => $(l.out_chs)\")\n    (l.activation == identity) || print(io, \", \", l.activation)\n    print(io, \")\")\nend\n\nfunction initialparameters(rng::AbstractRNG, d::ExplicitGCNConv)\n        return (weight=d.init_weight(rng, d.out_chs, d.in_chs),\n                bias=d.init_bias(rng, d.out_chs, 1))\nend\n\nfunction ExplicitGCNConv(Ã, ch::Pair{Int,Int}, activation = identity;\n                         init_weight=glorot_normal, init_bias=zeros32) \n    return ExplicitGCNConv{typeof(activation), typeof(init_weight), typeof(init_bias)}(Ã, first(ch), last(ch), activation, \n                                                                                       init_weight, init_bias)\nend\n\nfunction (l::ExplicitGCNConv)(x::AbstractMatrix, ps, st::NamedTuple)\n    z = ps.weight * x * l.Ã\n    return l.activation.(z .+ ps.bias), st\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Neural-Graph-Ordinary-Differential-Equations","page":"Neural Graph Differential Equations","title":"Neural Graph Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Let us now define the final model. We will use two GNN layers for approximating the gradients for the neural ODE. We use one additional GCNConv layer to project the data to a latent space and the a Dense layer to project it from the latent space to the predictions. Finally a softmax layer gives us the probability of the input belonging to each target category.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"function diffeqsol_to_array(x::ODESolution{T, N, <:AbstractVector{<:CuArray}}) where {T, N}\n    return dropdims(gpu(x); dims=3)\nend\ndiffeqsol_to_array(x::ODESolution) = dropdims(Array(x); dims=3)\n\ngnn = Chain(ExplicitGCNConv(Ã, nhidden => nhidden, relu),\n            ExplicitGCNConv(Ã, nhidden => nhidden, relu))\n\nnode = NeuralODE(gnn, (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                 reltol = 1e-3, abstol = 1e-3, save_start = false)                \n\nmodel = Chain(ExplicitGCNConv(Ã, nin => nhidden, relu),\n              node,\n              diffeqsol_to_array,\n              Dense(nhidden, nout))","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Training-Configuration","page":"Neural Graph Differential Equations","title":"Training Configuration","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Loss-Function-and-Accuracy","page":"Neural Graph Differential Equations","title":"Loss Function and Accuracy","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"We shall be using the standard categorical crossentropy loss function which is used for multiclass classification tasks.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"logitcrossentropy(ŷ, y) = mean(-sum(y .* logsoftmax(ŷ); dims=1))\n\nfunction loss(x, y, mask, model, ps, st)\n    ŷ, st = model(x, ps, st)\n    return logitcrossentropy(ŷ[:,mask], y), st\nend\n\nfunction eval_loss_accuracy(X, y, mask, model, ps, st)\n    ŷ, _ = model(X, ps, st)\n    l = logitcrossentropy(ŷ[:,mask], y[:,mask])\n    acc = mean(onecold(ŷ[:,mask]) .== onecold(y[:,mask]))\n    return (loss = round(l, digits=4), acc = round(acc*100, digits=2))\nend","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Setup-Model","page":"Neural Graph Differential Equations","title":"Setup Model","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"We need to manually set up our mode with Lux, and convert the paramters to ComponentArray so that they can work well with sensitivity algorithms.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"rng = Random.default_rng()\nRandom.seed!(rng, 0)\n\nps, st = Lux.setup(rng, model)\nps = ComponentArray(ps) |> device\nst = st |> device","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Optimizer","page":"Neural Graph Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"For this task we will be using the ADAM optimizer with a learning rate of 0.01.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"opt = Optimisers.Adam(0.01f0)\nst_opt = Optimisers.setup(opt,ps)","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/#Training-Loop","page":"Neural Graph Differential Equations","title":"Training Loop","text":"","category":"section"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"Finally, we use the package Optimisers to learn the parameters ps. We run the training loop for epochs number of iterations.","category":"page"},{"location":"modules/SciMLSensitivity/neural_ode/neural_gde/","page":"Neural Graph Differential Equations","title":"Neural Graph Differential Equations","text":"for _ in 1:epochs\n    (l,st), back = pullback(p->loss(X, ytrain, train_mask, model, p, st), ps)\n    gs = back((one(l), nothing))[1]\n    st_opt, ps = Optimisers.update(st_opt, ps, gs)\n    @show eval_loss_accuracy(X, y, val_mask, model, ps, st)\nend","category":"page"},{"location":"modules/LinearSolve/#LinearSolve.jl:-High-Performance-Unified-Linear-Solvers","page":"Home","title":"LinearSolve.jl: High-Performance Unified Linear Solvers","text":"","category":"section"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"LinearSolve.jl is a unified interface for the linear solving packages of Julia. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue.","category":"page"},{"location":"modules/LinearSolve/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"To install LinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"LinearSolve\")","category":"page"},{"location":"modules/LinearSolve/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums\nsee also SciML Community page","category":"page"},{"location":"modules/LinearSolve/#Roadmap","page":"Home","title":"Roadmap","text":"","category":"section"},{"location":"modules/LinearSolve/","page":"Home","title":"Home","text":"Wrappers for every linear solver in the Julia language is on the roadmap. If there are any important ones that are missing that you would like to see added, please open an issue. The current algorithms should support automatic differentiation. Pre-defined preconditioners would be a welcome addition.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#External-Modeling-Packages","page":"External Modeling Packages","title":"External Modeling Packages","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"This is a list of modeling packages built upon the JuliaDiffEq ecosystem.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#DynamicalSystems.jl","page":"External Modeling Packages","title":"DynamicalSystems.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"DynamicalSystems.jl is a package for the exploration of continuous and discrete dynamical systems, with focus on nonlinear dynamics and chaos.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"It uses DifferentialEquations.jl for all evolution regarding continuous systems while still retaining unified interface for discrete systems.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"A quick summary of features: Lyapunov exponents, generalized entropies (Renyi entropy), generalized & fractal dimensions, delay coordinates embedding (reconstruction), chaos detection, Lyapunov exponents of a numerical timeseries, finding periodic orbits of any order for maps.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#BioEnergeticFoodWebs.jl","page":"External Modeling Packages","title":"BioEnergeticFoodWebs.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"BioEnergeticFoodWebs.jl is a package for simulations of biomass flows in food webs.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#SwitchTimeOpt.jl","page":"External Modeling Packages","title":"SwitchTimeOpt.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"SwitchTimeOpt.jl is a Julia package to easily define and efficiently solve switching time optimization (STO) problems for linear and nonlinear systems.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#VehicleModels.jl","page":"External Modeling Packages","title":"VehicleModels.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"VehicleModels.jl is a package for simulating vehicle models.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#MADS.jl","page":"External Modeling Packages","title":"MADS.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"MADS.jl is a package data and model analysis. It adds many sensitivity analysis, uncertainty quantification, and model selection routines.","category":"page"},{"location":"modules/DiffEqDocs/models/external_modeling/#QuantumOptics.jl","page":"External Modeling Packages","title":"QuantumOptics.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"QunatumOptics.jl is a package for simulation of quantum systems.","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/#API","page":"API","title":"API","text":"","category":"section"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"AbstractMultiScaleArray\nprint_human_readable","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/#MultiScaleArrays.AbstractMultiScaleArray","page":"API","title":"MultiScaleArrays.AbstractMultiScaleArray","text":"Defining A MultiScaleModel: The Interface\n\nThe required interface is as follows. Leaf types must extend AbstractMultiScaleArrayLeaf, the highest level of the model or the head extends MultiScaleModelHead, and all intermediate types extend AbstractMultiScaleArray. The leaf has an array values::Vector{B}. Each type above then contains three fields:\n\nnodes::Vector{T}\nvalues::Vector{B}\nend_idxs::Vector{Int}\n\nNote that the ordering of the fields matters. B is the BottomType, which has to be the same as the eltype for the array in the leaf types. T is another AbstractMultiScaleArray. Thus at each level, anAbstractMultiScaleArray contains some information of its own (values), the next level down in the heirarchy (nodes), and caching for indices (end_idxs). You can add and use extra fields as you please, and even make the types immutable.\n\nThe MultiScaleModel API\n\nThe resulting type acts as an array. A leaf type l acts exactly as an array with l[i] == l.values[i]. Higher nodes also act as a linear array. If ln is level n in the heirarchy, then ln.nodes is the vector of level n-1 objects, and ln.values are its \"intrinsic values\". There is an indexing scheme on ln, where:\n\nln[i,j,k] gets the kth n-3 object in the jth n-2 object in the ith level n-1 object. Of course, this recurses for the whole hierarchy.\nln[i] provides a linear index through all .nodes and .values values in every lower level and ln.values itself.\n\nThus typeof(ln) <: AbstractVector{B} where B is the eltype of its leaves and all .values's.\n\nIn addition, iterators are provided to make it easy to iterate through levels. For h being the head node, level_iter(h,n) iterates through all level objects n levels down from the top, while level_iter_idx(h,n) is an enumeration (node,y,z) where node are the nth from the head objects, with h[y:z] being the values it holds in the linear indexing.\n\nIndexing and Iteration\n\nThe head node then acts as the king. It is designed to have functionality which mimics a vector in order for usage in DifferentialEquations or Optim. So for example\n\nembryo[12]\n\nreturns the \"12th protein\", counting by Embryo > Tissue > Population > Cell in order of the vectors. The linear indexing exists for every AbstractMultiScaleArray. These types act as full linear vectors, so standard operations do the sensical operations:\n\nembryo[10] = 4.0 # changes protein concentration 10\nembryo[2,3,1] # Gives the 1st cell in the 3rd population of the second tissue\nembryo[:] # generates a vector of all of the protein concentrations\neachindex(embryo) # generates an iterator for the indices\n\nContinuous models can thus be written at the protein level and will work seamlessly with DifferentialEquations or Optim which will treat it like a vector of protein concentrations. Using the iterators, note that we can get each cell population by looping through 2 levels below the top, so\n\nfor cell in level_iter(embryo,3)\n  # Do something with the cells!\nend\n\nor the multiple level iter, which is the one generally used in DifferentialEquations.jl functions:\n\nfor (cell, dcell) in LevelIter(3,embryo, dembryo)\n    # If these are similar structures, `cell` and `dcell` are the similar parts\n    cell_ode(dcell,cell,p,t)\nend\n\nLevelIterIdx can give the indices along with iteration:\n\nfor (cell, y, z) in LevelIterIdx(embryo, 3)\n    # cell = embryo[y:z]\nend\n\nHowever, the interesting behavior comes from event handling. Since embryo will be the \"vector\" for the differential equation or optimization problem, it will be the value passed to the event handling. MultiScaleArrays includes behavior for changing the structure. For example:\n\ntissue3 = construct(Tissue, deepcopy([population, population2]))\nadd_node!(embryo, tissue3) # Adds a new tissue to the embryo\nremove_node!(embryo, 2, 1) # Removes population 1 from tissue 2 of the embryo\n\nCombined with event handling, this allows for dynamic structures to be derived from low level behaviors.\n\nHeterogeneous Nodes via Tuples\n\nNote that tuples can be used as well. This allows for type-stable broadcasting with heterogeneous nodes. This could be useful for mixing types inside of the nodes. For example:\n\nstruct PlantSettings{T} x::T end\nstruct OrganParams{T} y::T end\n\nstruct Organ{B<:Number,P} <: AbstractMultiScaleArrayLeaf{B}\n    values::Vector{B}\n    name::Symbol\n    params::P\nend\n\nstruct Plant{B,S,N<:Tuple{Vararg{<:Organ{<:Number}}}} <: AbstractMultiScaleArray{B}\n    nodes::N\n    values::Vector{B}\n    end_idxs::Vector{Int}\n    settings::S\nend\n\nstruct Community{B,N<:Tuple{Vararg{<:Plant{<:Number}}}} <: AbstractMultiScaleArray{B}\n    nodes::N\n    values::Vector{B}\n    end_idxs::Vector{Int}\nend\n\nmutable struct Scenario{B,N<:Tuple{Vararg{<:Community{<:Number}}}} <: AbstractMultiScaleArrayHead{B}\n    nodes::N\n    values::Vector{B}\n    end_idxs::Vector{Int}\nend\n\norgan1 = Organ([1.1,2.1,3.1], :Shoot, OrganParams(:grows_up))\norgan2 = Organ([4.1,5.1,6.1], :Root, OrganParams(\"grows down\"))\norgan3 = Organ([1.2,2.2,3.2], :Shoot, OrganParams(true))\norgan4 = Organ([4.2,5.2,6.2], :Root, OrganParams(1//3))\nplant1 = construct(Plant, (deepcopy(organ1), deepcopy(organ2)), Float64[], PlantSettings(1))\nplant2 = construct(Plant, (deepcopy(organ3), deepcopy(organ4)), Float64[], PlantSettings(1.0))\ncommunity = construct(Community, (deepcopy(plant1), deepcopy(plant2), ))\nscenario = construct(Scenario, (deepcopy(community),))\n\n(of course at the cost of mutability).\n\n\n\n\n\n","category":"type"},{"location":"modules/MultiScaleArrays/multiscalearray/#MultiScaleArrays.print_human_readable","page":"API","title":"MultiScaleArrays.print_human_readable","text":"print_human_readable(embryo)\n# +|Tissue;                                                 |Tissue                                                 \n#  +|Popula;           |Popula;           |Popula;          +|Popula;           |Popula;           |Popula          \n#   +Cell; Cell; Cell; +Cell; Cell; Cell; +Cell; Cell; Cell; +Cell; Cell; Cell; +Cell; Cell; Cell; +Cell; Cell; Cell\n\nprint_human_readable(embryo;NcharPerName=2)\n# +|Ti;                                   |Ti                                   \n#  +|Po;         |Po;         |Po;        +|Po;         |Po;         |Po        \n#   +Ce; Ce; Ce; +Ce; Ce; Ce; +Ce; Ce; Ce; +Ce; Ce; Ce; +Ce; Ce; Ce; +Ce; Ce; Ce\n\nHere, if the 'AbstractMultiScaleArrayLeaf's contain several fields, you can specify them with fields = [field1,field2,...]\n\nprint_human_readable(embryo;NcharPerName=2,fields=[:values])\n# +|Ti;                                                                                                                                                                             |Ti                                                                                                                                                                             \n#  +|Po;                                                       |Po;                                                       |Po;                                                      +|Po;                                                       |Po;                                                       |Po                                                      \n#   +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]; +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]; +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]; +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]; +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]; +va: [1.0, 2.0, 3.0]; va: [3.0, 2.0, 5.0]; va: [4.0, 6.0]\n\nif your screen is small, then print a sub-part of the AbstractMultiScaleArray:\n\nprint_human_readable(embryo.nodes[1].nodes[1];fields=[:values])\n# +values: [1.0, 2.0, 3.0]; values: [3.0, 2.0, 5.0]; values: [4.0, 6.0]\n\n\n\n\n\n","category":"function"},{"location":"modules/MultiScaleArrays/multiscalearray/#Extensions","page":"API","title":"Extensions","text":"","category":"section"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"Note that this only showed the most basic MultiScaleArray. These types can be extended as one pleases. For example, we can change the definition of the cell to have:","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"struct Cell{B} <: AbstractMultiScaleArrayLeaf{B}\n    values::Vector{B}\n    celltype::Symbol\nend","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"Note that the ordering of the fields matters here: the extra fields must come after the standard fields (so for a leaf it comes after values, for a standard multiscale array it would come after nodes,values,end_idxs). Then we'd construct cells with cell3 = Cell([3.0; 2.0; 5.0], :BCell), and can give it a cell type. This information is part of the call, so","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"for (cell, y, z) in level_iter_idx(embryo, 2)\n    f(t, cell, @view embryo[y:z])\nend","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"can allow one to check the cell.celltype in f an apply a different ODE depending on the cell type. You can add fields however you want, so you can use them to name cells and track lineages.","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"Showing the use of values, you just pass it to the constructor. Let's pass it an array of 3 values:","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"tissue = construct(Tissue, deepcopy([population; population2]), [0.0; 0.0; 0.0])","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"We can selectively apply some function on these values via:","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"for (tissue, y, z) in level_iter_idx(embryo, 1)\n    f(t, tissue, @view embryo[y:z])\nend","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"and mutate tis.values in f. For example, we could have","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"function f(du, tissue::Tissue, p, t)\n    du .+= randn(3)\nend","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"applies normal random numbers to the three values. We could use this to add to the model the fact that tissue.values[1:3] are the tissue's position, and f would then be adding Brownian motion.","category":"page"},{"location":"modules/MultiScaleArrays/multiscalearray/","page":"API","title":"API","text":"Of course, you can keep going and kind of do whatever you want. The power is yours!","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/#pemethod","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"When identifying linear systems from noisy data, the prediction-error method [Ljung] is close to a gold standard when it comes to the quality of the models it produces, but is also one of the computationally more expensive methods due to its reliance on iterative, gradient-based estimation. When we are identifying nonlinear models, we typically do not have the luxury of closed-form, non-iterative solutions, while PEM is easier to adopt to the nonlinear setting.[Larsson]","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Fundamentally, PEM changes the problem from minimizing a loss based on the simulation performance, to minimizing a loss based on shorter-term predictions. There are several benefits of doing so, and this example will highlight two:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The loss is often easier to optimize.\nIn addition to an accurate simulator, you also obtain a prediction for the system.\nWith PEM, it's possible to estimate disturbance models.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The last point will not be illustrated in this tutorial, but we will briefly expand upon it here. Gaussian, zero-mean measurement noise is usually not very hard to handle. Disturbances that affect the state of the system may, however, cause all sorts of havoc on the estimate. Consider wind affecting an aircraft, deriving a statistical and dynamical model of the wind may be doable, but unless you measure the exact wind affecting the aircraft, making use of the model during parameter estimation is impossible. The wind is an unmeasured load disturbance that affects the state of the system through its own dynamics model. Using the techniques illustrated in this tutorial, it's possible to estimate the influence of the wind during the experiment that generated the data and reduce or eliminate the bias it otherwise causes in the parameter estimates. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We will start by illustrating a common problem with simulation-error minimization. Imagine a pendulum with unknown length that is to be estimated. A small error in the pendulum length causes the frequency of oscillation to change. Over sufficiently large horizon, two sinusoidal signals with different frequencies become close to orthogonal to each other. If some form of squared-error loss is used, the loss landscape will be horribly non-convex in this case, indeed, we will illustrate exactly this below.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Another case that poses a problem for simulation-error estimation is when the system is unstable or chaotic. A small error in either the initial condition or the parameters may cause the simulation error to diverge and its gradient to become meaningless.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"In both of these examples, we may make use of measurements we have of the evolution of the system to prevent the simulation error from diverging. For instance, if we have measured the angle of the pendulum, we can make use of this measurement to adjust the angle during the simulation to make sure it stays close to the measured angle. Instead of performing a pure simulation, we instead say that we predict the state a while forward in time, given all the measurements up until the current time point. By minimizing this prediction rather than the pure simulation, we can often prevent the model error from diverging even though we have a poor initial guess. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We start by defining a model of the pendulum. The model takes a parameter L corresponding to the length of the pendulum. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"using DifferentialEquations, Optimization, OptimizationOptimJL, OptimizationPolyalgorithms, Plots, Statistics, DataInterpolations, ForwardDiff\n\ntspan = (0.1f0, Float32(20.0))\ntsteps = range(tspan[1], tspan[2], length = 1000)\n\nu0 = [0f0, 3f0] # Initial angle and angular velocity\n\nfunction simulator(du,u,p,t) # Pendulum dynamics\n    g = 9.82f0 # Gravitational constant\n    L = p isa Number ? p : p[1] # Length of the pendulum\n    gL = g/L\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -gL * sin(θ)\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We assume that the true length of the pendulum is L = 1, and generate some data from this system.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"prob = ODEProblem(simulator,u0,tspan,1.0) # Simulate with L = 1\nsol = solve(prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)\ny = sol[1,:] # This is the data we have available for parameter estimation\nplot(y, title=\"Pendulum simulation\", label=\"angle\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We also define functions that simulate the system and calculate the loss, given a parameter p corresponding to the length.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"function simulate(p)\n    _prob = remake(prob,p=p)\n    solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)[1,:]\nend\n\nfunction simloss(p)\n    yh = simulate(p)\n    e2 = yh\n    e2 .= abs2.(y .- yh)\n    return mean(e2)\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We now look at the loss landscape as a function of the pendulum length:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Ls = 0.01:0.01:2\nsimlosses = simloss.(Ls)\nfig_loss = plot(Ls, simlosses, title = \"Loss landscape\", xlabel=\"Pendulum length\", ylabel = \"MSE loss\", lab=\"Simulation loss\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"This figure is interesting, the loss is of course 0 for the true value L=1, but for values L  1, the overall slope actually points in the wrong direction! Moreover, the loss is oscillatory, indicating that this is a terrible function to optimize, and that we would need a very good initial guess for a local search to converge to the true value. Note, this example is chosen to be one-dimensional in order to allow these kinds of visualizations, and one-dimensional problems are typically not hard to solve, but the reasoning extends to higher-dimensional and harder problems.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We will now move on to defining a predictor model. Our predictor will be very simple, each time step, we will calculate the error e between the simulated angle theta and the measured angle y. A part of this error will be used to correct the state of the pendulum. The correction we use is linear and looks like Ke = K(y - theta). We have formed what is commonly referred to as a (linear) observer. The Kalman filter is a particular kind of linear observer, where K is calculated based on a statistical model of the disturbances that act on the system. We will stay with a simple, fixed-gain observer here for simplicity. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"To feed the sampled data into the continuous-time simulation, we make use of an interpolator. We also define new functions, predictor that contains the pendulum dynamics with the observer correction, a prediction function that performs the rollout (we're not using the word simulation to not confuse with the setting above) and a loss function.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"y_int = LinearInterpolation(y,tsteps)\n\nfunction predictor(du,u,p,t)\n    g = 9.82f0\n    L, K, y = p # pendulum length, observer gain and measurements\n    gL = g/L\n    θ  = u[1]\n    dθ = u[2]\n    yt = y(t)\n    e = yt - θ\n    du[1] = dθ + K*e\n    du[2] = -gL * sin(θ) \nend\n\npredprob = ODEProblem(predictor,u0,tspan,nothing)\n\nfunction prediction(p)\n    p_full = (p..., y_int)\n    _prob = remake(predprob,u0=eltype(p).(u0),p=p_full)\n    solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6)[1,:]\nend\n\nfunction predloss(p)\n    yh = prediction(p)\n    e2 = yh\n    e2 .= abs2.(y .- yh)\n    return mean(e2)\nend\n\npredlosses = map(Ls) do L\n    p = (L, 1) # use K = 1\n    predloss(p)\nend\n\nplot!(Ls, predlosses, lab=\"Prediction loss\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Once gain we look at the loss as a function of the parameter, and this time it looks a lot better. The loss is not convex, but the gradient points in the right direction over a much larger interval. Here, we arbitrarily set the observer gain to K=1, we will later let the optimizer learn this parameter.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"For completeness, we also perform estimation using both losses. We choose an initial guess we know will be hard for the simulation-error minimization just to drive home the point:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"L0 = [0.7] # Initial guess of pendulum length\nadtype = Optimization.AutoForwardDiff()\noptf = Optimization.OptimizationFunction((x,p)->simloss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, L0)\n\nressim = Optimization.solve(optprob, PolyOpt(),\n                                    maxiters = 5000)\nysim = simulate(ressim.u)\n\nplot(tsteps, [y ysim], label=[\"Data\" \"Simulation model\"])\n\np0 = [0.7, 1.0] # Initial guess of length and observer gain K\noptf2 = Optimization.OptimizationFunction((p,_)->predloss(p), adtype)\noptfunc2 = Optimization.instantiate_function(optf2, p0, adtype, nothing)\noptprob2 = Optimization.OptimizationProblem(optfunc2, p0)\n\nrespred = Optimization.solve(optprob2, PolyOpt(),\n                                    maxiters = 5000)\nypred = simulate(respred.u)\n\nplot!(tsteps, ypred, label=\"Prediction model\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"The estimated parameters (L K) are","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"respred.u","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"Now, we might ask ourselves why we used a correct on the form Ke and didn't instead set the angle in the simulation equal to the measurement. The reason is twofold","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"If our prediction of the angle is 100% based on the measurements, the model parameters do not matter for the prediction and we can thus not hope to learn their values.\nThe measurement is usually noisy, and we thus want to fuse the predictive power of the model with the information of the measurements. The Kalman filter is an optimal approach to this information fusion under special circumstances (linear model, Gaussian noise).","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"We thus let the optimization learn the best value of the observer gain in order to make the best predictions. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"As a last step, we perform the estimation also with some measurement noise to verify that it does something reasonable:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"yn = y .+ 0.1f0 .* randn.(Float32)\ny_int = LinearInterpolation(yn,tsteps) # redefine the interpolator to contain noisy measurements\n\noptf = Optimization.OptimizationFunction((x,p)->predloss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p0)\n\nresprednoise = Optimization.solve(optprob, PolyOpt(),\n                                    maxiters = 5000)\n\nyprednoise = prediction(resprednoise.u)\nplot!(tsteps, yprednoise, label=\"Prediction model with noisy measurements\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"resprednoise.u","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"This example has illustrated basic use of the prediction-error method for parameter estimation. In our example, the measurement we had corresponded directly to one of the states, and coming up with an observer/predictor that worked was not too hard. For more difficult cases, we may opt to use a nonlinear observer, such as an extended Kalman filter (EKF) or design a Kalman filter based on a linearization of the system around some operating point.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"As a last note, there are several other methods available to improve the loss landscape and avoid local minima, such as multiple-shooting. The prediction-error method can easily be combined with most of those methods. ","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"References:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"[Ljung]: Ljung, Lennart. \"System identification–-Theory for the user\".","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/prediction_error_method/","page":"Prediction error method (PEM)","title":"Prediction error method (PEM)","text":"[Larsson]: Larsson, Roger, et al. \"Direct prediction-error identification of unstable nonlinear systems applied to flight test data.\"","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Controlling-Stochastic-Differential-Equations","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"In this tutorial, we show how to use DiffEqFlux to control the time evolution of a system described by a stochastic differential equations (SDE). Specifically, we consider a continuously monitored qubit described by an SDE in the Ito sense with multiplicative scalar noise (see [1] for a reference):","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"dψ = b(ψ(t) Ω(t))ψ(t) dt + σ(ψ(t))ψ(t) dW_t ","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use a predictive model to map the quantum state of the qubit, ψ(t), at each time to the control parameter Ω(t) which rotates the quantum state about the x-axis of the Bloch sphere to ultimately prepare and stabilize the qubit in the excited state.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Copy-Pasteable-Code","page":"Controlling Stochastic Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"Before getting to the explanation, here's some code to start with. We will follow a full explanation of the definition and training process:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# load packages\nusing DiffEqFlux\nusing SciMLSensitivity\nusing Optimization\nusing StochasticDiffEq, DiffEqCallbacks, DiffEqNoiseProcess\nusing Statistics, LinearAlgebra, Random\nusing Plots\n\n\n#################################################\nlr = 0.01f0\nepochs = 100\n\nnumtraj = 16 # number of trajectories in parallel simulations for training\nnumtrajplot = 32 # .. for plotting\n\n# time range for the solver\ndt = 0.0005f0\ntinterval = 0.05f0\ntstart = 0.0f0\nNintervals = 20 # total number of intervals, total time = t_interval*Nintervals\ntspan = (tstart,tinterval*Nintervals)\nts = Array(tstart:dt:(Nintervals*tinterval+dt)) # time array for noise grid\n\n# Hamiltonian parameters\nΔ = 20.0f0\nΩmax = 10.0f0 # control parameter (maximum amplitude)\nκ = 0.3f0\n\n# loss hyperparameters\nC1 = Float32(1.0)  # evolution state fidelity\n\nstruct Parameters{flType,intType,tType}\n  lr::flType\n  epochs::intType\n  numtraj::intType\n  numtrajplot::intType\n  dt::flType\n  tinterval::flType\n  tspan::tType\n  Nintervals::intType\n  ts::Vector{flType}\n  Δ::flType\n  Ωmax::flType\n  κ::flType\n  C1::flType\nend\n\nmyparameters = Parameters{typeof(dt),typeof(numtraj), typeof(tspan)}(\n  lr, epochs, numtraj, numtrajplot, dt, tinterval, tspan, Nintervals, ts,\n  Δ, Ωmax, κ, C1)\n\n################################################\n# Define Neural Network\n\n# state-aware\nnn = FastChain(\n  FastDense(4, 32, relu),\n  FastDense(32, 1, tanh))\n\np_nn = initial_params(nn) # random initial parameters\n\n\n###############################################\n# initial state anywhere on the Bloch sphere\nfunction prepare_initial(dt, n_par)\n  # shape 4 x n_par\n  # input number of parallel realizations and dt for type inference\n  # random position on the Bloch sphere\n  theta = acos.(2*rand(typeof(dt),n_par).-1)  # uniform sampling for cos(theta) between -1 and 1\n  phi = rand(typeof(dt),n_par)*2*pi  # uniform sampling for phi between 0 and 2pi\n  # real and imaginary parts ceR, cdR, ceI, cdI\n  u0 = [cos.(theta/2), sin.(theta/2).*cos.(phi), false*theta, sin.(theta/2).*sin.(phi)]\n  return vcat(transpose.(u0)...) # build matrix\nend\n\n# target state\n# ψtar = |up>\n\nu0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n\n###############################################\n# Define SDE\n\nfunction qubit_drift!(du,u,p,t)\n  # expansion coefficients |Ψ> = ce |e> + cd |d>\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  # Δ: atomic frequency\n  # Ω: Rabi frequency for field in x direction\n  # κ: spontaneous emission\n  Δ, Ωmax, κ = p[end-2:end]\n  nn_weights = p[1:end-3]\n  Ω = (nn(u, nn_weights).*Ωmax)[1]\n\n  @inbounds begin\n    du[1] = 1//2*(ceI*Δ-ceR*κ+cdI*Ω)\n    du[2] = -cdI*Δ/2 + 1*ceR*(cdI*ceI+cdR*ceR)*κ+ceI*Ω/2\n    du[3] = 1//2*(-ceR*Δ-ceI*κ-cdR*Ω)\n    du[4] = cdR*Δ/2 + 1*ceI*(cdI*ceI+cdR*ceR)*κ-ceR*Ω/2\n  end\n  return nothing\nend\n\nfunction qubit_diffusion!(du,u,p,t)\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  κ = p[end]\n\n  du .= false\n\n  @inbounds begin\n    #du[1] = zero(ceR)\n    du[2] += sqrt(κ)*ceR\n    #du[3] = zero(ceR)\n    du[4] += sqrt(κ)*ceI\n  end\n  return nothing\nend\n\n# normalization callback\ncondition(u,t,integrator) = true\nfunction affect!(integrator)\n  integrator.u .= integrator.u/norm(integrator.u)\nend\ncallback = DiscreteCallback(condition, affect!, save_positions=(false, false))\n\nCreateGrid(t,W1) = NoiseGrid(t,W1)\nZygote.@nograd CreateGrid #avoid taking grads of this function\n\n# set scalar random process\nW = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\nW1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\nNG = CreateGrid(myparameters.ts,W1)\n\n# get control pulses\np_all = [p_nn; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n# define SDE problem\nprob = SDEProblem{true}(qubit_drift!, qubit_diffusion!, vec(u0[:,1]), myparameters.tspan, p_all,\n   callback=callback, noise=NG\n   )\n\n#########################################\n# compute loss\nfunction g(u,p,t)\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  p[1]*mean((cdR.^2 + cdI.^2) ./ (ceR.^2 + cdR.^2 + ceI.^2 + cdI.^2))\nend\n\n\nfunction loss(p; alg=EM(), sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP()))\n\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n  u0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:, i]))\n    W = sqrt(myparameters.dt) * randn(typeof(myparameters.dt), size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts, W1)\n\n    remake(prob,\n      p=pars,\n      u0=u0tmp,\n      callback=callback,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n    prob_func=prob_func,\n    safetycopy=true\n  )\n\n  _sol = solve(ensembleprob, alg, EnsembleThreads(),\n    sensealg=sensealg,\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false,\n    trajectories=myparameters.numtraj, batch_size=myparameters.numtraj)\n  A = convert(Array, _sol)\n\n  l = g(A, [myparameters.C1], nothing)\n  # returns loss value\n  return l\nend\n\n#########################################\n# visualization -- run for new batch\nfunction visualize(p; alg=EM())\n\n  u0 = prepare_initial(myparameters.dt, myparameters.numtrajplot)\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:, i]))\n    W = sqrt(myparameters.dt) * randn(typeof(myparameters.dt), size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts, W1)\n\n    remake(prob,\n      p=pars,\n      u0=u0tmp,\n      callback=callback,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n    prob_func=prob_func,\n    safetycopy=true\n  )\n\n  u = solve(ensembleprob, alg, EnsembleThreads(),\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false, #abstol=1e-6, reltol=1e-6,\n    trajectories=myparameters.numtrajplot, batch_size=myparameters.numtrajplot)\n\n\n  ceR = @view u[1, :, :]\n  cdR = @view u[2, :, :]\n  ceI = @view u[3, :, :]\n  cdI = @view u[4, :, :]\n  infidelity = @. (cdR^2 + cdI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n  meaninfidelity = mean(infidelity)\n  loss = myparameters.C1 * meaninfidelity\n\n  @info \"Loss: \" loss\n\n  fidelity = @. (ceR^2 + ceI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n\n  mf = mean(fidelity, dims=2)[:]\n  sf = std(fidelity, dims=2)[:]\n\n  pl1 = plot(0:myparameters.Nintervals, mf,\n    ribbon=sf,\n    ylim=(0, 1), xlim=(0, myparameters.Nintervals),\n    c=1, lw=1.5, xlabel=\"steps i\", ylabel=\"Fidelity\", legend=false)\n\n  pl = plot(pl1, legend=false, size=(400, 360))\n  return pl, loss\nend\n\n# burn-in loss\nl = loss(p_nn)\n# callback to visualize training\nvisualization_callback = function (p, l; doplot=false)\n  println(l)\n\n  if doplot\n    pl, _ = visualize(p)\n    display(pl)\n  end\n\n  return false\nend\n\n# Display the ODE with the initial parameter values.\nvisualization_callback(p_nn, l; doplot=true)\n\n\n###################################\n# training loop\n@info \"Start Training..\"\n\n# optimize the parameters for a few epochs with ADAM on time span\n# Setup and run the optimization\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, p_nn)\nres = Optimization.solve(optprob, ADAM(myparameters.lr), callback=visualization_callback, maxiters=100)\n\n# plot optimized control\nvisualization_callback(res.u, loss(res.u); doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Step-by-step-description","page":"Controlling Stochastic Differential Equations","title":"Step-by-step description","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Load-packages","page":"Controlling Stochastic Differential Equations","title":"Load packages","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"using DiffEqFlux\nusing SciMLSensitivity\nusing Optimization\nusing StochasticDiffEq, DiffEqCallbacks, DiffEqNoiseProcess\nusing Statistics, LinearAlgebra, Random\nusing Plots","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Parameters","page":"Controlling Stochastic Differential Equations","title":"Parameters","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We define the parameters of the qubit and hyper-parameters of the training process.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"lr = 0.01f0\nepochs = 100\n\nnumtraj = 16 # number of trajectories in parallel simulations for training\nnumtrajplot = 32 # .. for plotting\n\n# time range for the solver\ndt = 0.0005f0\ntinterval = 0.05f0\ntstart = 0.0f0\nNintervals = 20 # total number of intervals, total time = t_interval*Nintervals\ntspan = (tstart,tinterval*Nintervals)\nts = Array(tstart:dt:(Nintervals*tinterval+dt)) # time array for noise grid\n\n# Hamiltonian parameters\nΔ = 20.0f0\nΩmax = 10.0f0 # control parameter (maximum amplitude)\nκ = 0.3f0\n\n# loss hyperparameters\nC1 = Float32(1.0)  # evolution state fidelity\n\nstruct Parameters{flType,intType,tType}\n  lr::flType\n  epochs::intType\n  numtraj::intType\n  numtrajplot::intType\n  dt::flType\n  tinterval::flType\n  tspan::tType\n  Nintervals::intType\n  ts::Vector{flType}\n  Δ::flType\n  Ωmax::flType\n  κ::flType\n  C1::flType\nend\n\nmyparameters = Parameters{typeof(dt),typeof(numtraj), typeof(tspan)}(\n  lr, epochs, numtraj, numtrajplot, dt, tinterval, tspan, Nintervals, ts,\n  Δ, Ωmax, κ, C1)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"In plain terms, the quantities that were defined are:","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"lr = learning rate of the optimizer\nepochs = number of epochs in the training process\nnumtraj = number of simulated trajectories in the training process\nnumtrajplot = number of simulated trajectories to visualize the performance\ndt = time step for solver (initial dt if adaptive)\ntinterval = time spacing between checkpoints\ntspan = time span\nNintervals = number of checkpoints\nts = discretization of the entire time interval, used for NoiseGrid\nΔ = detuning between the qubit and the laser\nΩmax = maximum frequency of the control laser\nκ = decay rate\nC1 = loss function hyper-parameter","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Controller","page":"Controlling Stochastic Differential Equations","title":"Controller","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use a neural network to control the parameter Ω(t). Alternatively, one could also, e.g., use tensor layers, Flux.jl, or Lux.jl.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# state-aware\nnn = FastChain(\n  FastDense(4, 32, relu),\n  FastDense(32, 1, tanh))\n\np_nn = initial_params(nn) # random initial parameters","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Initial-state","page":"Controlling Stochastic Differential Equations","title":"Initial state","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We prepare n_par initial states, uniformly distributed over the Bloch sphere. To avoid complex numbers in our simulations, we split the state of the qubit","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"  ψ(t) = c_e(t) (10) + c_d(t) (01)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"into its real and imaginary part.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# initial state anywhere on the Bloch sphere\nfunction prepare_initial(dt, n_par)\n  # shape 4 x n_par\n  # input number of parallel realizations and dt for type inference\n  # random position on the Bloch sphere\n  theta = acos.(2*rand(typeof(dt),n_par).-1)  # uniform sampling for cos(theta) between -1 and 1\n  phi = rand(typeof(dt),n_par)*2*pi  # uniform sampling for phi between 0 and 2pi\n  # real and imaginary parts ceR, cdR, ceI, cdI\n  u0 = [cos.(theta/2), sin.(theta/2).*cos.(phi), false*theta, sin.(theta/2).*sin.(phi)]\n  return vcat(transpose.(u0)...) # build matrix\nend\n\n# target state\n# ψtar = |e>\n\nu0 = prepare_initial(myparameters.dt, myparameters.numtraj)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Defining-the-SDE","page":"Controlling Stochastic Differential Equations","title":"Defining the SDE","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We define the drift and diffusion term of the qubit. The SDE doesn't preserve the norm of the quantum state. To ensure the normalization of the state, we add a DiscreteCallback after each time step. Further, we use a NoiseGrid from the DiffEqNoiseProcess package, as one possibility to simulate a 1D Brownian motion. Note that the NN is placed directly into the drift function, thus the control parameter Ω is continuously updated.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# Define SDE\nfunction qubit_drift!(du,u,p,t)\n  # expansion coefficients |Ψ> = ce |e> + cd |d>\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  # Δ: atomic frequency\n  # Ω: Rabi frequency for field in x direction\n  # κ: spontaneous emission\n  Δ, Ωmax, κ = p[end-2:end]\n  nn_weights = p[1:end-3]\n  Ω = (nn(u, nn_weights).*Ωmax)[1]\n\n  @inbounds begin\n    du[1] = 1//2*(ceI*Δ-ceR*κ+cdI*Ω)\n    du[2] = -cdI*Δ/2 + 1*ceR*(cdI*ceI+cdR*ceR)*κ+ceI*Ω/2\n    du[3] = 1//2*(-ceR*Δ-ceI*κ-cdR*Ω)\n    du[4] = cdR*Δ/2 + 1*ceI*(cdI*ceI+cdR*ceR)*κ-ceR*Ω/2\n  end\n  return nothing\nend\n\nfunction qubit_diffusion!(du,u,p,t)\n  ceR, cdR, ceI, cdI = u # real and imaginary parts\n\n  κ = p[end]\n\n  du .= false\n\n  @inbounds begin\n    #du[1] = zero(ceR)\n    du[2] += sqrt(κ)*ceR\n    #du[3] = zero(ceR)\n    du[4] += sqrt(κ)*ceI\n  end\n  return nothing\nend\n\n# normalization callback\ncondition(u,t,integrator) = true\nfunction affect!(integrator)\n  integrator.u.=integrator.u/norm(integrator.u)\nend\ncallback = DiscreteCallback(condition,affect!,save_positions=(false,false))\n\nCreateGrid(t,W1) = NoiseGrid(t,W1)\nZygote.@nograd CreateGrid #avoid taking grads of this function\n\n# set scalar random process\nW = sqrt(myparameters.dt)*randn(typeof(myparameters.dt),size(myparameters.ts)) #for 1 trajectory\nW1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\nNG = CreateGrid(myparameters.ts,W1)\n\n# get control pulses\np_all = [p_nn; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n# define SDE problem\nprob = SDEProblem{true}(qubit_drift!, qubit_diffusion!, vec(u0[:,1]), myparameters.tspan, p_all,\n   callback=callback, noise=NG\n   )","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Compute-loss-function","page":"Controlling Stochastic Differential Equations","title":"Compute loss function","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We'd like to prepare the excited state of the qubit. An appropriate choice for the loss function is the infidelity of the state ψ(t) with respect to the excited state. We create a parallelized EnsembleProblem, where the prob_func creates a new NoiseGrid for every trajectory and loops over the initial states. The number of parallel trajectories and the used batch size can be tuned by the kwargs trajectories=.. and batchsize=.. in the solve call. See also the parallel ensemble simulation docs for a description of the available ensemble algorithms. To optimize only the parameters of the neural network, we use pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# compute loss\nfunction g(u,p,t)\n  ceR = @view u[1,:,:]\n  cdR = @view u[2,:,:]\n  ceI = @view u[3,:,:]\n  cdI = @view u[4,:,:]\n  p[1]*mean((cdR.^2 + cdI.^2) ./ (ceR.^2 + cdR.^2 + ceI.^2 + cdI.^2))\nend\n\nfunction loss(p; alg=EM(), sensealg=BacksolveAdjoint(autojacvec=ReverseDiffVJP()))\n\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n  u0 = prepare_initial(myparameters.dt, myparameters.numtraj)\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:, i]))\n    W = sqrt(myparameters.dt) * randn(typeof(myparameters.dt), size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts, W1)\n\n    remake(prob,\n      p=pars,\n      u0=u0tmp,\n      callback=callback,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n    prob_func=prob_func,\n    safetycopy=true\n  )\n\n  _sol = solve(ensembleprob, alg, EnsembleThreads(),\n    sensealg=sensealg,\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false,\n    trajectories=myparameters.numtraj, batch_size=myparameters.numtraj)\n  A = convert(Array, _sol)\n\n  l = g(A, [myparameters.C1], nothing)\n  # returns loss value\n  return l\nend","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Visualization","page":"Controlling Stochastic Differential Equations","title":"Visualization","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"To visualize the performance of the controller, we plot the mean value and standard deviation of the fidelity of a bunch of trajectories (myparameters.numtrajplot) as a function of the time steps at which loss values are computed.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"function visualize(p; alg=EM())\n\n  u0 = prepare_initial(myparameters.dt, myparameters.numtrajplot)\n  pars = [p; myparameters.Δ; myparameters.Ωmax; myparameters.κ]\n\n  function prob_func(prob, i, repeat)\n    # prepare initial state and applied control pulse\n    u0tmp = deepcopy(vec(u0[:, i]))\n    W = sqrt(myparameters.dt) * randn(typeof(myparameters.dt), size(myparameters.ts)) #for 1 trajectory\n    W1 = cumsum([zero(myparameters.dt); W[1:end-1]], dims=1)\n    NG = CreateGrid(myparameters.ts, W1)\n\n    remake(prob,\n      p=pars,\n      u0=u0tmp,\n      callback=callback,\n      noise=NG)\n  end\n\n  ensembleprob = EnsembleProblem(prob,\n    prob_func=prob_func,\n    safetycopy=true\n  )\n\n  u = solve(ensembleprob, alg, EnsembleThreads(),\n    saveat=myparameters.tinterval,\n    dt=myparameters.dt,\n    adaptive=false, #abstol=1e-6, reltol=1e-6,\n    trajectories=myparameters.numtrajplot, batch_size=myparameters.numtrajplot)\n\n\n  ceR = @view u[1, :, :]\n  cdR = @view u[2, :, :]\n  ceI = @view u[3, :, :]\n  cdI = @view u[4, :, :]\n  infidelity = @. (cdR^2 + cdI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n  meaninfidelity = mean(infidelity)\n  loss = myparameters.C1 * meaninfidelity\n\n  @info \"Loss: \" loss\n\n  fidelity = @. (ceR^2 + ceI^2) / (ceR^2 + cdR^2 + ceI^2 + cdI^2)\n\n  mf = mean(fidelity, dims=2)[:]\n  sf = std(fidelity, dims=2)[:]\n\n  pl1 = plot(0:myparameters.Nintervals, mf,\n    ribbon=sf,\n    ylim=(0, 1), xlim=(0, myparameters.Nintervals),\n    c=1, lw=1.5, xlabel=\"steps i\", ylabel=\"Fidelity\", legend=false)\n\n  pl = plot(pl1, legend=false, size=(400, 360))\n  return pl, loss\nend\n# callback to visualize training\nvisualization_callback = function (p, l; doplot=false)\n  println(l)\n\n  if doplot\n    pl, _ = visualize(p)\n    display(pl)\n  end\n\n  return false\nend","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#Training","page":"Controlling Stochastic Differential Equations","title":"Training","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"We use the ADAM optimizer to optimize the parameters of the neural network. In each epoch, we draw new initial quantum states, compute the forward evolution, and, subsequently, the gradients of the loss function with respect to the parameters of the neural network. sensealg allows one to switch between the different sensitivity modes. InterpolatingAdjoint and BacksolveAdjoint are the two possible continuous adjoint sensitivity methods. The necessary correction between Ito and Stratonovich integrals is computed under the hood in the SciMLSensitivity package.","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"# optimize the parameters for a few epochs with ADAM on time span\n# Setup and run the optimization\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\n\noptprob = Optimization.OptimizationProblem(optf, p_nn)\nres = Optimization.solve(optprob, ADAM(myparameters.lr), callback=visualization_callback, maxiters=100)\n\n# plot optimized control\nvisualization_callback(res.u, loss(res.u); doplot=true)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"(Image: Evolution of the fidelity as a function of time)","category":"page"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/#References","page":"Controlling Stochastic Differential Equations","title":"References","text":"","category":"section"},{"location":"modules/SciMLSensitivity/optimal_control/SDE_control/","page":"Controlling Stochastic Differential Equations","title":"Controlling Stochastic Differential Equations","text":"[1] Schäfer, Frank, Pavel Sekatski, Martin Koppenhöfer, Christoph Bruder, and Michal Kloc. \"Control of stochastic quantum dynamics by differentiable programming.\" Machine Learning: Science and Technology 2, no. 3 (2021): 035004.","category":"page"},{"location":"modules/Optimization/tutorials/rosenbrock/#Solving-the-Rosenbrock-Problem-in-10-Ways","page":"Solving the Rosenbrock Problem in >10 Ways","title":"Solving the Rosenbrock Problem in >10 Ways","text":"","category":"section"},{"location":"modules/Optimization/tutorials/rosenbrock/","page":"Solving the Rosenbrock Problem in >10 Ways","title":"Solving the Rosenbrock Problem in >10 Ways","text":"This tutorial is a demonstration of many different solvers to demonstrate the flexibility of Optimization.jl. This is a gauntlet of many solvers to get a feel for common workflows of the package and give copy-pastable starting points.","category":"page"},{"location":"modules/Optimization/tutorials/rosenbrock/","page":"Solving the Rosenbrock Problem in >10 Ways","title":"Solving the Rosenbrock Problem in >10 Ways","text":"note: Note\nThis example uses many different solvers of Optimization.jl. Each solver subpackage needs to be installed separate. For example, for the details on  the installation and usage of OptimizationOptimJL.jl package, see the  Optim.jl page.","category":"page"},{"location":"modules/Optimization/tutorials/rosenbrock/","page":"Solving the Rosenbrock Problem in >10 Ways","title":"Solving the Rosenbrock Problem in >10 Ways","text":"# Define the problem to solve\nusing Optimization, ForwardDiff, Zygote, Test, Random\n\nrosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p  = [1.0, 100.0]\n\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nl1 = rosenbrock(x0, _p)\nprob = OptimizationProblem(f, x0, _p)\n\n## Optim.jl Solvers\n\nusing OptimizationOptimJL\n\n# Start with some derivative-free optimizers\n\nsol = solve(prob, SimulatedAnnealing())\nprob = OptimizationProblem(f, x0, _p, lb=[-1.0, -1.0], ub=[0.8, 0.8])\nsol = solve(prob, SAMIN())\n\nl1 = rosenbrock(x0)\nprob = OptimizationProblem(rosenbrock, x0)\nsol = solve(prob, NelderMead())\n\n# Now a gradient-based optimizer with forward-mode automatic differentiation\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= cons)\nprob = OptimizationProblem(optf, x0)\nsol = solve(prob, BFGS())\n\n# Now a second order optimizer using Hessians generated by forward-mode automatic differentiation\n\nsol = solve(prob, Newton())\n\n# Now a second order Hessian-free optimizer\n\nsol = solve(prob, Optim.KrylovTrustRegion())\n\n# Now derivative-based optimizers with various constraints\n\ncons = (x,p) -> [x[1]^2 + x[2]^2]\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= cons)\nprob = OptimizationProblem(optf, x0)\nsol = solve(prob, IPNewton())\n\nprob = OptimizationProblem(optf, x0, lcons = [-Inf], ucons = [Inf])\nsol = solve(prob, IPNewton())\n\nprob = OptimizationProblem(optf, x0, lcons = [-5.0], ucons = [10.0])\nsol = solve(prob, IPNewton())\n\nprob = OptimizationProblem(optf, x0, lcons = [-Inf], ucons = [Inf], \n                           lb = [-500.0,-500.0], ub=[50.0,50.0])\nsol = solve(prob, IPNewton())\n\nfunction con2_c(x,p)\n    [x[1]^2 + x[2]^2, x[2]*sin(x[1])-x[1]]\nend\n\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= con2_c)\nprob = OptimizationProblem(optf, x0, lcons = [-Inf,-Inf], ucons = [Inf,Inf])\nsol = solve(prob, IPNewton())\n\ncons_circ = (x,p) -> [x[1]^2 + x[2]^2]\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff();cons= cons_circ)\nprob = OptimizationProblem(optf, x0, lcons = [-Inf], ucons = [0.25^2])\nsol = solve(prob, IPNewton())\n\n# Now let's switch over to OptimizationOptimisers with reverse-mode AD\n\nusing OptimizationOptimisers\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, x0)\nsol = solve(prob, Adam(0.05), maxiters = 1000, progress = false)\n\n## Try out CMAEvolutionStrategy.jl's evolutionary methods\n\nusing OptimizationCMAEvolutionStrategy\nsol = solve(prob, CMAEvolutionStrategyOpt())\n\n## Now try a few NLopt.jl solvers with symbolic differentiation via ModelingToolkit.jl\n\nusing OptimizationNLopt, ModelingToolkit\noptf = OptimizationFunction(rosenbrock, Optimization.AutoModelingToolkit())\nprob = OptimizationProblem(optf, x0)\n\nsol = solve(prob, Opt(:LN_BOBYQA, 2))\nsol = solve(prob, Opt(:LD_LBFGS, 2))\n\n## Add some box constarints and solve with a few NLopt.jl methods\n\nprob = OptimizationProblem(optf, x0, lb=[-1.0, -1.0], ub=[0.8, 0.8])\nsol = solve(prob, Opt(:LD_LBFGS, 2))\nsol = solve(prob, Opt(:G_MLSL_LDS, 2), nstart=2, local_method = Opt(:LD_LBFGS, 2), maxiters=10000)\n\n## Evolutionary.jl Solvers\n\nusing OptimizationEvolutionary\nsol = solve(prob, CMAES(μ =40 , λ = 100),abstol=1e-15)\n\n## BlackBoxOptim.jl Solvers\n\nusing OptimizationBBO\nprob = Optimization.OptimizationProblem(rosenbrock, x0, lb=[-1.0, -1.0], ub=[0.8, 0.8])\nsol = solve(prob, BBO())","category":"page"},{"location":"modules/Optimization/tutorials/rosenbrock/","page":"Solving the Rosenbrock Problem in >10 Ways","title":"Solving the Rosenbrock Problem in >10 Ways","text":"And this is only a small subset of what Optimization.jl has to offer!","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/#Linear-Solve-with-Caching-Interface","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"","category":"section"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"In many cases one may want to cache information that is reused between different linear solves. For example, if one is going to perform:","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A\\b1\nA\\b2","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"then it would be more efficient to LU-factorize one time and reuse the factorization:","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"lu!(A)\nA\\b1\nA\\b2","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"LinearSolve.jl's caching interface automates this process to use the most efficient means of solving and resolving linear systems. To do this with LinearSolve.jl, you simply init a cache, solve, replace b, and solve again. This looks like:","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"using LinearSolve\n\nn = 4\nA = rand(n,n)\nb1 = rand(n); b2 = rand(n)\nprob = LinearProblem(A, b1)\n\nlinsolve = init(prob)\nsol1 = solve(linsolve)\n\nsol1.u\n#=\n4-element Vector{Float64}:\n -0.9247817429364165\n -0.0972021708185121\n  0.6839050402960025\n  1.8385599677530706\n=#\n\nlinsolve = LinearSolve.set_b(sol1.cache,b2)\nsol2 = solve(linsolve)\n\nsol2.u\n#=\n4-element Vector{Float64}:\n  1.0321556637762768\n  0.49724400693338083\n -1.1696540870182406\n -0.4998342686003478\n=#","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"Then refactorization will occur when a new A is given:","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"A2 = rand(n,n)\nlinsolve = LinearSolve.set_A(sol2.cache,A2)\nsol3 = solve(linsolve)\n\nsol3.u\n#=\n4-element Vector{Float64}:\n -6.793605395935224\n  2.8673042300837466\n  1.1665136934977371\n -0.4097250749016653\n=#","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The factorization occurs on the first solve, and it stores the factorization in the cache. You can retrieve this cache via sol.cache, which is the same object as the init but updated to know not to re-solve the factorization.","category":"page"},{"location":"modules/LinearSolve/tutorials/caching_interface/","page":"Linear Solve with Caching Interface","title":"Linear Solve with Caching Interface","text":"The advantage of course with using LinearSolve.jl in this form is that it is efficient while being agnostic to the linear solver. One can easily swap in iterative solvers, sparse solvers, etc. and it will do all of the tricks like caching symbolic factorizations if the sparsity pattern is unchanged.","category":"page"},{"location":"modules/HighDimPDE/getting_started/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/#General-workflow","page":"Getting started","title":"General workflow","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"The general workflow for using HighDimPDE.jl is as follows:","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Define a Partial Integro Differential Equation problem","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Modules = [HighDimPDE]\nPages   = [\"HighDimPDE.jl\"]","category":"page"},{"location":"modules/HighDimPDE/getting_started/#HighDimPDE.HighDimPDE","page":"Getting started","title":"HighDimPDE.HighDimPDE","text":"(Image: ) (Image: ) (Image: Build Status)\n\nHighDimPDE.jl\n\nHighDimPDE.jl is a Julia package to solve Highly Dimensional non-local, non-linear PDEs of the form\n\n$\n\n\\begin{aligned}     (\\partialt u)(t,x) &= \\int{\\Omega} f\\big(t,x,{\\bf x}, u(t,x),u(t,{\\bf x}), ( \\nablax u )(t,x ),( \\nablax u )(t,{\\bf x} ) \\big) d{\\bf x} \\\n    & \\quad + \\big\\langle \\mu(t,x), ( \\nablax u )( t,x ) \\big\\rangle + \\tfrac{1}{2} \\text{Trace} \\big(\\sigma(t,x) [ \\sigma(t,x) ]^* ( \\text{Hess}x u)(t, x ) \\big). \\end{aligned} $\n\nwhere u colon 0T times Omega to mathbbR Omega subseteq mathbbR^d is subject to initial and boundary conditions, and where d is large.\n\nDocumentation\n\n(Image: ) (Image: )\n\nInstallation\n\nOpen Julia and type the following\n\nusing Pkg;\nPkg.add(\"HighDimPDE.jl\")\n\nThis will download the latest version from the git repo and download all dependencies.\n\nGetting started\n\nSee documentation and test folders.\n\nReference\n\nBoussange, V., Becker, S., Jentzen, A., Kuckuck, B., Pellissier, L., Deep learning approximations for non-local nonlinear PDEs with Neumann boundary conditions. arXiv (2022)\n\n<!– - Becker, S., Braunwarth, R., Hutzenthaler, M., Jentzen, A., von Wurstemberger, P., Numerical simulations for full history recursive multilevel Picard approximations for systems of high-dimensional partial differential equations. arXiv (2020)\n\nBeck, C., Becker, S., Cheridito, P., Jentzen, A., Neufeld, A., Deep splitting method for parabolic PDEs. arXiv (2019)\nHan, J., Jentzen, A., E, W., Solving high-dimensional partial differential equations using deep learning. arXiv (2018) –>\n\n<!– ## Acknowledgements HighDimPDE.jl is inspired from Sebastian Becker's scripts in Python, TensorFlow and C++. Pr. Arnulf Jentzen largely contributed to the theoretical developments of the solver algorithms implemented. –>\n\n\n\n\n\n","category":"module"},{"location":"modules/HighDimPDE/getting_started/#HighDimPDE.PIDEProblem","page":"Getting started","title":"HighDimPDE.PIDEProblem","text":"PIDEProblem(g, f, μ, σ, x, tspan, p = nothing, x0_sample=nothing, neumann_bc=nothing)\n\nDefines a Partial Integro Differential Problem, of the form \n\nbeginaligned\n    fracdudt = tfrac12 textTr(sigma sigma^T) Delta u(x t) + mu nabla u(x t)  \n    quad + int f(x y u(x t) u(y t) ( nabla_x u )(x t) ( nabla_x u )(y t) p t) dy\nendaligned\n\nwith u(x0) = g(x).\n\nArguments\n\ng : initial condition, of the form g(x, p, t).\nf : nonlinear function, of the form f(x, y, u(x, t), u(y, t), ∇u(x, t), ∇u(x, t), p, t).\nμ : drift function, of the form μ(x, p, t).\nσ : diffusion function σ(x, p, t).\nx: point where u(x,t) is approximated. Is required even in the case where x0_sample is provided.\ntspan: timespan of the problem.\np: the parameter vector.\nx0_sample : sampling method for x0. Can be UniformSampling(a,b), NormalSampling(σ_sampling, shifted), or NoSampling (by default). If NoSampling, only solution at the single point x is evaluated.\nneumann_bc: if provided, neumann boundary conditions on the hypercube neumann_bc[1] × neumann_bc[2]. \n\n\n\n\n\n","category":"type"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Select a solver algorithm\nSolve the problem","category":"page"},{"location":"modules/HighDimPDE/getting_started/#Examples","page":"Getting started","title":"Examples","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Let's illustrate that with some examples.","category":"page"},{"location":"modules/HighDimPDE/getting_started/#MLP","page":"Getting started","title":"MLP","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/#Local-PDE","page":"Getting started","title":"Local PDE","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the Fisher KPP PDE in dimension 10 with MLP.","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - u) + frac12sigma^2Delta_xu tag1","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0,0.5) # time horizon\nx0 = fill(0.,d)  # initial point\ng(x) = exp(- sum(x.^2) ) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = max(0.0, v_x) * (1 -  max(0.0, v_x)) # nonlocal nonlinear part of the\nprob = PIDEProblem(g, f, μ, σ, x0, tspan) # defining the problem\n\n## Definition of the algorithm\nalg = MLP() # defining the algorithm. We use the Multi Level Picard algorithm\n\n## Solving with multiple threads \nsol = solve(prob, alg, multithreading=true)","category":"page"},{"location":"modules/HighDimPDE/getting_started/#Non-local-PDE-with-Neumann-boundary-conditions","page":"Getting started","title":"Non local PDE with Neumann boundary conditions","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Let's include in the previous equation non local competition, i.e.","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"partial_t u = u (1 - int_Omega u(ty)dy) + frac12sigma^2Delta_xu tag2","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"where Omega = -12 12^d, and let's assume Neumann Boundary condition on Omega.","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0,0.5) # time horizon\nx0 = fill(0.,d)  # initial point\ng(x) = exp( -sum(x.^2) ) # initial condition\nμ(x, p, t) = 0.0 # advection coefficients\nσ(x, p, t) = 0.1 # diffusion coefficients\nmc_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, t) = max(0.0, v_x) * (1 -  max(0.0, v_y)) \nprob = PIDEProblem(g, f, μ, \n                    σ, x0, tspan) # defining x0_sample is sufficient to implement Neumann boundary conditions\n\n## Definition of the algorithm\nalg = MLP(mc_sample = mc_sample ) \n\nsol = solve(prob, alg, multithreading=true)","category":"page"},{"location":"modules/HighDimPDE/getting_started/#DeepSplitting","page":"Getting started","title":"DeepSplitting","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the previous equation with DeepSplitting.","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"using HighDimPDE\n\n## Definition of the problem\nd = 10 # dimension of the problem\ntspan = (0.0, 0.5) # time horizon\nx0 = fill(0f0,d)  # initial point\ng(x) = exp.(- sum(x.^2, dims=1) ) # initial condition\nμ(x, p, t) = 0.0f0 # advection coefficients\nσ(x, p, t) = 0.1f0 # diffusion coefficients\nx0_sample = UniformSampling(fill(-5f-1, d), fill(5f-1, d))\nf(x, y, v_x, v_y, ∇v_x, ∇v_y, p, t) = v_x .* (1f0 .- v_y)\nprob = PIDEProblem(g, f, μ, \n                    σ, x0, tspan, \n                    x0_sample = x0_sample)\n\n## Definition of the neural network to use\nusing Flux # needed to define the neural network\n\nhls = d + 50 #hidden layer size\n\nnn = Flux.Chain(Dense(d, hls, tanh),\n        Dense(hls, hls, tanh),\n        Dense(hls, 1)) # neural network used by the scheme\n\nopt = ADAM(1e-2)\n\n## Definition of the algorithm\nalg = DeepSplitting(nn,\n                    opt = opt,\n                    mc_sample = x0_sample)\n\nsol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000)","category":"page"},{"location":"modules/HighDimPDE/getting_started/#Solving-on-the-GPU","page":"Getting started","title":"Solving on the GPU","text":"","category":"section"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"DeepSplitting can run on the GPU for (much) improved performance. To do so, just set use_cuda = true.","category":"page"},{"location":"modules/HighDimPDE/getting_started/","page":"Getting started","title":"Getting started","text":"sol = solve(prob, \n            alg, \n            0.1, \n            verbose = true, \n            abstol = 2e-3,\n            maxiters = 1000,\n            batch_size = 1000,\n            use_cuda=true)","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#API","page":"API","title":"API","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/#Simulation","page":"API","title":"Simulation","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"NBodySimulation\nrun_simulation","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.NBodySimulation","page":"API","title":"NBodySimulator.NBodySimulation","text":"Simulation is an entity determining parameters of the experiment: time span of simulation, global physical constants, borders of the simulation cell, external magnetic or electric fields, etc. The required arguments for NBodySImulation constructor are the system to be tested and the time span of the simulation.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.run_simulation","page":"API","title":"NBodySimulator.run_simulation","text":"Run the N-body simulation.\n\n\n\n\n\n","category":"function"},{"location":"modules/NBodySimulator/nbodysimulator/#Bodies","page":"API","title":"Bodies","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"NBodySimulator.Body\nMassBody\nChargedParticle\nMagneticParticle\ngenerate_bodies_in_cell_nodes","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.Body","page":"API","title":"NBodySimulator.Body","text":"Bodies or Particles are the objects which will interact with each other and for which the equations of Newton's 2nd law are solved during the simulation process.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.generate_bodies_in_cell_nodes","page":"API","title":"NBodySimulator.generate_bodies_in_cell_nodes","text":"Places similar particles in the nodes of a cubic cell with their velocities distributed in accordance with the Maxwell–Boltzmann law\n\n\n\n\n\n","category":"function"},{"location":"modules/NBodySimulator/nbodysimulator/#Potentials","page":"API","title":"Potentials","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"PotentialParameters\nNBodySimulator.get_accelerating_function\nGravitationalParameters\nMagnetostaticParameters\nElectrostaticParameters\nLennardJonesParameters\nSPCFwParameters","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.PotentialParameters","page":"API","title":"NBodySimulator.PotentialParameters","text":"The potentials or force field determines the interaction of particles and, therefore, their acceleration.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#Thermostats","page":"API","title":"Thermostats","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"NBodySimulator.Thermostat\nNBodySimulator.NullThermostat\nAndersenThermostat\nBerendsenThermostat\nNoseHooverThermostat\nLangevinThermostat","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.Thermostat","page":"API","title":"NBodySimulator.Thermostat","text":"Usually, during the simulation, a system is required to be at a particular temperature. NBodySimulator contains several thermostats for that purpose.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.NullThermostat","page":"API","title":"NBodySimulator.NullThermostat","text":"No thermostat.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#Boundary-Conditions","page":"API","title":"Boundary Conditions","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"CubicPeriodicBoundaryConditions\nPeriodicBoundaryConditions\nInfiniteBox","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#Systems","page":"API","title":"Systems","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"PotentialNBodySystem\nGravitationalSystem\nWaterSPCFw","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.PotentialNBodySystem","page":"API","title":"NBodySimulator.PotentialNBodySystem","text":"Structure that represents systems with a custom set of potentials. In other words, the user determines the ways in which the particles are allowed to interact. One can pass the bodies and parameters of interaction potentials into that system. In the case the potential parameters are not set, the particles will move at constant velocities without acceleration during the simulation.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#Analyze","page":"API","title":"Analyze","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"NBodySimulator.SimulationResult\ntemperature\nrdf\nmsd\ninitial_energy\nkinetic_energy\npotential_energy\ntotal_energy","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.SimulationResult","page":"API","title":"NBodySimulator.SimulationResult","text":"SimulationResult sould provide an interface for working with properties of a separate particle and with physical properties of the whole system.\n\n\n\n\n\n","category":"type"},{"location":"modules/NBodySimulator/nbodysimulator/#Protein-Database-File","page":"API","title":"Protein Database File","text":"","category":"section"},{"location":"modules/NBodySimulator/nbodysimulator/","page":"API","title":"API","text":"load_water_molecules_from_pdb\nsave_to_pdb","category":"page"},{"location":"modules/NBodySimulator/nbodysimulator/#NBodySimulator.load_water_molecules_from_pdb","page":"API","title":"NBodySimulator.load_water_molecules_from_pdb","text":"Molecules for the SPC/Fw water model can be imported from a PDB file.\n\n\n\n\n\n","category":"function"},{"location":"modules/NonlinearSolve/#NonlinearSolve.jl:-High-Performance-Unified-Nonlinear-Solvers","page":"Home","title":"NonlinearSolve.jl: High-Performance Unified Nonlinear Solvers","text":"","category":"section"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"NonlinearSolve.jl is a unified interface for the nonlinear solving packages of Julia. It includes its own high-performance nonlinear solvers which include the ability to swap out to fast direct and iterative linear solvers, along with the ability to use sparse automatic differentiation for Jacobian construction and Jacobian-vector products. It interfaces with other packages of the Julia ecosystem to make it easy to test alternative solver packages and pass small types to control algorithm swapping. It also interfaces with the ModelingToolkit.jl world of symbolic modeling to allow for automatically generating high-performance code.","category":"page"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"Performance is key: the current methods are made to be highly performant on scalar and statically sized small problems, with options for large-scale systems. If you run into any performance issues, please file an issue. Note that this package is distinct from SciMLNLSolve.jl. Consult the NonlinearSystemSolvers page for information on how to import solvers from different packages.","category":"page"},{"location":"modules/NonlinearSolve/#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"To install NonlinearSolve.jl, use the Julia package manager:","category":"page"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"NonlinearSolve\")","category":"page"},{"location":"modules/NonlinearSolve/#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to ModelingToolkit.\nThere are a few community forums:\nthe #diffeq-bridged channel in the Julia Slack\nJuliaDiffEq on Gitter\non the Julia Discourse forums (look for the modelingtoolkit tag\nsee also SciML Community page","category":"page"},{"location":"modules/NonlinearSolve/#Roadmap","page":"Home","title":"Roadmap","text":"","category":"section"},{"location":"modules/NonlinearSolve/","page":"Home","title":"Home","text":"The current algorithms should support automatic differentiation, though improved adjoint overloads are planned to be added in the current update (which will make use of the f(u,p) form). Future updates will include standard methods for larger scale nonlinear solving like Newton-Krylov methods.","category":"page"},{"location":"modules/NeuralPDE/developer/debugging/#Debugging-PINN-Solutions","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"","category":"section"},{"location":"modules/NeuralPDE/developer/debugging/#Note-this-is-all-not-current-right-now!","page":"Debugging PINN Solutions","title":"Note this is all not current right now!","text":"","category":"section"},{"location":"modules/NeuralPDE/developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"Let's walk through debugging functions for the physics-informed neural network PDE solvers.","category":"page"},{"location":"modules/NeuralPDE/developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"using NeuralPDE, ModelingToolkit, Flux, Zygote\nimport ModelingToolkit: Interval, infimum, supremum\n# 2d wave equation, neumann boundary condition\n@parameters x, t\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n#2D PDE\nC=1\neq  = Dtt(u(x,t)) ~ C^2*Dxx(u(x,t))\n\n# Initial and boundary conditions\nbcs = [u(0,t) ~ 0.,\n       u(1,t) ~ 0.,\n       u(x,0) ~ x*(1. - x),\n       Dt(u(x,0)) ~ 0. ]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           t ∈ Interval(0.0,1.0)]\n\n# Neural network\nchain = FastChain(FastDense(2,16,Flux.σ),FastDense(16,16,Flux.σ),FastDense(16,1))\ninit_params = DiffEqFlux.initial_params(chain)\n\neltypeθ = eltype(init_params)\nphi = NeuralPDE.get_phi(chain)\nderivative = NeuralPDE.get_numeric_derivative()\n\nu_ = (cord, θ, phi)->sum(phi(cord, θ))\n\nphi([1,2], init_params)\n\nphi_ = (p) -> phi(p, init_params)[1]\ndphi = Zygote.gradient(phi_,[1.,2.])\n\ndphi1 = derivative(phi,u_,[1.,2.],[[ 0.0049215667, 0.0]],1,init_params)\ndphi2 = derivative(phi,u_,[1.,2.],[[0.0,  0.0049215667]],1,init_params)\nisapprox(dphi[1][1], dphi1, atol=1e-8)\nisapprox(dphi[1][2], dphi2, atol=1e-8)\n\n\nindvars = [x,t]\ndepvars = [u(x, t)]\ndict_depvars_input = Dict(:u => [:x, :t])\ndim = length(domains)\ndx = 0.1\nmultioutput = chain isa AbstractArray\nstrategy = NeuralPDE.GridTraining(dx)\nintegral = NeuralPDE.get_numeric_integral(strategy, indvars, multioutput, chain, derivative)\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq,indvars,depvars,phi,derivative,integral,multioutput,init_params,strategy)\n\njulia> expr_pde_loss_function = NeuralPDE.build_symbolic_loss_function(eq,indvars,depvars,dict_depvars_input,phi,derivative,integral,multioutput,init_params,strategy)\n\n:((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667], [0.0, 0.0049215667]], 2, var\"##θ#529\") .- derivative.(phi, u, cord, Array{Float32,1}[[0.0049215667, 0.0], [0.0049215667, 0.0]], 2, var\"##θ#529\")\n              end\n          end\n      end)\n\njulia> bc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n4-element Array{Array{Any,1},1}:\n [:t]\n [:t]\n [:x]\n [:x]\n\n_bc_loss_functions = [NeuralPDE.build_loss_function(bc,indvars,depvars,\n                                                     phi,derivative,integral,multioutput,init_params,strategy,\n                                                     bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n\njulia> expr_bc_loss_functions = [NeuralPDE.build_symbolic_loss_function(bc,indvars,depvars,dict_depvars_input,\n                                                                        phi,derivative,integral,multioutput,init_params,strategy,\n                                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n4-element Array{Expr,1}:\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- (*).(x, (+).(1.0, (*).(-1, x)))\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667]], 1, var\"##θ#529\") .- 0.0\n              end\n          end\n      end)\n\ntrain_sets = NeuralPDE.generate_training_sets(domains,dx,[eq],bcs,eltypeθ,indvars,depvars)\npde_train_set,bcs_train_set = train_sets\n\njulia> pde_train_set\n1-element Array{Array{Float32,2},1}:\n [0.1 0.2 … 0.8 0.9; 0.1 0.1 … 1.0 1.0]\n\n\njulia> bcs_train_set\n4-element Array{Array{Float32,2},1}:\n [0.0 0.0 … 0.0 0.0; 0.0 0.1 … 0.9 1.0]\n [1.0 1.0 … 1.0 1.0; 0.0 0.1 … 0.9 1.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n\n\npde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains,[eq],bcs,eltypeθ,indvars,depvars,NeuralPDE.StochasticTraining(100))\n\njulia> pde_bounds\n1-element Vector{Vector{Any}}:\n [Float32[0.01, 0.99], Float32[0.01, 0.99]]\n\njulia> bcs_bounds\n4-element Vector{Vector{Any}}:\n [0, Float32[0.0, 1.0]]\n [1, Float32[0.0, 1.0]]\n [Float32[0.0, 1.0], 0]\n [Float32[0.0, 1.0], 0]\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,strategy)\n\n@named pde_system = PDESystem(eq,bcs,domains,indvars,depvars)\nprob = NeuralPDE.discretize(pde_system,discretization)\n\nexpr_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\nexpr_pde_loss_function , expr_bc_loss_functions = expr_prob\n","category":"page"},{"location":"modules/Catalyst/tutorials/basic_examples/#Basic-Chemical-Reaction-Network-Examples","page":"Basic Chemical Reaction Network Examples","title":"Basic Chemical Reaction Network Examples","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/basic_examples/#Example:-Birth-Death-Process","page":"Basic Chemical Reaction Network Examples","title":"Example: Birth-Death Process","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/basic_examples/","page":"Basic Chemical Reaction Network Examples","title":"Basic Chemical Reaction Network Examples","text":"rs = @reaction_network begin\n  c1, X --> 2X\n  c2, X --> 0\n  c3, 0 --> X\nend c1 c2 c3\np = (:c1 => 1.0, :c2 => 2.0, :c3 => 50.) \ntspan = (0.,4.)\nu0 = [:X => 5.]\n\n# solve ODEs\noprob = ODEProblem(rs, u0, tspan, p)\nosol  = solve(oprob, Tsit5())\n\n# solve for Steady-States\nssprob = SteadyStateProblem(rs, u0, p)\nsssol  = solve(ssprob, SSRootfind())\n\n# solve SDEs\nsprob = SDEProblem(rs, u0, tspan, p)\nssol  = solve(sprob, EM(), dt=.01)\n\n# solve JumpProblem\nu0 = [:X => 5]\ndprob = DiscreteProblem(rs, u0, tspan, p)\njprob = JumpProblem(rs, dprob, Direct())\njsol = solve(jprob, SSAStepper())","category":"page"},{"location":"modules/Catalyst/tutorials/basic_examples/#Example:-Michaelis-Menten-Enzyme-Kinetics","page":"Basic Chemical Reaction Network Examples","title":"Example: Michaelis-Menten Enzyme Kinetics","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/basic_examples/","page":"Basic Chemical Reaction Network Examples","title":"Basic Chemical Reaction Network Examples","text":"rs = @reaction_network begin\n  c1, S + E --> SE\n  c2, SE --> S + E\n  c3, SE --> P + E\nend c1 c2 c3\np = (:c1 => 0.00166, :c2 => 0.0001, :c3 => 0.1) \ntspan = (0., 100.)\nu0 = [:S => 301., :E => 100., :SE => 0., :P => 0.]  \n\n# solve ODEs\noprob = ODEProblem(rs, u0, tspan, p)\nosol  = solve(oprob, Tsit5())\n\n# solve JumpProblem\nu0 = [:S => 301, :E => 100, :SE => 0, :P => 0] \ndprob = DiscreteProblem(rs, u0, tspan, p)\njprob = JumpProblem(rs, dprob, Direct())\njsol = solve(jprob, SSAStepper())","category":"page"},{"location":"modules/Optimization/optimization_packages/flux/#Flux.jl","page":"Flux.jl","title":"Flux.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/flux/#Installation:-OptimizationFlux.jl","page":"Flux.jl","title":"Installation: OptimizationFlux.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/flux/","page":"Flux.jl","title":"Flux.jl","text":"To use this package, install the OptimizationFlux package:","category":"page"},{"location":"modules/Optimization/optimization_packages/flux/","page":"Flux.jl","title":"Flux.jl","text":"import Pkg; Pkg.add(\"OptimizationFlux\")","category":"page"},{"location":"modules/Optimization/optimization_packages/flux/","page":"Flux.jl","title":"Flux.jl","text":"warn: Warn\nFlux's optimizers are soon to be deprecated by Optimisers.jl Because of this, we recommend using the OptimizationOptimisers.jl setup instead of OptimizationFlux.jl","category":"page"},{"location":"modules/Optimization/optimization_packages/flux/#Local-Unconstrained-Optimizers","page":"Flux.jl","title":"Local Unconstrained Optimizers","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/flux/","page":"Flux.jl","title":"Flux.jl","text":"Flux.Optimise.Descent: Classic gradient descent optimizer with learning rate\nsolve(problem, Descent(η))\nη is the learning rate\nDefaults:\nη = 0.1\nFlux.Optimise.Momentum: Classic gradient descent optimizer with learning rate and momentum\nsolve(problem, Momentum(η, ρ))\nη is the learning rate\nρ is the momentum\nDefaults:\nη = 0.01\nρ = 0.9\nFlux.Optimise.Nesterov: Gradient descent optimizer with learning rate and Nesterov momentum\nsolve(problem, Nesterov(η, ρ))\nη is the learning rate\nρ is the Nesterov momentum\nDefaults:\nη = 0.01\nρ = 0.9\nFlux.Optimise.RMSProp: RMSProp optimizer\nsolve(problem, RMSProp(η, ρ))\nη is the learning rate\nρ is the momentum\nDefaults:\nη = 0.001\nρ = 0.9\nFlux.Optimise.ADAM: ADAM optimizer\nsolve(problem, ADAM(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nFlux.Optimise.RADAM: Rectified ADAM optimizer\nsolve(problem, RADAM(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nFlux.Optimise.AdaMax: AdaMax optimizer\nsolve(problem, AdaMax(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nFlux.Optimise.ADAGRad: ADAGrad optimizer\nsolve(problem, ADAGrad(η))\nη is the learning rate\nDefaults:\nη = 0.1\nFlux.Optimise.ADADelta: ADADelta optimizer\nsolve(problem, ADADelta(ρ))\nρ is the gradient decay factor\nDefaults:\nρ = 0.9\nFlux.Optimise.AMSGrad: AMSGrad optimizer\nsolve(problem, AMSGrad(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nFlux.Optimise.NADAM: Nesterov variant of the ADAM optimizer\nsolve(problem, NADAM(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\nFlux.Optimise.ADAMW: ADAMW optimizer\nsolve(problem, ADAMW(η, β::Tuple))\nη is the learning rate\nβ::Tuple is the decay of momentums\ndecay is the decay to weights\nDefaults:\nη = 0.001\nβ::Tuple = (0.9, 0.999)\ndecay = 0","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Continuous-Time-Jump-Processes-and-Gillespie-Methods","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In this tutorial we will describe how to define and simulate continuous-time jump processes, also known in biological fields as stochastic chemical kinetics (i.e. Gillespie) models. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The discrete stochastic simulations we consider are a form of jump equation with a \"trivial\" (non-existent) differential equation. We will first demonstrate how to build these types of models using the biological modeling functionality of Catalyst.jl, then describe how to build them directly and more generally using JumpProcesses.jl jump types, and finally show how to couple discrete stochastic simulations to differential equation models.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Illustrative-Model:-SIR-disease-dynamics","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Illustrative Model: SIR disease dynamics","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To illustrate the jump process solvers, we will build an SIR model which matches the tutorial from Gillespie.jl. SIR stands for susceptible, infected, and recovered, and is a model of disease spread. When a susceptible person comes in contact with an infected person, the disease has a chance of infecting the susceptible person. This \"chance\" is determined by the number of susceptible persons and the number of infected persons, since in larger populations there is a greater chance that two people come into contact. Every infected person will in turn have a rate at which they recover. In our model we'll assume there are no births or deaths, and a recovered individual is protected from reinfection.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We'll begin by giving the mathematical equations for the jump processes of the number of susceptible (S(t)), number of infected (I(t)), and number of recovered (R(t)). In the next section we give a more intuitive and biological description of the model for users that are less familiar with jump processes. Let Y_i(t), i = 12, denote independent unit Poisson processes. Our basic mathematical model for the evolution of (S(t)I(t)R(t)), written using Kurtz's time-change representation, is then","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS(t) = S(0) - Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright) \nI(t) = I(0) + Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright) \n        - Y_2 left( int_0^t nu I(s^-)   ds right) \nR(t) = R(0) + Y_2 left( int_0^t nu I(s^-)   ds right)\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, our model involves two jumps with rate functions, also known as intensities or propensities, given by beta S(t) I(t) and nu I(t) respectively. These model the infection of susceptible individuals and recovery of infected individuals.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Defining-the-SIR-Model-using-Reactions-via-Catalyst","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the SIR Model using Reactions via Catalyst","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For those less-familiar with the time-change representation, we now give a more intuitive explanation of the model, and then demonstrate how it can be written as a serious of chemical reactions in Catalyst.jl and seamlessly converted into a form that can be used with the JumpProcesses.jl solvers. Users interested in how to directly define jumps using the lower-level JumpProcesses interface can skip to Building and Simulating the Jump Process using the JumpProcesses Low-level Interface.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The SIR model described above involves two basic chemical reactions,","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS + I oversetbetato 2 I \nI oversetnuto R\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where beta and nu are the rate constants of the reactions (with units of probability per time). In a jump process (stochastic chemical kinetics) model, we keep track of the non-negative integer number of each species at each time (i.e. (S(t) I(t) R(t)) above). Each reaction has an associated rate function (i.e. intensity or propensity) giving the probability per time it can occur when the system is in state (S(t)I(t)R(t)):","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextReaction  textRate Functions \nhline\nS + I oversetbetato 2 I  beta S(t) I(t) \nI oversetnuto R  nu I(t)\nendmatrix","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beta is determined by factors like the type of the disease. It can be interpreted as the probability per time one pair of susceptible and infected people encounter each other, with the susceptible person becoming sick. The overall rate (i.e. probability per time) that some susceptible person gets sick is then given by the rate constant multiplied by the number of possible pairs of susceptible and infected people. This formulation is known as the law of mass action. Similarly, we have that each individual infected person is assumed to recover with probability per time nu, so that the probability per time some infected person becomes recovered is nu times the number of infected people, i.e. nu I(t). ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Rate functions give the probability per time for each of the two types of jumps to occur, and hence determine when the state of our system changes. To fully specify our model we also need to specify how the state changes when a jump occurs, giving what are called affect functions in JumpProcesses. For example, when the S + I to 2 I reaction occurs and some susceptible person becomes infected, the subsequent (instantaneous) state change is that","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS to S - 1  I to I + 1\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Likewise, when the I to R reaction occurs so that some infected person becomes recovered the state change is","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nI to I - 1  R to R + 1\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Using Catalyst.jl we can input our full reaction network in a form that can be easily used with JumpProcesses's solvers:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"# ]add Catalyst\nusing Catalyst\nsir_model = @reaction_network begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice that the order the variables are introduced in the model is S, then I, then R, and thus this is the canonical ordering of the variables.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Process-from-Catalyst-Models","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Process from Catalyst Models","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"First, we have to define some kind of differential equation that we can \"solve\" to simulate the jump process. Since we want integer, discrete changes in the numbers of the different types of people, we will build a DiscreteProblem. We do this by giving the constructor u0, the initial condition, and tspan, the timespan. Here, we will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0. We use the parameters β = 0.1/1000 and ν = 0.01. Thus we build the problem via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"p     = (0.1/1000,0.01)   \nu₀    = [999,1,0]\ntspan = (0.0,250.0)\nprob  = DiscreteProblem(sir_model, u₀, tspan, p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The Catalyst reaction network can be converted into various DifferentialEquations.jl problem types, including JumpProblems, ODEProblems, or SDEProblems. To turn it into a jump problem representing the SIR jump process model, we load JumpProcesses and simply do:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using JumpProcesses\njump_prob = JumpProblem(sir_model, prob, Direct())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of reactions using Gillespie's Direct stochastic simulation algorithm (SSA). See Constant Rate Jump Aggregators below for other supported SSAs.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the JumpProcesses solvers. Since our model is a pure jump process (no continuously-varying components), we will use SSAStepper() to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Plots; plot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: SIR Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Process-using-the-JumpProcesses-Low-level-Interface","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Process using the JumpProcesses Low-level Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now show how to directly use JumpProcesses's low-level interface to construct and solve our jump process model for (S(t)I(t)R(t)). Each individual jump that can occur is represented through specifying two pieces of information; a rate function (i.e. intensity or propensity) for the jump and an affect function for the jump. The former gives the probability per time a particular jump can occur given the current state of the system, and hence determines the time at which jumps can happen. The later specifies the instantaneous change in the state of the system when the jump occurs. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In our SIR model we have two possible jumps that can occur (one for susceptibles becoming infected and one for infected becoming recovered), with the corresponding (mathematical) rates and affects given by","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextRates  textAffects\nhline \nbeta S(t) I(t)  S to S - 1 I to I + 1 \nnu I(t)  I to I - 1  R to R + 1\nendmatrix","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"JumpProcesses offers three different ways to represent jumps: MassActionJump, ConstantRateJump, and VariableRateJump. Choosing which to use is a trade off between the desired generality of the rate and affect functions vs. the computational performance of the resulting simulated system. In general","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Jump Type Performance Generality\nMassActionJump Fastest Restrictive rates/affects\nConstantRateJump Somewhat Slower Much more general\nVariableRateJump Slowest Completely general","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"It is recommended to try to encode jumps using the most performant option that supports the desired generality of the underlying rate and affect functions. Below we describe the different jump types, and show how the SIR model can be formulated using first ConstantRateJumps and then MassActionJumps (VariableRateJumps are considered later).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#ConstantRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: ConstantRateJump","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump = ConstantRateJump(rate, affect!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where rate is a function rate(u,p,t) and affect! is a function of the integrator affect!(integrator) (for details on the integrator, see the integrator interface docs). Here u corresponds to the current state of the system; for our SIR model u[1]=S(t), u[2]=I(t) and u[3]=R(t). p corresponds to the parameters of the model, just as used for passing parameters to derivative functions in ODE solvers. Thus, to define the two possible jumps for our model we take (with β=.1/1000.0 and ν=.01).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using JumpProcesses\nβ = 0.1 / 1000.0; ν = .01;\np = (β,ν)\nrate1(u,p,t) = p[1]*u[1]*u[2]  # β*S*I\nfunction affect1!(integrator)\n  integrator.u[1] -= 1         # S -> S - 1\n  integrator.u[2] += 1         # I -> I + 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = p[2]*u[2]      # ν*I\nfunction affect2!(integrator)\n  integrator.u[2] -= 1        # I -> I - 1\n  integrator.u[3] += 1        # R -> R + 1\nend\njump2 = ConstantRateJump(rate2,affect2!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0 so that","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀    = [999,1,0]\ntspan = (0.0,250.0)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Since we want integer, discrete changes in the numbers of the different types of people, we will build a DiscreteProblem. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"prob = DiscreteProblem(u₀, tspan, p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then use JumpProblem from JumpProcesses to augment the discrete problem with jumps and select the stochastic simulation algorithm (SSA) to use in sampling the jump processes. To create a JumpProblem we would simply do:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of jumps that occur using Gillespie's Direct stochastic simulation algorithm (SSA). See Constant Rate Jump Aggregators for other supported SSAs.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the JumpProcesses solvers. Since our model is a pure jump process (no continuously-varying components), we will use SSAStepper() to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Plots; plot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: SIR Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, in systems with more than a few jumps (more than ~10), it can be advantageous to use more sophisticated SSAs than Direct. For such systems it is recommended to use SortingDirect, RSSA or RSSACR, see the list of JumpProcesses SSAs at Constant Rate Jump Aggregators.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*Caution-about-Constant-Rate-Jumps*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Caution about Constant Rate Jumps","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"ConstantRateJumps are quite general, but they do have one restriction. They assume that the rate functions are constant at all times between two consecutive jumps of the system. i.e. any species/states or parameters that the rate function depends on must not change between the times at which two consecutive jumps occur. Such conditions are violated if one has a time dependent parameter like beta(t) or if some of the solution components, say u[2], may also evolve through a coupled ODE or SDE (see below for examples). For problems where the rate function may change between consecutive jumps, VariableRateJumps must be used.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Thus in the examples above,","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate1(u,p,t) = p[1]*u[1]*u[2]\nrate2(u,p,t) = p[2]*u[2]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"both must be constant other than changes due to some other ConstantRateJump or MassActionJump (the same restriction applies to MassActionJumps). Since these rates only change when u[1] or u[2] is changed, and u[1] and u[2] only change when one of the jumps occur, this setup is valid. However, a rate of t*p[1]*u[1]*u[2] would not be valid because the rate would change during the interval, as would p[2]*u[1]*u[4] when u[4] is the solution to a continuous problem such as an ODE or SDE. Thus one must be careful to follow this rule when choosing rates.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If your problem must have the rates depend on a continuously changing quantity, you need to use the VariableRateJump.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#SSAStepper","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"SSAStepper","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Any common interface algorithm can be used to perform the time-stepping since it is implemented over the callback interface. This allows for hybrid systems that mix ODEs, SDEs and jumps. In many cases we may have a pure jump system that only involves ConstantRateJumps and/or MassActionJumps (see below). When that's the case, a substantial performance benefit may be gained by using SSAStepper(). Note, SSAStepper is a more limited time-stepper which only supports discrete events, and does not allow simultaneous coupled ODEs or SDEs. It is, however, very efficient for pure jump problems.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#save_positions_docs","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Reducing Memory Use: Controlling Saving Behavior","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that jumps act via the callback interface which defaults to saving at each event. The reason is because this is required in order to accurately resolve every discontinuity exactly (and this is what allows for perfectly vertical lines in plots!). However, in many cases when using jump problems you may wish to decrease the saving pressure given by large numbers of jumps. To do this, you set save_positions in the JumpProblem. Just like for other callbacks, this is a tuple (bool1,bool2) which sets whether to save before or after a jump. If we do not want to save at every jump, we would thus pass:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2, save_positions=(false,false))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now the saving controls associated with the integrator are the only ones to note. For example, we can use saveat=0.5 to save at an evenly spaced grid:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper(), saveat=0.5)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#MassActionJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: MassActionJump","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For jumps that can be represented as mass action reactions, a further specialization of the jump type is possible that offers improved computational performance; MassActionJump. Suppose the system has N chemical species S_1dotsS_N. A general mass action reaction has the form","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"R_1 S_1 + R_2 S_2 + dots + R_N S_N oversetkrightarrow P_1 S_1 + P_2 S_2 + dots + P_N S_N","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where the non-negative integers (R_1dotsR_N) denote the reactant stoichiometry of the reaction, and the non-negative integers (P_1dotsP_N) the product stoichiometry. The net stoichiometry is the net change in each chemical species from the reaction occurring one time, given by (P_1-R_1dotsP_N-R_N).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"As an example, consider again the SIR model defined in the @reaction_network above. The species are then (S,I,R). The first reaction has rate β, reactant stoichiometry (1,1,0), product stoichiometry (0,2,0), and net stoichiometry (-1,1,0). The second reaction has rate ν, reactant stoichiometry (0,1,0), product stoichiometry (0,0,1), and net stoichiometry (0,-1,1).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can manually encode this system as a mass action jump by specifying the indexes of the rate constants in p, the reactant stoichiometry, and the net stoichiometry as follows:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rateidxs = [1, 2]    # i.e. [β,ν]\nreactant_stoich =\n[\n  [1 => 1, 2 => 1],         # 1*S and 1*I\n  [2 => 1]                  # 1*I\n]\nnet_stoich =\n[\n  [1 => -1, 2 => 1],        # -1*S and 1*I\n  [2 => -1, 3 => 1]         # -1*I and 1*R\n]\nmass_act_jump = MassActionJump(reactant_stoich, net_stoich; param_idxs=rateidxs)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, one typically should define one MassActionJump that encodes each possible jump that can be represented via a mass action reaction. This is in contrast to ConstantRateJumps or VariableRateJumps where separate instances are created for each distinct jump type.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Just like for ConstantRateJumps, to then simulate the system we create a JumpProblem and call solve:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about MassActionJumps see Defining a Mass Action Jump. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, for chemical reaction systems, Catalyst.jl automatically groups reactions into their optimal jump representation.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Defining-the-Jumps-Directly:-Mixing-ConstantRateJump-and-MassActionJump","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: Mixing ConstantRateJump and MassActionJump","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Suppose we now want to add in to the SIR model another jump that can not be represented as a mass action reaction. We can create a new ConstantRateJump and simulate a hybrid system using both the MassActionJump for the two previous reactions, and the new ConstantRateJump. Let's suppose we want to let susceptible people be born with the following jump rate:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"birth_rate(u,p,t) = 10.0*u[1]/(200. + u[1]) + 10.\nfunction birth_affect!(integrator)\n  integrator.u[1] += 1\nend\nbirth_jump = ConstantRateJump(birth_rate, birth_affect!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then simulate the hybrid system as","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump, birth_jump)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: gillespie_hybrid_jumps)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#Adding-Jumps-to-a-Differential-Equation","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding Jumps to a Differential Equation","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If we instead used some form of differential equation instead of a DiscreteProblem, we would couple the jumps/reactions to the differential equation. Let's define an ODE problem, where the continuous part only acts on some new 4th component:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using OrdinaryDiffEq\nfunction f(du,u,p,t)\n  du[4] = u[2]*u[3]/100000 - u[1]*u[4]/100000\nend\nu₀   = [999.0,1.0,0.0,100.0]\nprob = ODEProblem(f,u₀,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice we gave the 4th component a starting value of 100.0, and used floating point numbers for the initial condition since some solution components now evolve continuously. The same steps as above will allow us to solve this hybrid equation when using ConstantRateJumps (or MassActionJumps). For example, we can solve it using the Tsit5() method via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)\nsol = solve(jump_prob,Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: gillespie_ode)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#VariableRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding a VariableRateJump","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now let's consider adding a reaction whose rate changes continuously with the differential equation. To continue our example, let's let there be a new jump/reaction with rate depending on u[4]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate3(u,p,t) = 1e-2*u[4]\nfunction affect3!(integrator)\n  integrator.u[2] += 1\nend\njump3 = VariableRateJump(rate3,affect3!)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, since rate3 depends on a variable that evolves continuously, and hence is not constant between jumps, we must use a VariableRateJump.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Solving the equation is exactly the same:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀   = [999.0,1.0,0.0,1.0]\nprob = ODEProblem(f,u₀,tspan,p)\njump_prob = JumpProblem(prob,Direct(),jump,jump2,jump3)\nsol = solve(jump_prob,Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: variable_rate_gillespie)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that VariableRateJumps require a continuous problem, like an ODE/SDE/DDE/DAE problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Lastly, we are not restricted to ODEs. For example, we can solve the same jump problem except with multiplicative noise on u[4] by using an SDEProblem instead:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using StochasticDiffEq\nfunction g(du,u,p,t)\n  du[4] = 0.1u[4]\nend\n\nprob = SDEProblem(f,g,[999.0,1.0,0.0,1.0],(0.0,250.0), p)\njump_prob = JumpProblem(prob,Direct(),jump,jump2,jump3)\nsol = solve(jump_prob,SRIW1())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: sde_gillespie)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about VariableRateJumps see Defining a Variable Rate Jump.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#RegularJumps-and-Tau-Leaping","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"RegularJumps and Tau-Leaping","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The previous parts described how to use ConstantRateJumps, MassActionJumps, and VariableRateJumps to add jumps to differential equation algorithms over the callback interface. However, in many cases you do not need to step to every jump time. Instead, regular jumping allows you to pool together jumps and perform larger updates in a statistically-correct but more efficient manner.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For RegularJumps, we pool together the jumps we wish to perform. Here our rate is a vector equation which computes the rates of each jump process together:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"function rate(out,u,p,t)\n    out[1] = (0.1/1000.0)*u[1]*u[2]\n    out[2] = 0.01u[2]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"and then we compute the total change matrix c","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"function c(dc,u,p,t,mark)\n    dc[1,1] = -1\n    dc[2,1] = 1\n    dc[2,2] = -1\n    dc[3,2] = 1\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where each column is a different jump process. We then declare the form of dc and build a RegularJump:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"dc = zeros(3,2)\nrj = RegularJump(rate,c,dc;constant_c=true)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"From there we build a JumpProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"prob = DiscreteProblem([999.0,1.0,0.0],(0.0,250.0))\njump_prob = JumpProblem(prob,Direct(),rj)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that when a JumpProblem has a RegularJump, special algorithms are required. This is detailed on the jump solvers page. One such algorithm is SimpleTauLeaping, which we use as follows:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob,SimpleTauLeaping();dt=1.0)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#FAQ","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"FAQ","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*1.-My-simulation-is-really-slow-and/or-using-a-lot-of-memory,-what-can-I-do?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"1. My simulation is really slow and/or using a lot of memory, what can I do?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To reduce memory use, use save_positions=(false,false) in the JumpProblem constructor as described earlier to turn off saving the system state before and after every jump. Combined with use of saveat in the call to solve this can dramatically reduce memory usage.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"While Direct is often fastest for systems with 10 or less ConstantRateJumps or MassActionJumps, if your system has many jumps or one jump occurs most frequently, other stochastic simulation algorithms may be faster. See Constant Rate Jump Aggregators and the subsequent sections there for guidance on choosing different SSAs (called aggregators in JumpProcesses).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*2.-When-running-many-consecutive-simulations,-for-example-within-an-EnsembleProblem-or-loop,-how-can-I-update-JumpProblems?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"2. When running many consecutive simulations, for example within an EnsembleProblem or loop, how can I update JumpProblems?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In Remaking JumpProblems we show how to modify parameters, the initial condition, and other components of a generated JumpProblem. This can be useful when trying to call solve many times while avoiding reallocations of the internal aggregators for each new parameter value or initial condition.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*3.-How-do-I-use-callbacks-with-ConstantRateJump-or-MassActionJump-systems?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"3. How do I use callbacks with ConstantRateJump or MassActionJump systems?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Callbacks can be used with ConstantRateJumps and MassActionJumps. When solving a pure jump system with SSAStepper, only discrete callbacks can be used (otherwise a different time stepper is needed). ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, when modifying u or p within a callback, you must call reset_aggregated_jumps!(integrator) after making updates. This ensures that the underlying jump simulation algorithms know to reinitialize their internal data structures. Leaving out this call will lead to incorrect behavior!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"A simple example that uses a MassActionJump and changes the parameters at a specified time in the simulation using a DiscreteCallback is","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using JumpProcesses\nrs = [[1 => 1],[2=>1]]\nns = [[1 => -1, 2 => 1],[1=>1,2=>-1]]\np  = [1.0,0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1,2])\nu₀ = [100,0]\ntspan = (0.0,40.0)\ndprob = DiscreteProblem(u₀,tspan,p)\njprob = JumpProblem(dprob,Direct(),maj)\npcondit(u,t,integrator) = t==20.0\nfunction paffect!(integrator)\n  integrator.p[1] = 0.0\n  integrator.p[2] = 1.0\n  reset_aggregated_jumps!(integrator)\nend\nsol = solve(jprob, SSAStepper(), tstops=[20.0], callback=DiscreteCallback(pcondit,paffect!))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here at time 20.0 we turn off production of u[2] while activating production of u[1], giving","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: callback_gillespie)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*4.-How-can-I-define-collections-of-many-different-jumps-and-pass-them-to-JumpProblem?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"4. How can I define collections of many different jumps and pass them to JumpProblem?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can use JumpSets to collect jumps together, and then pass them into JumpProblems directly. For example, using the MassActionJump and ConstantRateJump defined earlier we can write","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jset = JumpSet(mass_act_jump, birth_jump)\njump_prob = JumpProblem(prob, Direct(), jset)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If you have many jumps in tuples or vectors it is easiest to use the keyword argument-based constructor:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"cj1 = ConstantRateJump(rate1,affect1!)\ncj2 = ConstantRateJump(rate2,affect2!)\ncjvec = [cj1,cj2]\n\nvj1 = VariableRateJump(rate3,affect3!)\nvj2 = VariableRateJump(rate4,affect4!)\nvjtuple = (vj1,vj2)\n\njset = JumpSet(; constant_jumps=cjvec, variable_jumps=vjtuple, \n                 massaction_jumps=mass_act_jump)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*5.-How-can-I-set-the-random-number-generator-used-in-the-jump-process-sampling-algorithms-(SSAs)?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"5. How can I set the random number generator used in the jump process sampling algorithms (SSAs)?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Random number generators can be passed to JumpProblem via the rng keyword argument. Continuing the previous example:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"#] add RandomNumbers\nusing RandomNumbers\njprob = JumpProblem(dprob, Direct(), maj, rng=Xorshifts.Xoroshiro128Star(rand(UInt64)))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"uses the Xoroshiro128Star generator from RandomNumbers.jl.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*6.-What-are-these-aggregators-and-aggregations-in-JumpProcesses?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"6. What are these aggregators and aggregations in JumpProcesses?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"JumpProcesses provides a variety of methods for sampling the time the next ConstantRateJump or MassActionJump occurs, and which jump type happens at that time. These methods are examples of stochastic simulation algorithms (SSAs), also known as Gillespie methods, Doob's method, or Kinetic Monte Carlo methods. In the JumpProcesses terminology we call such methods \"aggregators\", and the cache structures that hold their basic data \"aggregations\". See Constant Rate Jump Aggregators for a list of the available SSA aggregators.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/#*7.-How-should-jumps-be-ordered-in-dependency-graphs?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"7. How should jumps be ordered in dependency graphs?","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Internally, JumpProcesses SSAs (aggregators) order all MassActionJumps first, then all ConstantRateJumps. i.e. in the example","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using JumpProcesses\nrs = [[1 => 1],[2=>1]]\nns = [[1 => -1, 2 => 1],[1=>1,2=>-1]]\np  = [1.0,0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1,2])\nrate1(u,p,t) = u[1]\nfunction affect1!(integrator) \n  u[1] -= 1\nend\ncj1 = ConstantRateJump(rate1,affect1)\nrate2(u,p,t) = u[2]\nfunction affect2!(integrator)\n  u[2] -= 1\nend \ncj2 = ConstantRateJump(rate2,affect2)\njset = JumpSet(; constant_jumps=[cj1,cj2], massaction_jump=maj)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The four jumps would be ordered by the first jump in maj, the second jump in maj, cj1, and finally cj2. Any user-generated dependency graphs should then follow this ordering when assigning an integer id to each jump. ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"See also Constant Rate Jump Aggregators Requiring Dependency Graphs for more on dependency graphs needed for the various SSAs.","category":"page"},{"location":"modules/DiffEqDocs/types/sdae_types/#SDAE-Problems","page":"SDAE Problems","title":"SDAE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sdae_types/#Mathematical-Specification-of-a-Stochastic-Differential-Algebraic-Equation-(SDAE)-Problem","page":"SDAE Problems","title":"Mathematical Specification of a Stochastic Differential-Algebraic Equation (SDAE) Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"To define an SDAE, you simply define an SDE Problem with the forcing function f, the noise function g, a mass matrix M and the initial condition u₀ which define the SDAE in mass matrix form:","category":"page"},{"location":"modules/DiffEqDocs/types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"M du = f(upt)dt + Σgᵢ(upt)dWⁱ","category":"page"},{"location":"modules/DiffEqDocs/types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"f and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.","category":"page"},{"location":"modules/DiffEqDocs/types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"Nonsingular mass matrices correspond to constraint equations and thus a stochastic DAE.","category":"page"},{"location":"modules/DiffEqDocs/types/sdae_types/#Example","page":"SDAE Problems","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"const mm_A = [-2.0 1 4\n            4 -2 1\n            0 0 0]\nfunction f!(du,u,p,t)\n  du[1] = u[1]\n  du[2] = u[2]\n  du[3] = u[1] + u[2] + u[3] - 1\nend\n\nfunction g!(du,u,p,t)\n  @. du = 0.1\nend\n\nprob = SDEProblem(SDEFunction(f!,g!;mass_matrix=mm_A),g!,\n                  ones(3),(0.0,1.0))","category":"page"},{"location":"modules/DiffEqParamEstim/methods/collocation_loss/#Two-Stage-method-(Non-Parametric-Collocation)","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"","category":"section"},{"location":"modules/DiffEqParamEstim/methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"The two-stage method is a collocation method for estimating parameters without requiring repeated solving of the differential equation. It does so by determining a smoothed estimated trajectory of the data (local quadratic polynomial fit by least squares) and optimizing the derivative function and the data's timepoints to match the derivatives of the smoothed trajectory. This method has less accuracy than other methods but is much faster, and is a good method to try first to get in the general \"good parameter\" region, to then finish using one of the other methods.","category":"page"},{"location":"modules/DiffEqParamEstim/methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"function two_stage_method(prob::DEProblem,tpoints,data;kernel= :Epanechnikov,\n                          loss_func = L2DistLoss,mpg_autodiff = false,\n                          verbose = false,verbose_steps = 100)","category":"page"},{"location":"modules/DiffEqDocs/features/performance_overloads/#performance_overloads","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/performance_overloads/","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"The DiffEq ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the DiffEqFunction types which can be passed to the problems.","category":"page"},{"location":"modules/DiffEqDocs/features/performance_overloads/#Built-In-Jacobian-Options","page":"Jacobians, Gradients, etc.","title":"Built-In Jacobian Options","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/performance_overloads/","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"warning: Warning\nThis subsection on Jacobian options only applies to the Julia-based solvers (OrdinaryDiffEq.jl, StochasticDiffEq.jl, etc.). Wrappers of traditional C/Fortran codes, such as Sundials.jl, always default to finite differencing using the in-solver code.","category":"page"},{"location":"modules/DiffEqDocs/features/performance_overloads/","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"All applicable stiff differential equation solvers in the Julia ecosystem (OrdinaryDiffEq.jl, StochasticDiffEq.jl, DelayDiffEq.jl, etc.) take the following arguments for handling the automatic Jacobian construction with the following defaults:","category":"page"},{"location":"modules/DiffEqDocs/features/performance_overloads/","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"chunk_size: The chunk size used with ForwardDiff.jl. Defaults to Val{0}() and thus uses the internal ForwardDiff.jl algorithm for the choice.\nautodiff: Specifies whether to use automatic differentiation via ForwardDiff.jl or finite differencing via FiniteDiff.jl. Defaults to Val{true}() for automatic differentation.\nstandardtag: Specifies whether to use package-specific tags instead of the ForwardDiff default function-specific tags. For more information see this blog post. Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defalts to nothing, which means it will be chosen true/false depending on circumstances of the solver, such as whether a Krylov subspace method is used for linsolve.\ndiff_type: The type of differentiation used in FiniteDiff.jl if autodiff=false. Defalts to Val{:forward}, with altnerative choices of Val{:central} and Val{:complex}. ","category":"page"},{"location":"modules/DiffEqDocs/features/performance_overloads/#Passing-Jacobian-Function-Definitions","page":"Jacobians, Gradients, etc.","title":"Passing Jacobian Function Definitions","text":"","category":"section"},{"location":"modules/DiffEqDocs/features/performance_overloads/","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"If one wishes to directly define a Jacobian function for use in the solver, then the defined method is passed to the AbstractSciMLFunction type associated with the DEProblem. For example, ODEProblem definitions have a spot for jac in the ODEFunction specification. For more information on how to define Jacobians for the specific problems, see the appropriate problem type page, for example, the ODE problem page","category":"page"},{"location":"modules/SciMLStyle/#SciML-Style-Guide-for-Julia","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"(Image: SciML Code Style)","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"The SciML Style Guide is a style guide for the Julia programming language. It is used by the SciML Open Source Scientific Machine Learning Organization. As such, it is open to discussion with the community. Please file an issue or open a PR to discuss changes to the style guide.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Table of Contents","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Code Style Badge\nOverarching Dogmas of the SciML Style\nConsistency vs Adherence\nCommunity Contribution Guidelines\nOpen source contributions are allowed to start small and grow over time\nGeneric code is preferred unless code is known to be specific\nInternal types should match the types used by users when possible\nTrait definition and adherence to generic interface is preferred when possible\nMacros should be limited and only be used for syntactic sugar\nErrors should be caught as high as possible, and error messages should be contextualized for newcommers\nSubpackaging and interface packages is preferred over conditional modules via Requires.jl\nFunctions should either attempt to be non-allocating and reuse caches, or treat inputs as immutable\nOut-Of-Place and Immutability is preferred when sufficient performant\nTests should attempt to cover a wide gamut of input types\nWhen in doubt, a submodule should become a subpackage or separate package\nGlobals should be avoided whenever possible\nType-stable and Type-grounded code is preferred wherever possible\nClosures should be avoided whenever possible\nNumerical functionality should use the appropriate generic numerical interfaces\nFunctions should capture one underlying principle\nInternal choices should be exposed as options whenever possible\nPrefer code reuse over rewrites whenever possible\nPrefer to not shadow functions\nSpecific Rules\nHigh Level Rules\nGeneral Naming Principles\nComments\nModules\nFunctions\nFunction Argument Precedence\nTests and Continuous Integration\nWhitespace\nNamedTuples\nNumbers\nTernary Operator\nFor loops\nFunction Type Annotations\nStruct Type Annotations\nTypes and Type Annotations\nPackage version specifications\nDocumentation\nError Handling\nArrays\nVS-Code Settings\nJuliaFormatter","category":"page"},{"location":"modules/SciMLStyle/#Code-Style-Badge","page":"SciML Style Guide for Julia","title":"Code Style Badge","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Let contributors know your project is following the SciML Style Guide by adding the badge to your README.md.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"[![SciML Code Style](https://img.shields.io/static/v1?label=code%20style&message=SciML&color=9558b2&labelColor=389826)](https://github.com/SciML/SciMLStyle)","category":"page"},{"location":"modules/SciMLStyle/#Overarching-Dogmas-of-the-SciML-Style","page":"SciML Style Guide for Julia","title":"Overarching Dogmas of the SciML Style","text":"","category":"section"},{"location":"modules/SciMLStyle/#Consistency-vs-Adherence","page":"SciML Style Guide for Julia","title":"Consistency vs Adherence","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"According to PEP8:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is the most important.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"But most importantly: know when to be inconsistent – sometimes the style guide just doesn't apply. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Some code within the SciML organization is old, on life support, donated by researchers to be maintained. Consistency is the number one goal, so updating to match the style guide should happen on a repo-by-repo basis, i.e. do not update one file to match the style guide (leaving all other files behind).","category":"page"},{"location":"modules/SciMLStyle/#Community-Contribution-Guidelines","page":"SciML Style Guide for Julia","title":"Community Contribution Guidelines","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"For a comprehensive set of community contribution guidelines, refer to ColPrac. A relevant point to highlight PRs should do one thing. In the context of style, this means that PRs which update the style of a package's code should not be mixed with fundamental code contributions. This separation makes it easier to ensure that large style improvement are isolated from substantive (and potentially breaking) code changes.","category":"page"},{"location":"modules/SciMLStyle/#Open-source-contributions-are-allowed-to-start-small-and-grow-over-time","page":"SciML Style Guide for Julia","title":"Open source contributions are allowed to start small and grow over time","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"If the standard for code contributions is that every PR needs to support every possible input type that anyone can think of, the barrier would be too high for newcomers. Instead, the principle is to be as correct as possible to begin with, and grow the generic support over time. All recommended functionality should be tested, any known generality issues should be documented in an issue (and with a @test_broken test when possible). However, a function which is known to not be GPU-compatible is not grounds to block merging, rather its an encouragement for a follow-up PR to improve the general type support!","category":"page"},{"location":"modules/SciMLStyle/#Generic-code-is-preferred-unless-code-is-known-to-be-specific","page":"SciML Style Guide for Julia","title":"Generic code is preferred unless code is known to be specific","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"For example, the code:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"function f(A, B)\n    for i in 1:length(A)\n        A[i] = A[i] + B[i]\n    end\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"would not be preferred for two reasons. One is that it assumes A uses one-based indexing, which would fail in cases like OffsetArrays and FFTViews. Another issue is that it requires indexing, while not all array types support indexing (for example, CuArrays). A more generic compatible implementation of this function would be to use broadcast, for example:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"function f(A, B)\n    @. A = A + B\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"which would allow support for a wider variety of array types.","category":"page"},{"location":"modules/SciMLStyle/#Internal-types-should-match-the-types-used-by-users-when-possible","page":"SciML Style Guide for Julia","title":"Internal types should match the types used by users when possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"If f(A) takes the input of some collections and computes an output from those collections, then it should be expected that if the user gives A as an Array, the computation should be done via Arrays. If A was a CuArray, then it should be expected that the computation should be internally done using a CuArray (or appropriately error if not supported). For these reasons, constructing arrays via generic methods, like similar(A), is preferred when writing f instead of using non-generic constructors like Array(undef,size(A)) unless the function is documented as being non-generic.","category":"page"},{"location":"modules/SciMLStyle/#Trait-definition-and-adherence-to-generic-interface-is-preferred-when-possible","page":"SciML Style Guide for Julia","title":"Trait definition and adherence to generic interface is preferred when possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Julia provides many different interfaces, for example:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Iteration\nIndexing\nBroadcast","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Those interfaces should be followed when possible. For example, when defining broadcast overloads, one should implement a BroadcastStyle as suggested by the documentation instead of simply attempting to bypass the broadcast system via copyto! overloads.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"When interface functions are missing, these should be added to Base Julia or an interface package, like ArrayInterface.jl. Such traits should be declared and used when appropriate. For example, if a line of code requires mutation, the trait ArrayInterface.ismutable(A) should be checked before attempting to mutate, and informative error messages should be written to capture the immutable case (or, an alternative code which does not mutate should be given).","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"One example of this principle is demonstrated in the generation of Jacobian matrices. In many scientific applications, one may wish to generate a Jacobian cache from the user's input u0. A naive way to generate this Jacobian is J = similar(u0,length(u0),length(u0)). However, this will generate a Jacobian J such that J isa Matrix.","category":"page"},{"location":"modules/SciMLStyle/#Macros-should-be-limited-and-only-be-used-for-syntactic-sugar","page":"SciML Style Guide for Julia","title":"Macros should be limited and only be used for syntactic sugar","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Macros define new syntax, and for this reason they tend to be less composable than other coding styles and require prior familiarity to be easily understood. One principle to keep in mind is, \"can the person reading the code easily picture what code is being generated?\". For example, a user of Soss.jl may not know what code is being generated by:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"@model (x, α) begin\n    σ ~ Exponential()\n    β ~ Normal()\n    y ~ For(x) do xj\n        Normal(α + β * xj, σ)\n    end\n    return y\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"and thus using such a macro as the interface is not preferred when possible. However, a macro like @muladd is trivial to picture on a code (it recursively transforms a*b + c to muladd(a,b,c) for more accuracy and efficiency), so using such a macro for example:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"julia> @macroexpand(@muladd k3 = f(t + c3 * dt, @. uprev + dt * (a031 * k1 + a032 * k2)))\n:(k3 = f((muladd)(c3, dt, t), (muladd).(dt, (muladd).(a032, k2, (*).(a031, k1)), uprev)))","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"is recommended. Some macros in this category are:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"@inbounds\n@muladd\n@view\n@named\n@.\n@..","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Some performance macros, like @simd, @threads, or @turbo from LoopVectorization.jl, make an exception in that their generated code may be foreign to many users. However, they still are classified as appropriate uses as they are syntactic sugar since they do (or should) not change the behavior of the program in measurable ways other than performance.","category":"page"},{"location":"modules/SciMLStyle/#Errors-should-be-caught-as-high-as-possible,-and-error-messages-should-be-contextualized-for-newcomers","page":"SciML Style Guide for Julia","title":"Errors should be caught as high as possible, and error messages should be contextualized for newcomers","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Whenever possible, defensive programming should be used to check for potential errors before they are encountered deeper within a package. For example, if one knows that f(u0,p) will error unless u0 is the size of p, this should be caught at the start of the function to throw a domain specific error, for example \"parameters and initial condition should be the same size\".","category":"page"},{"location":"modules/SciMLStyle/#Subpackaging-and-interface-packages-is-preferred-over-conditional-modules-via-Requires.jl","page":"SciML Style Guide for Julia","title":"Subpackaging and interface packages is preferred over conditional modules via Requires.jl","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Requires.jl should be avoided at all costs. If an interface package exists, such as ChainRulesCore.jl for defining automatic differentiation rules without requiring a dependency on the whole ChainRules.jl system, or RecipesBase.jl which allows for defining Plots.jl plot recipes without a dependency on Plots.jl, a direct dependency on these interface packages is preferred.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Otherwise, instead of resorting to a conditional dependency using Requires.jl, it is preferred one creates subpackages, i.e. smaller independent packages kept within the same Github repository with independent versioning and package management. An example of this is seen in Optimization.jl which has subpackages like OptimizationBBO.jl for BlackBoxOptim.jl support.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Some important interface packages to know about are:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"ChainRulesCore.jl\nRecipesBase.jl\nArrayInterface.jl\nCommonSolve.jl\nSciMLBase.jl","category":"page"},{"location":"modules/SciMLStyle/#Functions-should-either-attempt-to-be-non-allocating-and-reuse-caches,-or-treat-inputs-as-immutable","page":"SciML Style Guide for Julia","title":"Functions should either attempt to be non-allocating and reuse caches, or treat inputs as immutable","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Mutating codes and non-mutating codes fall into different worlds. When a code is fully immutable, the compiler can better reason about dependencies, optimize the code, and check for correctness. However, many times a code making the fullest use of mutation can outperform even what the best compilers of today can generate. That said, the worst of all worlds is when code mixes mutation with non-mutating code. Not only is this a mishmash of coding styles, it has the potential non-locality and compiler proof issues of mutating code while not fully benefiting from the mutation.","category":"page"},{"location":"modules/SciMLStyle/#Out-Of-Place-and-Immutability-is-preferred-when-sufficient-performant","page":"SciML Style Guide for Julia","title":"Out-Of-Place and Immutability is preferred when sufficient performant","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Mutation is used to get more performance by decreasing the amount of heap allocations. However, if it's not helpful for heap allocations in a given spot, do not use mutation. Mutation is scary and should be avoided unless it gives an immediate benefit. For example, if matrices are sufficiently large, then A*B is as fast as mul!(C,A,B), and thus writing A*B is preferred (unless the rest of the function is being careful about being fully non-allocating, in which case this should be mul! for consistency).","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Similarly, when defining types, using struct is preferred to mutable struct unless mutating the struct is a common occurrence. Even if mutating the struct is a common occurrence, see whether using SetField.jl is sufficient. The compiler will optimize the construction of immutable structs, and thus this can be more efficient if it's not too much of a code hassle.","category":"page"},{"location":"modules/SciMLStyle/#Tests-should-attempt-to-cover-a-wide-gamut-of-input-types","page":"SciML Style Guide for Julia","title":"Tests should attempt to cover a wide gamut of input types","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Code coverage numbers are meaningless if one does not consider the input types. For example, one can hit all of the code with Array, but that does not test whether CuArray is compatible! Thus it's always good to think of coverage not in terms of lines of code but in terms of type coverage. A good list of number types to think about are:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Float64\nFloat32\nComplex\nDual\nBigFloat","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Array types to think about testing are:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Array\nOffsetArray\nCuArray","category":"page"},{"location":"modules/SciMLStyle/#When-in-doubt,-a-submodule-should-become-a-subpackage-or-separate-package","page":"SciML Style Guide for Julia","title":"When in doubt, a submodule should become a subpackage or separate package","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Keep packages to one core idea. If there's something separate enough to be a submodule, could it instead be a separate well-tested and documented package to be used by other packages? Most likely yes.","category":"page"},{"location":"modules/SciMLStyle/#Globals-should-be-avoided-whenever-possible","page":"SciML Style Guide for Julia","title":"Globals should be avoided whenever possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Global variables should be avoided whenever possible. When required, global variables should be consts and have an all uppercase name separated with underscores (e.g. MY_CONSTANT). They should be defined at the top of the file, immediately after imports and exports but before an __init__ function. If you truly want mutable global style behaviour you may want to look into mutable containers.","category":"page"},{"location":"modules/SciMLStyle/#Type-stable-and-Type-grounded-code-is-preferred-wherever-possible","page":"SciML Style Guide for Julia","title":"Type-stable and Type-grounded code is preferred wherever possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Type-stable and type-grounded code helps the compiler create not only more optimized code, but also faster to compile code. Always keep containers well-typed, functions specializing on the appropriate arguments, and types concrete.","category":"page"},{"location":"modules/SciMLStyle/#Closures-should-be-avoided-whenever-possible","page":"SciML Style Guide for Julia","title":"Closures should be avoided whenever possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Closures can cause accidental type instabilities that are difficult to track down and debug; in the long run it saves time to always program defensively and avoid writing closures in the first place, even when a particular closure would not have been problematic. A similar argument applies to reading code with closures; if someone is looking for type instabilities, this is faster to do when code does not contain closures. Furthermore, if you want to update variables in an outer scope, do so explicitly with Refs or self defined structs. For example,","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"map(Base.Fix2(getindex, i), vector_of_vectors)","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"is preferred over","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"map(v -> v[i], vector_of_vectors)","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"or","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"[v[i] for v in vector_of_vectors]","category":"page"},{"location":"modules/SciMLStyle/#Numerical-functionality-should-use-the-appropriate-generic-numerical-interfaces","page":"SciML Style Guide for Julia","title":"Numerical functionality should use the appropriate generic numerical interfaces","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"While you can use A\\b to do a linear solve inside of a package, that does not mean that you should. This interface is only sufficient for performing factorizations, and so that limits the scaling choices, the types of A that can be supported, etc. Instead, linear solves within packages should use LinearSolve.jl. Similarly, nonlinear solves should use NonlinearSolve.jl. Optimization should use Optimization.jl. Etc. This allows the full generic choice to be given to the user without depending on every solver package (effectively recreating the generic interfaces within each package).","category":"page"},{"location":"modules/SciMLStyle/#Functions-should-capture-one-underlying-principle","page":"SciML Style Guide for Julia","title":"Functions should capture one underlying principle","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Functions mean one thing. Every dispatch of + should be \"the meaning of addition on these types\". While in theory you could add dispatches to + that mean something different, that will fail in generic code for which + means addition. Thus for generic code to work, code needs to adhere to one meaning for each function. Every dispatch should be an instantiation of that meaning.","category":"page"},{"location":"modules/SciMLStyle/#Internal-choices-should-be-exposed-as-options-whenever-possible","page":"SciML Style Guide for Julia","title":"Internal choices should be exposed as options whenever possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Whenever possible, numerical values and choices within scripts should be exposed as options to the user. This promotes code reusability beyond the few cases the author may have expected.","category":"page"},{"location":"modules/SciMLStyle/#Prefer-code-reuse-over-rewrites-whenever-possible","page":"SciML Style Guide for Julia","title":"Prefer code reuse over rewrites whenever possible","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"If a package has a function you need, use the package. Add a dependency if you need to. If the function is missing a feature, prefer to add that feature to said package and then add it as a dependency. If the dependency is potentially troublesome, for example because it has a high load time, prefer to spend time helping said package fix these issues and add the dependency. Only when it does not seem possible to make the package \"good enough\" should using the package be abandoned. If it is abandoned, consider building a new package for this functionality as you need it, and then make it a dependency.","category":"page"},{"location":"modules/SciMLStyle/#Prefer-to-not-shadow-functions","page":"SciML Style Guide for Julia","title":"Prefer to not shadow functions","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Two functions can have the same name in Julia by having different namespaces. For example, X.f and Y.f can be two different functions, with different dispatches, but the same name. This should be avoided whenever possible. Instead of creating MyPackage.sort, consider adding dispatches to Base.sort for your types if these new dispatches match the underlying principle of the function. If it doesn't, prefer to use a different name. While using MyPackage.sort is not conflicting, it is going to be confusing for most people unfamiliar with your code, so MyPackage.special_sort would be more helpful to newcomers reading the code.","category":"page"},{"location":"modules/SciMLStyle/#Specific-Rules","page":"SciML Style Guide for Julia","title":"Specific Rules","text":"","category":"section"},{"location":"modules/SciMLStyle/#High-Level-Rules","page":"SciML Style Guide for Julia","title":"High Level Rules","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Use 4 spaces per indentation level, no tabs.\nTry to adhere to a 92 character line length limit.","category":"page"},{"location":"modules/SciMLStyle/#General-Naming-Principles","page":"SciML Style Guide for Julia","title":"General Naming Principles","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"All type names should be CamelCase.\nAll struct names should be CamelCase.\nAll module names should be CamelCase.\nAll function names should be snake_case (all lowercase).\nAll variable names should be snake_case (all lowercase).\nAll constant names should be SNAKE_CASE (all uppercase).\nAll abstract type names should begin with Abstract.\nAll type variable names should be a single capital letter, preferably related to the value being typed.\nWhole words are usually better than abbreviations or single letters.\nVariables meant to be internal or private to a package should be denoted by prepending two underscores, i.e. __.\nSingle letters can be okay when naming a mathematical entity, i.e. an entity whose purpose or non-mathematical \"meaning\" is likely only known by downstream callers. For example, a and b would be appropriate names when implementing *(a::AbstractMatrix, b::AbstractMatrix), since the \"meaning\" of those arguments (beyond their mathematical meaning as matrices, which is already described by the type) is only known by the caller.\nUnicode is fine within code where it increases legibility, but in no case should Unicode be used in public APIs. This is to allow support for terminals which cannot use Unicode: if a keyword argument must be η, then it can be exclusionary to uses on clusters which do not support Unicode inputs.","category":"page"},{"location":"modules/SciMLStyle/#Comments","page":"SciML Style Guide for Julia","title":"Comments","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"TODO to mark todo comments and XXX to mark comments about currently broken code\nQuote code in comments using backticks (e.g. variable_name).\nWhen possible, code should be changed to incorporate information that would have been in a comment. For example, instead of commenting # fx applies the effects to a tree, simply change the function and variable names apply_effects(tree).\nComments referring to Github issues and PRs should add the URL in the comments. Only use inline comments if they fit within the line length limit. If your comment cannot be fitted inline then place the comment above the content to which it refers:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\n\n# Number of nodes to predict. Again, an issue with the workflow order. Should be updated\n# after data is fetched.\np = 1\n\n# No:\n\np = 1  # Number of nodes to predict. Again, an issue with the workflow order. Should be\n# updated after data is fetched.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"In general, comments above a line of code or function are preferred to inline comments.","category":"page"},{"location":"modules/SciMLStyle/#Modules","page":"SciML Style Guide for Julia","title":"Modules","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Module imports should occur at the top of a file or right after a module declaration.\nModule imports in packages should either use import or explicitly declare the imported functionality, for example using Dates: Year, Month, Week, Day, Hour, Minute, Second, Millisecond.\nImport and using statements should be separated, and should be divided by a blank line.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nimport A: a\nimport C\n\nusing B\nusing D: d\n\n# No:\nimport A: a\nusing B\nimport C\nusing D: d","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Exported variables should be considered as part of the public API, and changing their interface constitutes a breaking change.\nAny exported variables should be sufficiently unique. I.e., do not export f as that is very likely to clash with something else.\nA file that includes the definition of a module, should not include any other code that runs outside that module. i.e. the module should be declared at the top of the file with the module keyword and end at the bottom of the file. No other code before, or after (except for module docstring before). In this case the code with in the module block should not be indented.\nSometimes, e.g. for tests, or for namespacing an enumeration, it is desirable to declare a submodule midway through a file. In this case the code within the submodule should be indented.","category":"page"},{"location":"modules/SciMLStyle/#Functions","page":"SciML Style Guide for Julia","title":"Functions","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Only use short-form function definitions when they fit on a single line:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nfoo(x::Int64) = abs(x) + 3\n\n# No:\nfoobar(array_data::AbstractArray{T}, item::T) where {T <: Int64} = T[\n    abs(x) * abs(item) + 3 for x in array_data\n]","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Inputs should be required unless a default is historically expected or likely to be applicable to >95% of use cases. For example, the tolerance of a differential equation solver was set to a default of abstol=1e-6,reltol=1e-3 as a generally correct plot in most cases, and is an expectation from back in the 90's. In that case, using the historically expected and most often useful default tolerances is justified. However, if one implements GradientDescent, the learning rate needs to be adjusted for each application (based on the size of the gradient), and thus a default of GradientDescent(learning_rate = 1) is not recommended.\nArguments which do not have defaults should be preferrably made into positional arguments. The newer syntax of required keyword arguments can be useful but should not be abused. Notable exceptions are cases where \"either or\" arguments are accepted, for example of defining g or dgdu is sufficient, then making them both keyword arguments with = nothing and checking that either is not nothing (and throwing an appropriate error) is recommended if distinct dispatches with different types is not possible. \nWhen calling a function always separate your keyword arguments from your positional arguments with a semicolon. This avoids mistakes in ambiguous cases (such as splatting a Dict).\nWhen writing a function that sends a lot of keyword arguments to another function, say sending keyword arguments to a differential equation solver, use a named tuple keyword argument instead of splatting the keyword arguments. For example, use diffeq_solver_kwargs = (; abstol=1e-6, reltol=1e-6,) as the API and use solve(prob, alg; diffeq_solver_kwargs...) instead of splatting all keyword arguments.\nFunctions which mutate arguments should be appended with !.\nAvoid type piracy. I.e., do not add methods to functions you don't own on types you don't own. Either own the types or the function.\nFunctions should prefer instances instead of types for arguments. For example, for a solver type Tsit5, the interface should use solve(prob,Tsit5()), not solve(prob,Tsit5). The reason for this is multifold. For one, passing a type has different specialization rules, so functionality can be slower unless ::Type{Tsit5} is written in the dispatches which use it. Secondly, this allows for default and keyword arguments to extend the choices, which may become useful for some types down the line. Using this form allows adding more options in a non-breaking manner.\nIf the number of arguments is too large to fit into a 92 character line, then use as many arguments as possible within a line and start each new row with the same indentation, preferably at the same column as the ( but this can be moved left if the function name is very long. For example:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes\nfunction my_large_function(argument1, argument2,\n                           argument3, argument4,\n                           argument5, x, y, z)\n\n# No\nfunction my_large_function(argument1,\n                           argument2,\n                           argument3,\n                           argument4,\n                           argument5,\n                           x,\n                           y,\n                           z)","category":"page"},{"location":"modules/SciMLStyle/#Function-Argument-Precedence","page":"SciML Style Guide for Julia","title":"Function Argument Precedence","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Function argument. Putting a function argument first permits the use of do blocks for passing multiline anonymous functions.\nI/O stream. Specifying the IO object first permits passing the function to functions such as sprint, e.g. sprint(show, x).\nInput being mutated. For example, in fill!(x, v), x is the object being mutated and it appears before the value to be inserted into x.\nType. Passing a type typically means that the output will have the given type. In parse(Int, \"1\"), the type comes before the string to parse. There are many such examples where the type appears first, but it's useful to note that in read(io, String), the IO argument appears before the type, which is in keeping with the order outlined here.\nInput not being mutated. In fill!(x, v), v is not being mutated and it comes after x.\nKey. For associative collections, this is the key of the key-value pair(s). For other indexed collections, this is the index.\nValue. For associative collections, this is the value of the key-value pair(s). In cases like fill!(x, v), this is v.\nEverything else. Any other arguments.\nVarargs. This refers to arguments that can be listed indefinitely at the end of a function call. For example, in Matrix{T}(undef, dims), the dimensions can be given as a Tuple, e.g. Matrix{T}(undef, (1,2)), or as Varargs, e.g. Matrix{T}(undef, 1, 2).\nKeyword arguments. In Julia keyword arguments have to come last anyway in function definitions; they're listed here for the sake of completeness.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"The vast majority of functions will not take every kind of argument listed above; the numbers merely denote the precedence that should be used for any applicable arguments to a function.","category":"page"},{"location":"modules/SciMLStyle/#Tests-and-Continuous-Integration","page":"SciML Style Guide for Julia","title":"Tests and Continuous Integration","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"The high level runtests.jl file should only be used to shuttle to other test files.\nEvery set of tests should be included into a @safetestset. A standard @testset does not fully enclose all defined values, such as functions defined in a @testset, and thus can \"leak\".\nTest includes should be written in one line, for example:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"@time @safetestset \"Jacobian Tests\" begin include(\"interface/jacobian_tests.jl\") end","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Every test script should be fully reproducible in isolation. I.e., one should be able to copy paste that script and receive the results.\nTest scripts should be grouped based on categories, for example tests of the interface vs tests for numerical convergence. Grouped tests should be kept in the same folder.\nA GROUP environment variable should be used to specify test groups for parallel testing in continuous integration. A fallback group All should be used to specify all of the tests that should be run when a developer runs ]test Package locally. As an example, see the OrdinaryDiffEq.jl test structure\nTests should include downstream tests to major packages which use the functionality, to ensure continued support. Any update which breaks the downstream tests should follow with a notification to the downstream package of why the support was broken (preferably in the form of a PR which fixes support), and the package should be given a major version bump in the next release if the changed functionality was part of the public API.\nCI scripts should use the default settings unless required.\nCI scripts should test the Long-Term Support (LTS) release and the current stable release. Nightly tests are only necessary for packages which a heavy reliance on specific compiler details.\nAny package supporting GPUs should include continuous integration for GPUs.\nDoctests should be enabled except for on the examples which are computationally-prohibitive to have as part of continuous integration.","category":"page"},{"location":"modules/SciMLStyle/#Whitespace","page":"SciML Style Guide for Julia","title":"Whitespace","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Avoid extraneous whitespace immediately inside parentheses, square brackets or braces.\n```julia\nYes:\nspam(ham[1], [eggs])\nNo:\nspam( ham[ 1 ], [ eggs ] )   ```\nAvoid extraneous whitespace immediately before a comma or semicolon:\n```julia\nYes:\nif x == 4 @show(x, y); x, y = y, x end\nNo:\nif x == 4 @show(x , y) ; x , y = y , x end   ```\nAvoid whitespace around : in ranges. Use brackets to clarify expressions on either side.\n```julia\nYes:\nham[1:9]   ham[9:-3:0]   ham[1:step:end]   ham[lower:upper-1]   ham[lower:upper - 1]   ham[lower:(upper + offset)]   ham[(lower + offset):(upper + offset)]\nNo:\nham[1: 9]   ham[9 : -3: 1]   ham[lower : upper - 1]   ham[lower + offset:upper + offset]  # Avoid as it is easy to read as ham[lower + (offset:upper) + offset]   ```\nAvoid using more than one space around an assignment (or other) operator to align it with another:\n```julia\nYes:\nx = 1   y = 2   long_variable = 3\nNo:\nx             = 1   y             = 2   long_variable = 3   ```\nSurround most binary operators with a single space on either side: assignment (=), updating operators (+=, -=, etc.), numeric comparisons operators (==, <, >, !=, etc.), lambda operator (->). Binary operators may be excluded from this guideline include: the range operator (:), rational operator (//), exponentiation operator (^), optional arguments/keywords (e.g. f(x = 1; y = 2)).\n```julia\nYes:\ni = j + 1   submitted += 1   x^2 < y\nNo:\ni=j+1   submitted +=1   x^2<y   ```\nAvoid using whitespace between unary operands and the expression:\n```julia\nYes:\n-1   [1 0 -1]\nNo:\n1\n[1 0 - 1]  # Note: evaluates to [1 -1]   ```\nAvoid extraneous empty lines. Avoid empty lines between single line method definitions   and otherwise separate functions with one empty line, plus a comment if required:\n```julia\nYes:\nNote: an empty line before the first long-form domaths method is optional.\ndomaths(x::Number) = x + 5   domaths(x::Int) = x + 10   function domaths(x::String)       return \"A string is a one-dimensional extended object postulated in string theory.\"   end\ndophilosophy() = \"Why?\"\nNo:\ndomath(x::Number) = x + 5\ndomath(x::Int) = x + 10","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"function domath(x::String)\n    return \"A string is a one-dimensional extended object postulated in string theory.\"\nend\n\n\ndophilosophy() = \"Why?\"\n```","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Function calls which cannot fit on a single line within the line limit should be broken up such that the lines containing the opening and closing brackets are indented to the same level while the parameters of the function are indented one level further. In most cases the arguments and/or keywords should each be placed on separate lines. Note that this rule conflicts with the typical Julia convention of indenting the next line to align with the open bracket in which the parameter is contained. If working in a package with a different convention follow the convention used in the package over using this guideline.\n```julia\nYes:\nf(a, b)   constraint = conicform!(SOCElemConstraint(temp2 + temp3, temp2 - temp3, 2 * temp1),                            uniqueconic_forms)\nNo:\nNote: f call is short enough to be on a single line\nf(       a,       b,   )   constraint = conicform!(SOCElemConstraint(temp2 + temp3,                                              temp2 - temp3, 2 * temp1),                            uniqueconic_forms)   ```\nGroup similar one line statements together.\n```julia\nYes:\nfoo = 1   bar = 2   baz = 3\nNo:\nfoo = 1\nbar = 2\nbaz = 3   ```\nUse blank-lines to separate different multi-line blocks.\n```julia\nYes:\nif foo       println(\"Hi\")   end\nfor i in 1:10       println(i)   end\nNo:\nif foo       println(\"Hi\")   end   for i in 1:10       println(i)   end   ```\nAfter a function definition, and before an end statement do not include a blank line.\n```julia\nYes:\nfunction foo(bar::Int64, baz::Int64)       return bar + baz   end\nNo:\nfunction foo(bar::Int64, baz::Int64)\n  return bar + baz\nend\nNo:\nfunction foo(bar::In64, baz::Int64)       return bar + baz\nend   ```\nUse line breaks between control flow statements and returns.\n```julia\nYes:\nfunction foo(bar; verbose = false)       if verbose           println(\"baz\")       end\n  return bar\nend\nOk:\nfunction foo(bar; verbose = false)       if verbose           println(\"baz\")       end       return bar   end   ```","category":"page"},{"location":"modules/SciMLStyle/#NamedTuples","page":"SciML Style Guide for Julia","title":"NamedTuples","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"The = character in NamedTuples should be spaced as in keyword arguments. Space should be put between the name and its value. The empty NamedTuple should be written NamedTuple() not (;)","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nxy = (x = 1, y = 2)\nx = (x = 1,)  # Trailing comma required for correctness.\nx = (; kwargs...)  # Semicolon required to splat correctly.\n\n# No:\nxy = (x=1, y=2)\nxy = (;x=1,y=2)","category":"page"},{"location":"modules/SciMLStyle/#Numbers","page":"SciML Style Guide for Julia","title":"Numbers","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Floating-point numbers should always include a leading and/or trailing zero:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\n0.1\n2.0\n3.0f0\n\n# No:\n.1\n2.\n3.f0","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Always prefer the type Int to Int32 or Int64 unless one has a specific reason to choose the bit size.","category":"page"},{"location":"modules/SciMLStyle/#Ternary-Operator","page":"SciML Style Guide for Julia","title":"Ternary Operator","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Ternary operators (?:) should generally only consume a single line. Do not chain multiple ternary operators. If chaining many conditions, consider using an if-elseif-else conditional, dispatch, or a dictionary.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nfoobar = foo == 2 ? bar : baz\n\n# No:\nfoobar = foo == 2 ?\n    bar :\n    baz\nfoobar = foo == 2 ? bar : foo == 3 ? qux : baz","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"As an alternative, you can use a compound boolean expression:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nfoobar = if foo == 2\n    bar\nelse\n    baz\nend\n\nfoobar = if foo == 2\n    bar\nelseif foo == 3\n    qux\nelse\n    baz\nend","category":"page"},{"location":"modules/SciMLStyle/#For-loops","page":"SciML Style Guide for Julia","title":"For loops","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"For loops should always use in, never = or ∈. This also applies to list and generator comprehensions","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes\nfor i in 1:10\n    #...\nend\n\n[foo(x) for x in xs]\n\n# No:\nfor i = 1:10\n    #...\nend\n\n[foo(x) for x ∈ xs]","category":"page"},{"location":"modules/SciMLStyle/#Function-Type-Annotations","page":"SciML Style Guide for Julia","title":"Function Type Annotations","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Annotations for function definitions should be as general as possible.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nsplicer(arr::AbstractArray, step::Integer) = arr[begin:step:end]\n\n# No:\nsplicer(arr::Array{Int}, step::Int) = arr[begin:step:end]","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Using as generic types as possible allows for a variety of inputs and allows your code to be more general:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"julia> splicer(1:10, 2)\n1:2:9\n\njulia> splicer([3.0, 5, 7, 9], 2)\n2-element Array{Float64,1}:\n 3.0\n 7.0","category":"page"},{"location":"modules/SciMLStyle/#Struct-Type-Annotations","page":"SciML Style Guide for Julia","title":"Struct Type Annotations","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Annotations on type fields need to be given a little more thought since field access is not concrete unless the compiler can infer the type (see type-dispatch design for details). Since well-inferred code is preferred, abstract type annotations, i.e.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"mutable struct MySubString <: AbstractString\n    string::AbstractString\n    offset::Integer\n    endof::Integer\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"are not recommended. Instead a concretely-typed struct:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"mutable struct MySubString <: AbstractString\n    string::String\n    offset::Int\n    endof::Int\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"is preferred. If generality is required, then parametric typing is preferred, i.e.:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"mutable struct MySubString{T<:Integer} <: AbstractString\n    string::String\n    offset::T\n    endof::T\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Untyped fields should be explicitly typed Any, i.e.:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"struct StructA\n    a::Any\nend","category":"page"},{"location":"modules/SciMLStyle/#Macros","page":"SciML Style Guide for Julia","title":"Macros","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Do not add spaces between assignments when there are multiple assignments.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Yes:\n@parameters a = b\n@parameters a=b c=d\n\nNo:\n@parameters a = b c = d","category":"page"},{"location":"modules/SciMLStyle/#Types-and-Type-Annotations","page":"SciML Style Guide for Julia","title":"Types and Type Annotations","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Avoid elaborate union types. Vector{Union{Int,AbstractString,Tuple,Array}} should probably be Vector{Any}. This will reduce the amount of extra strain on compilation checking many branches.\nUnions should be kept to two or three types only for branch splitting. Unions of three types should be kept to a minimum for compile times.\nDo not use === to compare types. Use isa or <: instead.","category":"page"},{"location":"modules/SciMLStyle/#Package-version-specifications","page":"SciML Style Guide for Julia","title":"Package version specifications","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Use Semantic Versioning\nFor simplicity, avoid including the default caret specifier when specifying package version requirements.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"# Yes:\nDataFrames = \"0.17\"\n\n# No:\nDataFrames = \"^0.17\"","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"For accuracy, do not use constructs like >= to avoid upper bounds.\nEvery dependency should have a bound.\nAll packages should use CompatHelper and attempt to stay up to date with the dependencies.\nThe lower bound on dependencies should be the last tested version.","category":"page"},{"location":"modules/SciMLStyle/#Documentation","page":"SciML Style Guide for Julia","title":"Documentation","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Documentation should always attempt to be at the highest level possible. I.e., documentation of an interface that all methods follow is preferred to documenting every method, and documenting the interface of an abstract type is preferred to documenting all of the subtypes individually. All instances should then refer to the higher level documentation.\nDocumentation should use Documenter.jl.\nTutorials should come before reference materials.\nEvery package should have a starting tutorial that covers \"the 90% use case\", i.e. the ways that most people will want to use the package.\nThe tutorial should show a complete workflow and be opinionated in said workflow. For example, when writing a tutorial about a simulator, pick a plotting package and show to plot it.\nVariable names in tutorials are important. If you use u0, then all other codes will copy that naming scheme. Show potential users the right way to use your code with the right naming.\nWhen applicable, tutorials on how to use the \"high performance advanced features\" should be separated from the beginning tutorial.\nAll documentation should summarize contents before going into specifics of API docstrings.\nMost modules, types and functions should have docstrings.\nPrefer documenting accessor functions instead of fields when possible. Documented fields are part of the public API and changing their contents/name constitutes a breaking change.\nOnly exported functions are required to be documented.\nAvoid documenting methods common overloads ==.\nTry to document a function and not individual methods where possible as typically all methods will have similar docstrings.\nIf you are adding a method to a function which already has a docstring only add a docstring if the behaviour of your function deviates from the existing docstring.\nDocstrings are written in Markdown and should be concise.\nDocstring lines should be wrapped at 92 characters.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n    bar(x[, y])\n\nCompute the Bar index between `x` and `y`. If `y` is missing, compute the Bar index between\nall pairs of columns of `x`.\n\"\"\"\nfunction bar(x, y) ...","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"It is recommended that you have a blank line between the headings and the content when the content is of sufficient length.\nTry to be consistent within a docstring whether you use this additional whitespace.\nFollow one of the following templates for types and functions when possible:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Type Template (should be skipped if is redundant with the constructor(s) docstring):","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n    MyArray{T, N}\n\nMy super awesome array wrapper!\n\n# Fields\n- `data::AbstractArray{T, N}`: stores the array being wrapped\n- `metadata::Dict`: stores metadata about the array\n\"\"\"\nstruct MyArray{T, N} <: AbstractArray{T, N}\n    data::AbstractArray{T, N}\n    metadata::Dict\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Function Template (only required for exported functions):","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n    mysearch(array::MyArray{T}, val::T; verbose = true) where {T} -> Int\n\nSearches the `array` for the `val`. For some reason we don't want to use Julia's\nbuiltin search :)\n\n# Arguments\n- `array::MyArray{T}`: the array to search\n- `val::T`: the value to search for\n\n# Keywords\n- `verbose::Bool = true`: print out progress details\n\n# Returns\n- `Int`: the index where `val` is located in the `array`\n\n# Throws\n- `NotFoundError`: I guess we could throw an error if `val` isn't found.\n\"\"\"\nfunction mysearch(array::AbstractArray{T}, val::T) where {T}\n    ...\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"The @doc doc\"\"\" \"\"\" formulation from the Markdown standard library should be used whenever there is LaTeX.\nOnly public fields of types must be documented. Undocumented fields are considered non-public internals.\nIf your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args... and/or kwargs....","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n    Manager(args...; kwargs...) -> Manager\n\nA cluster manager which spawns workers.\n\n# Arguments\n\n- `min_workers::Integer`: The minimum number of workers to spawn or an exception is thrown\n- `max_workers::Integer`: The requested number of workers to spawn\n\n# Keywords\n\n- `definition::AbstractString`: Name of the job definition to use. Defaults to the\n    definition used within the current instance.\n- `name::AbstractString`: ...\n- `queue::AbstractString`: ...\n\"\"\"\nfunction Manager(...)\n    ...\nend","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Feel free to document multiple methods for a function within the same docstring. Be careful to only do this for functions you have defined.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n    Manager(max_workers; kwargs...)\n    Manager(min_workers:max_workers; kwargs...)\n    Manager(min_workers, max_workers; kwargs...)\n\nA cluster manager which spawns workers.\n\n# Arguments\n\n- `min_workers::Int`: The minimum number of workers to spawn or an exception is thrown\n- `max_workers::Int`: The requested number of workers to spawn\n\n# Keywords\n\n- `definition::AbstractString`: Name of the job definition to use. Defaults to the\n    definition used within the current instance.\n- `name::AbstractString`: ...\n- `queue::AbstractString`: ...\n\"\"\"\nfunction Manager end\n","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"If the documentation for bullet-point exceeds 92 characters the line should be wrapped and slightly indented. Avoid aligning the text to the :.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"\"\"\"\n...\n\n# Keywords\n- `definition::AbstractString`: Name of the job definition to use. Defaults to the\n    definition used within the current instance.\n\"\"\"","category":"page"},{"location":"modules/SciMLStyle/#Error-Handling","page":"SciML Style Guide for Julia","title":"Error Handling","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"error(\"string\") should be avoided. Defining and throwing exception types is preferred. See the manual on exceptions for more details.\nTry to avoid try/catch. Use it as minimally as possible. Attempt to catch potential issues before running code, not after.","category":"page"},{"location":"modules/SciMLStyle/#Arrays","page":"SciML Style Guide for Julia","title":"Arrays","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Avoid splatting (...) whenever possible. Prefer iterators such as collect, vcat, hcat, etc. instead.","category":"page"},{"location":"modules/SciMLStyle/#Line-Endings","page":"SciML Style Guide for Julia","title":"Line Endings","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Always use Unix style \\n line ending.","category":"page"},{"location":"modules/SciMLStyle/#VS-Code-Settings","page":"SciML Style Guide for Julia","title":"VS-Code Settings","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"If you are a user of VS Code we recommend that you have the following options in your Julia syntax specific settings. To modify these settings open your VS Code Settings with <kbd>CMD</kbd>+<kbd>,</kbd> (Mac OS) or <kbd>CTRL</kbd>+<kbd>,</kbd> (other OS), and add to your settings.json:","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"{\n    \"[julia]\": {\n        \"editor.detectIndentation\": false,\n        \"editor.insertSpaces\": true,\n        \"editor.tabSize\": 4,\n        \"files.insertFinalNewline\": true,\n        \"files.trimFinalNewlines\": true,\n        \"files.trimTrailingWhitespace\": true,\n        \"editor.rulers\": [92],\n        \"files.eol\": \"\\n\"\n    },\n}","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Additionally you may find the Julia VS-Code plugin useful.","category":"page"},{"location":"modules/SciMLStyle/#JuliaFormatter","page":"SciML Style Guide for Julia","title":"JuliaFormatter","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Note: the sciml style is only available in JuliaFormatter v1.0 or later","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"One can add .JuliaFormatter.toml with the content","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"style = \"sciml\"","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"in the root of a repository, and run","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"using JuliaFormatter, SomePackage\nformat(joinpath(dirname(pathof(SomePackage)), \"..\"))","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"to format the package automatically.","category":"page"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Add FormatCheck.yml to enable the formatting CI. The CI will fail if the repository needs additional formatting. Thus, one should run format before committing.","category":"page"},{"location":"modules/SciMLStyle/#References","page":"SciML Style Guide for Julia","title":"References","text":"","category":"section"},{"location":"modules/SciMLStyle/","page":"SciML Style Guide for Julia","title":"SciML Style Guide for Julia","text":"Many of these style choices were derived from the Julia style guide, the YASGuide, and the Blue style guide.","category":"page"},{"location":"modules/NBodySimulator/#NBodySimulator.jl","page":"Home","title":"NBodySimulator.jl","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Simulating systems of N interacting bodies.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"This project is under development at the moment. The implementation of potential calculations is fairly experimental and has not been extensively verified yet. You can test simulation of different systems now but be aware of possible changes in the future.","category":"page"},{"location":"modules/NBodySimulator/#Basic-Components","page":"Home","title":"Basic Components","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"There are three basic components required for any simulation of systems of N-bodies: bodies, system, and simulation.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Bodies or Particles are the objects which will interact with each other and for which the equations of Newton's 2nd law are solved during the simulation process. Three parameters of a body are necessary, namely: initial location, initial velocity, and mass. MassBody structure represents such particles:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"using StaticArrays\nr = SVector(.0,.0,.0)\nv = SVector(.1,.2,.5)\nmass = 1.25\nbody = MassBody(r,v,mass)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"For the sake of simulation speed, it is advised to use static arrays.  ","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"A System covers bodies and necessary parameters for correct simulation of interaction between particles. For example, to create an entity for a system of gravitationally interacting particles, one needs to use GravitationalSystem constructor:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"const G = 6.67e-11 # m^3/kg/s^2\nsystem = GravitationalSystem(bodies, G)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Simulation is an entity determining parameters of the experiment: time span of simulation, global physical constants, borders of the simulation cell, external magnetic or electric fields, etc. The required arguments for NBodySImulation constructor are the system to be tested and the time span of the simulation.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"tspan = (.0, 10.0)\nsimulation = NBodySimulation(system, tspan)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"There are different types of bodies but they are just containers of particle parameters. The interaction and acceleration of particles are defined by the potentials or force fields.","category":"page"},{"location":"modules/NBodySimulator/#Generating-bodies","page":"Home","title":"Generating bodies","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The package exports quite a useful function for placing similar particles in the nodes of a cubic cell with their velocities distributed in accordance with the Maxwell–Boltzmann law:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"N = 100 # number of bodies/particles\nm = 1.0 # mass of each of them\nv = 10.0 # mean velocity\nL = 21.0 # size of the cell side\n\nbodies = generate_bodies_in_cell_nodes(N, m, v, L)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Molecules for the SPC/Fw water model can be imported from a PDB file:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"molecules = load_water_molecules_from_pdb(\"path_to_pdb_file.pdb\")","category":"page"},{"location":"modules/NBodySimulator/#Potentials","page":"Home","title":"Potentials","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The potentials or force field determines the interaction of particles and, therefore, their acceleration.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"There are several structures for basic physical interactions:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"g_parameters = GravitationalParameters(G)\nm_parameters = MagnetostaticParameters(μ_4π)\nel_potential = ElectrostaticParameters(k, cutoff_radius)\njl_parameters = LennardJonesParameters(ϵ, σ, cutoff_radius)\nspc_water_paramters = SPCFwParameters(rOH, ∠HOH, k_bond, k_angle)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The Lennard-Jones potential is used in molecular dynamics simulations for approximating interactions between neutral atoms or molecules. The SPC/Fw water model is used in water simulations. The meaning of arguments for SPCFwParameters constructor will be clarified further in this documentation.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"PotentialNBodySystem structure represents systems with a custom set of potentials. In other words, the user determines the ways in which the particles are allowed to interact. One can pass the bodies and parameters of interaction potentials into that system. In the case the potential parameters are not set, the particles will move at constant velocities without acceleration during the simulation.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"system = PotentialNBodySystem(bodies, Dict(:gravitational => g_parameters, :electrostatic => el_potential))","category":"page"},{"location":"modules/NBodySimulator/#Custom-Potential","page":"Home","title":"Custom Potential","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"There exists an example of a simulation of an N-body system at absolutely custom potential.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Here is shown how to create custom acceleration functions using tools of NBodySimulator.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"First of all, it is necessary to create a structure for parameters for the custom potential.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"struct CustomPotentialParameters <: PotentialParameters\n    a::AbstractFloat\nend","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Next, the acceleration function for the potential is required. The custom potential defined here creates a force acting on all the particles proportionate to their masses. The first argument of the function determines the potential for which the acceleration should be calculated in this method.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"import NBodySimulator.get_accelerating_function\nfunction get_accelerating_function(p::CustomPotentialParameters, simulation::NBodySimulation)\n    ms = get_masses(simulation.system)\n    (dv, u, v, t, i) -> begin custom_accel = SVector(0.0, 0.0, p.a); dv .= custom_accel*ms[i] end\nend","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"After the parameters and acceleration function have been created, one can instantiate a system of particles interacting with a set of potentials which includes the just-created custom potential:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"parameters = CustomPotentialParameters(-9.8)\nsystem = PotentialNBodySystem(bodies, Dict(:custom_potential_params => parameters))","category":"page"},{"location":"modules/NBodySimulator/#Gravitational-Interaction","page":"Home","title":"Gravitational Interaction","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Using NBodySimulator, it is possible to simulate gravitational interaction of celestial bodies. In fact, any structure for bodies can be used for simulation of gravitational interaction since all those structures are required to have mass as one of their parameters:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"body1 = MassBody(SVector(0.0, 1.0, 0.0), SVector( 5.775e-6, 0.0, 0.0), 2.0)\nbody2 = MassBody(SVector(0.0,-1.0, 0.0), SVector(-5.775e-6, 0.0, 0.0), 2.0)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"When solving a gravitational problem, one needs to specify the gravitational constant G.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"G = 6.673e-11","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Now we have enough parameters to create a GravitationalSystem object:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"system = GravitationalSystem([body1,body2], G)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Usually, we solve an N-body problem for a certain period of time:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"tspan = (0.0, 1111150.0)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The created objects determine the simulation we want to run:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"simulation = NBodySimulation(system, tspan)\nsim_result = run_simulation(simulation)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"And, finally, we can animate our solution showing two equal bodies rotating on the same orbit:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"using Plots\nanimate(sim_result, \"path_to_animated_particles.gif\")","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"<img src=\"https://user-images.githubusercontent.com/16945627/39958539-d2cf779c-561d-11e8-96a8-ffc3a595be8b.gif\" alt=\"Here should appear a gif of rotating bodies\" width=\"350\"/>","category":"page"},{"location":"modules/NBodySimulator/#Electrostatic-Interaction","page":"Home","title":"Electrostatic Interaction","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Interaction between charged particles obeys Coulomb's law. The movement of such bodies can be simulated using ChargedParticle and ChargedParticles structures.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The following example shows how to model two oppositely charged particles. If one body is more massive than another, it will be possible to observe rotation of the light body around the heavy one without adjusting their positions in space. The constructor for the ChargedParticles system requires bodies and Coulomb's constant k to be passed as arguments.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"r = 100.0 # m\nq1 = 1e-3 # C\nq2 = -1e-3 # C\nm1 = 100.0 # kg\nm2 = 0.1 # kg\nv2 = sqrt(abs(k * q1 * q2 / m2 / r)) # m/s - using the centrifugal acceleration\nt = 2 * pi * r / v2 # s  - for one rotation\np1 = ChargedParticle(SVector(0.0, 0.0, 0.0), SVector(0.0, 0, 0.0), m1, q1)\np2 = ChargedParticle(SVector(100.0, 0.0, 0.0), SVector(0.0, v2, 0.0), m2, q2)\nsystem = ChargedParticles([p1, p2], k)\nsimulation = NBodySimulation(system, (0.0, t))\nsim_result = run_simulation(simulation)","category":"page"},{"location":"modules/NBodySimulator/#Magnetic-Interaction","page":"Home","title":"Magnetic Interaction","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"An N-body system consisting of MagneticParticles can be used for simulation of interacting magnetic dipoles, though such dipoles cannot rotate in space. Such a model can represent single domain particles interacting under the influence of a strong external magnetic field.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"In order to create a magnetic particle, one specifies its location in space, velocity, and the vector of its magnetic moment. The following code shows how we can construct an iron particle:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"iron_dencity = 7800 # kg/m^3\nmagnetization_saturation = 1.2e6 # A/m\n\nmass =  5e-6 # kg\nr = SVector(-0.005,0.0,0.0) # m\nv = SVector(0.0,0.0,0.0) # m/s\nmagnetic_moment = SVector(0.0, 0.0, magnetization_saturation * mass / iron_dencity) # A*m^2\n\np1 = MagneticParticle(r, v, mass, magnetic_moment)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"For the second particle, we will use a shorter form:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"p2 = MagneticParticle(SVector(0.005, 0.0, 0.0), SVector(0.0, 0.0, 0.0), 5e-6, SVector(0.0,0.0,0.00077))","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"To calculate magnetic interactions properly, one should also specify the value for the constant μ<sub>0</sub>/4π or its substitute. Having created parameters for the magnetostatic potential, one can now instantiate a system of particles which should interact magnetically. For that purpose, we use PotentialNBodySystem and pass particles and potential parameters as arguments.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"parameters = MagnetostaticParameters(μ_4π)\nsystem = PotentialNBodySystem([p1, p2], Dict(:magnetic => parameters))\nsimulation = NBodySimulation(system, (t1, t2))\nsim_result = run_simulation(simulation, VelocityVerlet(), dt=τ)","category":"page"},{"location":"modules/NBodySimulator/#Molecular-Dynamics-(MD)","page":"Home","title":"Molecular Dynamics (MD)","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"NBodySimulator allows one to conduct molecular dynamic simulations for the Lennard-Jones liquids, SPC/Fw model of water, and other molecular systems thanks to implementations of basic interaction potentials between atoms and molecules:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Lennard-Jones\nelectrostatic and magnetostatic\nharmonic bonds\nharmonic valence angle generated by pairs of bonds","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The comprehensive examples of liquid argon and water simulations can be found in the examples folder. Here only the basic principles of the molecular dynamics simulations using NBodySimulator are presented using liquid argon as a classical MD system for beginners.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"First of all, one needs to define parameters of the simulation:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"T = 120.0 # °K\nT0 = 90.0 # °K\nkb = 8.3144598e-3 # kJ/(K*mol)\nϵ = T * kb\nσ = 0.34 # nm\nρ = 1374/1.6747# Da/nm^3\nm = 39.95# Da\nN = 216\nL = (m*N/ρ)^(1/3)#10.229σ\nR = 0.5*L   \nv_dev = sqrt(kb * T / m)\nbodies = generate_bodies_in_cell_nodes(N, m, v_dev, L)\n\nτ = 0.5e-3 # ps or 1e-12 s\nt1 = 0.0\nt2 = 2000τ","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Liquid argon consists of neutral molecules so the Lennard-Jones potential runs their interaction:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"parameters = LennardJonesParameters(ϵ, σ, R)\nlj_system = PotentialNBodySystem(bodies, Dict(:lennard_jones => parameters));","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Then, a thermostat and boundary conditions should be selected and instantiated:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"thermostat = NoseHooverThermostat(T0, 200τ)\npbc = CubicPeriodicBoundaryConditions(L)\nsimulation = NBodySimulation(lj_system, (t1, t2), pbc, thermostat, kb);\nresult = run_simulation(simulation, VelocityVerlet(), dt=τ)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"It is recommended to use CubicPeriodicBoundaryConditions since cubic boxes are among the most popular boundary conditions in MD. There are different variants of the NBodySimulation constructor for MD:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"simulation = NBodySimulation(lj_system, (t1, t2));\nsimulation = NBodySimulation(lj_system, (t1, t2), pbc);\nsimulation = NBodySimulation(lj_system, (t1, t2), pbc, thermostat);\nsimulation = NBodySimulation(lj_system, (t1, t2), pbc, thermostat, kb);","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The default boundary conditions are InfiniteBox without any limits, default thermostat is NullThermostat (which does no thermostating), and the default Boltzmann constant kb equals its value in SI, i.e., 1.38e-23 J/K.","category":"page"},{"location":"modules/NBodySimulator/#Water-Simulations","page":"Home","title":"Water Simulations","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"In NBodySImulator the SPC/Fw water model is implemented. For using this model, one has to specify parameters of the Lennard-Jones potential between the oxygen atoms of water molecules, parameters of the electrostatic potential for the corresponding interactions between atoms of different molecules and parameters for harmonic potentials representing bonds between atoms and the valence angle made from bonds between hydrogen atoms and the oxygen atom.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"bodies = generate_bodies_in_cell_nodes(N, mH2O, v, L)\njl_parameters = LennardJonesParameters(ϵOO, σOO, R)\ne_parameters = ElectrostaticParameters(k, Rel)\nspc_paramters = SPCFwParameters(rOH, ∠HOH, k_bond, k_angle)\nwater = WaterSPCFw(bodies, mH, mO, qH, qO,  jl_parameters, e_parameters, spc_paramters);","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"For each water molecule here, rOH is the equilibrium distance between a hydrogen atom and the oxygen atom, ∠HOH denotes the equilibrium angle made of those two bonds, k_bond and k_angle are the elastic coefficients for the corresponding harmonic potentials.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Further, one can pass the water system into the NBodySimulation constructor as a usual system of N-bodies.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"simulation = NBodySimulation(water, (t1, t2), pbc, kb);","category":"page"},{"location":"modules/NBodySimulator/#Thermostats","page":"Home","title":"Thermostats","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Usually, during the simulation, a system is required to be at a particular temperature. NBodySimulator contains several thermostats for that purpose. Here the thermostating of liquid argon is presented, for thermostating of water one can refer to this post","category":"page"},{"location":"modules/NBodySimulator/#[Andersen-Thermostat](http://www.sklogwiki.org/SklogWiki/index.php/Andersen_thermostat)","page":"Home","title":"Andersen Thermostat","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"τ = 0.5e-3 # timestep of integration and simulation\nT0 = 90\nν = 0.05/τ\nthermostat = AndersenThermostat(90, ν)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(Image: andersen thermostating)","category":"page"},{"location":"modules/NBodySimulator/#[Berendsen-Thermostat](http://www2.mpip-mainz.mpg.de/andrienk/journal_club/thermostats.pdf)","page":"Home","title":"Berendsen Thermostat","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"τB = 2000τ\nthermostat = BerendsenThermostat(90, τB)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(Image: berendsen thermostating)","category":"page"},{"location":"modules/NBodySimulator/#[Nosé–Hoover-Thermostat](http://www.sklogwiki.org/SklogWiki/index.php/Nos%C3%A9-Hoover_thermostat)","page":"Home","title":"Nosé–Hoover Thermostat","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"τNH = 200τ\nthermostat = NoseHooverThermostat(T0, 200τ)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(Image: nose-hoover thermostating)","category":"page"},{"location":"modules/NBodySimulator/#Langevin-Thermostat","page":"Home","title":"Langevin Thermostat","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"γ = 10.0\nthermostat = LangevinThermostat(90, γ)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(Image: langevin thermostating)","category":"page"},{"location":"modules/NBodySimulator/#Analyzing-the-Result-of-Simulation","page":"Home","title":"Analyzing the Result of Simulation","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Once the simulation is completed, one can analyze the result and obtain some useful characteristics of the system.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The function run_simulation returns a structure containing the initial parameters of the simulation and the solution of the differential equation (DE) required for the description of the corresponding system of particles. There are different functions which help to interpret the solution of DEs into physical quantities.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"One of the main characteristics of a system during molecular dynamics simulations is its thermodynamic temperature. The value of the temperature at a particular time t can be obtained via calling this function:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"T = temperature(result, t)","category":"page"},{"location":"modules/NBodySimulator/#[Radial-distribution-functions](https://en.wikipedia.org/wiki/Radial_distribution_function)","page":"Home","title":"Radial distribution functions","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The RDF is another popular and essential characteristic of molecules or similar systems of particles. It shows the reciprocal location of particles averaged by the time of simulation.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(rs, grf) = rdf(result)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The dependence of grf on rs shows radial distribution of particles at different distances from an average particle in a system. Here the radial distribution function for the classic system of liquid argon is presented: (Image: rdf for liquid argon)","category":"page"},{"location":"modules/NBodySimulator/#Mean-Squared-Displacement-(MSD)","page":"Home","title":"Mean Squared Displacement (MSD)","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"The MSD characteristic can be used to estimate the shift of particles from their initial positions.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"(ts, dr2) = msd(result)","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"For a standard liquid argon system, the displacement grows with time: (Image: rdf for liquid argon)","category":"page"},{"location":"modules/NBodySimulator/#Energy-Functions","page":"Home","title":"Energy Functions","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Energy is a highly important physical characteristic of a system. The module provides four functions to obtain it, though the total_energy function just sums potential and kinetic energy:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"e_init = initial_energy(simulation)\ne_kin = kinetic_energy(result, t)\ne_pot = potential_energy(result, t)\ne_tot = total_energy(result, t)","category":"page"},{"location":"modules/NBodySimulator/#Plotting-Images","page":"Home","title":"Plotting Images","text":"","category":"section"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Using tools of NBodySimulator, one can export the results of a simulation into a Protein Database File. VMD is a well-known tool for visualizing molecular dynamics, which can read data from PDB files.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"save_to_pdb(result, \"path_to_a_new_pdb_file.pdb\" )","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"In the future it will be possible to export results via FileIO interface and its save function.","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Using Plots.jl, one can draw positions of particles at any time of simulation or create an animation of moving particles, molecules of water:","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"using Plots\nplot(result)\nanimate(result, \"path_to_file.gif\")","category":"page"},{"location":"modules/NBodySimulator/","page":"Home","title":"Home","text":"Makie.jl also has a recipe for plotting the results of N-body simulations. The example is presented in the documentation.","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/#Numerically-Solving-Integrals","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"","category":"section"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"For basic multidimensional quadrature we can construct and solve a IntegralProblem:","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"using Integrals\nf(x,p) = sum(sin.(x))\nprob = IntegralProblem(f,ones(2),3ones(2))\nsol = solve(prob,HCubatureJL(),reltol=1e-3,abstol=1e-3)","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"If we would like to parallelize the computation, we can use the batch interface to compute multiple points at once. For example, here we do allocation-free multithreading with Cubature.jl:","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"using Integrals, Cubature, Base.Threads\nfunction f(dx,x,p)\n  Threads.@threads for i in 1:size(x,2)\n    dx[i] = sum(sin.(@view(x[:,i])))\n  end\nend\nprob = IntegralProblem(f,ones(2),3ones(2),batch=2)\nsol = solve(prob,CubatureJLh(),reltol=1e-3,abstol=1e-3)","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"If we would like to compare the results against Cuba.jl's Cuhre method, then the change is a one-argument change:","category":"page"},{"location":"modules/Integrals/tutorials/numerical_integrals/","page":"Numerically Solving Integrals","title":"Numerically Solving Integrals","text":"using IntegralsCuba\nsol = solve(prob,CubaCuhre(),reltol=1e-3,abstol=1e-3)","category":"page"},{"location":"modules/StructuralIdentifiability/input/input/#Parsing-input-ODE-system","page":"Parsing input ODE system","title":"Parsing input ODE system","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/input/input/","page":"Parsing input ODE system","title":"Parsing input ODE system","text":"@ODEmodel(ex::Expr...)\nODE\nset_parameter_values","category":"page"},{"location":"modules/StructuralIdentifiability/input/input/#StructuralIdentifiability.@ODEmodel-Tuple{Vararg{Expr}}","page":"Parsing input ODE system","title":"StructuralIdentifiability.@ODEmodel","text":"Macro for creating an ODE from a list of equations. Also injects all variables into the global scope.\n\nThis macro accepts a sybolically written ODE system and generates an ODE structure instance:\n\node = @ODEmodel(\n    x1'(t) = -k1 * x1(t),\n    y1(t) = x1(t)\n)\n\n\n\n\n\n","category":"macro"},{"location":"modules/StructuralIdentifiability/input/input/#StructuralIdentifiability.ODE","page":"Parsing input ODE system","title":"StructuralIdentifiability.ODE","text":"The main structure that represents input ODE system.\n\nStores information about states (x_vars), outputs (y_vars), inputs (u_vars), parameters (parameters) and the equations.\n\nThis structure is constructed via @ODEmodel macro.\n\n\n\n\n\n","category":"type"},{"location":"modules/StructuralIdentifiability/input/input/#StructuralIdentifiability.set_parameter_values","page":"Parsing input ODE system","title":"StructuralIdentifiability.set_parameter_values","text":"set_parameter_values(ode, param_values)\n\nInput:\n\node - an ODE as above\nparam_values - values for (possibly, some of) the parameters as dictionary parameter => value\n\nOutput: \n\nnew ode with the parameters in param_values plugged with the given numbers\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/input/input/#Create-Compartmental-Model","page":"Parsing input ODE system","title":"Create Compartmental Model","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/input/input/","page":"Parsing input ODE system","title":"Parsing input ODE system","text":"linear_compartment_model","category":"page"},{"location":"modules/StructuralIdentifiability/input/input/#StructuralIdentifiability.linear_compartment_model","page":"Parsing input ODE system","title":"StructuralIdentifiability.linear_compartment_model","text":"linear_compartment_model(graph, inputs, outputs, leaks)\n\nInput: defines a linear compartment model with nodes numbered from 1 to n by\n\ngraph - and array of integer arrays representing the adjacency lists of the graph\ninputs - array of input nodes\noutputs - array of output nodes\nleaks - array of sink nodes\n\nOutput:\n\nthe corresponding ODE system in the notation of https://doi.org/10.1007/s11538-015-0098-0\n\n\n\n\n\n","category":"function"},{"location":"modules/MethodOfLines/MOLFiniteDifference/#molfd","page":"MOLFiniteDifference","title":"Discretization","text":"","category":"section"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"struct MOLFiniteDifference{G} <: DiffEqBase.AbstractDiscretization\n    dxs\n    time\n    approx_order::Int\n    upwind_order::Int\n    grid_align::G\nend\n\n# Constructors. If no order is specified, both upwind and centered differences will be 2nd order\nfunction MOLFiniteDifference(dxs, time=nothing; approx_order = 2, upwind_order = 1, grid_align=CenterAlignedGrid())\n    \n    if approx_order % 2 != 0\n        @warn \"Discretization approx_order must be even, rounding up to $(approx_order+1)\"\n    end\n    @assert approx_order >= 1 \"approx_order must be at least 1\"\n    @assert upwind_order >= 1 \"upwind_order must be at least 1\"\n    \n    return MOLFiniteDifference{typeof(grid_align)}(dxs, time, approx_order, upwind_order, grid_align)\nend","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"eq = [your system of equations, see examples for possibilities]\nbcs = [your boundary conditions, see examples for possibilities]\n\ndomain = [your domain, a vector of Intervals i.e. x ∈ Interval(x_min, x_max)]\n\n@named pdesys = PDESystem(eq, bcs, domains, [t, x, y], [u(t, x, y)])\n\ndiscretization = MOLFiniteDifference(dxs, \n                                      <your choice of continuous variable, usually time>; \n                                      upwind_order = <Currently unstable at any value other than 1>, \n                                      approx_order = <Order of derivative approximation, starting from 2> \n                                      grid_align = <your grid type choice>)\nprob = discretize(pdesys, discretization)","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"Where dxs is a vector of pairs of parameters to the grid step in this dimension, i.e. [x=>0.2, y=>0.1]. For a non uniform rectilinear grid, replace any or all of the step sizes with the grid you'd like to use with that variable, must be an AbstractVector but not a StepRangeLen.","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"Note that the second argument to MOLFiniteDifference is optional, all parameters can be discretized if all required boundary conditions are specified.","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"Currently supported grid types: center_align and edge_align. Edge align will give better accuracy with Neumann boundary conditions.","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"center_align: naive grid, starting from lower boundary, ending on upper boundary with step of dx","category":"page"},{"location":"modules/MethodOfLines/MOLFiniteDifference/","page":"MOLFiniteDifference","title":"MOLFiniteDifference","text":"edge_align: offset grid, set halfway between the points that would be generated with center_align, with extra points at either end that are above and below the supremum and infimum by dx/2. This improves accuracy for Neumann BCs.","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Lobachevsky-surrogate-tutorial","page":"Lobachevsky","title":"Lobachevsky surrogate tutorial","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky splines function is a function that used for univariate and multivariate scattered interpolation. Introduced by Lobachevsky in 1842 to investigate errors in astronomical measurements.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"We are going to use a Lobachevsky surrogate to optimize f(x)=sin(x)+sin(103 * x).","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"First of all import Surrogates and Plots.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Sampling","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"We choose to sample f in 4 points between 0 and 4 using the sample function. The sampling points are chosen using a Sobol sequence, this can be done by passing SobolSample() to the sample function.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"f(x) = sin(x) + sin(10/3 * x)\nn_samples = 5\nlower_bound = 1.0\nupper_bound = 4.0\nx = sample(n_samples, lower_bound, upper_bound, SobolSample())\ny = f.(x)\nscatter(x, y, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\", xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Building-a-surrogate","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"With our sampled points we can build the Lobachevsky surrogate using the LobachevskySurrogate function.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"lobachevsky_surrogate behaves like an ordinary function which we can simply plot. Alpha is the shape parameters and n specify how close you want lobachevsky function to radial basis function.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"alpha = 2.0\nn = 6\nlobachevsky_surrogate = LobachevskySurrogate(x, y, lower_bound, upper_bound, alpha = 2.0, n = 6)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound))\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Optimizing","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Having built a surrogate, we can now use it to search for minima in our original function f.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"To optimize using our surrogate we call surrogate_optimize method. We choose to use Stochastic RBF as optimization technique and again Sobol sampling as sampling technique.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"@show surrogate_optimize(f, SRBF(), lower_bound, upper_bound, lobachevsky_surrogate, SobolSample())\nscatter(x, y, label=\"Sampled points\")\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound))\nplot!(lobachevsky_surrogate, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound))","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"In the example below, it shows how to use lobachevsky_surrogate for higher dimension problems.","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Lobachevsky-Surrogate-Tutorial-(ND):","page":"Lobachevsky","title":"Lobachevsky Surrogate Tutorial (ND):","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"First of all we will define the Schaffer function we are going to build surrogate for. Notice, one how its argument is a vector of numbers, one for each coordinate, and its output is a scalar.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"using Plots # hide\ndefault(c=:matter, legend=false, xlabel=\"x\", ylabel=\"y\") # hide\nusing Surrogates # hide\n\nfunction schaffer(x)\n    x1=x[1]\n    x2=x[2]\n    fact1 = x1 ^2;\n    fact2 = x2 ^2;\n    y = fact1 + fact2;\nend","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Sampling-2","page":"Lobachevsky","title":"Sampling","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Let's define our bounds, this time we are working in two dimensions. In particular we want our first dimension x to have bounds 0, 8, and 0, 8 for the second dimension. We are taking 60 samples of the space using Sobol Sequences. We then evaluate our function on all of the sampling points.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"n_samples = 60\nlower_bound = [0.0, 0.0]\nupper_bound = [8.0, 8.0]\n\nxys = sample(n_samples, lower_bound, upper_bound, SobolSample())\nzs = schaffer.(xys);","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"x, y = 0:8, 0:8 # hide\np1 = surface(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nxs = [xy[1] for xy in xys] # hide\nys = [xy[2] for xy in xys] # hide\nscatter!(xs, ys, zs) # hide\np2 = contour(x, y, (x1,x2) -> schaffer((x1,x2))) # hide\nscatter!(xs, ys) # hide\nplot(p1, p2, title=\"True function\") # hide","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Building-a-surrogate-2","page":"Lobachevsky","title":"Building a surrogate","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Using the sampled points we build the surrogate, the steps are analogous to the 1-dimensional case.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Lobachevsky = LobachevskySurrogate(xys, zs,  lower_bound, upper_bound, alpha = [2.4,2.4], n=8)","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2, title=\"Surrogate\") # hide","category":"page"},{"location":"modules/Surrogates/lobachevsky/#Optimizing-2","page":"Lobachevsky","title":"Optimizing","text":"","category":"section"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"With our surrogate we can now search for the minima of the function.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"Notice how the new sampled points, which were created during the optimization process, are appended to the xys array. This is why its size changes.","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"surrogate_optimize(schaffer, SRBF(), lower_bound, upper_bound, Lobachevsky, SobolSample(), maxiters=1, num_new_samples=10)","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"size(Lobachevsky.x)","category":"page"},{"location":"modules/Surrogates/lobachevsky/","page":"Lobachevsky","title":"Lobachevsky","text":"p1 = surface(x, y, (x, y) -> Lobachevsky([x y])) # hide\nxys = Lobachevsky.x # hide\nxs = [i[1] for i in xys] # hide\nys = [i[2] for i in xys] # hide\nzs = schaffer.(xys) # hide\nscatter!(xs, ys, zs, marker_z=zs) # hide\np2 = contour(x, y, (x, y) -> Lobachevsky([x y])) # hide\nscatter!(xs, ys, marker_z=zs) # hide\nplot(p1, p2) # hide","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/#sensealg","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"Automatic differentiation control is done through the sensealg keyword argument. Hooks exist in the high level interfaces for solve which shuttle the definitions of automatic differentiation overloads to dispatches defined in DiffEqSensitivity.jl (should be renamed SciMLSensitivity.jl as it expands). This is done by first entering a top-level solve definition, for example:","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"function solve(prob::AbstractDEProblem, args...; sensealg=nothing,\n  u0=nothing, p=nothing, kwargs...)\n  u0 = u0 !== nothing ? u0 : prob.u0\n  p = p !== nothing ? p : prob.p\n  if sensealg === nothing && haskey(prob.kwargs, :sensealg)\n    sensealg = prob.kwargs[:sensealg]\n  end\n  solve_up(prob, sensealg, u0, p, args...; kwargs...)\nend","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"solve_up then drops down the differentiable arguments as positional arguments, which is required for the ChainRules.jl interface. Then the ChainRules overloads are written on the solve_up calls, like:","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"function ChainRulesCore.frule(::typeof(solve_up), prob,\n  sensealg::Union{Nothing,AbstractSensitivityAlgorithm},\n  u0, p, args...;\n  kwargs...)\n  _solve_forward(prob, sensealg, u0, p, args...; kwargs...)\nend\n\nfunction ChainRulesCore.rrule(::typeof(solve_up), prob::SciMLBase.AbstractDEProblem,\n  sensealg::Union{Nothing,AbstractSensitivityAlgorithm},\n  u0, p, args...;\n  kwargs...)\n  _solve_adjoint(prob, sensealg, u0, p, args...; kwargs...)\nend","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"Default definitions then exist to throw an informative error if the sensitivity mechanism is not added:","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"function _concrete_solve_adjoint(args...; kwargs...)\n  error(\"No adjoint rules exist. Check that you added `using DiffEqSensitivity`\")\nend\n\nfunction _concrete_solve_forward(args...; kwargs...)\n  error(\"No sensitivity rules exist. Check that you added `using DiffEqSensitivity`\")\nend","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"The sensitivity mechanism is kept in a separate package because of the high dependency and load time cost introduced by the automatic differentiation libraries. Different choices of automatic differentiation are then selected by the sensealg keyword argument in solve, which is made into a positional argument in the _solve_adjoint and other functions in order to allow dispatch.","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/#SensitivityADPassThrough","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"SensitivityADPassThrough","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"The special sensitivity algorithm SensitivityADPassThrough is used to ignore the internal sensitivity dispatches and instead do automatic differentiation directly through the solver. Generally this sensealg is only used internally.","category":"page"},{"location":"modules/SciMLBase/interfaces/Differentiation/#Note-about-ForwardDiff","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Note about ForwardDiff","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Differentiation/","page":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","title":"Automatic Differentiation and Sensitivity Algorithms (Adjoints)","text":"ForwardDiff does not use ChainRules.jl and thus it completely ignores the special handling.","category":"page"},{"location":"modules/DiffEqOperators/operators/operator_overview/#Operator-Overview","page":"Operator Overview","title":"Operator Overview","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/operator_overview/","page":"Operator Overview","title":"Operator Overview","text":"The operators in DiffEqOperators.jl are instantiations of the AbstractSciMLOperator interface. This is documented in SciMLBase. Thus each of the operators have the functions and traits as defined for the operator interface. In addition, the DiffEqOperators.jl operators satisfy the following properties:","category":"page"},{"location":"modules/DiffEqOperators/operators/operator_overview/","page":"Operator Overview","title":"Operator Overview","text":"Derivative * Boundary gives a GhostDerivative operator, representing a derivative operator which respects boundary conditions\nBoundary conditions generate extended vectors in a non-allocating form\nOperators can be concretized into matrices","category":"page"},{"location":"modules/DiffEqOperators/operators/operator_overview/#Operator-Compositions","page":"Operator Overview","title":"Operator Compositions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/operator_overview/","page":"Operator Overview","title":"Operator Overview","text":"Multiplying two DiffEqOperators will build a DiffEqOperatorComposition, while adding two DiffEqOperators will build a DiffEqOperatorCombination. Multiplying a DiffEqOperator by a scalar will produce a DiffEqScaledOperator. All will inherit the appropriate action.","category":"page"},{"location":"modules/DiffEqOperators/operators/operator_overview/#Efficiency-of-Composed-Operator-Actions","page":"Operator Overview","title":"Efficiency of Composed Operator Actions","text":"","category":"section"},{"location":"modules/DiffEqOperators/operators/operator_overview/","page":"Operator Overview","title":"Operator Overview","text":"Composed operator actions utilize NNLib.jl in order to do cache-efficient convolution operations in higher-dimensional combinations.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Augmented-Neural-Ordinary-Differential-Equations","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Copy-Pasteable-Code","page":"Augmented Neural Ordinary Differential Equations","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader\n\nfunction random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend\n\nfunction concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend\n\ndiffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : AugmentedNDELayer(node, augment_dim)\n    return Chain((x, p=node.p) -> node(x, p),\n                 Array,\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend\n\nfunction plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend\n\nloss_node(x, y) = mean((model(x) .- y) .^ 2)\n\nprintln(\"Generating Dataset\")\n\ndataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)\n\niter = 0\ncb = function()\n    global iter \n    iter += 1\n    if iter % 10 == 0\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend\n\nmodel, parameters = construct_model(1, 2, 64, 0)\nopt = ADAM(0.005)\n\nprintln(\"Training Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(parameters, model), dataloader, opt, cb = cb)\nend\n\nplt_node = plot_contour(model)\n\nmodel, parameters = construct_model(1, 2, 64, 1)\nopt = ADAM(0.005)\n\nprintln()\nprintln(\"Training Augmented Neural ODE\")\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(parameters, model), dataloader, opt, cb = cb)\nend\n\nplt_anode = plot_contour(model)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Step-by-Step-Explanation","page":"Augmented Neural Ordinary Differential Equations","title":"Step-by-Step Explanation","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Loading-required-packages","page":"Augmented Neural Ordinary Differential Equations","title":"Loading required packages","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"using DiffEqFlux, DifferentialEquations\nusing Statistics, LinearAlgebra, Plots\nusing Flux.Data: DataLoader","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Generating-a-toy-dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Generating a toy dataset","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In this example, we will be using data sampled uniformly in two concentric circles and then train our Neural ODEs to do regression on that values. We assign 1 to any point which lies inside the inner circle, and -1 to any point which lies between the inner and outer circle. Our first function random_point_in_sphere samples points uniformly between 2 concentric circles/spheres of radii min_radius and max_radius respectively.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function random_point_in_sphere(dim, min_radius, max_radius)\n    distance = (max_radius - min_radius) .* (rand(1) .^ (1.0 / dim)) .+ min_radius\n    direction = randn(dim)\n    unit_direction = direction ./ norm(direction)\n    return distance .* unit_direction\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we will construct a dataset of these points and use Flux's DataLoader to automatically minibatch and shuffle the data.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function concentric_sphere(dim, inner_radius_range, outer_radius_range,\n                           num_samples_inner, num_samples_outer; batch_size = 64)\n    data = []\n    labels = []\n    for _ in 1:num_samples_inner\n        push!(data, reshape(random_point_in_sphere(dim, inner_radius_range...), :, 1))\n        push!(labels, ones(1, 1))\n    end\n    for _ in 1:num_samples_outer\n        push!(data, reshape(random_point_in_sphere(dim, outer_radius_range...), :, 1))\n        push!(labels, -ones(1, 1))\n    end\n    data = cat(data..., dims=2)\n    labels = cat(labels..., dims=2)\n    return DataLoader((data |> gpu, labels |> gpu); batchsize=batch_size, shuffle=true,\n                      partial=false)\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Models","page":"Augmented Neural Ordinary Differential Equations","title":"Models","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We consider 2 models in this tutorial. The first is a simple Neural ODE which is described in detail in this tutorial. The other one is an Augmented Neural ODE [1]. The idea behind this layer is very simple. It augments the input to the Neural DE Layer by appending zeros. So in order to use any arbitrary DE Layer in combination with this layer, simply assume that the input to the DE Layer is of size size(x, 1) + augment_dim instead of size(x, 1) and construct that layer accordingly.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"In order to run the models on GPU, we need to manually transfer the models to GPU. First one is the network predicting the derivatives inside the Neural ODE and the other one is the last layer in the Chain.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"diffeqarray_to_array(x) = reshape(gpu(x), size(x)[1:2])\n\nfunction construct_model(out_dim, input_dim, hidden_dim, augment_dim)\n    input_dim = input_dim + augment_dim\n    node = NeuralODE(Chain(Dense(input_dim, hidden_dim, relu),\n                           Dense(hidden_dim, hidden_dim, relu),\n                           Dense(hidden_dim, input_dim)) |> gpu,\n                     (0.f0, 1.f0), Tsit5(), save_everystep = false,\n                     reltol = 1e-3, abstol = 1e-3, save_start = false) |> gpu\n    node = augment_dim == 0 ? node : (AugmentedNDELayer(node, augment_dim) |> gpu)\n    return Chain((x, p=node.p) -> node(x, p),\n                 Array,\n                 diffeqarray_to_array,\n                 Dense(input_dim, out_dim) |> gpu), node.p |> gpu\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Plotting-the-Results","page":"Augmented Neural Ordinary Differential Equations","title":"Plotting the Results","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here, we define an utility to plot our model regression results as a heatmap.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"function plot_contour(model, npoints = 300)\n    grid_points = zeros(2, npoints ^ 2)\n    idx = 1\n    x = range(-4.0, 4.0, length = npoints)\n    y = range(-4.0, 4.0, length = npoints)\n    for x1 in x, x2 in y\n        grid_points[:, idx] .= [x1, x2]\n        idx += 1\n    end\n    sol = reshape(model(grid_points |> gpu), npoints, npoints) |> cpu\n\n    return contour(x, y, sol, fill = true, linewidth=0.0)\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Training-Parameters","page":"Augmented Neural Ordinary Differential Equations","title":"Training Parameters","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Loss-Functions","page":"Augmented Neural Ordinary Differential Equations","title":"Loss Functions","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use the L2 distance between the model prediction model(x) and the actual prediction y as the optimization objective.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"loss_node(x, y) = mean((model(x) .- y) .^ 2)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Dataset","page":"Augmented Neural Ordinary Differential Equations","title":"Dataset","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Next, we generate the dataset. We restrict ourselves to 2 dimensions as it is easy to visualize. We sample a total of 4000 data points.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"dataloader = concentric_sphere(2, (0.0, 2.0), (3.0, 4.0), 2000, 2000; batch_size = 256)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Callback-Function","page":"Augmented Neural Ordinary Differential Equations","title":"Callback Function","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Additionally we define a callback function which displays the total loss at specific intervals.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"iter = 0\ncb = function()\n    global iter += 1\n    if iter % 10 == 1\n        println(\"Iteration $iter || Loss = $(loss_node(dataloader.data[1], dataloader.data[2]))\")\n    end\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Optimizer","page":"Augmented Neural Ordinary Differential Equations","title":"Optimizer","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"We use ADAM as the optimizer with a learning rate of 0.005","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"opt = ADAM(0.005)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Training-the-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Neural ODE","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"To train our neural ode model, we need to pass the appropriate learnable parameters, parameters which is returned by the construct_models function. It is simply the node.p vector. We then train our model for 20 epochs.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 0)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(model, parameters), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Here is what the contour plot should look for Neural ODE. Notice that the regression is not perfect due to the thin artifact which connects the circles.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: node)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Training-the-Augmented-Neural-ODE","page":"Augmented Neural Ordinary Differential Equations","title":"Training the Augmented Neural ODE","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Our training configuration will be same as that of Neural ODE. Only in this case we have augmented the input with a single zero. This makes the problem 3 dimensional and as such it is possible to find a function which can be expressed by the neural ode. For more details and proofs please refer to [1].","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"model, parameters = construct_model(1, 2, 64, 1)\n\nfor _ in 1:10\n    Flux.train!(loss_node, Flux.params(model, parameters), dataloader, opt, cb = cb)\nend","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"For the augmented Neural ODE we notice that the artifact is gone.","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"(Image: anode)","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#Expected-Output","page":"Augmented Neural Ordinary Differential Equations","title":"Expected Output","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"Generating Dataset\nTraining Neural ODE\nIteration 10 || Loss = 0.9802582\nIteration 20 || Loss = 0.6727416\nIteration 30 || Loss = 0.5862373\nIteration 40 || Loss = 0.5278132\nIteration 50 || Loss = 0.4867624\nIteration 60 || Loss = 0.41630346\nIteration 70 || Loss = 0.3325938\nIteration 80 || Loss = 0.28235924\nIteration 90 || Loss = 0.24069068\nIteration 100 || Loss = 0.20503852\nIteration 110 || Loss = 0.17608969\nIteration 120 || Loss = 0.1491399\nIteration 130 || Loss = 0.12711425\nIteration 140 || Loss = 0.10686825\nIteration 150 || Loss = 0.089558244\n\nTraining Augmented Neural ODE\nIteration 10 || Loss = 1.3911372\nIteration 20 || Loss = 0.7694144\nIteration 30 || Loss = 0.5639633\nIteration 40 || Loss = 0.33187616\nIteration 50 || Loss = 0.14787851\nIteration 60 || Loss = 0.094676435\nIteration 70 || Loss = 0.07363529\nIteration 80 || Loss = 0.060333826\nIteration 90 || Loss = 0.04998395\nIteration 100 || Loss = 0.044843454\nIteration 110 || Loss = 0.042587914\nIteration 120 || Loss = 0.042706195\nIteration 130 || Loss = 0.040252227\nIteration 140 || Loss = 0.037686247\nIteration 150 || Loss = 0.036247417","category":"page"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/#References","page":"Augmented Neural Ordinary Differential Equations","title":"References","text":"","category":"section"},{"location":"modules/DiffEqFlux/examples/augmented_neural_ode/","page":"Augmented Neural Ordinary Differential Equations","title":"Augmented Neural Ordinary Differential Equations","text":"[1] Dupont, Emilien, Arnaud Doucet, and Yee Whye Teh. \"Augmented neural ODEs.\" In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pp. 3140-3150. 2019.","category":"page"},{"location":"modules/DiffEqDocs/analysis/global_sensitivity/#gsa","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Global Sensitivity Analysis (GSA) methods are used to quantify the uncertainty in output of a model w.r.t. the parameters, their individual contributions, or the contribution of their interactions. The GSA interface allows for utilizing batched functions for parallel computation of GSA quantities.","category":"page"},{"location":"modules/DiffEqDocs/analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"For more information, see the documentation on GlobalSensitivity.jl.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/#Transfer-Learning-with-Neural-Adapter","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"warn: Warn\nThis documentation page is out of date.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"neural_adapter is method that trains a neural network using the results from an already obtained prediction.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"This allows reusing the obtained prediction results and pre-training states of the neural network to get a new prediction, or reuse the results of predictions to train a related task (for example, the same task with a different domain). It makes it possible to create more flexible training schemes.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/#Retrain-the-prediction","page":"Transfer Learning with Neural Adapter","title":"Retrain the prediction","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Using the example of 2D Poisson equation, it is shown how, using method neural_adapter, to retrain the prediction of one neural network to another.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: image)","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\n# Initial and boundary conditions\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0,1.0),\n           y ∈ Interval(0.0,1.0)]\nquadrature_strategy = NeuralPDE.QuadratureTraining(reltol=1e-2,abstol=1e-2,\n                                                   maxiters =50, batch=100)\ninner = 8\naf = Lux.tanh\nchain1 = Chain(Dense(2,inner,af),\n               Dense(inner,inner,af),\n               Dense(inner,1))\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain1,\n                                             quadrature_strategy)\n\n@named pde_system = PDESystem(eq,bcs,domains,[x,y],[u(x, y)])\nprob = NeuralPDE.discretize(pde_system,discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system,discretization)\n\nres = Optimization.solve(prob, BFGS();  maxiters=2000)\nphi = discretization.phi\n\ninner_ = 12\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2,inner_,af),\n                   Dense(inner_,inner_,af),\n                   Dense(inner_,inner_,af),\n                   Dense(inner_,1))\n\ninit_params2 = Float64.(ComponentArray(Lux.setup(Random.default_rng(), chain)[1]))\n\n# the rule by which the training will take place is described here in loss function\nfunction loss(cord,θ)\n    chain2(cord,θ) .- phi(cord,res.u)\nend\n\nstrategy = NeuralPDE.GridTraining(0.02)\n\nprob_ = NeuralPDE.neural_adapter(loss, init_params2, pde_system, strategy)\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres_ = Optimization.solve(prob_, BFGS();callback = callback, maxiters=1000)\n\nphi_ = NeuralPDE.get_phi(chain2)\n\nxs,ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\n\nu_predict = reshape([first(phi([x,y],res.u)) for x in xs for y in ys],(length(xs),length(ys)))\nu_predict_ =  reshape([first(phi_([x,y],res_.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u = u_predict .- u_real\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\np1 = plot(xs, ys, u_predict, linetype=:contourf,title = \"first predict\");\np2 = plot(xs, ys, u_predict_,linetype=:contourf,title = \"second predict\");\np3 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np4 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error 1\");\np5 = plot(xs, ys, diff_u_,linetype=:contourf,title = \"error 2\");\nplot(p1,p2,p3,p4,p5)\n","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: neural_adapter)","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/#Domain-decomposition","page":"Transfer Learning with Neural Adapter","title":"Domain decomposition","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"In this example, we first obtain a prediction of 2D Poisson equation on subdomains. We split up full domain into 10 sub problems by x, and create separate neural networks for each sub interval. If x domain ∈ [x0, xend] so, it is decomposed on 10 part: sub x domains = {[x0, x1], ... [xi,xi+1], ..., x9,xend]}. And then using the method neural_adapter, we retrain the banch of 10 predictions to the one prediction for full domain of task.","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: domain_decomposition)","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq  = Dxx(u(x,y)) + Dyy(u(x,y)) ~ -sin(pi*x)*sin(pi*y)\n\nbcs = [u(0,y) ~ 0.0, u(1,y) ~ -sin(pi*1)*sin(pi*y),\n       u(x,0) ~ 0.0, u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n\n# Space\nx_0 = 0.0\nx_end = 1.0\nx_domain = Interval(x_0, x_end)\ny_domain = Interval(0.0, 1.0)\ndomains = [x ∈ x_domain,\n           y ∈ y_domain]\n\ncount_decomp = 10\n\n# Neural network\naf = Lux.tanh\ninner = 10\nchains = [Lux.Chain(Dense(2, inner, af), Dense(inner, inner, af), Dense(inner, 1)) for _ in 1:count_decomp]\ninit_params = map(c->Float64.(ComponentArray(Lux.setup(Random.default_rng(), c)[1])),chains)\n\nxs_ = infimum(x_domain):1/count_decomp:supremum(x_domain)\nxs_domain = [(xs_[i], xs_[i+1]) for i in 1:length(xs_)-1]\ndomains_map = map(xs_domain) do (xs_dom)\n    x_domain_ = Interval(xs_dom...)\n    domains_ = [x ∈ x_domain_,\n                y ∈ y_domain]\nend\n\nanalytic_sol_func(x,y) = (sin(pi*x)*sin(pi*y))/(2pi^2)\nfunction create_bcs(x_domain_,phi_bound)\n    x_0, x_e =  x_domain_.left, x_domain_.right\n    if x_0 == 0.0\n        bcs = [u(0,y) ~ 0.0,\n               u(x_e,y) ~ analytic_sol_func(x_e,y),\n               u(x,0) ~ 0.0,\n               u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n        return bcs\n    end\n    bcs = [u(x_0,y) ~ phi_bound(x_0,y),\n           u(x_e,y) ~ analytic_sol_func(x_e,y),\n           u(x,0) ~ 0.0,\n           u(x,1) ~ -sin(pi*x)*sin(pi*1)]\n    bcs\nend\n\nreses = []\nphis = []\npde_system_map = []\n\nfor i in 1:count_decomp\n    println(\"decomposition $i\")\n    domains_ = domains_map[i]\n    phi_in(cord) = phis[i-1](cord,reses[i-1].minimizer)\n    phi_bound(x,y) = phi_in(vcat(x,y))\n    @register phi_bound(x,y)\n    Base.Broadcast.broadcasted(::typeof(phi_bound), x,y) = phi_bound(x,y)\n    bcs_ = create_bcs(domains_[1].domain, phi_bound)\n    @named pde_system_ = PDESystem(eq, bcs_, domains_, [x, y], [u(x, y)])\n    push!(pde_system_map,pde_system_)\n    strategy = NeuralPDE.GridTraining([0.1/count_decomp, 0.1])\n\n    discretization = NeuralPDE.PhysicsInformedNN(chains[i], strategy; init_params=init_params[i])\n\n    prob = NeuralPDE.discretize(pde_system_,discretization)\n    symprob = NeuralPDE.symbolic_discretize(pde_system_,discretization)\n    res_ = Optimization.solve(prob, BFGS(), maxiters=1000)\n    phi = discretization.phi\n    push!(reses, res_)\n    push!(phis, phi)\nend\n\nfunction compose_result(dx)\n    u_predict_array = Float64[]\n    diff_u_array = Float64[]\n    ys = infimum(domains[2].domain):dx:supremum(domains[2].domain)\n    xs_ = infimum(x_domain):dx:supremum(x_domain)\n    xs = collect(xs_)\n    function index_of_interval(x_)\n        for (i,x_domain) in enumerate(xs_domain)\n            if x_<= x_domain[2] && x_>= x_domain[1]\n                return i\n            end\n        end\n    end\n    for x_ in xs\n        i = index_of_interval(x_)\n        u_predict_sub = [first(phis[i]([x_,y],reses[i].minimizer)) for y in ys]\n        u_real_sub = [analytic_sol_func(x_,y)  for y in ys]\n        diff_u_sub = abs.(u_predict_sub .- u_real_sub)\n        append!(u_predict_array,u_predict_sub)\n        append!(diff_u_array,diff_u_sub)\n    end\n    xs,ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\n    u_predict = reshape(u_predict_array,(length(xs),length(ys)))\n    diff_u = reshape(diff_u_array, (length(xs),length(ys)))\n    u_predict, diff_u\nend\ndx= 0.01\nu_predict, diff_u = compose_result(dx)\n\n\ninner_ = 18\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2,inner_,af),\n                   Dense(inner_,inner_,af),\n                   Dense(inner_,inner_,af),\n                   Dense(inner_,inner_,af),\n                   Dense(inner_,1))\n\ninit_params2 = Float64.(ComponentArray(Lux.setup(Random.default_rng(), chain2)[1]))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\n\nlosses = map(1:count_decomp) do i\n    loss(cord,θ) = chain2(cord,θ) .- phis[i](cord,reses[i].minimizer)\nend\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nprob_ = NeuralPDE.neural_adapter(losses,init_params2, pde_system_map,NeuralPDE.GridTraining([0.1/count_decomp,0.1]))\nres_ = Optimization.solve(prob_, BFGS();callback = callback, maxiters=2000)\nprob_ = NeuralPDE.neural_adapter(losses,res_.minimizer, pde_system_map, NeuralPDE.GridTraining([0.05/count_decomp,0.05]))\nres_ = Optimization.solve(prob_, BFGS();callback = callback,  maxiters=1000)\n\nphi_ = NeuralPDE.get_phi(chain2)\n\nxs,ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_ = reshape([first(phi_([x,y],res_.minimizer)) for x in xs for y in ys],(length(xs),length(ys)))\nu_real = reshape([analytic_sol_func(x,y) for x in xs for y in ys], (length(xs),length(ys)))\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\n\np1 = plot(xs, ys, u_predict, linetype=:contourf,title = \"predict 1\");\np2 = plot(xs, ys, u_predict_,linetype=:contourf,title = \"predict 2\");\np3 = plot(xs, ys, u_real, linetype=:contourf,title = \"analytic\");\np4 = plot(xs, ys, diff_u,linetype=:contourf,title = \"error 1\");\np5 = plot(xs, ys, diff_u_,linetype=:contourf,title = \"error 2\");\nplot(p1,p2,p3,p4,p5)","category":"page"},{"location":"modules/NeuralPDE/tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: decomp)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/#Parameter-Estimation","page":"Parameter Estimation","title":"Parameter Estimation","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The parameters of a model, generated by Catalyst, can be estimated using various packages available in the Julia ecosystem. Refer here for more extensive information. Below follows a quick tutorial of how DiffEqFlux can be used to fit a parameter set to data.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"First, we fetch the required packages.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"using OrdinaryDiffEq\nusing DiffEqFlux, Flux\nusing Catalyst","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Next, we declare our model. For our example, we will use the Brusselator, a simple oscillator.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"brusselator = @reaction_network begin\n    A, ∅ → X\n    1, 2X + Y → 3X\n    B, X → Y\n    1, X → ∅\nend A B\np_real = [1., 2.]","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"We simulate our model, and from the simulation generate sampled data points (with added noise), to which we will attempt to fit a parameter et.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"u0 = [1.0, 1.0]\ntspan = (0.0, 30.0)\n\nsample_times = range(tspan[1],stop=tspan[2],length=100)\nprob = ODEProblem(brusselator, u0, tspan, p_real)\nsol_real = solve(prob, Rosenbrock23(), tstops=sample_times)\n\nsample_vals = [sol_real.u[findfirst(sol_real.t .>= ts)][var] * (1+(0.1rand()-0.05)) for var in 1:2, ts in sample_times];","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"We can plot the real solution, as well as the noisy samples.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"using Plots\nplot(sol_real,size=(1200,400),label=\"\",framestyle=:box,lw=3,color=[:darkblue :darkred])\nplot!(sample_times,sample_vals',seriestype=:scatter,color=[:blue :red],label=\"\")","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: parameter_estimation_plot1)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Next, we create an optimisation function. For a given initial estimate of the parameter values, p, this function will fit parameter values to our data samples. However, it will only do so on the interval [0,tend].","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"function optimise_p(p_init,tend)\n    function loss(p)\n        sol = solve(remake(prob,tspan=(0.,tend),p=p), Rosenbrock23(), tstops=sample_times)\n        vals = hcat(map(ts -> sol.u[findfirst(sol.t .>= ts)], sample_times[1:findlast(sample_times .<= tend)])...)    \n        loss = sum(abs2, vals .- sample_vals[:,1:size(vals)[2]])   \n        return loss, sol\n    end\n    return DiffEqFlux.sciml_train(loss,p_init,ADAM(0.1),maxiters = 100)\nend","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Next, we will fit a parameter set to the data on the interval [0,10].","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"p_estimate = optimise_p([5.,5.],10.).minimizer","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"We can compare this to the real solution, as well as the sample data","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"sol_estimate = solve(remake(prob,tspan=(0.,10.),p=p_estimate), Rosenbrock23())\nplot(sol_real,size=(1200,400),color=[:blue :red],framestyle=:box,lw=3,label=[\"X real\" \"Y real\"],linealpha=0.2)\nplot!(sample_times,sample_vals',seriestype=:scatter,color=[:blue :red],label=[\"Samples of X\" \"Samples of Y\"],alpha=0.4)\nplot!(sol_estimate,color=[:darkblue :darkred], linestyle=:dash,lw=3,label=[\"X estimated\" \"Y estimated\"],xlimit=tspan)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: parameter_estimation_plot2)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Next, we use this parameter estimation as the input to the next iteration of our fitting process, this time on the interval [0,20].","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"p_estimate = optimise_p(p_estimate,20.).minimizer\n\nsol_estimate = solve(remake(prob,tspan=(0.,20.),p=p_estimate), Rosenbrock23())\nplot(sol_real,size=(1200,400),color=[:blue :red],framestyle=:box,lw=3,label=[\"X real\" \"Y real\"],linealpha=0.2)\nplot!(sample_times,sample_vals',seriestype=:scatter,color=[:blue :red],label=[\"Samples of X\" \"Samples of Y\"],alpha=0.4)\nplot!(sol_estimate,color=[:darkblue :darkred], linestyle=:dash,lw=3,label=[\"X estimated\" \"Y estimated\"],xlimit=tspan)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: parameter_estimation_plot3)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Finally, we use this estimate as the input to fit a parameter set on the full interval of sampled data.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"p_estimate = optimise_p(p_estimate,30.).minimizer\n\nsol_estimate = solve(remake(prob,tspan=(0.,30.),p=p_estimate), Rosenbrock23())\nplot(sol_real,size=(1200,400),color=[:blue :red],framestyle=:box,lw=3,label=[\"X real\" \"Y real\"],linealpha=0.2)\nplot!(sample_times,sample_vals',seriestype=:scatter,color=[:blue :red],label=[\"Samples of X\" \"Samples of Y\"],alpha=0.4)\nplot!(sol_estimate,color=[:darkblue :darkred], linestyle=:dash,lw=3,label=[\"X estimated\" \"Y estimated\"],xlimit=tspan)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: parameter_estimation_plot4)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The final parameter set becomes [0.9996559014056948, 2.005632696191224] (the real one was [1.0, 2.0]).","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/#Why-we-fit-the-parameters-in-iterations.","page":"Parameter Estimation","title":"Why we fit the parameters in iterations.","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The reason we chose to fit the model on a smaller interval to begin with, and then extend the interval, is to avoid getting stuck in a local minimum. Here specifically, we chose our initial interval to be smaller than a full cycle of the oscillation. If we had chosen to fit a parameter set on the full interval immediately we would have received an inferior solution.","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"p_estimate = optimise_p([5.,5.],30.).minimizer\n\nsol_estimate = solve(remake(prob,tspan=(0.,30.),p=p_estimate), Rosenbrock23())\nplot(sol_real,size=(1200,400),color=[:blue :red],framestyle=:box,lw=3,label=[\"X real\" \"Y real\"],linealpha=0.2)\nplot!(sample_times,sample_vals',seriestype=:scatter,color=[:blue :red],label=[\"Samples of X\" \"Samples of Y\"],alpha=0.4)\nplot!(sol_estimate,color=[:darkblue :darkred], linestyle=:dash,lw=3,label=[\"X estimated\" \"Y estimated\"],xlimit=tspan)","category":"page"},{"location":"modules/Catalyst/tutorials/parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: parameter_estimation_plot5)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/#Global-Sensitivity-Analysis-of-the-Lotka-Volterra-model","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"","category":"section"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"The tutorial covers a workflow of using GlobalSensitivity.jl on the Lotka-Volterra differential equation. We showcase how to use multiple GSA methods, analyse their results and leverage Julia's parallelism capabilities to perform Global Sensitivity analysis at scale.","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using GlobalSensitivity, QuasiMonteCarlo, OrdinaryDiffEq, Statistics, CairoMakie\n\nfunction f(du,u,p,t)\n  du[1] = p[1]*u[1] - p[2]*u[1]*u[2] #prey\n  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2] #predator\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f,u0,tspan,p)\nt = collect(range(0, stop=10, length=200))\n\n\nf1 = function (p)\n    prob1 = remake(prob;p=p)\n    sol = solve(prob1,Tsit5();saveat=t)\nend\n\nbounds = [[1,5],[1,5],[1,5],[1,5]]\n\nreg_sens = gsa(f1, RegressionGSA(true), bounds)\nfig = Figure(resolution = (600, 400))\nax, hm = CairoMakie.heatmap(fig[1,1], reg_sens.partial_correlation, figure = (resolution = (600, 400),), axis = (xticksvisible = false,yticksvisible = false, yticklabelsvisible = false, xticklabelsvisible = false, title = \"Partial correlation\"))\nColorbar(fig[1, 2], hm)\nax, hm = CairoMakie.heatmap(fig[2,1], reg_sens.standard_regression, figure = (resolution = (600, 400),), axis = (xticksvisible = false,yticksvisible = false, yticklabelsvisible = false, xticklabelsvisible = false, title = \"Standard regression\"))\nColorbar(fig[2, 2], hm)\nfig","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"(Image: heatmapreg)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using StableRNGs\n_rng = StableRNG(1234)\nmorris_sens = gsa(f1, Morris(), bounds, rng = _rng)\nfig = Figure(resolution = (600, 400))\nscatter(fig[1,1], [1,2,3,4], morris_sens.means_star[1,:], color = :green, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Prey\",))\nscatter(fig[1,2], [1,2,3,4], morris_sens.means_star[2,:], color = :red, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Predator\",))\nfig","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"(Image: morrisscat)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"sobol_sens = gsa(f1, Sobol(), bounds, N=5000)\nefast_sens = gsa(f1, eFAST(), bounds)\nfig = Figure(resolution = (600, 400))\nbarplot(fig[1,1], [1,2,3,4], sobol_sens.S1[1, :], color = :green, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Prey (Sobol)\", ylabel = \"First order\"))\nbarplot(fig[2,1], [1,2,3,4], sobol_sens.ST[1, :], color = :green, axis = (xticksvisible = false, xticklabelsvisible = false, ylabel = \"Total order\"))\nbarplot(fig[1,2], [1,2,3,4], efast_sens.S1[1, :], color = :red, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Prey (eFAST)\"))\nbarplot(fig[2,2], [1,2,3,4], efast_sens.ST[1, :], color = :red, axis = (xticksvisible = false, xticklabelsvisible = false))\nfig\n\nfig = Figure(resolution = (600, 400))\nbarplot(fig[1,1], [1,2,3,4], sobol_sens.S1[2, :], color = :green, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Predator (Sobol)\", ylabel = \"First order\"))\nbarplot(fig[2,1], [1,2,3,4], sobol_sens.ST[2, :], color = :green, axis = (xticksvisible = false, xticklabelsvisible = false, ylabel = \"Total order\"))\nbarplot(fig[1,2], [1,2,3,4], efast_sens.S1[2, :], color = :red, axis = (xticksvisible = false, xticklabelsvisible = false, title = \"Predator (eFAST)\"))\nbarplot(fig[2,2], [1,2,3,4], efast_sens.ST[2, :], color = :red, axis = (xticksvisible = false, xticklabelsvisible = false))\nfig","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"(Image: sobolefastprey) (Image: sobolefastpred)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"using QuasiMonteCarlo\nN = 5000\nlb = [1.0, 1.0, 1.0, 1.0]\nub = [5.0, 5.0, 5.0, 5.0]\nsampler = SobolSample()\nA,B = QuasiMonteCarlo.generate_design_matrices(N,lb,ub,sampler)\nsobol_sens_desmat = gsa(f1,Sobol(),A,B)\n\n\nf_batch = function (p)\n  prob_func(prob,i,repeat) = remake(prob;p=p[:,i])\n  ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\n\n  sol = solve(ensemble_prob, Tsit5(), EnsembleThreads(); saveat=t, trajectories=size(p,2))\n\n  out = zeros(2,size(p,2))\n\n  for i in 1:size(p,2)\n    out[1,i] = mean(sol[i][1,:])\n    out[2,i] = maximum(sol[i][2,:])\n  end\n\n  return out\nend\n\nsobol_sens_batch = gsa(f_batch,Sobol(),A,B,batch=true)\n\n@time gsa(f1,Sobol(),A,B)\n@time gsa(f_batch,Sobol(),A,B,batch=true)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"f1 = function (p)\n           prob1 = remake(prob;p=p)\n           sol = solve(prob1,Tsit5();saveat=t)\n       end\nsobol_sens = gsa(f1, Sobol(nboot = 20), bounds, N=5000)\nfig = Figure(resolution = (600, 400))\nax, hm = CairoMakie.scatter(fig[1,1], sobol_sens.S1[1][1,2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(fig[1,1], sobol_sens.S1[1][2,2:end], label = \"Predator\", markersize = 4)\n\n# Legend(fig[1,2], ax)\n\nax, hm = CairoMakie.scatter(fig[1,2], sobol_sens.S1[2][1,2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(fig[1,2], sobol_sens.S1[2][2,2:end], label = \"Predator\", markersize = 4)\n\nax, hm = CairoMakie.scatter(fig[2,1], sobol_sens.S1[3][1,2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(fig[2,1], sobol_sens.S1[3][2,2:end], label = \"Predator\", markersize = 4)\n\nax, hm = CairoMakie.scatter(fig[2,2], sobol_sens.S1[4][1,2:end], label = \"Prey\", markersize = 4)\nCairoMakie.scatter!(fig[2,2], sobol_sens.S1[4][2,2:end], label = \"Predator\", markersize = 4)\n\ntitle = Label(fig[0,:], \"First order Sobol indices\")\nlegend = Legend(fig[2,3], ax)","category":"page"},{"location":"modules/GlobalSensitivity/tutorials/juliacon21/","page":"Global Sensitivity Analysis of the Lotka-Volterra model","title":"Global Sensitivity Analysis of the Lotka-Volterra model","text":"(Image: timeseriessobollv)","category":"page"},{"location":"modules/Surrogates/tensor_prod/#Tensor-product-function","page":"Tensor product","title":"Tensor product function","text":"","category":"section"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"The tensor product function is defined as: f(x) = prod_i=1^d cos(api x_i)","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Let's import Surrogates and Plots:","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Define the 1D objective function:","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"function f(x)\n    a = 0.5;\n    return cos(a*pi*x)\nend","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"n = 30\nlb = -5.0\nub = 5.0\na = 0.5\nx = sample(n, lb, ub, SobolSample())\ny = f.(x)\nxs = lb:0.001:ub\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-1, 1), legend=:top)\nplot!(xs, f.(xs), label=\"True function\", legend=:top)","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"Fitting and plotting different surrogates:","category":"page"},{"location":"modules/Surrogates/tensor_prod/","page":"Tensor product","title":"Tensor product","text":"loba_1 = LobachevskySurrogate(x, y, lb, ub)\nkrig = Kriging(x, y, lb, ub)\nscatter(x, y, label=\"Sampled points\", xlims=(lb, ub), ylims=(-2.5, 2.5), legend=:bottom)\nplot!(xs,f.(xs), label=\"True function\", legend=:top)\nplot!(xs, loba_1.(xs), label=\"Lobachevsky\", legend=:top)\nplot!(xs, krig.(xs), label=\"Kriging\", legend=:top)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/defining_problems/#Developing-A-New-Problem","page":"Developing A New Problem","title":"Developing A New Problem","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/contributing/defining_problems/","page":"Developing A New Problem","title":"Developing A New Problem","text":"New problems should be defined for new types of differential equations, new partial differential equations, and special subclasses of differential equations for which solvers can dispatch on for better performance.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/defining_problems/","page":"Developing A New Problem","title":"Developing A New Problem","text":"To develop a new problem, you need to make a new DEProblem and a new DESolution. These types belong in DiffEqBase and should be exported. The DEProblem type should hold all of the mathematical information about the problem (including all of the meshing information in both space and time), and the DESolution should hold all of the information for the solution. Then all that is required is to define a __solve(::DEProblem,alg;kwargs) which takes in the problem and returns a solution.","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/defining_problems/","page":"Developing A New Problem","title":"Developing A New Problem","text":"Then to check that the algorithm works, add a dispatch for test_convergence which makes a ConvergenceSimulation type. This type already has a plot recipe, so plotting functionality will already be embedded. This requires that your problem can take in a true solution, and has a field errors which is a dictionary of symbols for the different error estimates (L2,L infinity, etc.)","category":"page"},{"location":"modules/DiffEqDevDocs/contributing/defining_problems/","page":"Developing A New Problem","title":"Developing A New Problem","text":"After these steps, update the documentation to include the new problem types and the new associated solvers.","category":"page"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/#Bracketing-Solvers","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"solve(prob::NonlinearProblem,alg;kwargs)","category":"page"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"Solves for f(u)=0 in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"This page is solely focused on the bracketing methods for scalar nonlinear equations.","category":"page"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/#Recommended-Methods","page":"Bracketing Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"Falsi() can have a faster convergence and is discretely differentiable, but is less stable than Bisection.","category":"page"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/#Full-List-of-Methods","page":"Bracketing Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/#NonlinearSolve.jl","page":"Bracketing Solvers","title":"NonlinearSolve.jl","text":"","category":"section"},{"location":"modules/NonlinearSolve/solvers/BracketingSolvers/","page":"Bracketing Solvers","title":"Bracketing Solvers","text":"Falsi: A non-allocating regula falsi method\nBisection: A common bisection method","category":"page"},{"location":"modules/Optimization/tutorials/intro/#Basic-usage","page":"Basic usage","title":"Basic usage","text":"","category":"section"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"In this tutorial we introduce the basics of Optimization.jl by showing how to easily mix local optimizers from Optim.jl and global optimizers from BlackBoxOptim.jl on the Rosenbrock equation. The simplest copy-pasteable code to get started is the following:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"# Import the package and define the problem to optimize\nusing Optimization\nrosenbrock(u,p) =  (u[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\nu0 = zeros(2)\np  = [1.0,100.0]\n\nprob = OptimizationProblem(rosenbrock,u0,p)\n\n# Import a solver package and solve the optimization problem\nusing OptimizationOptimJL\nsol = solve(prob,NelderMead())\n\n# Import a different solver package and solve the optimization problem a different way\nusing OptimizationBBO\nprob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob,BBO_adaptive_de_rand_1_bin_radiuslimited())","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"Notice that Optimization.jl is the core glue package that holds all of the common pieces, but to solve the equations we need to use a solver package. Here, OptimizationOptimJL is for Optim.jl and OptimizationBBO is for BlackBoxOptim.jl.","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"The output of the first optimization task (with the NelderMead() algorithm) is given below:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"sol = solve(prob,NelderMead())","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"The solution from the original solver can always be obtained via original:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"sol.original","category":"page"},{"location":"modules/Optimization/tutorials/intro/#Controlling-Gradient-Calculations-(Automatic-Differentiation)","page":"Basic usage","title":"Controlling Gradient Calculations (Automatic Differentiation)","text":"","category":"section"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"Notice that both of the above methods were derivative-free methods, and thus no gradients were required to do the optimization. However, in many cases first order optimization (i.e. using gradients) is much more efficient. Defining gradients can be done in two ways. One way is to manually provide a gradient definition in the OptimizationFunction constructor. However, the more convenient way to obtain gradients is to provide an AD backend type. ","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"For example, let's now use the OptimizationOptimJL BFGS method to solve the same problem. We will import the forward-mode automatic differentiation library (using ForwardDiff) and then specify in the OptimizationFunction to automatically construct the derivative functions using ForwardDiff.jl. This looks like:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"using ForwardDiff\noptf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, u0, p)\nsol = solve(prob,BFGS())","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"We can inspect the original to see the statistics on the number of steps  required and gradients computed:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"sol.original","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"Sure enough, it's a lot less than the derivative-free methods!","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"However, the compute cost of forward-mode automatic differentiation scales via the number of inputs, and thus as our optimization problem grows large it slow down. To counteract this, for larger optimization problems (>100 state variables) one normally would want to use reverse-mode automatic differentiation. One common choice for reverse-mode automatic differentiation is Zygote.jl. We can demonstrate this via:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"using Zygote\noptf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())\nprob = OptimizationProblem(optf, u0, p)\nsol = solve(prob,BFGS())","category":"page"},{"location":"modules/Optimization/tutorials/intro/#Setting-Box-Constraints","page":"Basic usage","title":"Setting Box Constraints","text":"","category":"section"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"In many cases one knows the potential bounds on the solution values. In Optimization.jl, these can be supplied as the lb and ub arguments for the lower bounds and upper bounds respectively, supplying a vector of values with one per state variable. Let's now do our gradient-based optimization with box constraints by rebuilding the OptimizationProblem:","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"prob = OptimizationProblem(optf, u0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob,BFGS())","category":"page"},{"location":"modules/Optimization/tutorials/intro/","page":"Basic usage","title":"Basic usage","text":"For more information on handling constraints, in particular equality and inequality constraints, take a look at the constraints tutorial.","category":"page"},{"location":"modules/SciMLSensitivity/manual/direct_forward_sensitivity/#forward_sense","page":"Direct Forward Sensitivity Analysis of ODEs","title":"Direct Forward Sensitivity Analysis of ODEs","text":"","category":"section"},{"location":"modules/SciMLSensitivity/manual/direct_forward_sensitivity/","page":"Direct Forward Sensitivity Analysis of ODEs","title":"Direct Forward Sensitivity Analysis of ODEs","text":"ODEForwardSensitivityProblem\nextract_local_sensitivities","category":"page"},{"location":"modules/SciMLSensitivity/manual/direct_forward_sensitivity/#SciMLSensitivity.ODEForwardSensitivityProblem","page":"Direct Forward Sensitivity Analysis of ODEs","title":"SciMLSensitivity.ODEForwardSensitivityProblem","text":"function ODEForwardSensitivityProblem(f::Union{Function,DiffEqBase.AbstractODEFunction},                                       u0,tspan,p=nothing,                                       alg::AbstractForwardSensitivityAlgorithm = ForwardSensitivity();                                       kwargs...)\n\nLocal forward sensitivity analysis gives a solution along with a timeseries of the sensitivities. Thus if one wishes to have a derivative at every possible time point, directly using the ODEForwardSensitivityProblem can be the most efficient method.\n\nwarning: Warning\nODEForwardSensitivityProblem requires being able to solve   a differential equation defined by the parameter struct p. Thus while   DifferentialEquations.jl can support any parameter struct type, usage   with ODEForwardSensitivityProblem requires that p could be a valid   type for being the initial condition u0 of an array. This means that   many simple types, such as Tuples and NamedTuples, will work as   parameters in normal contexts but will fail during ODEForwardSensitivityProblem   construction. To work around this issue for complicated cases like nested structs,    look into defining p using AbstractArray libraries such as RecursiveArrayTools.jl    or ComponentArrays.jl.\n\nODEForwardSensitivityProblem Syntax\n\nODEForwardSensitivityProblem is similar to an ODEProblem, but takes an AbstractForwardSensitivityAlgorithm that describes how to append the forward sensitivity equation calculation to the time evolution to simultaneously compute the derivative of the solution with respect to parameters.\n\nODEForwardSensitivityProblem(f::SciMLBase.AbstractODEFunction,u0,\n                             tspan,p=nothing,\n                             sensealg::AbstractForwardSensitivityAlgorithm = ForwardSensitivity();\n                             kwargs...)\n\nOnce constructed, this problem can be used in solve just like any other ODEProblem.  The solution can be deconstructed into the ODE solution and sensitivities parts using the extract_local_sensitivities function, with the following dispatches:\n\nextract_local_sensitivities(sol, asmatrix::Val=Val(false)) # Decompose the entire time series\nextract_local_sensitivities(sol, i::Integer, asmatrix::Val=Val(false)) # Decompose sol[i]\nextract_local_sensitivities(sol, t::Union{Number,AbstractVector}, asmatrix::Val=Val(false)) # Decompose sol(t)\n\nFor information on the mathematics behind these calculations, consult the sensitivity math page\n\nExample using an ODEForwardSensitivityProblem\n\nTo define a sensitivity problem, simply use the ODEForwardSensitivityProblem type instead of an ODE type. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:\n\nfunction f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEForwardSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)\n\nThis generates a problem which the ODE solvers can solve:\n\nsol = solve(prob,DP8())\n\nNote that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:\n\nx,dp = extract_local_sensitivities(sol)\nx,dp = extract_local_sensitivities(sol,i)\nx,dp = extract_local_sensitivities(sol,t)\n\nIn each case, x is the ODE values and dp is the matrix of sensitivities The first gives the full timeseries of values and dp[i] contains the time series of the sensitivities of all components of the ODE with respect to ith parameter. The second returns the ith time step, while the third interpolates to calculate the sensitivities at time t. For example, if we do:\n\nx,dp = extract_local_sensitivities(sol)\nda = dp[1]\n\nthen da is the timeseries for fracpartial u(t)partial p. We can plot this\n\nplot(sol.t,da',lw=3)\n\ntransposing so that the rows (the timeseries) is plotted.\n\n(Image: Local Sensitivity Solution)\n\nHere we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions. However, as time goes on the sensitivity increases. This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems.\n\nWe can also quickly see that these values are equivalent to those given by automatic differentiation and numerical differentiation through the ODE solver:\n\nusing ForwardDiff, Calculus\nfunction test_f(p)\n  prob = ODEProblem(f,eltype(p).([1.0,1.0]),eltype(p).((0.0,10.0)),p)\n  solve(prob,Vern9(),abstol=1e-14,reltol=1e-14,save_everystep=false)[end]\nend\n\np = [1.5,1.0,3.0]\nfd_res = ForwardDiff.jacobian(test_f,p)\ncalc_res = Calculus.finite_difference_jacobian(test_f,p)\n\nHere we just checked the derivative at the end point.\n\nInternal representation of the Solution\n\nFor completeness, we detail the internal representation. When using ForwardDiffSensitivity, the representation is with Dual numbers under the standard interpretation. The values for the ODE's solution at time i are the ForwardDiff.value.(sol[i]) portions, and the derivative with respect to parameter j is given by ForwardDiff.partials.(sol[i])[j].\n\nWhen using ForwardSensitivity, the solution to the ODE are the first n components of the solution. This means we can grab the matrix of solution values like:\n\nx = sol[1:sol.prob.indvars,:]\n\nSince each sensitivity is a vector of derivatives for each function, the sensitivities are each of size sol.prob.indvars. We can pull out the parameter sensitivities from the solution as follows:\n\nda = sol[sol.prob.indvars+1:sol.prob.indvars*2,:]\ndb = sol[sol.prob.indvars*2+1:sol.prob.indvars*3,:]\ndc = sol[sol.prob.indvars*3+1:sol.prob.indvars*4,:]\n\nThis means that da[1,i] is the derivative of the x(t) by the parameter a at time sol.t[i]. Note that all of the functionality available to ODE solutions is available in this case, including interpolations and plot recipes (the recipes will plot the expanded system).\n\n\n\n","category":"type"},{"location":"modules/SciMLSensitivity/manual/direct_forward_sensitivity/#SciMLSensitivity.extract_local_sensitivities","page":"Direct Forward Sensitivity Analysis of ODEs","title":"SciMLSensitivity.extract_local_sensitivities","text":"extractlocalsensitivities\n\nExtracts the time series for the local sensitivities from the ODE solution. This requires that the ODE was defined via ODEForwardSensitivityProblem.\n\nextract_local_sensitivities(sol, asmatrix::Val=Val(false)) # Decompose the entire time series\nextract_local_sensitivities(sol, i::Integer, asmatrix::Val=Val(false)) # Decompose sol[i]\nextract_local_sensitivities(sol, t::Union{Number,AbstractVector}, asmatrix::Val=Val(false)) # Decompose sol(t)\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/types/steady_state_types/#Steady-State-Problems","page":"Steady State Problems","title":"Steady State Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"SteadyStateProblem","category":"page"},{"location":"modules/DiffEqDocs/types/steady_state_types/#SciMLBase.SteadyStateProblem","page":"Steady State Problems","title":"SciMLBase.SteadyStateProblem","text":"Defines an Defines a steady state ODE problem. Documentation Page: https://diffeq.sciml.ai/stable/types/steadystatetypes/\n\nMathematical Specification of a Steady State Problem\n\nTo define an Steady State Problem, you simply need to give the function f which defines the ODE:\n\nfracdudt = f(upt)\n\nand an initial guess u_0 of where f(u,p,t)=0. f should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nNote that for the steady-state to be defined, we must have that f is autonomous, that is f is independent of t. But the form which matches the standard ODE solver should still be used. The steady state solvers interpret the f by fixing t=0.\n\nProblem Type\n\nConstructors\n\nSteadyStateProblem(f::ODEFunction,u0,p=NullParameters();kwargs...)\nSteadyStateProblem{isinplace}(f,u0,p=NullParameters();kwargs...)\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. Additionally, the constructor from ODEProblems is provided:\n\nSteadyStateProblem(prob::ODEProblem)\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the ODE.\nu0: The initial guess for the steady state.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nSpecial Solution Fields\n\nThe SteadyStateSolution type is different from the other DiffEq solutions because it does not have temporal information.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/steady_state_types/#Solution-Type","page":"Steady State Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"NonlinearSolution","category":"page"},{"location":"modules/MethodOfLines/tutorials/params/#Adding-parameters","page":"Adding parameters","title":"Adding parameters","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/params/","page":"Adding parameters","title":"Adding parameters","text":"We can also build up more complicated systems with multiple dependent variables and parameters as follows","category":"page"},{"location":"modules/MethodOfLines/tutorials/params/","page":"Adding parameters","title":"Adding parameters","text":"using ModelingToolkit, MethodOfLines, OrdinaryDiffEq, DomainSets\n\n@parameters t x\n@parameters Dn, Dp\n@variables u(..) v(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs  = [Dt(u(t,x)) ~ Dn * Dxx(u(t,x)) + u(t,x)*v(t,x), \n        Dt(v(t,x)) ~ Dp * Dxx(v(t,x)) - u(t,x)*v(t,x)]\nbcs = [u(0,x) ~ sin(pi*x/2),\n       v(0,x) ~ sin(pi*x/2),\n       u(t,0) ~ 0.0, Dx(u(t,1)) ~ 0.0,\n       v(t,0) ~ 0.0, Dx(v(t,1)) ~ 0.0]\n\ndomains = [t ∈ Interval(0.0,1.0),\n           x ∈ Interval(0.0,1.0)]\n\n@named pdesys = PDESystem(eqs,bcs,domains,[t,x],[u(t,x),v(t,x)],[Dn=>0.5, Dp=>2])\n\ndiscretization = MOLFiniteDifference([x=>0.1],t)\n\nprob = discretize(pdesys,discretization) # This gives an ODEProblem since it's time-dependent\n\nsol = solve(prob,Tsit5())\n\ngrid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\ndiscrete_t = sol[t]\n\nusing Plots\n\nanim = @animate for i in 1:length(t)\n    p1 = plot(discrete_x, map(d -> sol[d][i], grid[u(t, x)]), label=\"u, t=$(discrete_t[i])[1:9] \"; legend=false, xlabel=\"x\",ylabel=\"u\",ylim=[0,1])\n    p2 = plot(discrete_x, map(d -> sol[d][i], grid[v(t, x)]), label=\"v, t=$(discrete_t[i])\"; legend=false, xlabel=\"x\", ylabel=\"v\",ylim=[0, 1])\n    plot(p1, p2)\nend\ngif(anim, \"plot.gif\",fps=30)","category":"page"},{"location":"modules/MethodOfLines/tutorials/params/#Remake-with-different-parameter-values","page":"Adding parameters","title":"Remake with different parameter values","text":"","category":"section"},{"location":"modules/MethodOfLines/tutorials/params/","page":"Adding parameters","title":"Adding parameters","text":"The system does not need to be re-discretized every time we want to plot with different parameters, the system can be remade with new parameters with remake. See the ModelingToolkit.jl docs for more ways to manipulate a prob post discretization.","category":"page"},{"location":"modules/MethodOfLines/tutorials/params/","page":"Adding parameters","title":"Adding parameters","text":"using ModelingToolkit, MethodOfLines, OrdinaryDiffEq, DomainSets\n\n@parameters t x\n@parameters Dn, Dp\n@variables u(..) v(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs  = [Dt(u(t, x)) ~ Dn * Dxx(u(t, x)) + u(t, x)*v(t,x),\n        Dt(v(t, x)) ~ Dp * Dxx(v(t, x)) - u(t, x)*v(t,x)]\nbcs = [u(0, x) ~ sin(pi*x/2),\n       v(0, x) ~ sin(pi*x/2),\n       u(t, 0) ~ 0.0, Dx(u(t, 1)) ~ 0.0,\n       v(t, 0) ~ 0.0, Dx(v(t, 1)) ~ 0.0]\n\ndomains = [t ∈ Interval(0.0, 1.0),\n           x ∈ Interval(0.0, 1.0)]\n\n@named pdesys = PDESystem(eqs, bcs, domains,[t, x], [u(t, x), v(t, x)], [Dn=>0.5, Dp=>2.0])\n\ndiscretization = MOLFiniteDifference([x=>0.1], t)\n\nprob = discretize(pdesys,discretization) # This gives an ODEProblem since it's time-dependent\n\nsols = []\nfor (Dnval, Dpval) in zip(rand(10), rand(10))\n    newprob = remake(prob, p = [Dnval, Dpval])\n    push!(sols, solve(newprob, Tsit5()));\nend\n\ngrid = get_discrete(pdesys, discretization)\ndiscrete_x = grid[x]\n\nusing Plots\nfor (j, sol) in enumerate(sols)\n    discrete_t = sol[t]\n    anim = @animate for i in 1:length(discrete_t)\n        p1 = plot(discrete_x, map(d -> sol[d][i], grid[u(t, x)]), label=\"u, t=$(discrete_t[i])\"; legend=false, xlabel=\"x\",ylabel=\"u\",ylim=[0,1])\n        p2 = plot(discrete_x, map(d -> sol[d][i], grid[v(t, x)]), label=\"v, t=$(discrete_t[i])\"; legend=false, xlabel=\"x\", ylabel=\"v\",ylim=[0, 1])\n        plot(p1, p2)\n    end\n    gif(anim, \"plot_$j.gif\",fps=10)\nend\n","category":"page"},{"location":"modules/DiffEqDocs/types/bvp_types/#BVP-Problems","page":"BVP Problems","title":"BVP Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"BVProblem","category":"page"},{"location":"modules/DiffEqDocs/types/bvp_types/#SciMLBase.BVProblem","page":"BVP Problems","title":"SciMLBase.BVProblem","text":"Defines an BVP problem. Documentation Page: https://diffeq.sciml.ai/stable/types/bvp_types/\n\nMathematical Specification of a BVP Problem\n\nTo define a BVP Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nfracdudt = f(upt)\n\nalong with an implicit function bc! which defines the residual equation, where\n\nbc(upt) = 0\n\nis the manifold on which the solution must live. A common form for this is the two-point BVProblem where the manifold defines the solution at two points:\n\nu(t_0) = a\nu(t_f) = b\n\nProblem Type\n\nConstructors\n\nTwoPointBVProblem{isinplace}(f,bc!,u0,tspan,p=NullParameters();kwargs...)\nBVProblem{isinplace}(f,bc!,u0,tspan,p=NullParameters();kwargs...)\n\nFor any BVP problem type, bc! is the inplace function:\n\nbc!(residual, u, p, t)\n\nwhere residual computed from the current u. u is an array of solution values where u[i] is at time t[i], while p are the parameters. For a TwoPointBVProblem, t = tspan. For the more general BVProblem, u can be all of the internal time points, and for shooting type methods u=sol the ODE solution. Note that all features of the ODESolution are present in this form. In both cases, the size of the residual matches the size of the initial condition.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf: The function for the ODE.\nbc: The boundary condition function.\nu0: The initial condition. Either the initial condition for the ODE as an initial value problem, or a Vector of values for u(t_i) for collocation methods\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/bvp_types/#Solution-Type","page":"BVP Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"BVProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"page"},{"location":"modules/FEniCS/#FEniCS.jl","page":"FEniCS.jl","title":"FEniCS.jl","text":"","category":"section"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"(Image: Join the chat at https://gitter.im/JuliaDiffEq/Lobby) (Image: Build Status) (Image: Coverage Status) (Image: codecov.io)","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"FEniCS.jl is a wrapper for the FEniCS library for finite element discretizations of PDEs. This wrapper includes three parts:","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Installation and direct access to FEniCS via a Conda installation. Alternatively one may use their current FEniCS installation.\nA low-level development API and provides some functionality to make directly dealing with the library a little bit easier, but still requires knowledge of FEniCS itself. Interfaces have been provided for the main functions and their attributes, and instructions to add further ones can be found here.\nA high-level API for usage with DifferentialEquations. An example can be seen solving the heat equation with high order adaptive timestepping.","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Various gists/jupyter notebooks have been created to provide a brief overview of the overall functionality, and of any differences between the pythonic FEniCS and the julian wrapper. DifferentialEquations.jl ecosystem. Paraview can also be used to visualize various results just like in FEniCS (see below).","category":"page"},{"location":"modules/FEniCS/#Installation-Instructions","page":"FEniCS.jl","title":"Installation Instructions","text":"","category":"section"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"To get the wrapper on your system,providing a FEniCS installation exists, follow the below steps:","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Add PyCall with the correct python environment corresponding to FEniCS. Then simply add FEniCS.jl using Pkg.add(\"FEniCS\")\nAlternatively, one can install Docker and then run the following command  ","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"docker run -ti ysimillides/fenics-julia-docker ","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"and once inside, 'julia' can be accessed by calling","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"julia","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Once inside the julia environment, simply add FEniCS with Pkg.add(\"FEniCS\"). All other dependencies are handled by the docker image.","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Note: Any suggestions/improvements/comments etc are always welcomed and can be made either on GitHub or via the gitter channel above. This wrapper was originally started via the Google Summer of Code program along with the help of Chris Rackauckas and Bart Janssens. This was continued via GSoC '18 along with the help of Chris Rackauckas and Timo Betcke.","category":"page"},{"location":"modules/FEniCS/#Tutorial","page":"FEniCS.jl","title":"Tutorial","text":"","category":"section"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"Below is a small demonstration of how a user would use our code to solve the Poisson equation with Dirichlet conditions. This directly mirrors one of the tutorials FEniCS provides ","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"using FEniCS\nmesh = UnitSquareMesh(8,8) \nV = FunctionSpace(mesh,\"P\",1)\nu_D = Expression(\"1+x[0]*x[0]+2*x[1]*x[1]\", degree=2)\nu = TrialFunction(V)\nbc1 = DirichletBC(V,u_D, \"on_boundary\")\nv = TestFunction(V)\nf = Constant(-6.0)\na = dot(grad(u),grad(v))*dx\nL = f*v*dx\nU = FeFunction(V)\nlvsolve(a,L,U,bc1) #linear variational solver\nerrornorm(u_D, U, norm=\"L2\")\nget_array(L) #this returns an array for the stiffness matrix\nget_array(U) #this returns an array for the solution values\nvtkfile = File(\"poisson/solution.pvd\")\nvtkfile << U.pyobject #exports the solution to a vtkfile","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"We can also plot the solution (this relies on FEniCS backend for plotting) or import it from our file into Paraview:","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"import PyPlot # plotting won't work if PyPlot is not imported\nFEniCS.Plot(U)\nFEniCS.Plot(mesh)\n","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"(Image: alt text)","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"(Image: alt text)","category":"page"},{"location":"modules/FEniCS/","page":"FEniCS.jl","title":"FEniCS.jl","text":"See the examples directory for more examples.","category":"page"},{"location":"modules/NeuralPDE/examples/ks/#Kuramoto–Sivashinsky-equation","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"Let's consider the Kuramoto–Sivashinsky equation, which contains a 4th-order derivative:","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"_t u(x t) + u(x t) _x u(x t) + alpha ^2_x u(x t) + beta ^3_x u(x t) + gamma ^4_x u(x t) =  0  ","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\alpha = \\gamma = 1 and \\beta = 4. The exact solution is:","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"u_e(x t) = 11 + 15 tanh theta - 15 tanh^2 theta - 15 tanh^3 theta  ","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\theta = 1 - x/2 and with initial and boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"beginalign*\n    u(  x 0) =     u_e(  x 0)  \n    u( 10 t) =     u_e( 10 t)  \n    u(-10 t) =     u_e(-10 t)  \n_x u( 10 t) = _x u_e( 10 t)  \n_x u(-10 t) = _x u_e(-10 t)  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"We use physics-informed neural networks.","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, t\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\nα = 1\nβ = 4\nγ = 1\neq = Dt(u(x,t)) + u(x,t)*Dx(u(x,t)) + α*Dx2(u(x,t)) + β*Dx3(u(x,t)) + γ*Dx4(u(x,t)) ~ 0\n\nu_analytic(x,t;z = -x/2+t) = 11 + 15*tanh(z) -15*tanh(z)^2 - 15*tanh(z)^3\ndu(x,t;z = -x/2+t) = 15/2*(tanh(z) + 1)*(3*tanh(z) - 1)*sech(z)^2\n\nbcs = [u(x,0) ~ u_analytic(x,0),\n       u(-10,t) ~ u_analytic(-10,t),\n       u(10,t) ~ u_analytic(10,t),\n       Dx(u(-10,t)) ~ du(-10,t),\n       Dx(u(10,t)) ~ du(10,t)]\n\n# Space and time domains\ndomains = [x ∈ Interval(-10.0,10.0),\n           t ∈ Interval(0.0,1.0)]\n# Discretization\ndx = 0.4; dt = 0.2\n\n# Neural network\nchain = Lux.Chain(Dense(2,12,Lux.σ),Dense(12,12,Lux.σ),Dense(12,1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining([dx,dt]))\n@named pde_system = PDESystem(eq,bcs,domains,[x,t],[u(x, t)])\nprob = discretize(pde_system,discretization)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob,opt; callback = callback, maxiters=2000)\nphi = discretization.phi","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"And some analysis:","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using Plots\n\nxs,ts = [infimum(d.domain):dx:supremum(d.domain) for (d,dx) in zip(domains,[dx/10,dt])]\n\nu_predict = [[first(phi([x,t],res.u)) for x in xs] for t in ts]\nu_real = [[u_analytic(x,t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x,t) -first(phi([x,t],res.u)))  for x in xs] for t in ts]\n\np1 =plot(xs,u_predict,title = \"predict\")\np2 =plot(xs,u_real,title = \"analytic\")\np3 =plot(xs,diff_u,title = \"error\")\nplot(p1,p2,p3)","category":"page"},{"location":"modules/NeuralPDE/examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: plotks)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/#Local-Identifiability-of-Differential-Models","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"In this tutorial, we will go over an example of solving a local identifiability problem for a simple system of ordinary differential equations.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"We will introduce how to use the input parsing in StructuralIdentifiability.jl and the local identifiability assessment functionality.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/#Input-System","page":"Local Identifiability of Differential Models","title":"Input System","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"We will consider a simple two-species competition model","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"x_1 = k (1 - x_1 - x_2) x_2=r(1-x_1-x_2)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"To make it a proper input for the algorithm, we add an output function y=x_1 that equals to the population density of species 1 at any time t.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/#Using-the-@ODEmodel-macro","page":"Local Identifiability of Differential Models","title":"Using the @ODEmodel macro","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"To parse the system of ordinary differential equations as above, we will use @ODEmodel macro. This is the easiest way to do so.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"We have two state variables x1, x2 (population densities), two parameters k, r (intrinsic growth rates), and one output function y. Note that there must be (t) to indicate time-dependent functions.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"After using the macro, we use assess_local_identifiability function for that. This function accepts the ODE model, the probability of correctness, and the type of identifiability we would like to inquire about.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"using StructuralIdentifiability\n\node = @ODEmodel(\n\tx1'(t) = k * (1 - x1(t) - x2(t)),\n\tx2'(t) = r * (1 - x1(t) - x2(t)),\n\ty(t) = x1(t)\n)\n\nlocal_id = assess_local_identifiability(ode, 0.99)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"The result shows that only the state variable's initial value x_1(0) is locally identifiable.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"Let us now add another output function y2(t):","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"using StructuralIdentifiability\n\node = @ODEmodel(\n\tx1'(t) = k * (1 - x1(t) - x2(t)),\n\tx2'(t) = r * (1 - x1(t) - x2(t)),\n\ty1(t) = x1(t),\n\ty2(t) = x2(t) # new output function!\n)\n\nlocal_id = assess_local_identifiability(ode, 0.99) # this is a different result!","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"As you can see, for this new model with an additional output, all parameters are reported as locally identifiable with probability 0.99. ","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/#Note-on-Probability-of-Correctness","page":"Local Identifiability of Differential Models","title":"Note on Probability of Correctness","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"We set the probability of correctness p to be 0.99. Why would we ever want a lower value? Great question! The underlying algorithm relies on operations being modulo a large enough prime characteristic mathcalPgeq kappa p where kappa is determined by the algorithm internally.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"The algorithm's complexity is proportional to the size of operands (see proposition 3.1 in the main paper[1]) and hence high probability of correctness may lead to higher size of coefficients during computation for some systems hence one may wish to lower p to save on runtime (though in practice this is very rare).","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/local_identifiability/","page":"Local Identifiability of Differential Models","title":"Local Identifiability of Differential Models","text":"[1]: A. Sedoglavic, A probabilistic algorithm to test local algebraic observability in polynomial time, Journal of Symbolic Computation, 2002.","category":"page"},{"location":"modules/NonlinearSolve/basics/NonlinearProblem/#Nonlinear-Problems","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"","category":"section"},{"location":"modules/NonlinearSolve/basics/NonlinearProblem/","page":"Nonlinear Problems","title":"Nonlinear Problems","text":"NonlinearProblem","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/#Optimization-of-Ordinary-Differential-Equations","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/#Copy-Paste-Code","page":"Optimization of Ordinary Differential Equations","title":"Copy-Paste Code","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"If you want to just get things running, try the following! Explanation will follow.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"using DifferentialEquations, Optimization, OptimizationPolyalgorithms, OptimizationOptimJL, \n      SciMLSensitivity, Zygote, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [α, β, δ, γ]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob = ODEProblem(lotka_volterra!, u0, tspan, p)\nsol = solve(prob, Tsit5())\n\n# Plot the solution\nusing Plots\nplot(sol)\nsavefig(\"LV_ode.png\")\n\nfunction loss(p)\n  sol = solve(prob, Tsit5(), p=p, saveat = tsteps)\n  loss = sum(abs2, sol.-1)\n  return loss, sol\nend\n\ncallback = function (p, l, pred)\n  display(l)\n  plt = plot(pred, ylim = (0, 6))\n  display(plt)\n  # Tell Optimization.solve to not halt the optimization. If return true, then\n  # optimization stops.\n  return false\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\n\nresult_ode = Optimization.solve(optprob, PolyOpt(),\n                                callback = callback,\n                                maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/#Explanation","page":"Optimization of Ordinary Differential Equations","title":"Explanation","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"First let's create a Lotka-Volterra ODE using DifferentialEquations.jl. For more details, see the DifferentialEquations.jl documentation. The Lotka-Volterra equations have the form:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"beginaligned\nfracdxdt = alpha x - beta x y      \nfracdydt = -delta y + gamma x y    \nendaligned","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"using DifferentialEquations, Optimization, OptimizationPolyalgorithms, OptimizationOptimJL, \n      SciMLSensitivity, Zygote, Plots\n\nfunction lotka_volterra!(du, u, p, t)\n  x, y = u\n  α, β, δ, γ = p\n  du[1] = dx = α*x - β*x*y\n  du[2] = dy = -δ*y + γ*x*y\nend\n\n# Initial condition\nu0 = [1.0, 1.0]\n\n# Simulation interval and intermediary points\ntspan = (0.0, 10.0)\ntsteps = 0.0:0.1:10.0\n\n# LV equation parameter. p = [α, β, δ, γ]\np = [1.5, 1.0, 3.0, 1.0]\n\n# Setup the ODE problem, then solve\nprob = ODEProblem(lotka_volterra!, u0, tspan, p)\nsol = solve(prob, Tsit5())\n\n# Plot the solution\nusing Plots\nplot(sol)\nsavefig(\"LV_ode.png\")","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: LV Solution Plot)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"For this first example, we do not yet include a neural network. We take AD-compatible solve function function that takes the parameters and an initial condition and returns the solution of the differential equation. Next we choose a loss function. Our goal will be to find parameters that make the Lotka-Volterra solution constant x(t)=1, so we define our loss as the squared distance from 1.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"function loss(p)\n  sol = solve(prob, Tsit5(), p=p, saveat = tsteps)\n  loss = sum(abs2, sol.-1)\n  return loss, sol\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Lastly, we use the Optimization.solve function to train the parameters using ADAM to arrive at parameters which optimize for our goal. Optimization.solve allows defining a callback that will be called at each step of our training loop. It takes in the current parameter vector and the returns of the last call to the loss function. We will display the current loss and make a plot of the current situation:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"callback = function (p, l, pred)\n  display(l)\n  plt = plot(pred, ylim = (0, 6))\n  display(plt)\n  # Tell Optimization.solve to not halt the optimization. If return true, then\n  # optimization stops.\n  return false\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"Let's optimize the model.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"adtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, p)\n\nresult_ode = Optimization.solve(optprob, PolyOpt(),\n                                callback = callback,\n                                maxiters = 100)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"In just seconds we found parameters which give a relative loss of 1e-16! We can get the final loss with result_ode.minimum, and get the optimal parameters with result_ode.u. For example, we can plot the final outcome and show that we solved the control problem and successfully found parameters to make the ODE solution constant:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"remade_solution = solve(remake(prob, p = result_ode.u), Tsit5(),      \n                        saveat = tsteps)\nplot(remade_solution, ylim = (0, 6))","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/optimization_ode/","page":"Optimization of Ordinary Differential Equations","title":"Optimization of Ordinary Differential Equations","text":"(Image: Final plot)","category":"page"},{"location":"modules/DiffEqDocs/analysis/dev_and_test/#Algorithm-Development-and-Testing","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"Algorithm developing and testing tools are provided by DiffEqDevTools.jl and are documented in the developer documentation.","category":"page"},{"location":"modules/DiffEqDocs/analysis/dev_and_test/#Installation","page":"Algorithm Development and Testing","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqDevTools.jl:","category":"page"},{"location":"modules/DiffEqDocs/analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"]add DiffEqDevTools\nusing DiffEqDevTools","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#parameter_estimation","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"Parameter estimation for differential equation models, also known as dynamic data analysis,  is provided by the DiffEq suite. In this introduction, we briefly present the relevant packages that facilitate parameter estimation, namely:","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"DiffEqFlux.jl\nTuring.jl\nDataDrivenDiffEq.jl\nDiffEqParamEstim.jl\nDiffEqBayes.jl","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"We also provide information regarding the respective strengths of these packages so that you can easily decide which one suits your needs best.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#DiffEqFlux.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqFlux.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"A very versatile and composable package, DiffEqFlux.jl allows for solving a wide range of differential equations, for instance: stiff universal ODEs, universal SDEs, universal PDEs, and other kinds of universal differential equations. As regards probabilistic programming, DiffEqFlux.jl works in conjunction with Turing.jl (see below). It is the most flexible and high performance parameter estimation system.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#Turing.jl","page":"Parameter Estimation and Bayesian Analysis","title":"Turing.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"In the context of differential equations and parameter estimation, Turing.jl allows for a Bayesian estimation of differential equations (used in conjunction with the high-level package DiffEqBayes.jl). For more examples on combining Turing.jl with DiffEqBayes.jl, see the documentation below. It is important to note that Turing.jl can also perform Bayesian estimation without relying on DiffEqBayes.jl (for an example, consult this tutorial).","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#DataDrivenDiffEq.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DataDrivenDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"The distinguishing feature of this package is that its ultimate goal is to identify the differential equation model that generated the input data. Depending on the user's needs, the package can provide structural identification of a given differential equation (output in a symbolic form) or structural estimation (output as a function for prediction purposes).","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#DiffEqParamEstim.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqParamEstim.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"This package is for simplified parameter estimation. While not as flexible of a system like DiffEqFlux.jl, it provides ready-made functions for doing standard optmization procedures like L2 fitting and MAP estimates. Among other features, it allows for the optimization of parameters in ODEs, stochastic problems, and delay differential equations.","category":"page"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/#DiffEqBayes.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqBayes.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"As the name suggests, this package has been designed to provide the estimation of differential equations parameters by means of Bayesian methods. It works in conjunction with Turing.jl,  CmdStan.jl,  DynamicHMC.jl, and  ApproxBayes.jl. While not as flexible as direct usage of DiffEqFlux.jl or Turing.jl, DiffEqBayes.jl can be an approachable interface for those not familiar with Bayesian estimation, and provides a nice way to use Stan from pure Julia.","category":"page"},{"location":"#The-SciML-Open-Source-Software-Ecosystem","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Source Software Ecosystem","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The SciML organization is a collection of tools for solving equations and modeling systems developed in the Julia programming language with bindings to other languages such as R and Python. The organization provides well-maintained  tools which compose together as a coherent ecosystem. It has a coherent development principle, unified APIs over large collections of equation solvers, pervasive differentiability and sensitivity analysis, and features many of the highest performance and parallel implementations one can find.","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"This documentation is made to pool together the docs of the various SciML libraries to paint the overarching picture, establish development norms, and document the shared/common functionality.","category":"page"},{"location":"#SciML:-Combining-Scientific-Computing-and-Machine-Learning","page":"The SciML Open Souce Software Ecosystem","title":"SciML: Combining Scientific Computing and Machine Learning","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"SciML is not standard machine learning,  SciML is the combination of scientific computing techniques with machine learning. Thus the SciML organization is not an organization for machine learning libraries (see  FluxML for machine learning in Julia), rather SciML is an organization dedicated to the development of scientific computing tools which work seamlessly in conjunction with next-generation machine learning workflows. This includes:","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"High performance and accurate tools for standard scientific computing modeling and simulation\nCompatibility with differentiable programming and automatic differentiation\nTools for building complex multiscale models\nMethods for handling inverse problems, model calibration, controls, and Bayesian analysis\nSymbolic modeling tools for generating efficient code for numerical equation solvers\nMethods for automatic discovery of (bio)physical equations","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"and much more. For an overview of the broad goals of the SciML organization, watch:","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The Use and Practice of Scientific Machine Learning\nState of SciML Scientific Machine Learning","category":"page"},{"location":"#Overview-of-Scientific-Computing-in-Julia-with-SciML","page":"The SciML Open Souce Software Ecosystem","title":"Overview of Scientific Computing in Julia with SciML","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Below is a simplification of the user-facing packages for use in scientific computing and SciML workflows.","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Workflow Element Associated Julia packages\nPlotting Plots*\nSparse matrix SparseArrays*\nInterpolation/approximation DataInterpolations*, ApproxFun*\nLinear system / least squares LinearSolve\nNonlinear system / rootfinding NonlinearSolve\nPolynomial roots Polynomials*\nIntegration Integrals\nNonlinear Optimization Optimization\nOther Optimization (linear, quadratic, convex, etc.) JuMP*\nInitial-value problem DifferentialEquations\nBoundary-value problem DifferentialEquations\nContinuous-Time Markov Chains (Poisson Jumps), Jump Diffusions JumpProcesses\nFinite differences FiniteDifferences*, FiniteDiff*\nAutomatic Differentiation ForwardDiff*, Enzyme*, DiffEqSensitivity\nBayesian Modeling Turing*\nDeep Learning Flux*\nAcausal Modeling / DAEs ModelingToolkit\nChemical Reaction Networks Catalyst\nSymbolic Computing Symbolics\nFast Fourier Transform FFTW*","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Partial Differential Equation Discretizations Associated Julia packages\nFinite Differences MethodOfLines\nDiscontinuous Galerkin Trixi*\nFinite Element Gridap*\nPhysics-Informed Neural Networks NeuralPDE\nNeural Operators NeuralOperators\nHigh Dimensional Deep Learning HighDimPDE","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"* Denotes a non-SciML package that is heavily tested against as part of SciML workflows and  has frequent collaboration with the SciML developers.","category":"page"},{"location":"#Domains-of-SciML","page":"The SciML Open Souce Software Ecosystem","title":"Domains of SciML","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The SciML common interface covers the following domains:","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Linear systems (LinearProblem)\nDirect methods for dense and sparse\nIterative solvers with preconditioning\nNonlinear Systems (NonlinearProblem)\nSystems of nonlinear equations\nScalar bracketing systems\nIntegrals (quadrature) (QuadratureProblem)\nDifferential Equations\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations) (DiscreteProblem and JumpProblem)\nOrdinary differential equations (ODEs) (ODEProblem)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods) (SplitODEProblem)\nStochastic ordinary differential equations (SODEs or SDEs) (SDEProblem)\nStochastic differential-algebraic equations (SDAEs) (SDEProblem with mass matrices)\nRandom differential equations (RODEs or RDEs) (RODEProblem)\nDifferential algebraic equations (DAEs) (DAEProblem and ODEProblem with mass matrices)\nDelay differential equations (DDEs) (DDEProblem)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs) (SDDEProblem)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions) (DEProblems with callbacks and JumpProblem)\nOptimization (OptimizationProblem)\nNonlinear (constrained) optimization\n(Stochastic/Delay/Differential-Algebraic) Partial Differential Equations (PDESystem)\nFinite difference and finite volume methods\nInterfaces to finite element methods\nPhysics-Informed Neural Networks (PINNs)\nIntegro-Differential Equations\nFractional Differential Equations\nSpecialized Forms \nPartial Integro-Differential Equations (PIPDEProblem)\nData-driven modeling\nDiscrete-time data-driven dynamical systems (DiscreteDataDrivenProblem)\nContinuous-time data-driven dynamical systems (ContinuousDataDrivenProblem)\nSymbolic regression (DirectDataDrivenProblem)\nUncertainty quantification and expected values (ExpectationProblem)","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The SciML common interface also includes ModelingToolkit.jl for defining such systems symbolically, allowing for optimizations like automated generation of parallel code, symbolic simplification, and generation of sparsity patterns.","category":"page"},{"location":"#Inverse-Problems,-Parameter-Estimation,-and-Structural-Identification","page":"The SciML Open Souce Software Ecosystem","title":"Inverse Problems, Parameter Estimation, and Structural Identification","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Parameter estimation and inverse problems are solved directly on their constituent problem types using tools like SciMLSensitivity.jl. Thus for example, there is no ODEInverseProblem, and instead ODEProblem is used to find the parameters p that solve the inverse problem. Check out the SciMLSensitivity documentation for a discussion on connections to automatic differentiation, optimization, and adjoints.","category":"page"},{"location":"#Common-Interface-High-Level","page":"The SciML Open Souce Software Ecosystem","title":"Common Interface High Level","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The SciML interface is common as the usage of arguments is standardized across all of the problem domains. Underlying high level ideas include:","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"All domains use the same interface of defining a SciMLProblem which is then solved via solve(prob,alg;kwargs), where alg is a SciMLAlgorithm. The keyword argument namings are standardized across the organization.\nSciMLProblems are generally defined by a SciMLFunction which can define extra details about a model function, such as its analytical Jacobian, its sparsity patterns and so on.\nThere is an organization-wide method for defining linear and nonlinear solvers used within other solvers, giving maximum control of performance to the user.\nTypes used within the packages are defined by the input types. For example, packages attempt to internally use the type of the initial condition as the type for the state within differential equation solvers.\nsolve calls should be thread-safe and parallel-safe.\ninit(prob,alg;kwargs) returns an iterator which allows for directly iterating over the solution process\nHigh performance is key. Any performance that is not at the top level is considered a bug and should be reported as such.\nAll functions have an in-place and out-of-place form, where the in-place form is made to utilize mutation for high performance on large-scale problems and the out-of-place form is for compatibility with tooling like static arrays and some reverse-mode automatic differentiation systems.","category":"page"},{"location":"#Flowchart-Example-for-PDE-Constrained-Optimal-Control","page":"The SciML Open Souce Software Ecosystem","title":"Flowchart Example for PDE-Constrained Optimal Control","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The following example showcases how the pieces of the common interface connect to solve a problem that mixes inference, symbolics, and numerics.","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"(Image: )","category":"page"},{"location":"#External-Binding-Libraries","page":"The SciML Open Souce Software Ecosystem","title":"External Binding Libraries","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"diffeqr\nSolving differential equations in R using DifferentialEquations.jl with ModelingToolkit for JIT compilation and GPU-acceleration\ndiffeqpy\nSolving differential equations in Python using DifferentialEquations.jl","category":"page"},{"location":"#Note-About-Third-Party-Libraries","page":"The SciML Open Souce Software Ecosystem","title":"Note About Third Party Libraries","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"The SciML documentation references and recommends many third party libraries for improving ones modeling, simulation, and analysis workflow in Julia. Take these as a positive affirmation of the quality of these libraries, as these libraries are commonly tested against by SciML developers and are in contact with the development teams of these groups. It also documents the libraries which are commonly chosen by SciML as dependencies. Do not take omissions as negative affirmations against a given library, i.e. a library left off of the list by SciML is not a negative endorsement. Rather, it means that compatibility with SciML is untested, SciML developers may have a personal preference for another choice, or SciML developers may be simply unaware of the library's existence. If one would like to add a third party library to the SciML documentation, open a pull request with the requested text. ","category":"page"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Note that the libraries in this documentation are only those that are meant to be used in the SciML extended universe of modeling, simulation, and analysis and thus there are many high quality libraries in other domains (machine learning, data science, etc.) which are purposefully not included. For an overview of the Julia package ecosystem, see the JuliaHub Search Engine.","category":"page"},{"location":"#Contributing","page":"The SciML Open Souce Software Ecosystem","title":"Contributing","text":"","category":"section"},{"location":"","page":"The SciML Open Souce Software Ecosystem","title":"The SciML Open Souce Software Ecosystem","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"modules/ExponentialUtilities/arnoldi/#Arnoldi-Iteration","page":"Arnoldi Iteration","title":"Arnoldi Iteration","text":"","category":"section"},{"location":"modules/ExponentialUtilities/arnoldi/","page":"Arnoldi Iteration","title":"Arnoldi Iteration","text":"arnoldi\narnoldi!\nlanczos!","category":"page"},{"location":"modules/ExponentialUtilities/arnoldi/#ExponentialUtilities.arnoldi","page":"Arnoldi Iteration","title":"ExponentialUtilities.arnoldi","text":"arnoldi(A,b[;m,tol,opnorm,iop]) -> Ks\n\nPerforms m anoldi iterations to obtain the Krylov subspace K_m(A,b).\n\nThe n x (m + 1) basis vectors getV(Ks) and the (m + 1) x m upper Hessenberg matrix getH(Ks) are related by the recurrence formula\n\nv_1=b,\\quad Av_j = \\sum_{i=1}^{j+1}h_{ij}v_i\\quad(j = 1,2,\\ldots,m)\n\niop determines the length of the incomplete orthogonalization procedure [1]. The default value of 0 indicates full Arnoldi. For symmetric/Hermitian A, iop will be ignored and the Lanczos algorithm will be used instead.\n\nRefer to KrylovSubspace for more information regarding the output.\n\nHappy-breakdown occurs whenver norm(v_j) < tol * opnorm, in this case the dimension of Ks is smaller than m.\n\n[1]: Koskela, A. (2015). Approximating the matrix exponential of an\n\nadvection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/arnoldi/#ExponentialUtilities.arnoldi!","page":"Arnoldi Iteration","title":"ExponentialUtilities.arnoldi!","text":"arnoldi!(Ks,A,b[;tol,m,opnorm,iop,init]) -> Ks\n\nNon-allocating version of arnoldi.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/arnoldi/#ExponentialUtilities.lanczos!","page":"Arnoldi Iteration","title":"ExponentialUtilities.lanczos!","text":"lanczos!(Ks,A,b[;tol,m,opnorm]) -> Ks\n\nA variation of arnoldi! that uses the Lanczos algorithm for Hermitian matrices.\n\n\n\n\n\n","category":"function"},{"location":"modules/ExponentialUtilities/arnoldi/#API","page":"Arnoldi Iteration","title":"API","text":"","category":"section"},{"location":"modules/ExponentialUtilities/arnoldi/","page":"Arnoldi Iteration","title":"Arnoldi Iteration","text":"KrylovSubspace\nfirststep!\narnoldi_step!\nlanczos_step!\ncoeff","category":"page"},{"location":"modules/ExponentialUtilities/arnoldi/#ExponentialUtilities.KrylovSubspace","page":"Arnoldi Iteration","title":"ExponentialUtilities.KrylovSubspace","text":"KrylovSubspace{T}(n,[maxiter=30]) -> Ks\n\nConstructs an uninitialized Krylov subspace, which can be filled by arnoldi!.\n\nThe dimension of the subspace, Ks.m, can be dynamically altered but should be smaller than maxiter, the maximum allowed arnoldi iterations.\n\ngetV(Ks) -> V\ngetH(Ks) -> H\n\nAccess methods for the (extended) orthonormal basis V and the (extended) Gram-Schmidt coefficients H. Both methods return a view into the storage arrays and has the correct dimensions as indicated by Ks.m.\n\nresize!(Ks, maxiter) -> Ks\n\nResize Ks to a different maxiter, destroying its contents.\n\nThis is an expensive operation and should be used scarcely.\n\n\n\n\n\n","category":"type"},{"location":"modules/GlobalSensitivity/methods/easi/#EASI-Method","page":"EASI Method","title":"EASI Method","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"struct EASI <: GSAMethod\n    max_harmonic::Int\n    dct_method::Bool\nend","category":"page"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"EASI has the following keyword arguments:","category":"page"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"max_harmonic: Maximum harmonic of the input frequency for which the output power spectrum is analyzed for. Defaults to 10.\ndct_method: Use Discrete Cosine Transform method to compute the power spectrum. Defaults to false.","category":"page"},{"location":"modules/GlobalSensitivity/methods/easi/#Method-Details","page":"EASI Method","title":"Method Details","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"The EASI method is a Fourier-based technique for performing variance-based methods of global sensitivity analysis for the computation of first order effects (Sobol’ indices), hence belonging into the same class of algorithms as FAST and RBD. It is a computationally cheap method for which existing data can be used. Unlike the FAST and RBD methods which use a specially generated sample set that contains suitable frequency data for the input factors, in EASI these frequencies are introduced by sorting and shuffling the available input samples.","category":"page"},{"location":"modules/GlobalSensitivity/methods/easi/#API","page":"EASI Method","title":"API","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"function gsa(f, method::EASI, p_range; N, batch = false, rng::AbstractRNG = Random.default_rng(), kwargs...)","category":"page"},{"location":"modules/GlobalSensitivity/methods/easi/#Example","page":"EASI Method","title":"Example","text":"","category":"section"},{"location":"modules/GlobalSensitivity/methods/easi/","page":"EASI Method","title":"EASI Method","text":"using GlobalSensitivity, Test\n\nfunction ishi_batch(X)\n    A= 7\n    B= 0.1\n    @. sin(X[1,:]) + A*sin(X[2,:])^2+ B*X[3,:]^4 *sin(X[1,:])\nend\nfunction ishi(X)\n    A= 7\n    B= 0.1\n    sin(X[1]) + A*sin(X[2])^2+ B*X[3]^4 *sin(X[1])\nend\n\nlb = -ones(4)*π\nub = ones(4)*π\n\nres1 = gsa(ishi,EASI(),[[lb[i],ub[i]] for i in 1:4],N=15000)\nres2 = gsa(ishi_batch,EASI(),[[lb[i],ub[i]] for i in 1:4],N=15000,batch=true)\n","category":"page"},{"location":"modules/Surrogates/optimizations/#Optimization-techniques","page":"Optimization","title":"Optimization techniques","text":"","category":"section"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"SRBF","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::SRBF,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"modules/Surrogates/optimizations/#Surrogates.surrogate_optimize-Tuple{Function, SRBF, Any, Any, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"The main idea is to pick the new evaluations from a set of candidate points where each candidate point is generated as an N(0, sigma^2) distributed perturbation from the current best solution. The value of sigma is modified based on progress and follows the same logic as in many trust region methods: we increase sigma if we make a lot of progress (the surrogate is accurate) and decrease sigma when we aren’t able to make progress (the surrogate model is inaccurate). More details about how sigma is updated is given in the original papers.\n\nAfter generating the candidate points, we predict their objective function value and compute the minimum distance to the previously evaluated point. Let the candidate points be denoted by C and let the function value predictions be s(x_i) and the distance values be d(x_i), both rescaled through a linear transformation to the interval [0,1]. This is done to put the values on the same scale. The next point selected for evaluation is the candidate point x that minimizes the weighted-distance merit function:\n\nmerit(x) = ws(x) + (1-w)(1-d(x))\n\nwhere 0 leq w leq 1. That is, we want a small function value prediction and a large minimum distance from the previously evaluated points. The weight w is commonly cycled between a few values to achieve both exploitation and exploration. When w is close to zero, we do pure exploration, while w close to 1 corresponds to exploitation.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"LCBS","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::LCBS,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"modules/Surrogates/optimizations/#Surrogates.surrogate_optimize-Tuple{Function, LCBS, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Lower Confidence Bound (LCB), a popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to minimize:\n\nLCB(x) = Ex - k * sqrt(Vx)\n\ndefault value k = 2.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"EI","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::EI,lb,ub,krig,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"modules/Surrogates/optimizations/#Surrogates.surrogate_optimize-Tuple{Function, EI, Any, Any, Any, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"This is an implementation of Expected Improvement (EI), arguably the most popular acquisition function in Bayesian optimization. Under a Gaussian process (GP) prior, the goal is to maximize expected improvement:\n\nEI(x) = Emax(f_best-f(x)0)\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"DYCORS","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::DYCORS,lb,ub,surrn::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"modules/Surrogates/optimizations/#Surrogates.surrogate_optimize-Tuple{Function, DYCORS, Any, Any, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"  surrogate_optimize(obj::Function,::DYCORS,lb::Number,ub::Number,surr1::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)\n\nThis is an implementation of the DYCORS strategy by Regis and Shoemaker: Rommel G Regis and Christine A Shoemaker. Combining radial basis function surrogates and dynamic coordinate search in high-dimensional expensive black-box optimization. Engineering Optimization, 45(5): 529–555, 2013. This is an extension of the SRBF strategy that changes how the candidate points are generated. The main idea is that many objective functions depend only on a few directions so it may be advantageous to perturb only a few directions. In particular, we use a perturbation probability to perturb a given coordinate and decrease this probability after each function evaluation so fewer coordinates are perturbed later in the optimization.\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"SOP","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,sop1::SOP,lb::Number,ub::Number,surrSOP::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=min(500*1,5000))","category":"page"},{"location":"modules/Surrogates/optimizations/#Surrogates.surrogate_optimize-Tuple{Function, SOP, Number, Number, AbstractSurrogate, SamplingAlgorithm}","page":"Optimization","title":"Surrogates.surrogate_optimize","text":"surrogateoptimize(obj::Function,::SOP,lb::Number,ub::Number,surr::AbstractSurrogate,sampletype::SamplingAlgorithm;maxiters=100,numnewsamples=100)\n\nSOP Surrogate optimization method, following closely the following papers:\n\n- SOP: parallel surrogate global optimization with Pareto center selection for computationally expensive single objective problems by Tipaluck Krityakierne\n- Multiobjective Optimization Using Evolutionary Algorithms by Kalyan Deb\n\n#Suggested number of new_samples = min(500*d,5000)\n\n\n\n\n\n","category":"method"},{"location":"modules/Surrogates/optimizations/#Adding-another-optimization-method","page":"Optimization","title":"Adding another optimization method","text":"","category":"section"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"To add another optimization method, you just need to define a new SurrogateOptimizationAlgorithm and write its corresponding algorithm, overloading the following:","category":"page"},{"location":"modules/Surrogates/optimizations/","page":"Optimization","title":"Optimization","text":"surrogate_optimize(obj::Function,::NewOptimizatonType,lb,ub,surr::AbstractSurrogate,sample_type::SamplingAlgorithm;maxiters=100,num_new_samples=100)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/#Parametric-Stoichiometry","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Catalyst supports stoichiometric coefficients that involve parameters, species, or even general expressions. In this tutorial we show several examples of how to use parametric stoichiometry, and discuss several caveats to be aware of.","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Note, this tutorial requires ModelingToolkit v8.5.4 or greater to work properly.","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/#Using-Symbolic-Stoichiometry","page":"Parametric Stoichiometry","title":"Using Symbolic Stoichiometry","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Let's first consider a simple reversible reaction where the number of reactants is a parameter, and the number of products is the product of two parameters. ","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"using Catalyst, Latexify, DifferentialEquations, ModelingToolkit, Plots\nrevsys = @reaction_network revsys begin\n    k₊, m*A --> (m*n)*B\n    k₋, B --> A\nend k₊ k₋ m n\nreactions(revsys)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Note, as always the @reaction_network macro sets all symbols not declared to be parameters to be species, so that in this example we have two species, A and B, and four parameters. In addition, the stoichiometry is applied to the right most symbol in a given term, i.e. in the first equation the substrate A has stoichiometry m and the product B has stoichiometry m*n. For example, in","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"rn = @reaction_network begin\n    k, A*C --> 2B\n    end k\nreactions(rn)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"we see three species, (A,B,C), however, A is treated as the stoichiometric coefficient of C, i.e.","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"rx = reactions(rn)[1]\nrx.substrates[1],rx.substoich[1]","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"We could have equivalently specified our systems directly via the Catalyst API. For example, for revsys we would could use","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"@parameters k₊,k₋,m,n\n@variables t, A(t), B(t)\nrxs = [Reaction(k₊,[A],[B],[m],[m*n]),\n       Reaction(k₋,[B],[A])] \nrevsys2 = ReactionSystem(rxs,t; name=:revsys)\nrevsys2 == revsys","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"which can be simplified using the @reaction macro to","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"rxs2 = [(@reaction k₊, m*A --> (m*n)*B),\n        (@reaction k₋, B --> A)]\nrevsys3 = ReactionSystem(rxs2,t; name=:revsys)\nrevsys3 == revsys","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Note, the @reaction macro assumes all symbols are parameters except the right most symbols in the reaction line (i.e. A and B). For example, in  @reaction k, F*A + 2(H*G+B) --> D, the substrates are (A,G,B) with stoichiometries (F,2*H,2).","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Let's now convert revsys to ODEs and look at the resulting equations:","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"osys = convert(ODESystem, revsys)\nequations(osys)\nshow(stdout, MIME\"text/plain\"(), equations(osys)) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Notice, as described in the Reaction rate laws used in simulations section, the default rate laws involve factorials in the stoichiometric coefficients. For this reason we must specify m and n as integers, and hence use a tuple for the parameter mapping","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"p  = (k₊ => 1.0, k₋ => 1.0, m => 2, n => 2)\nu₀ = [A => 1.0, B => 1.0]\noprob = ODEProblem(osys, u₀, (0.0,1.0), p)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"We can now solve and plot the system","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"sol = solve(oprob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"If we had used a vector to store parameters, m and n would be converted to floating point giving an error when solving the system.","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"An alternative approach to avoid the issues of using mixed floating point and integer variables is to disable the rescaling of rate laws as described in Reaction rate laws used in simulations section. This requires passing the combinatoric_ratelaws=false keyword to convert or to ODEProblem (if directly building the problem from a ReactionSystem instead of first converting to an ODESystem). For the previous example this gives the following (different) system of ODEs","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"osys = convert(ODESystem, revsys; combinatoric_ratelaws=false)\nequations(osys)\nshow(stdout, MIME\"text/plain\"(), equations(osys)) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Since we no longer have factorial functions appearing, our example will now run even with floating point values for m and n:","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"p  = (k₊ => 1.0, k₋ => 1.0, m => 2.0, n => 2.0)\noprob = ODEProblem(osys, u₀, (0.0,1.0), p)\nsol = solve(oprob, Tsit5())\nplot(sol)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/#Gene-expression-with-randomly-produced-amounts-of-protein","page":"Parametric Stoichiometry","title":"Gene expression with randomly produced amounts of protein","text":"","category":"section"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"As a second example, let's build the negative feedback model from MomentClosure.jl that involves a bursty reaction that produces a random amount of protein. ","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"In our model G₋ will denote the repressed state, and G₊ the active state where the gene can transcribe. P will denote the protein product of the gene. We will assume that proteins are produced in bursts that produce m proteins, where m is a (shifted) geometric random variable with mean b. To define m we must register the Distributions.Geometric distribution from Distributions.jl with Symbolics.jl, after which we can use it in symbolic expressions:","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"using Distributions: Geometric\n@register_symbolic Geometric(b)\n@parameters b\nm = rand(Geometric(1/b)) + 1\nnothing # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Note, as we require the shifted geometric distribution, we add one to Distributions.jl's Geometric random variable (which includes zero). ","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"We can now define our model","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"burstyrn = @reaction_network burstyrn begin\n    k₊, G₋ --> G₊\n    k₋*P^2, G₊ --> G₋\n    kₚ, G₊ --> G₊ + $m*P\n    γₚ, P --> ∅\nend k₊ k₋ kₚ γₚ\nreactions(burstyrn)\nshow(stdout, MIME\"text/plain\"(), reactions(burstyrn)) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"The parameter b does not need to be explicitly declared in the @reaction_network macro as it is detected when the expression rand(Geometric(1/b)) + 1 is substituted for m.","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"We next convert our network to a jump process representation","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"jsys = convert(JumpSystem, burstyrn; combinatoric_ratelaws=false)\nequations(jsys)\nshow(stdout, MIME\"text/plain\"(), equations(jsys)) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Notice, the equations of jsys have three MassActionJumps for the first three reactions, and one ConstantRateJump for the last reaction. If we examine the ConstantRateJump more closely we can see the generated rate and affect! functions for the bursty reaction that makes protein","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"equations(jsys)[4].rate\nshow(stdout, MIME\"text/plain\"(), equations(jsys)[4].rate) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"equations(jsys)[4].affect!\nshow(stdout, MIME\"text/plain\"(), equations(jsys)[4].affect!) # hide","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Finally, we can now simulate our jumpsystem","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"pmean = 200\nbval = 70\nγₚval = 1\nk₋val = 0.001\nk₊val = 0.05\nkₚval = pmean * γₚval * (k₋val * pmean^2 + k₊val) / (k₊val * bval)\np = symmap_to_varmap(jsys, (:k₊ => k₊val, :k₋ => k₋val, :kₚ => kₚval, :γₚ => γₚval, :b => bval))\nu₀ = symmap_to_varmap(jsys, [:G₊ => 1, :G₋ => 0, :P => 1])\ntspan = (0., 6.0)   # time interval to solve over\ndprob = DiscreteProblem(jsys, u₀, tspan, p)\njprob = JumpProblem(jsys, dprob, Direct())\nsol = solve(jprob, SSAStepper())\nplot(sol.t, sol[jsys.P], legend=false, xlabel=\"time\", ylabel=\"P(t)\")","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"To double check our results are consistent with MomentClosure.jl, let's calculate and plot the average amount of protein (which is also plotted in the MomentClosure.jl tutorial).","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"function getmean(jprob, Nsims, tv)\n    Pmean = zeros(length(tv))\n    @variables t, P(t)\n    for n in 1:Nsims\n        sol = solve(jprob, SSAStepper())        \n        Pmean .+= sol(tv, idxs=P)\n    end\n    Pmean ./= Nsims\nend\ntv = range(tspan[1],tspan[2],step=.1)\npsim_mean = getmean(jprob, 20000, tv)\nplot(tv, psim_mean, ylabel=\"average of P(t)\", xlabel=\"time\", xlim=(0.0,6.0), legend=false)","category":"page"},{"location":"modules/Catalyst/tutorials/symbolic_stoich/","page":"Parametric Stoichiometry","title":"Parametric Stoichiometry","text":"Comparing, we see similar averages for P(t).","category":"page"},{"location":"modules/StructuralIdentifiability/utils/ode/#Working-with-ODEs","page":"ODE Tools","title":"Working with ODEs","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/ode/","page":"ODE Tools","title":"ODE Tools","text":"Pages = [\"ode.md\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/ode/","page":"ODE Tools","title":"ODE Tools","text":"Modules = [StructuralIdentifiability]\nPages   = [\"ODE.jl\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/ode/#StructuralIdentifiability.PreprocessODE-Tuple{ODESystem, Array{Equation}}","page":"ODE Tools","title":"StructuralIdentifiability.PreprocessODE","text":"function PreprocessODE(de::ModelingToolkit.ODESystem, measured_quantities::Array{ModelingToolkit.Equation})\n\nInput:\n\nde - ModelingToolkit.ODESystem, a system for identifiability query\nmeasured_quantities - array of output functions\n\nOutput: \n\nODE object containing required data for identifiability assessment\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/ode/#StructuralIdentifiability._reduce_mod_p-Tuple{Nemo.fmpq_mpoly, Int64}","page":"ODE Tools","title":"StructuralIdentifiability._reduce_mod_p","text":"_reduce_mod_p(f, p)\n\nReduces a polynomial/rational function over Q modulo p\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/ode/#StructuralIdentifiability.power_series_solution-Union{Tuple{P}, Tuple{T}, Tuple{ODE{P}, Dict{P, T}, Dict{P, T}, Dict{P, Vector{T}}, Int64}} where {T<:AbstractAlgebra.FieldElem, P<:AbstractAlgebra.MPolyElem{T}}","page":"ODE Tools","title":"StructuralIdentifiability.power_series_solution","text":"power_series_solution(ode, param_values, initial_conditions, input_values, prec)\n\nInput:\n\node - an ode to solve\nparam_values - parameter values, must be a dictionary mapping parameter to a value\ninitial_conditions - initial conditions of ode, must be a dictionary mapping state variable to a value\ninput_values - power series for the inpiuts presented as a dictionary variable => list of coefficients\nprec - the precision of solutions\n\nOutput: \n\ncomputes a power series solution with precision prec presented as a dictionary variable => corresponding coordiante of the solution\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/ode/#StructuralIdentifiability.reduce_ode_mod_p-Tuple{ODE{<:AbstractAlgebra.MPolyElem{Nemo.fmpq}}, Int64}","page":"ODE Tools","title":"StructuralIdentifiability.reduce_ode_mod_p","text":"reduce_ode_mod_p(ode, p)\n\nInput: ode is an ODE over QQ, p is a prime number Output: the reduction mod p, throws an exception if p divides one of the denominators\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/ode/#StructuralIdentifiability.set_parameter_values-Union{Tuple{P}, Tuple{T}, Tuple{ODE{P}, Dict{P, T}}} where {T<:AbstractAlgebra.FieldElem, P<:AbstractAlgebra.MPolyElem{T}}","page":"ODE Tools","title":"StructuralIdentifiability.set_parameter_values","text":"set_parameter_values(ode, param_values)\n\nInput:\n\node - an ODE as above\nparam_values - values for (possibly, some of) the parameters as dictionary parameter => value\n\nOutput: \n\nnew ode with the parameters in param_values plugged with the given numbers\n\n\n\n\n\n","category":"method"},{"location":"modules/ModelingToolkit/tutorials/stochastic_diffeq/#Modeling-with-Stochasticity","page":"Modeling with Stochasticity","title":"Modeling with Stochasticity","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/stochastic_diffeq/","page":"Modeling with Stochasticity","title":"Modeling with Stochasticity","text":"All models with ODESystem are deterministic. SDESystem adds another element to the model: randomness. This is a stochastic differential equation which has a deterministic (drift) component and a stochastic (diffusion) component. Let's take the Lorenz equation from the first tutorial and extend it to have multiplicative noise.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/stochastic_diffeq/","page":"Modeling with Stochasticity","title":"Modeling with Stochasticity","text":"using ModelingToolkit, StochasticDiffEq\n\n# Define some variables\n@parameters σ ρ β\n@variables t x(t) y(t) z(t)\nD = Differential(t)\n\neqs = [D(x) ~ σ*(y-x),\n       D(y) ~ x*(ρ-z)-y,\n       D(z) ~ x*y - β*z]\n\nnoiseeqs = [0.1*x,\n            0.1*y,\n            0.1*z]\n\n@named de = SDESystem(eqs,noiseeqs,t,[x,y,z],[σ,ρ,β])\n\nu0map = [\n    x => 1.0,\n    y => 0.0,\n    z => 0.0\n]\n\nparammap = [\n    σ => 10.0,\n    β => 26.0,\n    ρ => 2.33\n]\n\nprob = SDEProblem(de,u0map,(0.0,100.0),parammap)\nsol = solve(prob,SOSRI())","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Common-Keyword-Arguments","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"The following defines the keyword arguments which are meant to be preserved throughout all of the AbstractSciMLProblem cases (where applicable).","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Default-Algorithm-Hinting","page":"Common Keyword Arguments","title":"Default Algorithm Hinting","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"To help choose the default algorithm, the keyword argument alg_hints is provided to solve. alg_hints is a Vector{Symbol} which describe the problem at a high level to the solver. The options are:","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"This functionality is derived via the benchmarks in SciMLBenchmarks.jl","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"Currently this is only implemented for the differential equation solvers.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Output-Control","page":"Common Keyword Arguments","title":"Output Control","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"These arguments control the output behavior of the solvers. It defaults to maximum output to give the best interactive user experience, but can be reduced all the way to only saving the solution at the final timepoint.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"The following options are all related to output control. See the \"Examples\" section at the end of this page for some example usage.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"dense: Denotes whether to save the extra pieces required for dense (continuous) output. Default is save_everystep && !isempty(saveat) for algorithms which have the ability to produce dense output, i.e. by default it's true unless the user has turned off saving on steps or has chosen a saveat value. If dense=false, the solution still acts like a function, and sol(t) is a linear interpolation between the saved time points.\nsaveat: Denotes specific times to save the solution at, during the solving phase. The solver will save at each of the timepoints in this array in the most efficient manner available to the solver. If only saveat is given, then the arguments save_everystep and dense are false by default. If saveat is given a number, then it will automatically expand to tspan[1]:saveat:tspan[2]. For methods where interpolation is not possible, saveat may be equivalent to tstops. The default value is [].\nsave_idxs: Denotes the indices for the components of the equation to save. Defaults to saving all indices. For example, if you are solving a 3-dimensional ODE, and given save_idxs = [1, 3], only the first and third components of the solution will be outputted. Notice that of course in this case the outputed solution will be two-dimensional.\ntstops: Denotes extra times that the timestepping algorithm must step to. This should be used to help the solver deal with discontinuities and singularities, since stepping exactly at the time of the discontinuity will improve accuracy. If a method cannot change timesteps (fixed timestep multistep methods), then tstops will use an interpolation, matching the behavior of saveat. If a method cannot change timesteps and also cannot interpolate, then tstops must be a multiple of dt or else an error will be thrown. Default is [].\nd_discontinuities: Denotes locations of discontinuities in low order derivatives. This will force FSAL algorithms which assume derivative continuity to re-evaluate the derivatives at the point of discontinuity. The default is [].\nsave_everystep: Saves the result at every step. Default is true if isempty(saveat).\nsave_on: Denotes whether intermediate solutions are saved. This overrides the settings of dense, saveat and save_everystep and is used by some applicatioins to manually turn off saving temporarily. Everyday use of the solvers should leave this unchanged. Defaults to true.\nsave_start: Denotes whether the initial condition should be included in the solution type as the first timepoint. Defaults to true.\nsave_end: Denotes whether the final timepoint is forced to be saved, regardless of the other saving settings. Defaults to true.\ninitialize_save: Denotes whether to save after the callback initialization phase (when u_modified=true). Defaults to true.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"Note that dense requires save_everystep=true and saveat=false.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Stepsize-Control","page":"Common Keyword Arguments","title":"Stepsize Control","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"These arguments control the timestepping routines.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Basic-Stepsize-Control","page":"Common Keyword Arguments","title":"Basic Stepsize Control","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"adaptive: Turns on adaptive timestepping for appropriate methods. Default is true.\nabstol: Absolute tolerance in adaptive timestepping. This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related).\nreltol: Relative tolerance in adaptive timestepping.  This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related).\ndt: Sets the initial stepsize. This is also the stepsize for fixed timestep methods. Defaults to an automatic choice if the method is adaptive.\ndtmax: Maximum dt for adaptive timestepping. Defaults are package-dependent.\ndtmin: Minimum dt for adaptive timestepping. Defaults are package-dependent.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Fixed-Stepsize-Usage","page":"Common Keyword Arguments","title":"Fixed Stepsize Usage","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"Note that if a method does not have adaptivity, the following rules apply:","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"If dt is set, then the algorithm will step with size dt each iteration.\nIf tstops and dt are both set, then the algorithm will step with either a size dt, or use a smaller step to hit the tstops point.\nIf tstops is set without dt, then the algorithm will step directly to each value in tstops\nIf neither dt nor tstops are set, the solver will throw an error.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Memory-Optimizations","page":"Common Keyword Arguments","title":"Memory Optimizations","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"alias_u0: allows the solver to alias the initial condition array that is contained in the problem struct. Defaults to false.\ncache: pass a solver cache to decrease the construction time. This is not implemented for any of the problem interfaces at this moment.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Miscellaneous","page":"Common Keyword Arguments","title":"Miscellaneous","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"maxiters: Maximum number of iterations before stopping.\ncallback: Specifies a callback function that is called between iterations.\nverbose: Toggles whether warnings are thrown when the solver exits early. Defaults to true.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Progress-Monitoring","page":"Common Keyword Arguments","title":"Progress Monitoring","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"These arguments control the usage of the progressbar in the logger.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"progress: Turns on/off the Juno progressbar. Default is false.\nprogress_steps: Numbers of steps between updates of the progress bar. Default is 1000.\nprogress_name: Controls the name of the progressbar. Default is the name of the problem type.\nprogress_message: Controls the message with the progressbar. Defaults to showing dt, t, the maximum of u.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"The progress bars all use the Julia Logging interface in order to be generic to the IDE or programming tool that is used. For more information on how this is all put together, see this discussion.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Error-Calculations","page":"Common Keyword Arguments","title":"Error Calculations","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"If you are using the test problems (i.e. SciMLFunctions where f.analytic is defined), then options control the errors which are calculated. By default, any cheap error estimates are always calculated. Extra keyword arguments include:","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"timeseries_errors\ndense_errors","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"for specifying more expensive errors.","category":"page"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/#Automatic-Differentiation-Control","page":"Common Keyword Arguments","title":"Automatic Differentiation Control","text":"","category":"section"},{"location":"modules/SciMLBase/interfaces/Common_Keywords/","page":"Common Keyword Arguments","title":"Common Keyword Arguments","text":"See the Automatic Differentiation page for a full description of sensealg","category":"page"},{"location":"modules/Surrogates/variablefidelity/#Variable-fidelity-Surrogates","page":"Variable Fidelity","title":"Variable fidelity Surrogates","text":"","category":"section"},{"location":"modules/Surrogates/variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"With the variable fidelity surrogate, we can specify two different surrogates: one for high fidelity data and one for low fidelity data. By default, the first half samples are considered high fidelity and the second half low fidelity.","category":"page"},{"location":"modules/Surrogates/variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"using Surrogates\nusing Plots\ndefault()","category":"page"},{"location":"modules/Surrogates/variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"n = 20\nlower_bound = 1.0\nupper_bound = 6.0\nx = sample(n,lower_bound,upper_bound,SobolSample())\nf = x -> 1/3*x\ny = f.(x)\nplot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"modules/Surrogates/variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"varfid = VariableFidelitySurrogate(x,y,lower_bound,upper_bound)","category":"page"},{"location":"modules/Surrogates/variablefidelity/","page":"Variable Fidelity","title":"Variable Fidelity","text":"plot(x, y, seriestype=:scatter, label=\"Sampled points\", xlims=(lower_bound, upper_bound), legend=:top)\nplot!(f, label=\"True function\",  xlims=(lower_bound, upper_bound), legend=:top)\nplot!(varfid, label=\"Surrogate function\",  xlims=(lower_bound, upper_bound), legend=:top)","category":"page"},{"location":"highlevels/abstractarray_libraries/#AbstractArray-Libraries","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/#RecursiveArrayTools.jl:-Arrays-of-Arrays-and-Even-Deeper","page":"AbstractArray Libraries","title":"RecursiveArrayTools.jl: Arrays of Arrays and Even Deeper","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Sometimes when one is creating a model, basic array types are not enough for expressing a complex concept. RecursiveArrayTools.jl gives many types, such as VectorOfArray and ArrayPartition, which allow for easily building nested array models in a way that conforms to the standard AbstractArray interface. While standard Vector{Array{Float64,N}} types may not be compatible with many equation solver libraries, these wrapped forms like VectorOfArray{Vector{Array{Float64,N}}} are, making it easy to use these more exotic array constructions.","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Note that SciML's interfaces use RecursiveArrayTools.jl extensively, for example, with the timeseries solution types being AbstractVectorOfArray.","category":"page"},{"location":"highlevels/abstractarray_libraries/#LabelledArrays.jl:-Named-Variables-in-Arrays-without-Overhead","page":"AbstractArray Libraries","title":"LabelledArrays.jl: Named Variables in Arrays without Overhead","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Sometimes you want to use a full domain-specific language like  ModelingToolkit. Other times, you wish arrays just had a slightly nicer syntax. Don't you wish you could write the Lorenz equations like:","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"function lorenz_f(du,u,p,t)\n  du.x = p.σ*(u.y-u.x)\n  du.y = u.x*(p.ρ-u.z) - u.y\n  du.z = u.x*u.y - p.β*u.z\nend","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"without losing any efficiency? LabelledArrays.jl provides the array types to do just that. All of the . accesses are resolved at compile-time so it's a zero-overhead interface.","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"note: Note\nWe recommend using ComponentArrays.jl for any instance where nested accesses is required, or where the . accesses need to be views to subsets of the array.","category":"page"},{"location":"highlevels/abstractarray_libraries/#MultiScaleArrays.jl:-Multiscale-Modeling-to-Compose-with-Equation-Solvers","page":"AbstractArray Libraries","title":"MultiScaleArrays.jl: Multiscale Modeling to Compose with Equation Solvers","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"(Image: )","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"How do you encode such real-world structures in a manner that is compatible with the SciML equation solver libraries? MultiScaleArrays.jl is an answer. MultiScaleArrays.jl gives a highly flexible interface for defining multi-level types which generates a corresponding interface as an AbstractArray. MultiScaleArrays.jl's flexibility includes the ease of resizing, allowing for models where the number of equations grows and shrinks as agents (cells) in the model divide and die.","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"note: Note\nWe recommend using ComponentArrays.jl instead in any instance where the resizing functionality is not used.","category":"page"},{"location":"highlevels/abstractarray_libraries/#Third-Party-Libraries-to-Note","page":"AbstractArray Libraries","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/#ComponentArrays.jl:-Arrays-with-Arbitrarily-Nested-Named-Components","page":"AbstractArray Libraries","title":"ComponentArrays.jl: Arrays with Arbitrarily Nested Named Components","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"What if you had a set of arrays of arrays with names, but you wanted to represent them on a single contiguous vector so that linear algebra was as fast as possible, while retaining . named accesses with zero-overhead? This is what ComponentArrays.jl provides, and as such it is one of the top recommendations of AbstractArray types to be used. Multi-level definitions such as x = ComponentArray(a=5, b=[(a=20., b=0), (a=33., b=0), (a=44., b=3)], c=c) are common-place, and allow for accessing via x.b.a etc. without any performance loss. ComponentArrays are fully compatible with the SciML equation solvers, thus they can be used as initial conditions. Here's a demonstration of the Lorenz equation using ComponentArrays with Parameters.jl's @unpack:","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"using ComponentArrays\nusing DifferentialEquations\nusing Parameters: @unpack\n\n\ntspan = (0.0, 20.0)\n\n\n## Lorenz system\nfunction lorenz!(D, u, p, t; f=0.0)\n    @unpack σ, ρ, β = p\n    @unpack x, y, z = u\n\n    D.x = σ*(y - x)\n    D.y = x*(ρ - z) - y - f\n    D.z = x*y - β*z\n    return nothing\nend\n\nlorenz_p = (σ=10.0, ρ=28.0, β=8/3)\nlorenz_ic = ComponentArray(x=0.0, y=0.0, z=0.0)\nlorenz_prob = ODEProblem(lorenz!, lorenz_ic, tspan, lorenz_p)","category":"page"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Is that beautiful? Yes it is.","category":"page"},{"location":"highlevels/abstractarray_libraries/#StaticArrays.jl:-Statically-Defined-Arrays","page":"AbstractArray Libraries","title":"StaticArrays.jl: Statically-Defined Arrays","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"StaticArrays.jl is a library for statically-defined arrays. Because these arrays have type-level information for size, they recompile the solvers for every new size. They can be dramatically faster for small sizes (up to approximately size 10), but for larger equations they increase compile time with little to not benefit.","category":"page"},{"location":"highlevels/abstractarray_libraries/#CUDA.jl:-NVIDIA-CUDA-Based-GPU-Array-Computations","page":"AbstractArray Libraries","title":"CUDA.jl: NVIDIA CUDA-Based GPU Array Computations","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"CUDA.jl is the library for defining arrays which live on NVIDIA GPUs (CuArray). SciML's libraries will respect the GPU-ness of the inputs, i.e., if the input arrays live on the GPU then the operations will all take place on the GPU or else the libraries will error if it's unable to do so. Thus using CUDA.jl's CuArray is how one GPU-accelerates any computation with the SciML organization's libraries. Simply use a CuArray as the initial condition to an ODE solve or as the initial guess for a nonlinear solve and the whole solve will recompile to take place on the GPU.","category":"page"},{"location":"highlevels/abstractarray_libraries/#AMDGPU.jl:-AMD-Based-GPU-Array-Computations","page":"AbstractArray Libraries","title":"AMDGPU.jl: AMD-Based GPU Array Computations","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"AMDGPU.jl is the library for defining arrays which live on AMD GPUs (ROCArray). SciML's libraries will respect the GPU-ness of the inputs, i.e., if the input arrays live on the GPU then the operations will all take place on the GPU or else the libraries will error if it's unable to do so. Thus using AMDGPU.jl's ROCArray is how one GPU-accelerates any computation with the SciML organization's libraries. Simply use a ROCArray as the initial condition to an ODE solve or as the initial guess for a nonlinear solve and the whole solve will recompile to take place on the GPU.","category":"page"},{"location":"highlevels/abstractarray_libraries/#FillArrays.jl:-Lazy-Arrays","page":"AbstractArray Libraries","title":"FillArrays.jl: Lazy Arrays","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"FillArrays.jl is a library for defining arrays with lazy values. For example, an O(1) representation of the identity matrix is given by Eye{Int}(5). FillArrays.jl is used extensively throughout the ecosystem to improve runtime and memory performance.","category":"page"},{"location":"highlevels/abstractarray_libraries/#BandedMatrices.jl:-Fast-Banded-Matrices","page":"AbstractArray Libraries","title":"BandedMatrices.jl: Fast Banded Matrices","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Banded matrices show up in many equation solver contexts, such as the Jacobians of many partial differential equations. While the base SparseMatrixCSC sparse matrix type can represent such matices, BandedMatrices.jl is a specialized format specifically for BandedMatrices which can be used to greatly improve performance of operations on a banded matrix.","category":"page"},{"location":"highlevels/abstractarray_libraries/#BlockBandedMatrices.jl:-Fast-Block-Banded-Matrices","page":"AbstractArray Libraries","title":"BlockBandedMatrices.jl: Fast Block-Banded Matrices","text":"","category":"section"},{"location":"highlevels/abstractarray_libraries/","page":"AbstractArray Libraries","title":"AbstractArray Libraries","text":"Block banded matrices show up in many equation solver contexts, such as the Jacobians of many systems of partial differential equations. While the base SparseMatrixCSC sparse matrix type can represent such matices, BlockBandedMatrices.jl is a specialized format specifically for BlockBandedMatrices which can be used to greatly improve performance of operations on a block-banded matrix.","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/#BlackBoxOptim.jl","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"BlackBoxOptim is a is a Julia package implementing (Meta-)heuristic/stochastic algorithms that do not require for the optimized function to be differentiable.","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/#Installation:-OptimizationBBO.jl","page":"BlackBoxOptim.jl","title":"Installation: OptimizationBBO.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"To use this package, install the OptimizationBBO package:","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"import Pkg; Pkg.add(\"OptimizationBBO\")","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/#Global-Optimizers","page":"BlackBoxOptim.jl","title":"Global Optimizers","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/#Without-Constraint-Equations","page":"BlackBoxOptim.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"The algorithms in BlackBoxOptim are performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"A BlackBoxOptim algorithm is called by BBO_ prefix followed by the algorithm name:","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"Natural Evolution Strategies:\nSeparable NES: BBO_separable_nes()\nExponential NES: BBO_xnes()\nDistance-weighted Exponential NES: BBO_dxnes()\nDifferential Evolution optimizers, 5 different:\nAdaptive DE/rand/1/bin: BBO_adaptive_de_rand_1_bin()\nAdaptive DE/rand/1/bin with radius limited sampling: BBO_adaptive_de_rand_1_bin_radiuslimited()\nDE/rand/1/bin: BBO_de_rand_1_bin()\nDE/rand/1/bin with radius limited sampling (a type of trivial geography): BBO_de_rand_1_bin_radiuslimited()\nDE/rand/2/bin: de_rand_2_bin()\nDE/rand/2/bin with radius limited sampling (a type of trivial geography): BBO_de_rand_2_bin_radiuslimited()\nDirect search:\nGenerating set search:\nCompass/coordinate search: BBO_generating_set_search()\nDirect search through probabilistic descent: BBO_probabilistic_descent()\nResampling Memetic Searchers:\nResampling Memetic Search (RS): BBO_resampling_memetic_search()\nResampling Inheritance Memetic Search (RIS): BBO_resampling_inheritance_memetic_search()\nStochastic Approximation:\nSimultaneous Perturbation Stochastic Approximation (SPSA): BBO_simultaneous_perturbation_stochastic_approximation()\nRandomSearch (to compare to): BBO_random_search()","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"The recommended optimizer is BBO_adaptive_de_rand_1_bin_radiuslimited()","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"The currently available algorithms are listed here","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/#Example","page":"BlackBoxOptim.jl","title":"Example","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"The Rosenbrock function can optimized using the BBO_adaptive_de_rand_1_bin_radiuslimited() as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/blackboxoptim/","page":"BlackBoxOptim.jl","title":"BlackBoxOptim.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited(), maxiters=100000, maxtime=1000.0)","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Parameter-Identifiability-in-ODE-Models","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Ordinary differential equations are commonly used for modeling real-world processes. The problem of parameter identifiability is one of the key design challenges for mathematical models. A parameter is said to be identifiable if one can recover its value from experimental data. Structural identifiability is a theoretical property of a model that answers this question. In this tutorial, we will show how to use StructuralIdentifiability.jl with ModelingToolkit.jl to assess identifiability of parameters in ODE models. The theory behind StructuralIdentifiability.jl is presented in paper [4].","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We will start by illustrating local identifiability in which a parameter is known up to finitely many values, and then proceed to determining global identifiability, that is, which parameters can be identified uniquely.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"To install StructuralIdentifiability.jl, simply run","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"using Pkg\nPkg.add(\"StructuralIdentifiability\")","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"The package has a standalone data structure for ordinary differential equations but is also compatible with ODESystem type from ModelingToolkit.jl.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Local-Identifiability","page":"Parameter Identifiability in ODE Models","title":"Local Identifiability","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Input-System","page":"Parameter Identifiability in ODE Models","title":"Input System","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We will consider the following model:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"begincases\nfracdx_4dt = - frack_5 x_4k_6 + x_4\nfracdx_5dt = frack_5 x_4k_6 + x_4 - frack_7 x_5(k_8 + x_5 + x_6)\nfracdx_6dt = frack_7 x_5(k_8 + x_5 + x_6) - frack_9  x_6  (k_10 - x_6) k_10\nfracdx_7dt = frack_9  x_6  (k_10 - x_6) k_10\ny_1 = x_4\ny_2 = x_5endcases","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"This model describes the biohydrogenation[1] process[2] with unknown initial conditions.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Using-the-ODESystem-object","page":"Parameter Identifiability in ODE Models","title":"Using the ODESystem object","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"To define the ode system in Julia, we use ModelingToolkit.jl.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We first define the parameters, variables, differential equations and the output equations.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"using StructuralIdentifiability, ModelingToolkit\n\n# define parameters and variables\n@variables t x4(t) x5(t) x6(t) x7(t) y1(t) y2(t)\n@parameters k5 k6 k7 k8 k9 k10\nD = Differential(t)\n\n# define equations\neqs = [\n    D(x4) ~ - k5 * x4 / (k6 + x4),\n    D(x5) ~ k5 * x4 / (k6 + x4) - k7 * x5/(k8 + x5 + x6),\n    D(x6) ~ k7 * x5 / (k8 + x5 + x6) - k9 * x6 * (k10 - x6) / k10,\n    D(x7) ~ k9 * x6 * (k10 - x6) / k10\n]\n\n# define the output functions (quantities that can be measured)\nmeasured_quantities = [y1 ~ x4, y2 ~ x5]\n\n# define the system\nde = ODESystem(eqs, t, name=:Biohydrogenation)\n","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"After that we are ready to check the system for local identifiability:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"# query local identifiability\n# we pass the ode-system\nlocal_id_all = assess_local_identifiability(de, measured_quantities=measured_quantities, p=0.99)\n                # [ Info: Preproccessing `ModelingToolkit.ODESystem` object\n                # 6-element Vector{Bool}:\n                #  1\n                #  1\n                #  1\n                #  1\n                #  1\n                #  1","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We can see that all states (except x_7) and all parameters are locally identifiable with probability 0.99. ","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Let's try to check specific parameters and their combinations","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"to_check = [k5, k7, k10/k9, k5+k6]\nlocal_id_some = assess_local_identifiability(de, measured_quantities=measured_quantities, funcs_to_check=to_check, p=0.99)\n                # 4-element Vector{Bool}:\n                #  1\n                #  1\n                #  1\n                #  1","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Notice that in this case, everything (except the state variable x_7) is locally identifiable, including combinations such as k_10k_9 k_5+k_6","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Global-Identifiability","page":"Parameter Identifiability in ODE Models","title":"Global Identifiability","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"In this part tutorial, let us cover an example problem of querying the ODE for globally identifiable parameters.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/#Input-System-2","page":"Parameter Identifiability in ODE Models","title":"Input System","text":"","category":"section"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Let us consider the following four-dimensional model with two outputs:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"begincases\n    x_1(t) = -b  x_1(t) + frac1  c + x_4(t)\n    x_2(t) = alpha  x_1(t) - beta  x_2(t)\n    x_3(t) = gamma  x_2(t) - delta  x_3(t)\n    x_4(t) = sigma  x_4(t)  frac(gamma x_2(t) - delta x_3(t)) x_3(t)\n    y(t) = x_1(t)\nendcases","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We will run a global identifiability check on this enzyme dynamics[3] model. We will use the default settings: the probability of correctness will be p=0.99 and we are interested in identifiability of all possible parameters","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Global identifiability needs information about local identifiability first, but the function we chose here will take care of that extra step for us.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Note: as of writing this tutorial, UTF-symbols such as Greek characters are not supported by one of the project's dependencies, see this issue.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"using StructuralIdentifiability, ModelingToolkit\n@parameters b c a beta g delta sigma\n@variables t x1(t) x2(t) x3(t) x4(t) y(t) y2(t)\nD = Differential(t)\n\neqs = [\n    D(x1) ~ -b * x1 + 1/(c + x4),\n    D(x2) ~ a * x1 - beta * x2,\n    D(x3) ~ g * x2 - delta * x3,\n    D(x4) ~ sigma * x4 * (g * x2 - delta * x3)/x3\n]\n\nmeasured_quantities = [y~x1+x2, y2~x2]\n\n\node = ODESystem(eqs, t, name=:GoodwinOsc)\n\n@time global_id = assess_identifiability(ode, measured_quantities=measured_quantities)\n                    # 30.672594 seconds (100.97 M allocations: 6.219 GiB, 3.15% gc time, 0.01% compilation time)\n                    # Dict{Num, Symbol} with 7 entries:\n                    #   a     => :globally\n                    #   b     => :globally\n                    #   beta  => :globally\n                    #   c     => :globally\n                    #   sigma => :globally\n                    #   g     => :nonidentifiable\n                    #   delta => :globally","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"We can see that only parameters a, g are unidentifiable and everything else can be uniquely recovered.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Let us consider the same system but with two inputs and we will try to find out identifiability with probability 0.9 for parameters c and b:","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"using StructuralIdentifiability, ModelingToolkit\n@parameters b c a beta g delta sigma\n@variables t x1(t) x2(t) x3(t) x4(t) y(t) u1(t) [input=true] u2(t) [input=true]\nD = Differential(t)\n\neqs = [\n    D(x1) ~ -b * x1 + 1/(c + x4),\n    D(x2) ~ a * x1 - beta * x2 - u1,\n    D(x3) ~ g * x2 - delta * x3 + u2,\n    D(x4) ~ sigma * x4 * (g * x2 - delta * x3)/x3\n]\nmeasured_quantities = [y~x1+x2, y2~x2]\n\n# check only 2 parameters\nto_check = [b, c]\n\node = ODESystem(eqs, t, name=:GoodwinOsc)\n\nglobal_id = assess_identifiability(ode, measured_quantities=measured_quantities, funcs_to_check=to_check, p=0.9)\n            # Dict{Num, Symbol} with 2 entries:\n            #   b => :globally\n            #   c => :globally","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"Both parameters b, c are globally identifiable with probability 0.9 in this case.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"[1]: R. Munoz-Tamayo, L. Puillet, J.B. Daniel, D. Sauvant, O. Martin, M. Taghipoor, P. Blavy Review: To be or not to be an identifiable model. Is this a relevant question in animal science modelling?, Animal, Vol 12 (4), 701-712, 2018. The model is the ODE system (3) in Supplementary Material 2, initial conditions are assumed to be unknown.","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"[2]: Moate P.J., Boston R.C., Jenkins T.C. and Lean I.J., Kinetics of Ruminal Lipolysis of Triacylglycerol and Biohydrogenationof Long-Chain Fatty Acids: New Insights from Old Data, Journal of Dairy Science 91, 731–742, 2008","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"[3]: Goodwin, B.C. Oscillatory behavior in enzymatic control processes, Advances in Enzyme Regulation, Vol 3 (C), 425-437, 1965","category":"page"},{"location":"modules/ModelingToolkit/tutorials/parameter_identifiability/","page":"Parameter Identifiability in ODE Models","title":"Parameter Identifiability in ODE Models","text":"[4]: Dong, R., Goodbrake, C., Harrington, H. A., & Pogudin, G. Computing input-output projections of dynamical models with applications to structural identifiability. arXiv preprint arXiv:2111.00991.","category":"page"},{"location":"highlevels/developer_documentation/#Developer-Documentation","page":"Developer Documentation","title":"Developer Documentation","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"For uniformity and clarity, the SciML Open Source Software Organization has many well-defined rules and practices for its development. However, we stress one important principle:","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"Do not be detered from contributing if you think you do not know everything. No one knows everything. These rules and styles are designed for iterative contributions. Open pull requests and contribute what you can with what you know, and the maintainers will help you learn and do the rest!","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"If you need any help contributing, please feel welcome joining our community channels.","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"The #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nJuliaDiffEq on Gitter\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"We welcome everybody.","category":"page"},{"location":"highlevels/developer_documentation/#Getting-Started-With-Contributing-to-SciML","page":"Developer Documentation","title":"Getting Started With Contributing to SciML","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"To get started contributing to SciML, check out the following resources:","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"Developing Julia Packages\nGetting Started with Julia (for Experienced Programmers)","category":"page"},{"location":"highlevels/developer_documentation/#SciMLStyle:-The-SciML-Style-Guide-for-Julia","page":"Developer Documentation","title":"SciMLStyle: The SciML Style Guide for Julia","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"This is a style guide for how to program in Julia for SciML contributions. It describes everything one needs to know, from preferred naming schemes of functions to fundamental dogmas for designing traits. We stress that this style guide is meant to be comprehensive for the sake of designing automatic formatters and teaching desired rules, but complete knoweldge and adherance to the style guide is not required for contributions!","category":"page"},{"location":"highlevels/developer_documentation/#COLPRAC:-Contributor's-Guide-on-Collaborative-Practices-for-Community-Packages","page":"Developer Documentation","title":"COLPRAC: Contributor's Guide on Collaborative Practices for Community Packages","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"What are the rules for when PRs should be merged? What are the rules for whether to tag a major, minor, or patch release? All of these development rules are defined in COLPRAC.","category":"page"},{"location":"highlevels/developer_documentation/#DiffEq-Developer-Documentation","page":"Developer Documentation","title":"DiffEq Developer Documentation","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"There are many solver libraries which share similar internals, such as OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl. This section of the documentation describes the internal systems of these packages and how they are used to quickly write efficient solvers.","category":"page"},{"location":"highlevels/developer_documentation/#Third-Party-Libraries-to-Note","page":"Developer Documentation","title":"Third Party Libraries to Note","text":"","category":"section"},{"location":"highlevels/developer_documentation/#Documenter.jl","page":"Developer Documentation","title":"Documenter.jl","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"Documenter.jl is the documentation generation library that the SciML organization uses, and thus its documentation is the documentation of the documentation.","category":"page"},{"location":"highlevels/developer_documentation/#JuliaFormatter.jl","page":"Developer Documentation","title":"JuliaFormatter.jl","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"JuliaFormatter.jl is the formatter used by the SciML organization to enforce the SciML Style. Setting style = \"sciml\" in a .JuliaFormatter.toml file of a repo and using the standard FormatCheck.yml as part of continuous integration makes JuliaFormatter check for SciML Style compliance on pull requests.","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"To run JuliaFormatter in a SciML repository, do:","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"using JuliaFomatter, DevedPackage\nJuliaFormatter.format(pkgdir(DevedPackage))","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"which will reformat the code according to the SciML Style.","category":"page"},{"location":"highlevels/developer_documentation/#Github-Actions-Continuous-Integrations","page":"Developer Documentation","title":"Github Actions Continuous Integrations","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"The SciML Organization uses continuous integration testing to always ensure tests are passing when merging pull requests. The organization uses the Github Actions supplied by Julia Actions to accomplish this. Common continuous integration scripts are:","category":"page"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"CI.yml, the standard CI script\nDownstream.yml, used to specify packages for downstream testing. This will make packages which depend on the current package also be tested to ensure that \"non-breaking changes\" do not actually break other packages.\nDocumentation.yml, used to run the documentation automatic generation with Documenter.jl\nFormatCheck.yml, used to check JuliaFormatter SciML Style compliance","category":"page"},{"location":"highlevels/developer_documentation/#CompatHelper","page":"Developer Documentation","title":"CompatHelper","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"CompatHelper is used to automatically create pull requests whenever a dependent package is upper bounded. The results of CompatHelper PRs should be checked to ensure that the latest version of the dependencies are grabbed for the test process. After successful CompatHelper PRs, i.e. if the increase of the upper bound did not cause a break to the tests, a new version tag should follow. It is setup by adding the CompatHelper.yml Github action.","category":"page"},{"location":"highlevels/developer_documentation/#TagBot","page":"Developer Documentation","title":"TagBot","text":"","category":"section"},{"location":"highlevels/developer_documentation/","page":"Developer Documentation","title":"Developer Documentation","text":"TagBot automatically creates tags in the Github repository whenever a package is registered to the Julia General repository. It is setup by adding the TagBot.yml Github action.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#integrator","page":"Integrator Interface","title":"Integrator Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator interface gives one the ability to interactively step through the numerical solving of a differential equation. Through this interface, one can easily monitor results, modify the problem during a run, and dynamically continue solving as one sees fit.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Initialization-and-Stepping","page":"Integrator Interface","title":"Initialization and Stepping","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"To initialize an integrator, use the syntax:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,alg;kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The keyword args which are accepted are the same as the solver options used by solve and the returned value is an integrator which satisfies typeof(integrator)<:DEIntegrator. One can manually choose to step via the step! command:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"step!(integrator)","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"which will take one successful step. Additionally:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"step!(integrator,dt[,stop_at_tdt=false])","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"passing a dt will make the integrator keep stepping until integrator.t+dt, and setting stop_at_tdt=true will add a tstop to force it to step to integrator.t+dt","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"To check whether or not the integration step was successful, you can call check_error(integrator) which returns one of the return codes.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"This type also implements an iterator interface, so one can step n times (or to the last tstop) using the take iterator:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for i in take(integrator,n) end","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"One can loop to the end by using solve!(integrator) or using the iterator interface:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for i in integrator end","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"In addition, some helper iterators are provided to help monitor the solution. For example, the tuples iterator lets you view the values:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for (u,t) in tuples(integrator)\n  @show u,t\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"and the intervals iterator lets you view the full interval:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for (uprev,tprev,u,t) in intervals(integrator)\n  @show tprev,t\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Additionally, you can make the iterator return specific time points via the TimeChoiceIterator:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"ts = range(0, stop=1, length=11)\nfor (u,t) in TimeChoiceIterator(integrator,ts)\n  @show u,t\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Lastly, one can dynamically control the \"endpoint\". The initialization simply makes prob.tspan[2] the last value of tstop, and many of the iterators are made to stop at the final tstop value. However, step! will always take a step, and one can dynamically add new values of tstops by modifiying the variable in the options field: add_tstop!(integrator,new_t).","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Finally, to solve to the last tstop, call solve!(integrator). Doing init and then solve! is equivalent to solve.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"SciMLBase.step!\nSciMLBase.check_error\nSciMLBase.check_error!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.step!","page":"Integrator Interface","title":"SciMLBase.step!","text":"step!(integ::DEIntegrator [, dt [, stop_at_tdt]])\n\nPerform one (successful) step on the integrator.\n\nAlternative, if a dt is given, then step! the integrator until there is a temporal difference ≥ dt in integ.t.  When true is passed to the optional third argument, the integrator advances exactly dt.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.check_error","page":"Integrator Interface","title":"SciMLBase.check_error","text":"check_error(integrator)\n\nCheck state of integrator and return one of the Return Codes\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.check_error!","page":"Integrator Interface","title":"SciMLBase.check_error!","text":"check_error!(integrator)\n\nSame as check_error but also set solution's return code (integrator.sol.retcode) and run postamble!.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Handing-Integrators","page":"Integrator Interface","title":"Handing Integrators","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator<:DEIntegrator type holds all of the information for the intermediate solution of the differential equation. Useful fields are:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"t - time of the proposed step\nu - value at the proposed step\np - user-provided data\nopts - common solver options\nalg - the algorithm associated with the solution\nf - the function being solved\nsol - the current state of the solution\ntprev - the last timepoint\nuprev - the value at the last timepoint\ntdir - the sign for the direction of time","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The p is the data which is provided by the user as a keyword arg in init. opts holds all of the common solver options, and can be mutated to change the solver characteristics. For example, to modify the absolute tolerance for the future timesteps, one can do:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator.opts.abstol = 1e-9","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The sol field holds the current solution. This current solution includes the interpolation function if available, and thus integrator.sol(t) lets one interpolate efficiently over the whole current solution. Additionally, a a \"current interval interpolation function\" is provided on the integrator type via integrator(t,deriv::Type=Val{0};idxs=nothing,continuity=:left). This uses only the solver information from the interval [tprev,t] to compute the interpolation, and is allowed to extrapolate beyond that interval.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Note-about-mutating","page":"Integrator Interface","title":"Note about mutating","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Be cautious: one should not directly mutate the t and u fields of the integrator. Doing so will destroy the accuracy of the interpolator and can harm certain algorithms. Instead if one wants to introduce discontinuous changes, one should use the callbacks. Modifications within a callback affect! surrounded by saves provides an error-free handling of the discontinuity.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"As low-level alternative to the callbacks, one can use set_t!, set_u! and set_ut! to mutate integrator states.  Note that certain integrators may not have efficient ways to modify u and t.  In such case, set_*! are as inefficient as reinit!.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"SciMLBase.set_t!\nSciMLBase.set_u!\nSciMLBase.set_ut!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.set_t!","page":"Integrator Interface","title":"SciMLBase.set_t!","text":"set_t!(integrator::DEIntegrator, t)\n\nSet current time point of the integrator to t.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.set_u!","page":"Integrator Interface","title":"SciMLBase.set_u!","text":"set_u!(integrator::DEIntegrator, u)\n\nSet current state of the integrator to u.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.set_ut!","page":"Integrator Interface","title":"SciMLBase.set_ut!","text":"set_ut!(integrator::DEIntegrator, u, t)\n\nSet current state of the integrator to u and t\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Integrator-vs-Solution","page":"Integrator Interface","title":"Integrator vs Solution","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator and the solution have very different actions because they have very different meanings. The typeof(sol) <: DESolution type is a type with history: it stores all of the (requested) timepoints and interpolates/acts using the values closest in time. On the other hand, the typeof(integrator)<:DEIntegrator type is a local object. It only knows the times of the interval it currently spans, the current caches and values, and the current state of the solver (the current options, tolerances, etc.). These serve very different purposes:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator's interpolation can extrapolate, both forward and backward in in time. This is used to estimate events and is internally used for predictions.\nThe integrator is fully mutable upon iteration. This means that every time an iterator affect is used, it will take timesteps from the current time. This means that first(integrator)!=first(integrator) since the integrator will step once to evaluate the left and then step once more (not backtracking). This allows the iterator to keep dynamically stepping, though one should note that it may violate some immutablity assumptions commonly made about iterators.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"If one wants the solution object, then one can find it in integrator.sol.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Function-Interface","page":"Integrator Interface","title":"Function Interface","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"In addition to the type interface, a function interface is provided which allows for safe modifications of the integrator type, and allows for uniform usage throughout the ecosystem (for packages/algorithms which implement the functions). The following functions make up the interface:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Saving-Controls","page":"Integrator Interface","title":"Saving Controls","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"savevalues!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.savevalues!","page":"Integrator Interface","title":"SciMLBase.savevalues!","text":"savevalues!(integrator::DEIntegrator,\n  force_save=false) -> Tuple{Bool, Bool}\n\nTry to save the state and time variables at the current time point, or the saveat point by using interpolation when appropriate. It returns a tuple that is (saved, savedexactly). If savevalues! saved value, then saved is true, and if savevalues! saved at the current time point, then savedexactly is true.\n\nThe saving priority/order is as follows:\n\nsave_on\nsaveat\nforce_save\nsave_everystep\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Caches","page":"Integrator Interface","title":"Caches","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"get_tmp_cache\nfull_cache","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.get_tmp_cache","page":"Integrator Interface","title":"SciMLBase.get_tmp_cache","text":"get_tmp_cache(i::DEIntegrator)\n\nReturns a tuple of internal cache vectors which are safe to use as temporary arrays. This should be used for integrator interface and callbacks which need arrays to write into in order to be non-allocating. The length of the tuple is dependent on the method.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.full_cache","page":"Integrator Interface","title":"SciMLBase.full_cache","text":"full_cache(i::DEIntegrator)\n\nReturns an iterator over the cache arrays of the method. This can be used to change internal values as needed.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#stepping_controls","page":"Integrator Interface","title":"Stepping Controls","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"u_modified!\nget_proposed_dt\nset_proposed_dt!\nterminate!\nchange_t_via_interpolation!\nadd_tstop!\nhas_tstop\nfirst_tstop\npop_tstop!\nadd_saveat!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.u_modified!","page":"Integrator Interface","title":"SciMLBase.u_modified!","text":"u_modified!(i::DEIntegrator,bool)\n\nSets bool which states whether a change to u occurred, allowing the solver to handle the discontinuity. By default, this is assumed to be true if a callback is used. This will result in the re-calculation of the derivative at t+dt, which is not necessary if the algorithm is FSAL and u does not experience a discontinuous change at the end of the interval. Thus if u is unmodified in a callback, a single call to the derivative calculation can be eliminated by u_modified!(integrator,false).\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.get_proposed_dt","page":"Integrator Interface","title":"SciMLBase.get_proposed_dt","text":"get_proposed_dt(i::DEIntegrator)\n\nGets the proposed dt for the next timestep.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.set_proposed_dt!","page":"Integrator Interface","title":"SciMLBase.set_proposed_dt!","text":"set_proposed_dt(i::DEIntegrator,dt)\nset_proposed_dt(i::DEIntegrator,i2::DEIntegrator)\n\nSets the proposed dt for the next timestep. If second argument isa DEIntegrator then it sets the timestepping of first argument to match that of second one. Note that due to PI control and step acceleration this is more than matching the factors in most cases.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.terminate!","page":"Integrator Interface","title":"SciMLBase.terminate!","text":"terminate!(i::DEIntegrator[, retcode = :Terminated])\n\nTerminates the integrator by emptying tstops. This can be used in events and callbacks to immediately end the solution process.  Optionally, retcode may be specified (see: Return Codes (RetCodes)).\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.change_t_via_interpolation!","page":"Integrator Interface","title":"SciMLBase.change_t_via_interpolation!","text":"change_t_via_interpolation!(integrator::DEIntegrator,t,modify_save_endpoint=Val{false})\n\nModifies the current t and changes all of the corresponding values using the local interpolation. If the current solution has already been saved, one can provide the optional value modify_save_endpoint to also modify the endpoint of sol in the same manner.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.add_tstop!","page":"Integrator Interface","title":"SciMLBase.add_tstop!","text":"add_tstop!(i::DEIntegrator,t)\n\nAdds a tstop at time t.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.has_tstop","page":"Integrator Interface","title":"SciMLBase.has_tstop","text":"has_tstop(i::DEIntegrator)\n\nChecks if integrator has any stopping times defined.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.first_tstop","page":"Integrator Interface","title":"SciMLBase.first_tstop","text":"first_tstop(i::DEIntegrator)\n\nGets the first stopping time of the integrator.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.pop_tstop!","page":"Integrator Interface","title":"SciMLBase.pop_tstop!","text":"pop_tstop!(i::DEIntegrator)\n\nPops the last stopping time from the integrator.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.add_saveat!","page":"Integrator Interface","title":"SciMLBase.add_saveat!","text":"add_saveat!(i::DEIntegrator,t)\n\nAdds a saveat time point at t.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Resizing","page":"Integrator Interface","title":"Resizing","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"resize!\ndeleteat!\naddat!\nresize_non_user_cache!\ndeleteat_non_user_cache!\naddat_non_user_cache!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Base.resize!","page":"Integrator Interface","title":"Base.resize!","text":"resize!(integrator::DEIntegrator,k::Int)\n\nResizes the DE to a size k. This chops off the end of the array, or adds blank values at the end, depending on whether k > length(integrator.u).\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Base.deleteat!","page":"Integrator Interface","title":"Base.deleteat!","text":"deleteat!(integrator::DEIntegrator,idxs)\n\nShrinks the ODE by deleting the idxs components.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.addat!","page":"Integrator Interface","title":"SciMLBase.addat!","text":"addat!(integrator::DEIntegrator,idxs,val)\n\nGrows the ODE by adding the idxs components. Must be contiguous indices.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.resize_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.resize_non_user_cache!","text":"resize_non_user_cache!(integrator::DEIntegrator,k::Int)\n\nResizes the non-user facing caches to be compatible with a DE of size k. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, resize! simply resizes full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.deleteat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.deleteat_non_user_cache!","text":"deleteat_non_user_cache!(integrator::DEIntegrator,idxs)\n\ndeleteat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, deleteat! simply deleteat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.addat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.addat_non_user_cache!","text":"addat_non_user_cache!(i::DEIntegrator,idxs)\n\naddat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, addat! simply addat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Reinitialization","page":"Integrator Interface","title":"Reinitialization","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"reinit!\nauto_dt_reset!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.reinit!","page":"Integrator Interface","title":"SciMLBase.reinit!","text":"reinit!(solver, prob)\n\nReinitialize solver to the original starting conditions\n\n\n\n\n\nreinit!(integrator::DEIntegrator,args...; kwargs...)\n\nThe reinit function lets you restart the integration at a new value.\n\nArguments\n\nu0: Value of u to start at. Default value is integrator.sol.prob.u0\n\nKeyword Arguments\n\nt0: Starting timepoint. Default value is integrator.sol.prob.tspan[1]\ntf: Ending timepoint. Default value is integrator.sol.prob.tspan[2]\nerase_sol=true: Whether to start with no other values in the solution, or keep the previous solution.\ntstops, d_discontinuities, & saveat: Cache where these are stored. Default is the original cache.\nreset_dt: Set whether to reset the current value of dt using the automatic dt determination algorithm. Default is (integrator.dtcache == zero(integrator.dt)) && integrator.opts.adaptive\nreinit_callbacks: Set whether to run the callback initializations again (and initialize_save is for that). Default is true.\nreinit_cache: Set whether to re-run the cache initialization function (i.e. resetting FSAL, not allocating vectors) which should usually be true for correctness. Default is true.\n\nAdditionally, once can access auto_dt_reset! which will run the auto dt initialization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.auto_dt_reset!","page":"Integrator Interface","title":"SciMLBase.auto_dt_reset!","text":"auto_dt_reset!(integrator::DEIntegrator)\n\nRun the auto dt initialization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#Misc","page":"Integrator Interface","title":"Misc","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"get_du\nget_du!","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.get_du","page":"Integrator Interface","title":"SciMLBase.get_du","text":"get_du(i::DEIntegrator)\n\nReturns the derivative at t.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/#SciMLBase.get_du!","page":"Integrator Interface","title":"SciMLBase.get_du!","text":"get_du!(out,i::DEIntegrator)\n\nWrite the current derivative at t into out.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"warning: Warning\nNote that not all of these functions will be implemented for every algorithm. Some have hard limitations. For example, Sundials.jl cannot resize problems. When a function is not limited, an error will be thrown.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Additional-Options","page":"Integrator Interface","title":"Additional Options","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The following options can additionally be specified in init (or be mutated in the opts) for further control of the integrator:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"advance_to_tstop: This makes step! continue to the next value in tstop.\nstop_at_next_tstop: This forces the iterators to stop at the next value of tstop.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"For example, if one wants to iterate but only stop at specific values, one can choose:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5],advance_to_tstop=true)\nfor (u,t) in tuples(integrator)\n  @test t ∈ [0.5,1.0]\nend","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"which will only enter the loop body at the values in tstops (here, prob.tspan[2]==1.0 and thus there are two values of tstops which are hit). Addtionally, one can solve! only to 0.5 via:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5])\nintegrator.opts.stop_at_next_tstop = true\nsolve!(integrator)","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/#Plot-Recipe","page":"Integrator Interface","title":"Plot Recipe","text":"","category":"section"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Like the DESolution type, a plot recipe is provided for the DEIntegrator type. Since the DEIntegrator type is a local state type on the current interval, plot(integrator) returns the solution on the current interval. The same options for the plot recipe are provided as for sol, meaning one can choose variables via the vars keyword argument, or change the plotdensity / turn on/off denseplot.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Additionally, since the integrator is an iterator, this can be used in the Plots.jl animate command to iteratively build an animation of the solution while solving the differential equation.","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"For an example of manually chaining together the iterator interface and plotting, one should try the following:","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"using DifferentialEquations, DiffEqProblemLibrary, Plots\n\n# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\n\nusing Plots\nintegrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5])\npyplot(show=true)\nplot(integrator)\nfor i in integrator\n  display(plot!(integrator,vars=(0,1),legend=false))\nend\nstep!(integrator); plot!(integrator,vars=(0,1),legend=false)\nsavefig(\"iteratorplot.png\")","category":"page"},{"location":"modules/DiffEqDocs/basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"(Image: Iterator Plot)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#linearsystemsolvers","page":"Linear System Solvers","title":"Linear System Solvers","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"solve(prob::LinearProlem,alg;kwargs)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Solves for Au=b in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#Recommended-Methods","page":"Linear System Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The default algorithm nothing is good for choosing an algorithm that will work, but one may need to change this to receive more performance or precision. If more precision is necessary, QRFactorization() and SVDFactorization() are the best choices, with SVD being the slowest but most precise.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For efficiency, RFLUFactorization is the fastest for dense LU-factorizations. FastLUFactorization will be faster than LUFactorization which is the Base.LinearAlgebra (\\ default) implementation of LU factorization. SimpleLUFactorization will be fast on very small matrices.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"For sparse LU-factorizations, KLUFactorization if there is less structure to the sparsity pattern and UMFPACKFactorization if there is more structure. Pardiso.jl's methods are also known to be very efficient sparse linear solvers.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"As sparse matrices get larger, iterative solvers tend to get more efficient than factorization methods if a lower tolerance of the solution is required.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Krylov.jl generally outperforms IterativeSolvers.jl and KrylovKit.jl, and is compatible with CPUs and GPUs, and thus is the generally preferred form for Krylov methods.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Finally, a user can pass a custom function for handling the linear solve using LinearSolveFunction() if existing solvers are not optimally suited for their application. The interface is detailed here","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#Full-List-of-Methods","page":"Linear System Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/#RecursiveFactorization.jl","page":"Linear System Solvers","title":"RecursiveFactorization.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"RFLUFactorization(): a fast pure Julia LU-factorization implementation using RecursiveFactorization.jl. This is by far the fastest LU-factorization implementation, usually outperforming OpenBLAS and MKL, but currently optimized only for Base Array with Float32 or Float64.  Additional optimization for  complex matrices is in the works.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#Base.LinearAlgebra","page":"Linear System Solvers","title":"Base.LinearAlgebra","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"These overloads tend to work for many array types, such as CuArrays for GPU-accelerated solving, using the overloads provided by the respective packages. Given that this can be customized per-package, details given below describe a subset of important arrays (Matrix, SparseMatrixCSC, CuMatrix, etc.)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LUFactorization(pivot=LinearAlgebra.RowMaximum()): Julia's built in lu.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices this will use UMFPACK from SuiteSparse. Note that this will not cache the symbolic factorization.\nOn CuMatrix it will use a CUDA-accelerated LU from CuSolver.\nOn BandedMatrix and BlockBandedMatrix it will use a banded LU.\nQRFactorization(pivot=LinearAlgebra.NoPivot(),blocksize=16): Julia's built in qr.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nOn sparse matrices this will use SPQR from SuiteSparse\nOn CuMatrix it will use a CUDA-accelerated QR from CuSolver.\nOn BandedMatrix and BlockBandedMatrix it will use a banded QR.\nSVDFactorization(full=false,alg=LinearAlgebra.DivideAndConquer()): Julia's built in svd.\nOn dense matrices this uses the current BLAS implementation of the user's computer which by default is OpenBLAS but will use MKL if the user does using MKL in their system.\nGenericFactorization(fact_alg): Constructs a linear solver from a generic factorization algorithm fact_alg which complies with the Base.LinearAlgebra factorization API. Quoting from Base:\nIf A is upper or lower triangular (or diagonal), no factorization of A is required and the system is solved with either forward or backward substitution. For non-triangular square matrices, an LU factorization is used. For rectangular A the result is the minimum-norm least squares solution computed by a pivoted QR factorization of A and a rank estimate of A based on the R factor. When A is sparse, a similar polyalgorithm is used. For indefinite matrices, the LDLt factorization does not use pivoting during the numerical factorization and therefore the procedure can fail even for invertible matrices.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#LinearSolve.jl","page":"Linear System Solvers","title":"LinearSolve.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"LinearSolve.jl contains some linear solvers built in.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"SimpleLUFactorization: a simple LU-factorization implementation without BLAS. Fast for small matrices.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#FastLapackInterface.jl","page":"Linear System Solvers","title":"FastLapackInterface.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLapackInterface.jl is a package that allows for a lower-level interface to the LAPACK calls to allow for preallocating workspaces to decrease the overhead of the wrappers. LinearSolve.jl provides a wrapper to these routines in a way where an initialized solver has a non-allocating LU factorization. In theory, this post-initialized solve should always be faster than the Base.LinearAlgebra version.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"FastLUFactorization the FastLapackInterface version of the LU factorizaiton. Notably, this version does not allow for choice of pivoting method.\nFastQRFactorization(pivot=NoPivot(),blocksize=32), the FastLapackInterface version of the QR factorizaiton.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#SuiteSparse.jl","page":"Linear System Solvers","title":"SuiteSparse.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"By default, the SuiteSparse.jl are implemented for efficiency by caching the symbolic factorization. I.e. if set_A is used, it is expected that the new A has the same sparsity pattern as the previous A. If this algorithm is to be used in a context where that assumption does not hold, set reuse_symbolic=false.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KLUFactorization(;reuse_symbolic=true): A fast sparse LU-factorization which specializes on sparsity patterns with \"less structure\".\nUMFPACKFactorization(;reuse_symbolic=true): A fast sparse multithreaded LU-factorization which specializes on sparsity patterns that are more structured.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#Pardiso.jl","page":"Linear System Solvers","title":"Pardiso.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package LinearSolvePardiso.jl","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The following algorithms are pre-specified:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKLPardisoFactorize(;kwargs...): A sparse factorization method.\nMKLPardisoIterate(;kwargs...): A mixed factorization+iterative method.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Those algorithms are defined via:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"MKLPardisoFactorize(;kwargs...) = PardisoJL(;fact_phase=Pardiso.NUM_FACT,\n                                             solve_phase=Pardiso.SOLVE_ITERATIVE_REFINE,\n                                             kwargs...)\nMKLPardisoIterate(;kwargs...) = PardisoJL(;solve_phase=Pardiso.NUM_FACT_SOLVE_REFINE,\n                                           kwargs...)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The full set of keyword arguments for PardisoJL are:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Base.@kwdef struct PardisoJL <: SciMLLinearSolveAlgorithm\n    nprocs::Union{Int, Nothing} = nothing\n    solver_type::Union{Int, Pardiso.Solver, Nothing} = nothing\n    matrix_type::Union{Int, Pardiso.MatrixType, Nothing} = nothing\n    fact_phase::Union{Int, Pardiso.Phase, Nothing} = nothing\n    solve_phase::Union{Int, Pardiso.Phase, Nothing} = nothing\n    release_phase::Union{Int, Nothing} = nothing\n    iparm::Union{Vector{Tuple{Int,Int}}, Nothing} = nothing\n    dparm::Union{Vector{Tuple{Int,Int}}, Nothing} = nothing\nend","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#CUDA.jl","page":"Linear System Solvers","title":"CUDA.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"Note that CuArrays are supported by GenericFactorization in the \"normal\" way. The following are non-standard GPU factorization routines.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"note: Note\nUsing this solver requires adding the package LinearSolveCUDA.jl","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"CudaOffloadFactorization(): An offloading technique used to GPU-accelerate CPU-based computations. Requires a sufficiently large A to overcome the data transfer costs.","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#IterativeSolvers.jl","page":"Linear System Solvers","title":"IterativeSolvers.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"IterativeSolversJL_CG(args...;kwargs...): A generic CG implementation\nIterativeSolversJL_GMRES(args...;kwargs...): A generic GMRES implementation\nIterativeSolversJL_BICGSTAB(args...;kwargs...): A generic BICGSTAB implementation\nIterativeSolversJL_MINRES(args...;kwargs...): A generic MINRES implementation","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"IterativeSolversJL(args...;\n                   generate_iterator = IterativeSolvers.gmres_iterable!,\n                   Pl=nothing, Pr=nothing,\n                   gmres_restart=0, kwargs...)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#Krylov.jl","page":"Linear System Solvers","title":"Krylov.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovJL_CG(args...;kwargs...): A generic CG implementation\nKrylovJL_GMRES(args...;kwargs...): A generic GMRES implementation\nKrylovJL_BICGSTAB(args...;kwargs...): A generic BICGSTAB implementation\nKrylovJL_MINRES(args...;kwargs...): A generic MINRES implementation","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovJL(args...; KrylovAlg = Krylov.gmres!,\n                  Pl=nothing, Pr=nothing,\n                  gmres_restart=0, window=0,\n                  kwargs...)","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/#KrylovKit.jl","page":"Linear System Solvers","title":"KrylovKit.jl","text":"","category":"section"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"KrylovKitJL_CG(args...;kwargs...): A generic CG implementation\nKrylovKitJL_GMRES(args...;kwargs...): A generic GMRES implementation","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"The general algorithm is:","category":"page"},{"location":"modules/LinearSolve/solvers/solvers/","page":"Linear System Solvers","title":"Linear System Solvers","text":"function KrylovKitJL(args...;\n                     KrylovAlg = KrylovKit.GMRES, gmres_restart = 0,\n                     kwargs...)","category":"page"},{"location":"modules/ModelingToolkit/internals/#Internal-Details","page":"Internal Details","title":"Internal Details","text":"","category":"section"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"This is a page for detailing some of the inner workings to help future contributors to the library.","category":"page"},{"location":"modules/ModelingToolkit/internals/#Observables-and-Variable-Elimination","page":"Internal Details","title":"Observables and Variable Elimination","text":"","category":"section"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"In the variable \"elimination\" algorithms, what is actually done is that variables are removed from being states and equations are moved into the observed category of the system. The observed equations are explicit algebraic equations which are then substituted out to completely eliminate these variables from the other equations, allowing the system to act as though these variables no longer exist.","category":"page"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"However, as a user may have wanted to interact with such variables, for example, plotting their output, these relationships are stored and are then used to generate the observed equation found in the SciMLFunction interface, so that sol[x] lazily reconstructs the observed variable when necessary. In this sense, there is an equivalence between observables and the variable elimination system.","category":"page"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"The procedure for variable elimination inside structural_simplify is","category":"page"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"ModelingToolkit.initialize_system_structure.\nModelingToolkit.alias_elimination. This step moves equations into observed(sys).\nModelingToolkit.dae_index_lowering by means of pantelides! (if the system is an ODESystem).\nModelingToolkit.tearing.","category":"page"},{"location":"modules/ModelingToolkit/internals/#Preparing-a-system-for-simulation","page":"Internal Details","title":"Preparing a system for simulation","text":"","category":"section"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"Before a simulation or optimization can be performed, the symbolic equations stored in an AbstractSystem must be converted into executable code. This step is typically occurs after the simplification explained above, and is performed when an instance of a SciMLBase.SciMLProblem, such as a ODEProblem, is constructed. The call chain typically looks like this, with the function names in the case of an ODESystem indicated in parenthesis","category":"page"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"Problem constructor (ODEProblem)\nBuild an DEFunction (process_DEProblem -> ODEFunction\nWrite actual executable code (generate_function)","category":"page"},{"location":"modules/ModelingToolkit/internals/","page":"Internal Details","title":"Internal Details","text":"Apart from generate_function, which generates the dynamics function, ODEFunction also builds functions for observed equations (build_explicit_observed_function) and jacobians (generate_jacobian) etc. These are all stored in the ODEFunction.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#Abstract-Noise-Processes","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"In addition to the NoiseProcess type, more general AbstractNoiseProcesses are defined. The NoiseGrid allows you to define a noise process from a set of pre-calculated points (the \"normal\" way). The NoiseApproximation allows you to define a new noise process as the solution to some stochastic differential equation. While these methods are only approximate, they are more general and allow the user to easily define their own colored noise to use in simulations.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"The NoiseWrapper allows one to wrap a NoiseProcess from a previous simulation to re-use it in a new simulation in a way that follows the same stochastic trajectory (even if different points are hit, for example solving with a smaller dt) in a distributionally-exact manner. It is demonstrated how the NoiseWrapper can be used to wrap the NoiseProcess of one SDE/RODE solution in order to re-use the same noise process in another simulation.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"The VirtualBrownianTree allows one to trade speed for O(1) memory usage. Instead of storing Brownian motion increments, the VirtualBrownianTree samples recursively from the midpoint tmid of Brownian bridges, using a splittable PRNG. The recursion terminates when the query time agrees within some tolerance with tmid or when the maximum depth of the tree is reached.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"Lastly, the NoiseFunction allows you to use any function of time as the noise process. Together, this functionality allows you to define any colored noise process and use this efficiently and accurately in your simulations.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#The-Standard-AbstractNoiseProcess","page":"Abstract Noise Processes","title":"The Standard AbstractNoiseProcess","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"NoiseProcess","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.NoiseProcess","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.NoiseProcess","text":"mutable struct NoiseProcess{T,N,Tt,T2,T3,ZType,F,F2,inplace,S1,S2,RSWM,C,RNGType} <: {T,N,Vector{T2},inplace}\n\nA NoiseProcess is a type defined as:\n\nNoiseProcess(t0,W0,Z0,dist,bridge;\n             iip=SciMLBase.isinplace(dist,3),\n             rswm = RSWM(),save_everystep=true,\n             rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n             reset = true, reseed = true)\n\nt0 is the first timepoint\nW0 is the first value of the process.\nZ0 is the first value of the pseudo-process. This is necessary for higher order algorithms. If it's not needed, set to nothing.\ndist the distribution for the steps over time.\nbridge the bridging distribution. Optional, but required for adaptivity and interpolating at new values.\nsave_everystep whether to save every step of the Brownian timeseries.\nrng the local RNG used for generating the random numbers.\nreset whether to reset the process with each solve.\nreseed whether to reseed the process with each solve.\n\nThe signature for the dist is\n\ndist!(rand_vec,W,dt,rng)\n\nfor inplace functions, and\n\nrand_vec = dist(W,dt,rng)\n\notherwise. The signature for bridge is\n\nbridge!(rand_vec,W,W0,Wh,q,h,rng)\n\nand the out of place syntax is\n\nrand_vec = bridge!(W,W0,Wh,q,h,rng)\n\nHere, W is the noise process, W0 is the left side of the current interval, Wh is the right side of the current interval, h is the interval length, and q is the proportion from the left where the interpolation is occuring.\n\nDirect Construction Example\n\nThe easiest way to show how to directly construct a NoiseProcess is by example. Here we will show how to directly construct a NoiseProcess which generates Gaussian white noise.\n\nThis is the noise process which uses randn!. A special dispatch is added for complex numbers for (randn()+im*randn())/sqrt(2). This function is DiffEqNoiseProcess.wiener_randn (or with ! respectively).\n\nThe first function that must be defined is the noise distribution. This is how to generate W(t+dt) given that we know W(x) for xt₀t. For Gaussian white noise, we know that\n\nW(dt)  N(0dt)\n\nfor W(0)=0 which defines the stepping distribution. Thus its noise distribution function is:\n\n@inline function WHITE_NOISE_DIST(W,dt,rng)\n  if typeof(W.dW) <: AbstractArray && !(typeof(W.dW) <: SArray)\n    return @fastmath sqrt(abs(dt))*wiener_randn(rng,W.dW)\n  else\n    return @fastmath sqrt(abs(dt))*wiener_randn(rng,typeof(W.dW))\n  end\nend\n\nfor the out of place versions, and for the inplace versions\n\nfunction INPLACE_WHITE_NOISE_DIST(rand_vec,W,dt,rng)\n  wiener_randn!(rng,rand_vec)\n  sqrtabsdt = @fastmath sqrt(abs(dt))\n  @. rand_vec *= sqrtabsdt\nend\n\nOptionally, we can provide a bridging distribution. This is the distribution of W(qh) for q01 given that we know W(0)=0 and W(h)=Wₕ. For Brownian motion, this is known as the Brownian Bridge, and is well known to have the distribution:\n\nW(qh)  N(qWₕ(1-q)qh)\n\nThus we have the out-of-place and in-place versions as:\n\nfunction WHITE_NOISE_BRIDGE(W,W0,Wh,q,h,rng)\n  if typeof(W.dW) <: AbstractArray\n    return @fastmath sqrt((1-q)*q*abs(h))*wiener_randn(rng,W.dW)+q*Wh\n  else\n    return @fastmath sqrt((1-q)*q*abs(h))*wiener_randn(rng,typeof(W.dW))+q*Wh\n  end\nend\nfunction INPLACE_WHITE_NOISE_BRIDGE(rand_vec,W,W0,Wh,q,h,rng)\n  wiener_randn!(rng,rand_vec)\n  #rand_vec .= sqrt((1.-q).*q.*abs(h)).*rand_vec.+q.*Wh\n  sqrtcoeff = @fastmath sqrt((1-q)*q*abs(h))\n  @. rand_vec = sqrtcoeff*rand_vec+q*Wh\nend\n\nThese functions are then placed in a noise process:\n\nNoiseProcess(t0,W0,Z0,WHITE_NOISE_DIST,WHITE_NOISE_BRIDGE;kwargs)\nNoiseProcess(t0,W0,Z0,INPLACE_WHITE_NOISE_DIST,INPLACE_WHITE_NOISE_BRIDGE;kwargs)\n\nNotice that we can optionally provide an alternative adaptive algorithm for the timestepping rejections. RSWM() defaults to the Rejection Sampling with Memory 3 algorithm (RSwM3).\n\nNote that the standard constructors are simply:\n\nWienerProcess(t0,W0,Z0=nothing) = NoiseProcess(t0,W0,Z0,WHITE_NOISE_DIST,WHITE_NOISE_BRIDGE;kwargs)\nWienerProcess!(t0,W0,Z0=nothing) = NoiseProcess(t0,W0,Z0,INPLACE_WHITE_NOISE_DIST,INPLACE_WHITE_NOISE_BRIDGE;kwargs)\n\nThese will generate a Wiener process, which can be stepped with step!(W,dt), and interpolated as W(t).\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#Alternative-AbstractNoiseProcess-Types","page":"Abstract Noise Processes","title":"Alternative AbstractNoiseProcess Types","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"In addition to the mathematically-defined noise processes above, there exists more generic functionality for building noise processes from other noise processes, from arbitrary functions, from arrays, and from approximations of stochastic differential equations.","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/","page":"Abstract Noise Processes","title":"Abstract Noise Processes","text":"NoiseWrapper\nNoiseFunction\nNoiseGrid\nNoiseApproximation\nVirtualBrownianTree\nSimpleNoiseProcess\nBoxWedgeTail\npCN","category":"page"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.NoiseWrapper","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.NoiseWrapper","text":"mutable struct NoiseWrapper{T,N,Tt,T2,T3,T4,ZType,inplace} <: AbstractNoiseProcess{T,N,Vector{T2},inplace}\n\nThis produces a new noise process from an old one, which will use its interpolation to generate the noise. This allows you to re-use a previous noise process not just with the same timesteps, but also with new (adaptive) timesteps as well. Thus this is very good for doing Multi-level Monte Carlo schemes and strong convergence testing.\n\nConstructor\n\nNoiseWrapper(source::AbstractNoiseProcess{T,N,Vector{T2},inplace};\n                      reset=true,reverse=false,indx=nothing) where {T,N,T2,inplace}\n\nNoiseWrapper Example\n\nIn this example, we will solve an SDE three times:\n\nFirst to generate a noise process\nSecond with the same timesteps to show the values are the same\nThird with half-sized timsteps\n\nFirst we will generate a noise process by solving an SDE:\n\nusing StochasticDiffEq,  DiffEqNoiseProcess\nf1(u, p, t) = 1.01u\ng1(u, p, t) = 1.01u\ndt = 1//2^(4)\nprob1 = SDEProblem(f1,g1,1.0,(0.0,1.0))\nsol1 = solve(prob1,EM(),dt=dt,save_noise = true)\n\nNow we wrap the noise into a NoiseWrapper and solve the same problem:\n\nW2 = NoiseWrapper(sol1.W)\nprob1 = SDEProblem(f1,g1,1.0,(0.0,1.0),noise=W2)\nsol2 = solve(prob1,EM(),dt=dt)\n\nWe can test\n\n@test sol1.u ≈ sol2.u\n\nto see that the values are essentially equal. Now we can use the same process to solve the same trajectory with a smaller dt:\n\nW3 = NoiseWrapper(sol1.W)\nprob2 = SDEProblem(f1,g1,1.0,(0.0,1.0),noise=W3)\n\ndt = 1//2^(5)\nsol3 = solve(prob2,EM(),dt=dt)\n\nWe can plot the results to see what this looks like:\n\nusing Plots\nplot(sol1)\nplot!(sol2)\nplot!(sol3)\n\n(Image: noise_process)\n\nIn this plot, sol2 covers up sol1 because they hit essentially the same values. You can see that sol3 its similar to the others, because it's using the same underlying noise process just sampled much finer.\n\nTo double check, we see that:\n\nplot(sol1.W)\nplot!(sol2.W)\nplot!(sol3.W)\n\n(Image: coupled_wiener)\n\nthe coupled Wiener processes coincide at every other time point, and the intermediate timepoints were calculated according to a Brownian bridge.\n\nAdaptive NoiseWrapper Example\n\nHere we will show that the same noise can be used with the adaptive methods using the NoiseWrapper. SRI and SRIW1 use slightly different error estimators, and thus give slightly different stepping behavior. We can see how they solve the same 2D SDE differently by using the noise wrapper:\n\nprob = SDEProblem(f1,g1,ones(2),(0.0,1.0))\nsol4 = solve(prob,SRI(),abstol=1e-8, save_noise = true)\n\nW2 = NoiseWrapper(sol4.W)\nprob2 = SDEProblem(f1,g1,ones(2),(0.0,1.0),noise=W2)\nsol5 = solve(prob2,SRIW1(),abstol=1e-8)\n\nusing Plots\nplot(sol4)\nplot!(sol5)\n\n(Image: SRI_SRIW1_diff)\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.NoiseFunction","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.NoiseFunction","text":"mutable struct NoiseFunction{T,N,wType,zType,Tt,T2,T3,inplace} <: AbstractNoiseProcess{T,N,nothing,inplace}\n\nThis allows you to use any arbitrary function W(t) as a NoiseProcess. This will use the function lazily, only caching values required to minimize function calls, but not store the entire noise array. This requires an initial time point t0 in the domain of W. A second function is needed if the desired SDE algorithm requires multiple processes.\n\nfunction NoiseFunction{iip}(t0,W,Z=nothing;\n                            noise_prototype=W(nothing,nothing,t0),\n                            reset=true) where iip\n\nAdditionally, one can use an in-place function W(out1,out2,t) for more efficient generation of the arrays for multi-dimensional processes. When the in-place version is used without a dispatch for the out-of-place version, the noise_prototype needs to be set.\n\nNoiseFunction Example\n\nThe NoiseFunction is pretty simple: pass a function. As a silly example, we can use exp as a noise process by doing:\n\nf(t) = exp(t)\nW = NoiseFunction(0.0,f)\n\nIf it's multi-dimensional and an in-place function is used, the noise_prototype must be given. For example:\n\nf(out,t) = (out.=exp(t))\nW = NoiseFunction(0.0,f,noise_prototype=rand(4))\n\nThis allows you to put arbitrarily weird noise into SDEs and RODEs. Have fun.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.NoiseGrid","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.NoiseGrid","text":"A noise grid builds a noise process from arrays of points. For example, you can generate your desired noise process as an array W with timepoints t, and use the constructor:\n\nNoiseGrid(t,W,Z=nothing;reset=true)\n\nto build the associated noise process. This process comes with a linear interpolation of the given points, and thus the grid does not have to match the grid of integration. Thus this can be used for adaptive solutions as well. However, one must make note that the fidelity of the noise process is linked to how fine the noise grid is determined: if the noise grid is sparse on points compared to the integration, then its distributional properties may be slightly perturbed by the linear interpolation. Thus its suggested that the grid size at least approximately match the number of time steps in the integration to ensure accuracy.\n\nFor a one-dimensional process, W should be an AbstractVector of Numbers. For multi-dimensional processes, W should be an AbstractVector of the noise_prototype.\n\nNoiseGrid\n\nIn this example, we will show you how to define your own version of Brownian motion using an array of pre-calculated points. In normal usage you should use WienerProcess instead since this will have distributionally-exact interpolations while the noise grid uses linear interpolations, but this is a nice example of the workflow.\n\nTo define a NoiseGrid you need to have a set of time points and a set of values for the process. Let's define a Brownian motion on (0.0,1.0) with a dt=0.001. To do this,\n\ndt = 0.001\nt = 0:dt:1\nbrownian_values = cumsum([0;[sqrt(dt)*randn() for i in 1:length(t)-1]])\n\nNow we build the NoiseGrid using these values:\n\nW = NoiseGrid(t,brownian_values)\n\nWe can then pass W as the noise argument of an SDEProblem to use it in an SDE.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.NoiseApproximation","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.NoiseApproximation","text":"In many cases, one would like to define a noise process directly by a stochastic differential equation which does not have an analytical solution. Of course, this will not be distributionally-exact and how well the properties match depends on how well the differential equation is integrated, but in many cases this can be used as a good approximation when other methods are much more difficult.\n\nA NoiseApproximation is defined by a DEIntegrator. The constructor for a NoiseApproximation is:\n\nNoiseApproximation(source1::DEIntegrator,source2::Union{DEIntegrator,Nothing}=nothing;reset=true)\n\nThe DEIntegrator should have a final time point of integration far enough such that it will not halt during the integration. For ease of use, you can use a final time point as Inf. Note that the time points do not have to match the time points of the future integration since the interpolant of the SDE solution will be used. Thus the limiting factor is error tolerance and not hitting specific points.\n\nNoiseApproximation Example\n\nIn this example we will show how to use the NoiseApproximation in order to build our own Geometric Brownian Motion from its stochastic differential equation definition. In normal usage, you should use the GeometricBrownianMotionProcess instead since that is more efficient and distributionally-exact.\n\nFirst, let's define the SDEProblem. Here will use a timespan (0.0,Inf) so that way the noise can be used over an indefinite integral.\n\nconst μ = 1.5\nconst σ = 1.2\nf(u, p, t) = μ*u\ng(u, p, t) = σ*u\nprob = SDEProblem(f,g,1.0,(0.0,Inf))\n\nNow we build the noise process by building the integrator and sending that integrator to the NoiseApproximation constructor:\n\nintegrator = init(prob,SRIW1())\nW = NoiseApproximation(integrator)\n\nWe can use this noise process like any other noise process. For example, we can now build a geometric Brownian motion whose noise process is colored noise that itself is a geometric Brownian motion:\n\nprob = SDEProblem(f,g,1.0,(0.0,Inf),noise=W)\n\nThe possibilities are endless.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.VirtualBrownianTree","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.VirtualBrownianTree","text":"A VirtualBrownianTree builds the noise process starting from an initial time t0, the first value of the proces W0, and (optionally) the first value Z0 for an auxiliary pseudo-process. The constructor is given as\n\nVirtualBrownianTree(t0,W0,Z0=nothing,dist=WHITE_NOISE_DIST,bridge=VBT_BRIDGE;kwargs...)\n\nwhere dist specifies the distribution that is used to generate the end point(s) Wend (Zend) of the noise process for the final time tend. bridge denotes the distribution of the employed Brownian bridge.  Per default tend is fixed to t0+1 but can be changed by passing a custom tend as a keyword argument. The following keyword arguments are available:\n\ntend is the end time of the noise process.\nWend is the end value of the noise process.\nZend is the end value of the pseudo-noise process.\natol represents the absolute tolerance determining when the recursion is  terminated.\ntree_depth allows one to store a cache of seeds, noise values, and times  to speed up the simulation by reducing the recursion steps.\nsearch_depth maximal search depth for the tree if atol is not reached.\nrng the splittable PRNG used for generating the random numbers.  Default: Threefry4x() from the Random123 package.\n\nVirtualBrownianTree Example\n\nIn this example, we define a multi-dimensional Brownian process based on a VirtualBrownianTree with a minimal tree_depth=0 such that memory consumption is minimized.\n\n  W0 = zeros(10)\n  W = VirtualBrownianTree(0.0,W0; tree_depth=0)\n\n  prob = NoiseProblem(W,(0.0,1.0))\n  sol = solve(prob;dt=1/10)\n\nUsing a look-up cache by increasing tree_depth can significantly reduce the runtime. Thus, the VirtualBrownianTree allows for trading off speed for memory in a simple manner.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.SimpleNoiseProcess","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.SimpleNoiseProcess","text":"mutable struct SimpleNoiseProcess{T,N,Tt,T2,T3,ZType,F,F2,inplace,RNGType} <: AbstractNoiseProcess{T,N,Vector{T2},inplace}\n\nLike NoiseProcess but without support for adaptivity. This makes it lightweight and slightly faster.\n\nwarn: Warn\nSimpleNoiseProcess should not be used with adaptive SDE solvers as it will lead to incorrect results.\n\nfunction SimpleNoiseProcess{iip}(t0,W0,Z0,dist,bridge;\n                                 save_everystep=true,\n                                 rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n                                 reset = true, reseed = true) where iip\n\nt0 is the first timepoint\nW0 is the first value of the process.\nZ0 is the first value of the pseudo-process. This is necessary for higher order algorithms. If it's not needed, set to nothing.\ndist the distribution for the steps over time.\nbridge the bridging distribution. Optional, but required for adaptivity and interpolating at new values.\nsave_everystep whether to save every step of the Brownian timeseries.\nrng the local RNG used for generating the random numbers.\nreset whether to reset the process with each solve.\nreseed whether to reseed the process with each solve.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.BoxWedgeTail","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.BoxWedgeTail","text":"mutable struct BoxWedgeTail{T,N,Tt,TA,T2,T3,ZType,F,F2,inplace,RNGType,tolType,\n  spacingType,jpdfType,boxType,wedgeType,tailType,distBWTType,distΠType} <: AbstractNoiseProcess{T,N,Vector{T2},inplace}\n\nThe method for random generation of stochastic area integrals due to Gaines and Lyons. The method is  based on Marsaglia's \"rectangle-wedge-tail\" approach for two dimensions. \n\n3 different groupings for the boxes are implemented.\n\nbox_grouping = :Columns (full, i.e., as large as possible, columns on a square spanned by dr and da)\nbox_grouping = :none (no grouping)\nbox_grouping = :MinEntropy (default, grouping that achieves a smaller entropy than the column wise grouping and thus allows for slightly faster sampling – but has a slightly larger amount of groups)\n\nThe sampling is based on the Distributions.jl package, i.e., to sample from one of the many distributions,  a uni-/bi-variate distribution from Distributions.jl is constructed and then rand(..) is used.\n\nConstructor\n\nBoxWedgeTail{iip}(t0,W0,Z0,dist,bridge;\n                      rtol=1e-8,nr=4,na=4,nz=10,\n                      box_grouping = :MinEntropy,\n                      sqeezing = true,\n                      save_everystep=true,\n                      rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n                      reset=true, reseed=true) where iip\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/abstract_noise_processes/#DiffEqNoiseProcess.pCN","page":"Abstract Noise Processes","title":"DiffEqNoiseProcess.pCN","text":"pCN(noise::AbstractNoiseProcess, ρ; reset=true,reverse=false,indx=nothing)\n\nCreate a new, but correlated noise process from noise and additional entropy with correlation ρ.\n\n\n\n\n\npCN(noise::NoiseGrid, ρ; reset=true, rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)))\n\nCreate a new, but correlated noise process from noise and additional entropy with correlation ρ. This update defines an autoregressive process in the space of Wiener (or noise process) trajectories which can be used as proposal distribution in Metropolis-Hastings algorithms (often called \"preconditioned Crank–Nicolson scheme\".)\n\nExternal links\n\nPreconditioned Crank–Nicolson algorithm on Wikipedia\n\n\n\n\n\n","category":"function"},{"location":"modules/Surrogates/gekpls/#GEKPLS-Surrogate-Tutorial","page":"GEKPLS","title":"GEKPLS Surrogate Tutorial","text":"","category":"section"},{"location":"modules/Surrogates/gekpls/","page":"GEKPLS","title":"GEKPLS","text":"Gradient Enhanced Kriging with Partial Least Squares Method (GEKPLS) is a surrogate modelling technique that brings down computation time and returns improved accuracy for high-dimensional problems. The Julia implementation of GEKPLS is adapted from the Python version by SMT which is based on this paper.  ","category":"page"},{"location":"modules/Surrogates/gekpls/","page":"GEKPLS","title":"GEKPLS","text":"The following are the inputs when building a GEKPLS surrogate: ","category":"page"},{"location":"modules/Surrogates/gekpls/","page":"GEKPLS","title":"GEKPLS","text":"X - The matrix containing the training points\ny - The vector containing the training outputs associated with each of the training points\ngrads - The gradients at each of the input X training points\nn_comp - Number of components to retain for the partial least squares regression (PLS)\ndelta_x -  The step size to use for the first order Taylor approximation\nxlimits - The lower and upper bounds for the training points\nextra_points - The number of additional points to use for the PLS \ntheta - The hyperparameter to use for the correlation model","category":"page"},{"location":"modules/Surrogates/gekpls/","page":"GEKPLS","title":"GEKPLS","text":"The following example illustrates how to use GEKPLS:","category":"page"},{"location":"modules/Surrogates/gekpls/","page":"GEKPLS","title":"GEKPLS","text":"\nusing Surrogates\nusing Zygote\n\nfunction vector_of_tuples_to_matrix(v)\n    #helper function to convert training data generated by surrogate sampling into a matrix suitable for GEKPLS\n    num_rows = length(v)\n    num_cols = length(first(v))\n    K = zeros(num_rows, num_cols)\n    for row in 1:num_rows\n        for col in 1:num_cols\n            K[row, col]=v[row][col]\n        end\n    end\n    return K\nend\n\nfunction vector_of_tuples_to_matrix2(v)\n    #helper function to convert gradients into matrix form\n    num_rows = length(v)\n    num_cols = length(first(first(v)))\n    K = zeros(num_rows, num_cols)\n    for row in 1:num_rows\n        for col in 1:num_cols\n            K[row, col] = v[row][1][col]\n        end\n    end\n    return K\nend\n\nfunction water_flow(x)\n    r_w = x[1]\n    r = x[2]\n    T_u = x[3]\n    H_u = x[4]\n    T_l = x[5]\n    H_l = x[6]\n    L = x[7]\n    K_w = x[8]\n    log_val = log(r/r_w)\n    return (2*pi*T_u*(H_u - H_l))/ ( log_val*(1 + (2*L*T_u/(log_val*r_w^2*K_w)) + T_u/T_l))\nend\n\nn = 1000\nd = 8\nlb = [0.05,100,63070,990,63.1,700,1120,9855]\nub = [0.15,50000,115600,1110,116,820,1680,12045]\nx = sample(n,lb,ub,SobolSample())\nX = vector_of_tuples_to_matrix(x)\ngrads = vector_of_tuples_to_matrix2(gradient.(water_flow, x))\ny = reshape(water_flow.(x),(size(x,1),1))\nxlimits = hcat(lb, ub)\nn_test = 100 \nx_test = sample(n_test,lb,ub,GoldenSample()) \nX_test = vector_of_tuples_to_matrix(x_test) \ny_true = water_flow.(x_test)\nn_comp = 2\ndelta_x = 0.0001\nextra_points = 2\ninitial_theta = [0.01 for i in 1:n_comp]\ng = GEKPLS(X, y, grads, n_comp, delta_x, xlimits, extra_points, initial_theta)\ny_pred = g(X_test)\nrmse = sqrt(sum(((y_pred - y_true).^2)/n_test)) #root mean squared error\nprintln(rmse) #0.0347\n","category":"page"},{"location":"modules/DiffEqFlux/#DiffEqFlux:-High-Level-Pre-Built-Architectures-for-Implicit-Deep-Learning","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux: High Level Pre-Built Architectures for Implicit Deep Learning","text":"","category":"section"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"DiffEqFlux.jl is an implicit deep learning library built using the SciML ecosystem. It is a high level interface that pulls together all of the tools with heuristics and helper functions to make training such deep implicit layer models fast and easy.","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"note: Note\nDiffEqFlux.jl is only for pre-built architectures and utility functions for deep implicit learning, mixing differential equations with machine learning. For details on automatic differentiation of equation solvers and adjoint techniques, and using these methods for doing things like callibrating models to data, nonlinear optimal control, and PDE-constrained optimization, see SciMLSensitivity.jl","category":"page"},{"location":"modules/DiffEqFlux/#Pre-Built-Architectures","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The approach of this package is the easy and efficient training of Universal Differential Equations. DiffEqFlux.jl provides architectures which match the interfaces of machine learning libraries such as Flux.jl and Lux.jl to make it easy to build continuous-time machine learning layers into larger machine learning applications.","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"The following layer functions exist:","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Neural Ordinary Differential Equations (Neural ODEs)\nCollocation-Based Neural ODEs (Neural ODEs without a solver, by far the fastest way!)\nMultiple Shooting Neural Ordinary Differential Equations\nNeural Stochastic Differential Equations (Neural SDEs)\nNeural Differential-Algebriac Equations (Neural DAEs)\nNeural Delay Differential Equations (Neural DDEs)\nAugmented Neural ODEs\nHamiltonian Neural Networks (with specialized second order and symplectic integrators)\nContinuous Normalizing Flows (CNF) and FFJORD","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Examples of how to build architectures from scratch, with tutorials on things like Graph Neural ODEs, can be found in the SciMLSensitivity.jl documentation.","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"WIP:","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"Lagrangian Neural Networks\nGalerkin Neural ODEs","category":"page"},{"location":"modules/DiffEqFlux/#Citation","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"Citation","text":"","category":"section"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"If you use DiffEqFlux.jl or are influenced by its ideas, please cite:","category":"page"},{"location":"modules/DiffEqFlux/","page":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","title":"DiffEqFlux.jl: High Level Scientific Machine Learning (SciML) Pre-Built Architectures","text":"@article{rackauckas2020universal,\n  title={Universal differential equations for scientific machine learning},\n  author={Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},\n  journal={arXiv preprint arXiv:2001.04385},\n  year={2020}\n}","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/#Using-ModelingToolkit.jl-With-StructuralIdentifiability.jl","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"In this tutorial, we will cover examples of solving identifiability problems for models defined with the syntax of ModelingToolkit.jl.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/#Input-System","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Input System","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"Let us consider the following ODE model with two outputs:","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"begincases\n    dotS = -b  S  (I + J + q  A)  N_inv\n    dotE = b  S  (I + J + q  A)  N_inv - k  E\n    dotA = k  (1 - r)  E - g_1  A\n    dotI = k  r  E - (alpha + g_1)  I\n    dotJ = alpha  I - g_2  J\n    dotC = alpha  I\n    y_1 = C\n    y_2 = N_inv\nendcases","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"This is an infectious desease model defined in [1].","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"The main difference between the input formats in ModelingToolkit.jl and StructuralIdentifiability.jl is that the output (measured values/functions) must be specified separately in ModelingToolkit.jl. In this example, measured quantities are presented by y_1, y_2.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"First, let us define the ODE. We will use @parameters and @variables macro to define parameters and time-depended functions in the ODE.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"using StructuralIdentifiability, ModelingToolkit\n\n@parameters b q N_inv k r alpha g1 g2\n@variables t S(t) E(t) A(t) I(t) J(t) C(t) y1(t) y2(t)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"The actual ODE will be defined using ODESystem structure from ModelingToolkit.jl:","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"D = Differential(t)\n\neqs = [\n    D(S) ~ -b * S * (I + J + q * A) * N_inv,\n    D(E) ~ b * S * (I + J + q * A) * N_inv - k * E,\n    D(A) ~ k * (1 - r) * E - g1 * A,\n    D(I) ~ k * r * E - (alpha + g1) * I,\n    D(J) ~ alpha * I - g2 * J,\n    D(C) ~ alpha * I,\n]\n\node = ODESystem(eqs, t, name = :SEIAJRCmodel)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"Finally, let us define the array of measured quantities and call the assess_identifiability function. This is the main function that determines local/global identifiability properties of each parameter and state. We will use the probability of correctness p=099.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"For ModelingToolkit.jl, both assess_identifiability and assess_local_identifiability functions accept keyword arguments: ","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"measured_quantities, also called \"output functions\" in identifiability literature; these are crucial for answering identifiability questions.\np, probability of correctness. This value equals 0.99 by default.\nfuncs_to_check, functions of parameters of which we wish to check identifiability.","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"measured_quantities = [y1 ~ C, y2 ~ N_inv]\n@time global_id = assess_identifiability(ode, measured_quantities=measured_quantities)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"Let us put all of the code above together:","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"using StructuralIdentifiability, ModelingToolkit\n\n@parameters b q N_inv k r alpha g1 g2\n@variables t S(t) E(t) A(t) I(t) J(t) C(t) y1(t) y2(t)\n\nD = Differential(t)\n\neqs = [\n    D(S) ~ -b * S * (I + J + q * A) * N_inv,\n    D(E) ~ b * S * (I + J + q * A) * N_inv - k * E,\n    D(A) ~ k * (1 - r) * E - g1 * A,\n    D(I) ~ k * r * E - (alpha + g1) * I,\n    D(J) ~ alpha * I - g2 * J,\n    D(C) ~ alpha * I,\n]\n\node = ODESystem(eqs, t, name = :SEIAJRCmodel)\n\nmeasured_quantities = [y1 ~ C, y2 ~ N_inv]\n@time global_id = assess_identifiability(ode, measured_quantities=measured_quantities)","category":"page"},{"location":"modules/StructuralIdentifiability/tutorials/using_modeling_toolkit/","page":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","title":"Using ModelingToolkit.jl With StructuralIdentifiability.jl","text":"[1]: K. Roosa and G. Chowell. Assessing parameter identifiability in compartmental dynamic models using a computational approach: application to infectious disease transmission models, Theor Biol Med Model 16, 1 (2019)","category":"page"},{"location":"modules/SymbolicNumericIntegration/symbolicnumericintegration/#API","page":"API","title":"API","text":"","category":"section"},{"location":"modules/SymbolicNumericIntegration/symbolicnumericintegration/","page":"API","title":"API","text":"integrate","category":"page"},{"location":"modules/NeuralPDE/manual/adaptive_losses/#adaptive_loss","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"The NeuralPDE discretize function allows for specifying adaptive loss function strategy which improve training performance by reweighing the equations as necessary to ensure the boundary conditions are well-statisfied, even in ill-conditioned scenarios. The following are the options for the adaptive_loss:","category":"page"},{"location":"modules/NeuralPDE/manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"NeuralPDE.NonAdaptiveLoss\nNeuralPDE.GradientScaleAdaptiveLoss\nNeuralPDE.MiniMaxAdaptiveLoss","category":"page"},{"location":"modules/NeuralPDE/manual/adaptive_losses/#NeuralPDE.NonAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.NonAdaptiveLoss","text":"NonAdaptiveLoss{T}(; pde_loss_weights = 1,\n                     bc_loss_weights = 1,\n                     additional_loss_weights = 1)\n\nA way of loss weighting the components of the loss function in the total sum that does not change during optimization\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/adaptive_losses/#NeuralPDE.GradientScaleAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.GradientScaleAdaptiveLoss","text":"GradientScaleAdaptiveLoss(reweight_every;\n                          weight_change_inertia = 0.9,\n                          pde_loss_weights = 1,\n                          bc_loss_weights = 1,\n                          additional_loss_weights = 1)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that BCi loss weights are scaled by the exponential moving average of max(|∇pdeloss|)/mean(|∇bciloss|) )\n\nPositional Arguments\n\nreweight_every: how often to reweight the BC loss functions, measured in iterations. Reweighting is somewhat expensive since it involves evaluating the gradient of each component loss function,\n\nKeyword Arguments\n\nweight_change_inertia: a real number that represents the inertia of the exponential moving average of the BC weight changes,\n\nReferences\n\nUnderstanding and mitigating gradient pathologies in physics-informed neural networks Sifan Wang, Yujun Teng, Paris Perdikaris https://arxiv.org/abs/2001.04536v1\n\nWith code reference: https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs\n\n\n\n\n\n","category":"type"},{"location":"modules/NeuralPDE/manual/adaptive_losses/#NeuralPDE.MiniMaxAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.MiniMaxAdaptiveLoss","text":"function MiniMaxAdaptiveLoss(reweight_every;\n                             pde_max_optimiser = Flux.ADAM(1e-4),\n                             bc_max_optimiser = Flux.ADAM(0.5),\n                             pde_loss_weights = 1,\n                             bc_loss_weights = 1,\n                             additional_loss_weights = 1)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that the loss weights are maximized by an internal optimiser, which leads to a behavior where loss functions that have not been satisfied get a greater weight,\n\nPositional Arguments\n\nreweight_every: how often to reweight the PDE and BC loss functions, measured in iterations.  reweighting is cheap since it re-uses the value of loss functions generated during the main optimisation loop\n\nKeyword Arguments\n\npde_max_optimiser: a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the PDE loss functions\nbc_max_optimiser: a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the BC loss functions\n\nReferences\n\nSelf-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism Levi McClenny, Ulisses Braga-Neto https://arxiv.org/abs/2009.04544\n\n\n\n\n\n","category":"type"},{"location":"modules/Optimization/optimization_packages/gcmaes/#GCMAES.jl","page":"GCMAES.jl","title":"GCMAES.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"GCMAES is a Julia package implementing the Gradient-based Covariance Matrix Adaptation Evolutionary Strategy which can utilize the gradient information to speed up the optimization process.","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/#Installation:-OptimizationGCMAES.jl","page":"GCMAES.jl","title":"Installation: OptimizationGCMAES.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"To use this package, install the OptimizationGCMAES package:","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"import Pkg; Pkg.add(\"OptimizationGCMAES\")","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/#Global-Optimizer","page":"GCMAES.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/gcmaes/#Without-Constraint-Equations","page":"GCMAES.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"The GCMAES algorithm is called by GCMAESOpt() and the initial search variance is set as a keyword argument σ0 (default: σ0 = 0.2)","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"The method in GCMAES is performing global optimization on problems without constraint equations. However, lower and upper constraints set by lb and ub in the OptimizationProblem are required.","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/#Example","page":"GCMAES.jl","title":"Example","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"The Rosenbrock function can optimized using the GCMAESOpt() without utilizing the gradient information as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, GCMAESOpt())","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"We can also utilise the gradient information of the optimization problem to aid the optimization as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/gcmaes/","page":"GCMAES.jl","title":"GCMAES.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock, Optimization.ForwardDiff)\nprob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0,-1.0], ub = [1.0,1.0])\nsol = solve(prob, GCMAESOpt())","category":"page"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#Functions-to-Assess-Parameter-Identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"ָFunctions to Assess Parameter Identifiability","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#Assessing-All-Types-of-Identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"Assessing All Types of Identifiability","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/","page":"ָFunctions to Assess Parameter Identifiability","title":"ָFunctions to Assess Parameter Identifiability","text":"assess_identifiability","category":"page"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#StructuralIdentifiability.assess_identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"StructuralIdentifiability.assess_identifiability","text":"assess_identifiability(ode::ODE{P}, p::Float64=0.99) where P <: MPolyElem{fmpq}\n\nInput:\n\node - the ODE model\np - probability of correctness.\n\nAssesses identifiability (both local and global) of a given ODE model (parameters detected automatically). The result is guaranteed to be correct with the probability at least p.\n\n\n\n\n\nassess_identifiability(ode, [funcs_to_check, p=0.99])\n\nInput:\n\node - the ODE model\np - probability of correctness.\n\nAssesses identifiability of a given ODE model. The result is guaranteed to be correct with the probability at least p.\n\nIf funcs_to_check are given, then the function will assess the identifiability of the provided functions and return a list of the same length with each element being one of :nonidentifiable, :locally, :globally.\n\nIf funcs_to_check are not given, the function will assess identifiability of the parameters, and the result will be a dictionary from the parameters to their identifiability properties (again, one of :nonidentifiable, :locally, :globally).\n\n\n\n\n\nassess_identifiability(ode::ModelingToolkit.ODESystem; measured_quantities=Array{ModelingToolkit.Equation}[], funcs_to_check=[], p = 0.99)\n\nInput:\n\node - the ModelingToolkit.ODESystem object that defines the model\nmeasured_quantities - the output functions of the model\nfuncs_to_check - functions of parameters for which to check the identifiability\np - probability of correctness.\n\nAssesses identifiability (both local and global) of a given ODE model (parameters detected automatically). The result is guaranteed to be correct with the probability at least p.\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#Assessing-Local-Identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"Assessing Local Identifiability","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/","page":"ָFunctions to Assess Parameter Identifiability","title":"ָFunctions to Assess Parameter Identifiability","text":"assess_local_identifiability","category":"page"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#StructuralIdentifiability.assess_local_identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"StructuralIdentifiability.assess_local_identifiability","text":"function assess_local_identifiability(ode::ModelingToolkit.ODESystem; measured_quantities=Array{ModelingToolkit.Equation}[], funcs_to_check=Array{}[], p::Float64=0.99, type=:SE)\n\nInput:\n\node - the ODESystem object from ModelingToolkit\nmeasured_quantities - the measureable outputs of the model\nfuncs_to_check - functions of parameters for which to check identifiability\np - probability of correctness\ntype - identifiability type (:SE for single-experiment, :ME for multi-experiment)\n\nOutput: \n\nfor type=:SE, the result is a dictionary from each parameter to boolean;\nfor type=:ME, the result is a tuple with the dictionary as in :SE case and array of number of experiments.\n\nThe function determines local identifiability of parameters in funcs_to_check or all possible parameters if funcs_to_check is empty\n\nThe result is correct with probability at least p.\n\ntype can be either :SE (single-experiment identifiability) or :ME (multi-experiment identifiability). The return value is a tuple consisting of the array of bools and the number of experiments to be performed. \"\n\n\n\n\n\nassess_local_identifiability(ode::ODE{P}, p::Float64 = 0.99, type=:SE) where P <: MPolyElem{Nemo.fmpq}\n\nInput:\n\node - the ODE model\np - probability of correctness\ntype - identifiability type (:SE for single-experiment, :ME for multi-experiment)\n\nOutput: \n\nfor type=:SE, the result is a dictionary from each parameter to boolean;\nfor type=:ME, the result is a tuple with the dictionary as in :SE case and array of number of experiments.\n\nThe main entrypoint for local identifiability checks.  Call this function to automatically take care of local identifiability of all parameters and initial conditions. The result is correct with probability at least p.\n\ntype can be either :SE (single-experiment identifiability) or :ME (multi-experiment identifiability). The return value is a tuple consisting of the array of bools and the number of experiments to be performed.\n\n\n\n\n\nassess_local_identifiability(ode::ODE{P}, funcs_to_check::Array{<: Any, 1}, p::Float64=0.99, type=:SE) where P <: MPolyElem{Nemo.fmpq}\n\nChecks the local identifiability/observability of the functions in funcs_to_check. The result is correct with probability at least p.\n\nCall this function if you have a specific collection of parameters of which you would like to check local identifiability.\n\ntype can be either :SE (single-experiment identifiability) or :ME (multi-experiment identifiability). If the type is :ME, states are not allowed to appear in the funcs_to_check.\n\n\n\n\n\n","category":"function"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#Assessing-Global-Identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"Assessing Global Identifiability","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/","page":"ָFunctions to Assess Parameter Identifiability","title":"ָFunctions to Assess Parameter Identifiability","text":"assess_global_identifiability","category":"page"},{"location":"modules/StructuralIdentifiability/identifiability/identifiability/#StructuralIdentifiability.assess_global_identifiability","page":"ָFunctions to Assess Parameter Identifiability","title":"StructuralIdentifiability.assess_global_identifiability","text":"assess_global_identifiability(ode::ODE{P}, p::Float64=0.99; var_change=:default) where P <: MPolyElem{fmpq}\n\nInput:\n\node - the ODE model\np - probability of correctness\nvar_change - a policy for variable change (:default, :yes, :no), affects only the runtime\n\nOutput: \n\na dictionary mapping each parameter to a boolean.\n\nChecks global identifiability for a parameters of the model provided in ode. Call this function to check global identifiability of all parameters automatically.\n\n\n\n\n\nassess_global_identifiability(ode, [funcs_to_check, p=0.99, var_change=:default])\n\nInput:\n\node - the ODE model\nfuncs_to_check - rational functions in parameters\np - probability of correctness\nvar_change - a policy for variable change (:default, :yes, :no),               affects only the runtime\n\nOutput: \n\narray of length length(funcs_to_check) with true/false values for global identifiability       or dictionary param => Bool if funcs_to_check are not given\n\nChecks global identifiability of functions of parameters specified in funcs_to_check.\n\n\n\n\n\n","category":"function"},{"location":"modules/DiffEqDocs/extras/timestepping/#timestepping","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"CurrentModule = OrdinaryDiffEq","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#Common-Setup","page":"Timestepping Method Descriptions","title":"Common Setup","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"All methods start by calculating a scaled error estimate on each scalar component of u:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"err^scaled_i = norm(err_i(abstol_i + max(uprev_iu_i)reltol_i))","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"On this scaled error estimate, we calculate the norm. This norm is usually the Hairer norm:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"norm(x) = sqrt(sum(x^2)length(x))","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"This norm works well because it does not change if we add new pieces to the differential equation: it scales our error by the number of equations so that independent equations will not step differently than a single solve.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"In all cases, the step is rejected if err^scaled1 since that means the error is larger than the tolerances, and the step is accepted if err^scaled1.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#Integral-Controller-(Standard-Controller)","page":"Timestepping Method Descriptions","title":"Integral Controller (Standard Controller)","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The proportional control algorithm is the \"standard algorithm\" for adaptive timestepping. Note that it is not the default in DifferentialEquations.jl because it is usually awful for performance, but it is explained first because it is the most widely taught algorithm and others build off of its techniques.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The control simply changes dt proportional to the error. There is an exponentiation based on the order of the algorithm which goes back to a result by Cechino for the optimal stepsize to reduce the error. The algorithm is:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"qtmp = integrator.EEst^(1/(alg_adaptive_order(integrator.alg)+1))/integrator.opts.gamma\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nintegrator.dtnew = integrator.dt/q","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"Thus q is the scaling factor for dt, and it must be between qmin and qmax. gamma is the safety factor, 0.9, for how much dt is decreased below the theoretical \"optimal\" value.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"Since proportional control is \"jagged\", i.e. can cause large changes between one step to the next, it can effect the stability of explicit methods. Thus it's only applied by default to low order implicit solvers.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"IController","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#OrdinaryDiffEq.IController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.IController","text":"IController()\n\nThe standard (integral) controller is the most basic step size controller. This controller is usually the first one introduced in numerical analysis classes but should only be used rarely in practice because of efficiency problems for many problems/algorithms.\n\nConstruct an integral (I) step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^(1/k) * Δtₙ\n\nwhere k = get_current_adaptive_order(alg, integrator.cache) + 1 and εᵢ is the inverse of the error estimate integrator.EEst scaled by the tolerance (Hairer, Nørsett, Wanner, 2008, Section II.4). The step size factor is multiplied by the safety factor gamma and clipped to the interval [qmin, qmax]. A step will be accepted whenever the estimated error integrator.EEst is less than or equal to unity. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nReferences\n\nHairer, Nørsett, Wanner (2008) Solving Ordinary Differential Equations I Nonstiff Problems DOI: 10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/extras/timestepping/#Proportional-Integral-Controller-(PI-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral Controller (PI Controller)","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The proportional-integral control algorithm is a standard control algorithm from control theory. It mixes proportional control with memory in order to make the timesteps more stable, which actually increases the adaptive stability region of the algorithm. This stability property means that it's well-suited for explicit solvers, and it's applied by default to the Rosenbrock methods as well. The form for the updates is:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"EEst,beta1,q11,qold,beta2 = integrator.EEst, integrator.opts.beta1, integrator.q11,integrator.qold,integrator.opts.beta2\n@fastmath q11 = EEst^beta1\n@fastmath q = q11/(qold^beta2)\nintegrator.q11 = q11\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),q/integrator.opts.gamma))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nq","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"beta1 is the gain on the proportional part, and beta2 is the gain for the history portion. qoldinit is the initialized value for the gain history.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PIController","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#OrdinaryDiffEq.PIController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PIController","text":"PIController(beta1, beta2)\n\nThe proportional-integral (PI) controller is a widespread step size controller with improved stability properties compared to the IController. This controller is the default for most algorithms in OrdinaryDiffEq.jl.\n\nConstruct a PI step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^β₁ * εₙ^β₂ * Δtₙ\n\nwhere εᵢ are inverses of the error estimates scaled by the tolerance (Hairer, Nørsett, Wanner, 2010, Section IV.2). The step size factor is multiplied by the safety factor gamma and clipped to the interval [qmin, qmax]. A step will be accepted whenever the estimated error integrator.EEst is less than or equal to unity. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nnote: Note\nThe coefficients beta1, beta2 are not scaled by the order of the method, in contrast to the PIDController. For the PIController, this scaling by the order must be done when the controller is constructed.\n\nReferences\n\nHairer, Nørsett, Wanner (2010) Solving Ordinary Differential Equations II Stiff and Differential-Algebraic Problems DOI: 10.1007/978-3-642-05221-7\nHairer, Nørsett, Wanner (2008) Solving Ordinary Differential Equations I Nonstiff Problems DOI: 10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/extras/timestepping/#Proportional-Integral-Derivative-Controller-(PID-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral-Derivative Controller (PID Controller)","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PIDController","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#OrdinaryDiffEq.PIDController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PIDController","text":"PIDController(beta1, beta2, beta3=zero(beta1);\n              limiter=default_dt_factor_limiter,\n              accept_safety=0.81)\n\nThe proportional-integral-derivative (PID) controller is a generalization of the PIController and can have improved stability and efficiency properties.\n\nConstruct a PID step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^(β₁/k) * εₙ^(β₂/k) * εₙ₋₁^(β₃/ k) * Δtₙ\n\nwhere k = min(alg_order, alg_adaptive_order) + 1 and εᵢ are inverses of the error estimates scaled by the tolerance (Söderlind, 2003). The step size factor is limited by the limiter with default value\n\nlimiter(x) = one(x) + atan(x - one(x))\n\nas proposed by Söderlind and Wang (2006). A step will be accepted whenever the predicted step size change is bigger than accept_safety. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nSome standard controller parameters suggested in the literature are\n\nController beta1 beta2 beta3\nbasic 1.00 0.00 0\nPI42 0.60 -0.20 0\nPI33 2//3 -1//3 0\nPI34 0.70 -0.40 0\nH211PI 1//6 1//6 0\nH312PID 1//18 1//9 1//18\n\nnote: Note\nIn contrast to the PIController, the coefficients beta1, beta2, beta3 are scaled by the order of the method. Thus, standard controllers such as PI42 can use the same coefficients beta1, beta2, beta3 for different algorithms.\n\nnote: Note\nIn contrast to other controllers, the PIDController does not use the keyword arguments qmin, qmax to limit the step size change or the safety factor gamma. These common keyword arguments are replaced by the limiter and accept_safety to guarantee a smooth behavior (Söderlind and Wang, 2006). Because of this, a PIDController behaves different from a PIController, even if beta1, beta2 are adapted accordingly and iszero(beta3).\n\nReferences\n\nSöderlind (2003) Digital Filters in Adaptive Time-Stepping DOI: 10.1145/641876.641877\nSöderlind, Wang (2006) Adaptive time-stepping and computational stability DOI: 10.1016/j.cam.2005.03.008\nRanocha, Dalcin, Parsani, Ketcheson (2021) Optimized Runge-Kutta Methods with Automatic Step Size Control for Compressible Computational Fluid Dynamics arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/extras/timestepping/#Gustafsson-Acceleration","page":"Timestepping Method Descriptions","title":"Gustafsson Acceleration","text":"","category":"section"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The Gustafsson acceleration algorithm accelerates changes so that way algorithms can more swiftly change to handle quick transients. This algorithm is thus well-suited for stiff solvers where this can be expected, and is the default for algorithms like the (E)SDIRK methods.","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"gamma = integrator.opts.gamma\nniters = integrator.cache.newton_iters\nfac = min(gamma,(1+2*integrator.alg.max_newton_iter)*gamma/(niters+2*integrator.alg.max_newton_iter))\nexpo = 1/(alg_order(integrator.alg)+1)\nqtmp = (integrator.EEst^expo)/fac\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nintegrator.qold = q\nq","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"In this case, niters is the number of Newton iterations which was required in the most recent step of the algorithm. Note that these values are used differently depending on acceptance and rejection. When the step is accepted, the following logic is applied:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"if integrator.success_iter > 0\n  expo = 1/(alg_adaptive_order(integrator.alg)+1)\n  qgus=(integrator.dtacc/integrator.dt)*(((integrator.EEst^2)/integrator.erracc)^expo)\n  qgus = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qgus/integrator.opts.gamma))\n  qacc=max(q,qgus)\nelse\n  qacc = q\nend\nintegrator.dtacc = integrator.dt\nintegrator.erracc = max(1e-2,integrator.EEst)\nintegrator.dt/qacc","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"When it rejects, its the same as the proportional control:","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"if integrator.success_iter == 0\n  integrator.dt *= 0.1\nelse\n  integrator.dt = integrator.dt/integrator.qold\nend","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PredictiveController","category":"page"},{"location":"modules/DiffEqDocs/extras/timestepping/#OrdinaryDiffEq.PredictiveController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PredictiveController","text":"PredictiveController()\n\nThe Gustafsson acceleration algorithm accelerates changes so that way algorithms can more swiftly change to handle quick transients. This algorithm is thus well-suited for stiff solvers where this can be expected, and is the default for algorithms like the (E)SDIRK methods.\n\ngamma = integrator.opts.gamma\nniters = integrator.cache.newton_iters\nfac = min(gamma,(1+2*integrator.alg.max_newton_iter)*gamma/(niters+2*integrator.alg.max_newton_iter))\nexpo = 1/(alg_order(integrator.alg)+1)\nqtmp = (integrator.EEst^expo)/fac\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nintegrator.qold = q\nq\n\nIn this case, niters is the number of Newton iterations which was required in the most recent step of the algorithm. Note that these values are used differently depending on acceptance and rejectance. When the step is accepted, the following logic is applied:\n\nif integrator.success_iter > 0\n  expo = 1/(alg_adaptive_order(integrator.alg)+1)\n  qgus=(integrator.dtacc/integrator.dt)*(((integrator.EEst^2)/integrator.erracc)^expo)\n  qgus = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qgus/integrator.opts.gamma))\n  qacc=max(q,qgus)\nelse\n  qacc = q\nend\nintegrator.dtacc = integrator.dt\nintegrator.erracc = max(1e-2,integrator.EEst)\nintegrator.dt/qacc\n\nWhen it rejects, it's the same as the IController:\n\nif integrator.success_iter == 0\n  integrator.dt *= 0.1\nelse\n  integrator.dt = integrator.dt/integrator.qold\nend\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/jump_types/#Jump-Problems","page":"Jump Problems","title":"Jump Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/#Mathematical-Specification-of-an-problem-with-jumps","page":"Jump Problems","title":"Mathematical Specification of an problem with jumps","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Jumps are defined as a Poisson process which changes states at some rate. When there are multiple possible jumps, the process is a compound Poisson process. On its own, a jump equation is a continuous-time Markov Chain where the time to the next jump is exponentially distributed as calculated by the rate. This type of process, known in biology as \"Gillespie discrete stochastic simulations\" and modeled by the Chemical Master Equation (CME), is the same thing as adding jumps to a DiscreteProblem. However, any differential equation can be extended by jumps as well. For example, we have an ODE with jumps, denoted by","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"fracdudt = f(upt) + sum_ic_i(upt)p_i(t)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"where p_i is a Poisson counter of rate lambda_i(upt). Extending a stochastic differential equation to have jumps is commonly known as a Jump Diffusion, and is denoted by","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"du = f(upt)dt + sum_jg_j(ut)dW_j(t) + sum_ic_i(upt)dp_i(t)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Types-of-Jumps:-Regular,-Variable,-Constant-Rate-and-Mass-Action","page":"Jump Problems","title":"Types of Jumps: Regular, Variable, Constant Rate and Mass Action","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"A RegularJump is a set of jumps that do not make structural changes to the underlying equation. These kinds of jumps only change values of the dependent variable (u) and thus can be treated in an inexact manner. Other jumps, such as those which change the size of u, require exact handling which is also known as time-adaptive jumping. These can only be specified as a ConstantRateJump, MassActionJump, or a VariableRateJump.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"We denote a jump as variable rate if its rate function is dependent on values which may change between constant rate jumps. For example, if there are multiple jumps whose rates only change when one of them occur, than that set of jumps is a constant rate jump. If a jump's rate depends on the differential equation, time, or by some value which changes outside of any constant rate jump, then it is denoted as variable.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"A MassActionJump is a specialized representation for a collection of constant rate jumps that can each be interpreted as a standard mass action reaction. For systems comprised of many mass action reactions, using the MassActionJump type will offer improved performance. Note, only one MassActionJump should be defined per JumpProblem; it is then responsible for handling all mass action reaction type jumps. For systems with both mass action jumps and non-mass action jumps, one can create one MassActionJump to handle the mass action jumps, and create a number of ConstantRateJumps to handle the non-mass action jumps.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RegularJumps are optimized for regular jumping algorithms like tau-leaping and hybrid algorithms. ConstantRateJumps and MassActionJumps are optimized for SSA algorithms. ConstantRateJumps, MassActionJumps and VariableRateJumps can be added to standard DiffEq algorithms since they are simply callbacks, while RegularJumps require special algorithms. ","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Defining-a-Regular-Jump","page":"Jump Problems","title":"Defining a Regular Jump","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a RegularJump is:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RegularJump(rate,c,numjumps;mark_dist = nothing)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate(out,u,p,t) is the function which computes the rate for every regular jump process\nc(du,u,p,t,counts,mark) is calculates the update given counts number of jumps for each jump process in the interval.\nnumjumps is the number of jump processes, i.e. the number of rate equations and the number of counts\nmark_dist is the distribution for the mark.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Defining-a-Constant-Rate-Jump","page":"Jump Problems","title":"Defining a Constant Rate Jump","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"ConstantRateJump(rate,affect!)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate(u,p,t) is a function which calculates the rate given the time and the state.\naffect!(integrator) is the effect on the equation, using the integrator interface.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Defining-a-Mass-Action-Jump","page":"Jump Problems","title":"Defining a Mass Action Jump","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a MassActionJump is:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"MassActionJump(reactant_stoich, net_stoich; scale_rates = true, param_idxs=nothing)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"reactant_stoich is a vector whose kth entry is the reactant stoichiometry of the kth reaction. The reactant stoichiometry for an individual reaction is assumed to be represented as a vector of Pairs, mapping species id to stoichiometric coefficient.\nnet_stoich is assumed to have the same type as reactant_stoich; a vector whose kth entry is the net stoichiometry of the kth reaction. The net stoichiometry for an individual reaction is again represented as a vector of Pairs, mapping species id to the net change in the species when the reaction occurs.\nscale_rates is an optional parameter that specifies whether the rate constants correspond to stochastic rate constants in the sense used by Gillespie, and hence need to be rescaled. The default, scale_rates=true, corresponds to rescaling the passed in rate constants. See below.\nparam_idxs is a vector of the indices within the parameter vector, p, that correspond to the rate constant for each jump.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Notes for Mass Action Jumps","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"When using MassActionJump the default behavior is to assume rate constants correspond to stochastic rate constants in the sense used by Gillespie (J. Comp. Phys., 1976, 22 (4)). This means that for a reaction such as 2A oversetkrightarrow B, the jump rate function constructed by MassActionJump would be k*A*(A-1)/2!. For a trimolecular reaction like 3A oversetkrightarrow B the rate function would be k*A*(A-1)*(A-2)/3!. To avoid having the reaction rates rescaled (by 1/2 and 1/6 for these two examples), one can pass the MassActionJump constructor the optional named parameter scale_rates=false, i.e. use\nMassActionJump(reactant_stoich, net_stoich; scale_rates = false, param_idxs)\nZero order reactions can be passed as reactant_stoichs in one of two ways. Consider the varnothing oversetkrightarrow A reaction with rate k=1:\np = [1.]\nreactant_stoich = [[0 => 1]]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nAlternatively one can create an empty vector of pairs to represent the reaction:\np = [1.]\nreactant_stoich = [Vector{Pair{Int,Int}}()]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nFor performance reasons, it is recommended to order species indices in stoichiometry vectors from smallest to largest. That is \nreactant_stoich = [[1 => 2, 3 => 1, 4 => 2], [2 => 2, 3 => 2]]\nis preferred over\nreactant_stoich = [[3 => 1, 1 => 2, 4 = > 2], [3 => 2, 2 => 2]]","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Defining-a-Variable-Rate-Jump","page":"Jump Problems","title":"Defining a Variable Rate Jump","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a VariableRateJump is:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"VariableRateJump(rate,affect!;\n                   idxs = nothing,\n                   rootfind=true,\n                   save_positions=(true,true),\n                   interp_points=10,\n                   abstol=1e-12,reltol=0)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Note that this is the same as defining a ContinuousCallback, except that instead of the condition function, you provide a rate(u,p,t) function for the rate at a given time and state.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Defining-a-Jump-Problem","page":"Jump Problems","title":"Defining a Jump Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To define a JumpProblem, you must first define the basic problem. This can be a DiscreteProblem if there is no differential equation, or an ODE/SDE/DDE/DAE if you would like to augment a differential equation with jumps. Denote this previously defined problem as prob. Then the constructor for the jump problem is:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,aggregator::Direct,jumps::JumpSet;\n            save_positions = typeof(prob) <: AbstractDiscreteProblem ? (false,true) : (true,true))","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The aggregator is the method for aggregating the constant jumps. These are defined below. jumps is a JumpSet which is just a gathering of jumps. Instead of passing a JumpSet, one may just pass a list of jumps themselves. For example:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,aggregator,jump1,jump2)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"and the internals will automatically build the JumpSet. save_positions is the save_positions argument built by the aggregation of the constant rate jumps.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Note that a JumpProblem/JumpSet can only have 1 RegularJump (since a RegularJump itself describes multiple processes together). Similarly, it can only have one MassActionJump (since it also describes multiple processes together).","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Constant-Rate-Jump-Aggregators","page":"Jump Problems","title":"Constant Rate Jump Aggregators","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Constant rate jump aggregators are the methods by which constant rate jumps, including MassActionJumps, are lumped together. This is required in all algorithms for both speed and accuracy. The current methods are:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Direct: the Gillespie Direct method SSA.\nRDirect: A variant of Gillespie's Direct method that uses rejection to sample the next reaction.\nDirectCR: The Composition-Rejection Direct method of Slepoy et al. For large networks and linear chain-type networks it will often give better performance than Direct. (Requires dependency graph, see below.)\nDirectFW: the Gillespie Direct method SSA with FunctionWrappers. This aggregator uses a different internal storage format for collections of ConstantRateJumps. \nFRM: the Gillespie first reaction method SSA. Direct should generally offer better performance and be preferred to FRM.\nFRMFW: the Gillespie first reaction method SSA with FunctionWrappers.\nNRM: The Gibson-Bruck Next Reaction Method. For some reaction network  structures this may offer better performance than Direct (for example,  large, linear chains of reactions). (Requires dependency graph, see below.) \nRSSA: The Rejection SSA (RSSA) method of Thanh et al. With RSSACR, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nRSSACR: The Rejection SSA (RSSA) with Composition-Rejection method of Thanh et al. With RSSA, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nSortingDirect: The Sorting Direct Method of McCollum et al. It will usually offer performance as good as Direct, and for some systems can offer substantially better performance. (Requires dependency graph, see below.)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To pass the aggregator, pass the instantiation of the type. For example:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,Direct(),jump1,jump2)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"will build a problem where the constant rate jumps are solved using Gillespie's Direct SSA method.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Constant-Rate-Jump-Aggregators-Requiring-Dependency-Graphs","page":"Jump Problems","title":"Constant Rate Jump Aggregators Requiring Dependency Graphs","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Italicized constant rate jump aggregators require the user to pass a dependency graph to JumpProblem. DirectCR, NRM and SortingDirect require a jump-jump dependency graph, passed through the named parameter dep_graph. i.e.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,DirectCR(),jump1,jump2; dep_graph=your_dependency_graph)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For systems with only MassActionJumps, or those generated from a Catalyst reaction_network, this graph will be auto-generated. Otherwise you must construct the dependency graph manually. Dependency graphs are represented as a Vector{Vector{Int}}, with the ith vector containing the indices of the jumps for which rates must be recalculated when the ith jump occurs. Internally, all MassActionJumps are ordered before ConstantRateJumps (with the latter internally ordered in the same order they were passed in).","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RSSA and RSSACR require two different types of dependency graphs, passed through the following JumpProblem kwargs:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"vartojumps_map - A Vector{Vector{Int}} mapping each variable index, i, to a set of jump indices. The jump indices correspond to jumps with rate functions that depend on the value of u[i].\njumptovars_map - A Vector{Vector{Int}}  mapping each jump index to a set  of variable indices. The corresponding variables are those that have their  value, u[i], altered when the jump occurs.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For systems generated from a Catalyst reaction_network these will be auto-generated. Otherwise you must explicitly construct and pass in these mappings.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Recommendations-for-Constant-Rate-Jumps","page":"Jump Problems","title":"Recommendations for Constant Rate Jumps","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For representing and aggregating constant rate jumps ","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Use a MassActionJump to handle all jumps that can be represented as mass action reactions. This will generally offer the fastest performance. \nUse ConstantRateJumps for any remaining jumps.\nFor a small number of jumps, < ~10, Direct will often perform as well as the other aggregators.\nFor > ~10 jumps SortingDirect will often offer better performance than Direct.\nFor large numbers of jumps with sparse chain like structures and similar jump rates, for example continuous time random walks, RSSACR, DirectCR and then NRM often have the best performance.\nFor very large networks, with many updates per jump, RSSA and RSSACR will often substantially outperform the other methods. ","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"In general, for systems with sparse dependency graphs if Direct is slow, one of SortingDirect, RSSA or RSSACR will usually offer substantially better performance. See DiffEqBenchmarks.jl for benchmarks on several example networks.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/#Remaking-JumpProblems","page":"Jump Problems","title":"Remaking JumpProblems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"When running many simulations, it can often be convenient to update the initial condition or simulation parameters without having to create and initialize a new JumpProblem. In such situations remake can be used to change the initial condition, time span, and the parameter vector. Note, the new JumpProblem will alias internal data structures from the old problem, including core components of the SSA aggregators. As such, only the new problem generated by remake should be used for subsequent simulations.","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"As an example, consider the following SIR model:","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate1(u,p,t) = (0.1/1000.0)*u[1]*u[2]\nfunction affect1!(integrator)\n  integrator.u[1] -= 1\n  integrator.u[2] += 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = 0.01u[2]\nfunction affect2!(integrator)\n  integrator.u[2] -= 1\n  integrator.u[3] += 1\nend\njump2 = ConstantRateJump(rate2,affect2!)\nu0    = [999,1,0]\np     = (0.1/1000,0.01)\ntspan = (0.0,250.0)\ndprob = DiscreteProblem(u0, tspan, p)\njprob = JumpProblem(dprob, Direct(), jump, jump2)\nsol   = solve(jprob, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"We can change any of u0, p and tspan by either making a new DiscreteProblem","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"u02    = [10,1,0]\np2     = (.1/1000, 0.0)\ntspan2 = (0.0,2500.0)\ndprob2 = DiscreteProblem(u02, tspan2, p2)\njprob2 = remake(jprob, prob=dprob2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"or by directly remaking with the new parameters","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"jprob2 = remake(jprob, u0=u02, p=p2, tspan=tspan2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To avoid ambiguities, the following will give an error","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"jprob2 = remake(jprob, prob=dprob2, u0=u02)","category":"page"},{"location":"modules/DiffEqDocs/types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"as will trying to update either p or tspan while passing a new DiscreteProblem using the prob kwarg.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#ode_example","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"This tutorial will introduce you to the functionality for solving ODEs. Other introductions can be found by checking out SciMLTutorials.jl. Additionally, a video tutorial walks through this material.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Example-1-:-Solving-Scalar-Equations","page":"Ordinary Differential Equations","title":"Example 1 : Solving Scalar Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"fracdudt = f(upt)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"on the time interval tin01 where f(upt)=αu. Here, u is the current state variable, p is our parameter variable (containing things like a reaction rate or the constant of gravity), and t is the current time.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(In our example, we know by calculus that the solution to this equation is  u(t)=u₀exp(αt), but we will use DifferentialEquations.jl to solve this  problem numerically, which is essential for problems where a symbolic solution  is not known.)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The general workflow is to define a problem, solve the problem, and then analyze the solution. The full code for solving this problem is:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nf(u,p,t) = 1.01*u\nu0 = 1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n\nusing Plots\nplot(sol,linewidth=5,title=\"Solution to the linear ODE with a thick line\",\n     xaxis=\"Time (t)\",yaxis=\"u(t) (in μm)\",label=\"My Thick Line!\") # legend=false\nplot!(sol.t, t->0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"where the pieces are described below.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Step-1:-Defining-a-Problem","page":"Ordinary Differential Equations","title":"Step 1: Defining a Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"To solve this numerically, we define a problem type by giving it the equation, the initial condition, and the timespan to solve over:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nf(u,p,t) = 1.01*u\nu0 = 1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that DifferentialEquations.jl will choose the types for the problem based on the types used to define the problem type. For our example, notice that u0 is a Float64, and therefore this will solve with the dependent variables being Float64. Since tspan = (0.0,1.0) is a tuple of Float64's, the independent variables will be solved using Float64's (note that the start time and end time must match types). You can use this to choose to solve with arbitrary precision numbers, unitful numbers, etc. Please see the notebook tutorials for more examples.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The problem types include many other features, including the ability to define mass matrices and hold callbacks for events. Each problem type has a page which details its constructor and the available fields. For ODEs, the appropriate page is here. In addition, a user can specify additional functions to be associated with the function in order to speed up the solvers. These are detailed at the performance overloads page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Step-2:-Solving-a-Problem","page":"Ordinary Differential Equations","title":"Step 2: Solving a Problem","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Controlling-the-Solvers","page":"Ordinary Differential Equations","title":"Controlling the Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"After defining a problem, you solve it using solve.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The solvers can be controlled using the available options are described on the Common Solver Options manual page. For example, we can lower the relative tolerance (in order to get a more correct result, at the cost of more timesteps) by using the command reltol:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"There are many controls for handling outputs. For example, we can choose to have the solver save every 0.1 time points by setting saveat=0.1. Chaining this with the tolerance choice looks like:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6,saveat=0.1)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"More generally, saveat can be any collection of time points to save at. Note that this uses interpolations to keep the timestep unconstrained to speed up the solution. In addition, if we only care about the endpoint, we can turn off intermediate saving in general:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6,save_everystep=false)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"which will only save the final time point.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Choosing-a-Solver-Algorithm","page":"Ordinary Differential Equations","title":"Choosing a Solver Algorithm","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"DifferentialEquations.jl has a method for choosing the default solver algorithm which will find an efficient method to solve your problem. To help users receive the right algorithm, DifferentialEquations.jl offers a method for choosing algorithms through hints. This default chooser utilizes the precisions of the number types and the keyword arguments (such as the tolerances) to select an algorithm. Additionally one can provide alg_hints to help choose good defaults using properties of the problem and necessary features for the solution. For example, if we have a stiff problem where we need high accuracy, but don't know the best stiff algorithm for this problem, we can use:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,alg_hints=[:stiff],reltol=1e-8,abstol=1e-8)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"You can also explicitly choose the algorithm to use. DifferentialEquations.jl offers a much wider variety of solver algorithms than traditional differential equations libraries. Many of these algorithms are from recent research and have been shown to be more efficient than the \"standard\" algorithms. For example, we can choose a 5th order Tsitouras method:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,Tsit5())","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the solver controls can be combined with the algorithm choice. Thus we can for example solve the problem using Tsit5() with a lower tolerance via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,Tsit5(),reltol=1e-8,abstol=1e-8)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In DifferentialEquations.jl, some good \"go-to\" choices for ODEs are:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"AutoTsit5(Rosenbrock23()) handles both stiff and non-stiff equations. This is a good algorithm to use if you know nothing about the equation.\nAutoVern7(Rodas5()) handles both stiff and non-stiff equations in a way that's efficient for high accuracy.\nTsit5() for standard non-stiff. This is the first algorithm to try in most cases.\nBS3() for fast low accuracy non-stiff.\nVern7() for high accuracy non-stiff.\nRodas4() or Rodas5() for small stiff equations with Julia-defined types, events, etc.\nKenCarp4() or TRBDF2() for medium sized (100-2000 ODEs) stiff equations\nRadauIIA5() for really high accuracy stiff equations\nQNDF() for large stiff equations","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For a comprehensive list of the available algorithms and detailed recommendations, Please see the solver documentation. Every problem type has an associated page detailing all of the solvers associated with the problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Step-3:-Analyzing-the-Solution","page":"Ordinary Differential Equations","title":"Step 3: Analyzing the Solution","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Handling-the-Solution-Type","page":"Ordinary Differential Equations","title":"Handling the Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The result of solve is a solution object. We can access the 5th value of the solution with:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"julia> sol[5]\n0.637","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"or get the time of the 8th timestep by:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"julia> sol.t[8]\n0.438","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Convenience features are also included. We can build an array using a comprehension over the solution tuples via:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"[t+u for (u,t) in tuples(sol)]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"or more generally","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"[t+2u for (u,t) in zip(sol.u,sol.t)]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"allows one to use more parts of the solution type. The object that is returned by default acts as a continuous solution via an interpolation. We can access the interpolated values by treating sol as a function, for example:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol(0.45) # The value of the solution at t=0.45","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note the difference between these: indexing with [i] is the value at the ith step, while (t) is an interpolation at time t!","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"If in the solver dense=true (this is the default unless saveat is used), then this interpolation is a high order interpolation and thus usually matches the error of the solution time points. The interpolations associated with each solver is detailed at the solver algorithm page. If dense=false (unless specifically set, this only occurs when save_everystep=false or saveat is used) then this defaults to giving a linear interpolation.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For more details on handling the output, see the solution handling page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Plotting-Solutions","page":"Ordinary Differential Equations","title":"Plotting Solutions","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"While one can directly plot solution time points using the tools given above, convenience commands are defined by recipes for Plots.jl. To plot the solution object, simply call plot:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"#]add Plots # You need to install Plots.jl before your first time using it!\nusing Plots\n#plotly() # You can optionally choose a plotting backend\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ode_tutorial_linear_plot)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"If you are in Juno, this will plot to the plot pane. To open an interactive GUI (dependent on the backend), use the gui command:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"gui()","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The plot function can be formatted using the attributes available in Plots.jl. Additional DiffEq-specific controls are documented at the plotting page.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For example, from the Plots.jl attribute page we see that the line width can be set via the argument linewidth. Additionally, a title can be set with title. Thus we add these to our plot command to get the correct output, fix up some axis labels, and change the legend (note we can disable the legend with legend=false) to get a nice looking plot:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,linewidth=5,title=\"Solution to the linear ODE with a thick line\",\n     xaxis=\"Time (t)\",yaxis=\"u(t) (in μm)\",label=\"My Thick Line!\") # legend=false","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can then add to the plot using the plot! command:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot!(sol.t,t->0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ode_tutorial_thick_linear)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Example-2:-Solving-Systems-of-Equations","page":"Ordinary Differential Equations","title":"Example 2: Solving Systems of Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In this example we will solve the Lorenz equations:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"beginaligned\nfracdxdt = σ(y-x) \nfracdydt = x(ρ-z) - y \nfracdzdt = xy - βz \nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Defining your ODE function to be in-place updating can have performance benefits. What this means is that, instead of writing a function which outputs its solution, you write a function which updates a vector that is designated to hold the solution. By doing this, DifferentialEquations.jl's solver packages are able to reduce the amount of array allocations and achieve better performance.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The way we do this is we simply write the output to the 1st input of the function. For example, our Lorenz equation problem would be defined by the function:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"and then we can use this function in a problem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"u0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Using the plot recipe tools defined on the plotting page, we can choose to do a 3D phase space plot between the different variables:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,vars=(1,2,3))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Lorenz System)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the default plot for multi-dimensional systems is an overlay of each timeseries. We can plot the timeseries of just the second component using the variable choices interface once more:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,vars=(0,2))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Lorenz Timeseries)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that here \"variable 0\" corresponds to the independent variable (\"time\").","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Defining-Parameterized-Functions","page":"Ordinary Differential Equations","title":"Defining Parameterized Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In many cases you may want to explicitly have parameters associated with your differential equations. This can be used by things like parameter estimation routines. In this case, you use the p values via the syntax:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function parameterized_lorenz!(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"and then we add the parameters to the ODEProblem:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"u0 = [1.0,0.0,0.0]\ntspan = (0.0,1.0)\np = [10.0,28.0,8/3]\nprob = ODEProblem(parameterized_lorenz!,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can make our functions look nicer by doing a few tricks. For example:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function parameterized_lorenz!(du,u,p,t)\n  x,y,z = u\n  σ,ρ,β = p\n  du[1] = dx = σ*(y-x)\n  du[2] = dy = x*(ρ-z) - y\n  du[3] = dz = x*y - β*z\nend","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the type for the parameters p can be anything: you can use arrays, static arrays, named tuples, etc. to enclose your parameters in a way that is sensible for your problem.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Since the parameters exist within the function, functions defined in this manner can also be used for sensitivity analysis, parameter estimation routines, and bifurcation plotting. This makes DifferentialEquations.jl a full-stop solution for differential equation analysis which also achieves high performance.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Example-3:-Solving-Nonhomogeneous-Equations-using-Parameterized-Functions","page":"Ordinary Differential Equations","title":"Example 3: Solving Nonhomogeneous Equations using Parameterized Functions","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Parameterized functions can also be used for building nonhomogeneous ordinary differential equations (these are also referred to as ODEs with nonzero right-hand sides). They are frequently used as models for dynamical systems with external (in general time-varying) inputs. As an example, consider a model of a pendulum consisting of a slender rod of length l and mass m:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"beginaligned\nfracmathrmdtheta(t)mathrmdt = omega(t)\nfracmathrmdomega(t)mathrmdt = - frac32fracglsintheta(t) + frac3ml^2M(t)\nendaligned","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"where θ and ω are the angular deviation of the pendulum from the vertical (hanging) orientation and the angular rate, respectively, M is an external torque (developed, say, by a wind or a motor), and finally, g stands for gravitational acceleration.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nusing Plots\n\nl = 1.0                             # length [m]\nm = 1.0                             # mass [kg]\ng = 9.81                            # gravitational acceleration [m/s²]\n\nfunction pendulum!(du,u,p,t)\n    du[1] = u[2]                    # θ'(t) = ω(t)\n    du[2] = -3g/(2l)*sin(u[1]) + 3/(m*l^2)*p(t) # ω'(t) = -3g/(2l) sin θ(t) + 3/(ml^2)M(t)\nend\n\nθ₀ = 0.01                           # initial angular deflection [rad]\nω₀ = 0.0                            # initial angular velocity [rad/s]\nu₀ = [θ₀, ω₀]                       # initial state vector\ntspan = (0.0,10.0)                  # time interval\n\nM = t->0.1sin(t)                    # external torque [Nm]\n\nprob = ODEProblem(pendulum!,u₀,tspan,M)\nsol = solve(prob)\n\nplot(sol,linewidth=2,xaxis=\"t\",label=[\"θ [rad]\" \"ω [rad/s]\"],layout=(2,1))","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Pendulum response)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note how the external time-varying torque M is introduced as a parameter in the pendulum! function. Indeed, as a general principle the parameters can be any type; here we specify M as time-varying by representing it by a function, which is expressed by appending the dependence on time (t) to the name of the parameter.  ","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note also that, in contrast with the time-varying parameter, the (vector of) state variables u, which is generally also time-varying, is always used without the explicit dependence on time (t).","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#ode_other_types","page":"Ordinary Differential Equations","title":"Example 4: Using Other Types for Systems of Equations","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"DifferentialEquations.jl can handle many different dependent variable types (generally, anything with a linear index should work!). So instead of solving a vector equation, let's let u be a matrix! To do this, we simply need to have u0 be a matrix, and define f such that it takes in a matrix and outputs a matrix. We can define a matrix of linear ODEs as follows:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"A  = [1. 0  0 -5\n      4 -2  4 -3\n     -4  0  0  1\n      5 -2  2  3]\nu0 = rand(4,2)\ntspan = (0.0,1.0)\nf(u,p,t) = A*u\nprob = ODEProblem(f,u0,tspan)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Here our ODE is on a 4x2 matrix, and the ODE is the linear system defined by multiplication by A. To solve the ODE, we do the same steps as before.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob)\nplot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ODE System Solution)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can instead use the in-place form by using Julia's in-place matrix multiplication function mul!:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using LinearAlgebra\nf(du,u,p,t) = mul!(du,A,u)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Additionally, we can use non-traditional array types as well. For example, StaticArrays.jl offers immutable arrays which are stack-allocated, meaning that their usage does not require any (slow) heap-allocations that arrays normally have. This means that they can be used to solve the same problem as above, with the only change being the type for the initial condition and constants:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using StaticArrays, DifferentialEquations\nA  = @SMatrix [ 1.0  0.0 0.0 -5.0\n                4.0 -2.0 4.0 -3.0\n               -4.0  0.0 0.0  1.0\n                5.0 -2.0 2.0  3.0]\nu0 = @SMatrix rand(4,2)\ntspan = (0.0,1.0)\nf(u,p,t) = A*u\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob)\nusing Plots; plot(sol)","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the analysis tools generalize over to systems of equations as well.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[4]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"still returns the solution at the fourth timestep. It also indexes into the array as well. The last value is the timestep, and the beginning values are for the component. This means","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[5,3]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"is the value of the 5th component (by linear indexing) at the 3rd timepoint, or","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[2,1,:]","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"is the timeseries for the component which is the 2nd row and 1 column.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Going-Beyond-ODEs:-How-to-Use-the-Documentation","page":"Ordinary Differential Equations","title":"Going Beyond ODEs: How to Use the Documentation","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Not everything can be covered in the tutorials. Instead, this tutorial will end by pointing you in the directions for the next steps.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Common-API-for-Defining,-Solving,-and-Plotting","page":"Ordinary Differential Equations","title":"Common API for Defining, Solving, and Plotting","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"One feature of DifferentialEquations.jl is that this pattern for solving equations is conserved across the different types of differential equations. Every equation has a problem type, a solution type, and the same solution handling (+ plotting) setup. Thus the solver and plotting commands in the Basics section applies to all sorts of equations, like stochastic differential equations and delay differential equations. Each of these different problem types are defined in the Problem Types section of the docs. Every associated solver algorithm is detailed in the Solver Algorithms section, sorted by problem type. The same steps for ODEs can then be used for the analysis of the solution.","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/#Additional-Features-and-Analysis-Tools","page":"Ordinary Differential Equations","title":"Additional Features and Analysis Tools","text":"","category":"section"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In many cases, the common workflow only starts with solving the differential equation. Many common setups have built-in solutions in DifferentialEquations.jl. For example, check out the features for:","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Handling, parallelizing, and analyzing large Ensemble experiments\nSaving the output to tabular formats like DataFrames and CSVs\nEvent handling\nParameter estimation (inverse problems)\nQuantification of numerical uncertainty and error","category":"page"},{"location":"modules/DiffEqDocs/tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Many more are defined in the relevant sections of the docs. Please explore the rest of the documentation, including tutorials for getting started with other types of equations. In addition, to get help, please either file an issue at the main repository or come have an informal discussion at our Gitter chatroom.","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ODESystem","page":"ODESystem","title":"ODESystem","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/#System-Constructors","page":"ODESystem","title":"System Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"ODESystem","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ModelingToolkit.ODESystem","page":"ODESystem","title":"ModelingToolkit.ODESystem","text":"struct ODESystem <: ModelingToolkit.AbstractODESystem\n\nA system of ordinary differential equations.\n\nFields\n\neqs\nThe ODEs defining the system.\niv\nIndependent variable.\nstates\nDependent (state) variables. Must not contain the independent variable.\nN.B.: If tornmatching !== nothing, this includes all variables. Actual ODE states are determined by the SelectedState() entries in `tornmatching`.\n\nps\nParameter variables. Must not contain the independent variable.\nvar_to_name\nArray variables.\nctrls\nControl parameters (some subset of ps).\nobserved\nObserved states.\ntgrad\nTime-derivative matrix. Note: this field will not be defined until calculate_tgrad is called on the system.\n\njac\nJacobian matrix. Note: this field will not be defined until calculate_jacobian is called on the system.\n\nctrl_jac\nControl Jacobian matrix. Note: this field will not be defined until calculate_control_jacobian is called on the system.\n\nWfact\nWfact matrix. Note: this field will not be defined until generate_factorized_W is called on the system.\n\nWfact_t\nWfact_t matrix. Note: this field will not be defined until generate_factorized_W is called on the system.\n\nname\nName: the name of the system\n\nsystems\nsystems: The internal systems. These are required to have unique names.\n\ndefaults\ndefaults: The default values to use when initial conditions and/or parameters are not supplied in ODEProblem.\n\ntorn_matching\ntorn_matching: Tearing result specifying how to solve the system.\n\nconnector_type\nconnector_type: type of the system\n\nconnections\nconnections: connections in a system\n\npreface\npreface: inject assignment statements before the evaluation of the RHS function.\n\ncontinuous_events\ncontinuous_events: A Vector{SymbolicContinuousCallback} that model events. The integrator will use root finding to guarantee that it steps at each zero crossing.\n\ntearing_state\ntearing_state: cache for intermediate tearing state\n\nsubstitutions\nsubstitutions: substitutions generated by tearing.\n\nExample\n\nusing ModelingToolkit\n\n@parameters σ ρ β\n@variables t x(t) y(t) z(t)\nD = Differential(t)\n\neqs = [D(x) ~ σ*(y-x),\n       D(y) ~ x*(ρ-z)-y,\n       D(z) ~ x*y - β*z]\n\n@named de = ODESystem(eqs,t,[x,y,z],[σ,ρ,β])\n\n\n\n\n\n","category":"type"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Composition-and-Accessor-Functions","page":"ODESystem","title":"Composition and Accessor Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"get_eqs(sys) or equations(sys): The equations that define the ODE.\nget_states(sys) or states(sys): The set of states in the ODE.\nget_ps(sys) or parameters(sys): The parameters of the ODE.\nget_iv(sys): The independent variable of the ODE.","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Transformations","page":"ODESystem","title":"Transformations","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"structural_simplify\r\node_order_lowering\r\ndae_index_lowering\r\nliouville_transform\r\nalias_elimination\r\ntearing","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ModelingToolkit.ode_order_lowering","page":"ODESystem","title":"ModelingToolkit.ode_order_lowering","text":"ode_order_lowering(sys::ODESystem) -> Any\n\n\nTakes a Nth order ODESystem and returns a new ODESystem written in first order form by defining new variables which represent the N-1 derivatives.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ModelingToolkit.StructuralTransformations.dae_index_lowering","page":"ODESystem","title":"ModelingToolkit.StructuralTransformations.dae_index_lowering","text":"dae_index_lowering(sys::ODESystem; kwargs...) -> ODESystem\n\nPerform the Pantelides algorithm to transform a higher index DAE to an index 1 DAE. kwargs are forwarded to pantelides!. End users are encouraged to call structural_simplify instead, which calls this function internally.\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ModelingToolkit.liouville_transform","page":"ODESystem","title":"ModelingToolkit.liouville_transform","text":"liouville_transform(sys::ModelingToolkit.AbstractODESystem)\n\n\nGenerates the Liouville transformed set of ODEs, which is the original ODE system with a new variable trJ appended, corresponding to the -tr(Jacobian). This variable is used for properties like uncertainty propagation from a given initial distribution density.\n\nFor example, if u=p*u and p follows a probability distribution f(p), then the probability density of a future value with a given choice of p is computed by setting the inital trJ = f(p), and the final value of trJ is the probability of u(t).\n\nExample:\n\nusing ModelingToolkit, OrdinaryDiffEq, Test\n\n@parameters t α β γ δ\n@variables x(t) y(t)\nD = Differential(t)\n\neqs = [D(x) ~ α*x - β*x*y,\n       D(y) ~ -δ*y + γ*x*y]\n\nsys = ODESystem(eqs)\nsys2 = liouville_transform(sys)\n@variables trJ\n\nu0 = [x => 1.0,\n      y => 1.0,\n      trJ => 1.0]\n\nprob = ODEProblem(sys2,u0,tspan,p)\nsol = solve(prob,Tsit5())\n\nWhere sol[3,:] is the evolution of trJ over time.\n\nSources:\n\nProbabilistic Robustness Analysis of F-16 Controller Performance: An Optimal Transport Approach\n\nAbhishek Halder, Kooktae Lee, and Raktim Bhattacharya https://abhishekhalder.bitbucket.io/F16ACC2013Final.pdf\n\n\n\n\n\n","category":"function"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Analyses","page":"ODESystem","title":"Analyses","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"ModelingToolkit.islinear\r\nModelingToolkit.isautonomous\r\nModelingToolkit.isaffine","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Applicable-Calculation-and-Generation-Functions","page":"ODESystem","title":"Applicable Calculation and Generation Functions","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"calculate_jacobian\r\ncalculate_tgrad\r\ncalculate_factorized_W\r\ngenerate_jacobian\r\ngenerate_tgrad\r\ngenerate_factorized_W\r\njacobian_sparsity","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Standard-Problem-Constructors","page":"ODESystem","title":"Standard Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"ODEFunction\r\nODEProblem\r\nSteadyStateFunction\r\nSteadyStateProblem","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#Torn-Problem-Constructors","page":"ODESystem","title":"Torn Problem Constructors","text":"","category":"section"},{"location":"modules/ModelingToolkit/systems/ODESystem/","page":"ODESystem","title":"ODESystem","text":"ODAEProblem","category":"page"},{"location":"modules/ModelingToolkit/systems/ODESystem/#ModelingToolkit.StructuralTransformations.ODAEProblem","page":"ODESystem","title":"ModelingToolkit.StructuralTransformations.ODAEProblem","text":"ODAEProblem{iip}(sys, u0map, tspan, parammap = DiffEqBase.NullParameters(); kw...)\n\nThis constructor acts similar to the one for ODEProblem with the following changes: ODESystems can sometimes be further reduced if structural_simplify has already been applied to them. In these cases, the constructor uses the knowledge of the strongly connected components calculated during the process of simplification as the basis for building pre-simplified nonlinear systems in the implicit solving. In summary: these problems are structurally modified, but could be more efficient and more stable. Note, the returned object is still of type ODEProblem.\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/models/physical/#Physical-Models","page":"Physical Models","title":"Physical Models","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"The physical modeling functionality is provided by DiffEqPhysics.jl and helps the user build and solve the differential equation based physical models.","category":"page"},{"location":"modules/DiffEqDocs/models/physical/#Hamiltonian-Problems","page":"Physical Models","title":"Hamiltonian Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"ODEs defined by Hamiltonians is described in the Dynamical ODEs section.","category":"page"},{"location":"modules/DiffEqDocs/models/physical/#N-Body-Problems","page":"Physical Models","title":"N-Body Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"N-Body problems can be solved by the implementation provided by NBodySimulator.jl using a defined potential:","category":"page"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"nprob = NBodyProblem(f, mass, vel, pos, tspan)","category":"page"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"where f is the potential function, mass is the mass matrix, pos and vel are ArrayPartitions for the intial positions and velocity, and tspan is the timespan to solve on.","category":"page"},{"location":"modules/DiffEqDocs/models/physical/#Example","page":"Physical Models","title":"Example","text":"","category":"section"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"In this example we will model the outer solar system planets.","category":"page"},{"location":"modules/DiffEqDocs/models/physical/","page":"Physical Models","title":"Physical Models","text":"using NBodySimulator\nG = 2.95912208286e-4\nM = [1.00000597682, 0.000954786104043, 0.000285583733151, 0.0000437273164546, 0.0000517759138449, 1/1.3e8]\ninvM = inv.(M)\nplanets = [\"Sun\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\"]\n\npos_x = [0.0,-3.5023653,9.0755314,8.3101420,11.4707666,-15.5387357]\npos_y = [0.0,-3.8169847,-3.0458353,-16.2901086,-25.7294829,-25.2225594]\npos_z = [0.0,-1.5507963,-1.6483708,-7.2521278,-10.8169456,-3.1902382]\npos = ArrayPartition(pos_x,pos_y,pos_z)\n\nvel_x = [0.0,0.00565429,0.00168318,0.00354178,0.00288930,0.00276725]\nvel_y = [0.0,-0.00412490,0.00483525,0.00137102,0.00114527,-0.00170702]\nvel_z = [0.0,-0.00190589,0.00192462,0.00055029,0.00039677,-0.00136504]\nvel = ArrayPartition(vel_x,vel_y,vel_z)\n\ntspan = (0.,200_000)\n\nconst ∑ = sum\nconst N = 6\npotential(p, t, x, y, z, M) = -G*∑(i->∑(j->(M[i]*M[j])/sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2 + (z[i]-z[j])^2), 1:i-1), 2:N)\nnprob = NBodyProblem(potential, M, vel, pos, tspan)\nsol = solve(nprob,Yoshida6(), dt=100)","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/#Using-GPUs-to-train-Physics-Informed-Neural-Networks-(PINNs)","page":"Using GPUs","title":"Using GPUs to train Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"the 2-dimensional PDE:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"_t u(x y t) = ^2_x u(x y t) + ^2_y u(x y t)  ","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with the initial and boundary conditions:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"beginalign*\nu(x y 0) = e^x+y cos(x + y)       \nu(0 y t) = e^y   cos(y + 4t)      \nu(2 y t) = e^2+y cos(2 + y + 4t)  \nu(x 0 t) = e^x   cos(x + 4t)      \nu(x 2 t) = e^x+2 cos(x + 2 + 4t)  \nendalign*","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"on the space and time domain:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"x in 0 2   y in 0 2    t in 0 2  ","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with physics-informed neural networks. The only major difference from the CPU case is that we must ensure that our initial parameters for the neural network are on the GPU. If that is done, then the internal computations will all take place on the GPU. This is done by using the gpu function on the Flux.Chain, like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using CUDA\nchain = Chain(Dense(3,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,1)) |> gpu","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"In total, this looks like:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using NeuralPDE, Flux, CUDA\nusing Optimization, OptimizationOptimJL, OptimizationOptimsiers\nimport ModelingToolkit: Interval\n\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min= 0.\nt_max = 2.0\nx_min = 0.\nx_max = 2.\ny_min = 0.\ny_max = 2.\n\n# 2D PDE\neq  = Dt(u(t,x,y)) ~ Dxx(u(t,x,y)) + Dyy(u(t,x,y))\n\nanalytic_sol_func(t,x,y) = exp(x+y)*cos(x+y+4t)\n# Initial and boundary conditions\nbcs = [u(t_min,x,y) ~ analytic_sol_func(t_min,x,y),\n       u(t,x_min,y) ~ analytic_sol_func(t,x_min,y),\n       u(t,x_max,y) ~ analytic_sol_func(t,x_max,y),\n       u(t,x,y_min) ~ analytic_sol_func(t,x,y_min),\n       u(t,x,y_max) ~ analytic_sol_func(t,x,y_max)]\n\n# Space and time domains\ndomains = [t ∈ Interval(t_min,t_max),\n           x ∈ Interval(x_min,x_max),\n           y ∈ Interval(y_min,y_max)]\n\n# Neural network\ninner = 25\nchain = Chain(Dense(3,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,inner,Flux.σ),\n              Dense(inner,1)) |> gpu\n\nstrategy = GridTraining(0.05)\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy)\n\n@named pde_system = PDESystem(eq,bcs,domains,[t,x,y],[u(t, x, y)])\nprob = discretize(pde_system,discretization)\nsymprob = symbolic_discretize(pde_system,discretization)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob,Adam(0.01);callback = callback,maxiters=2500)","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"We then use the remake function allows to rebuild the PDE problem to start a new optimization at the optimized parameters, and continue with a lower learning rate:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"prob = remake(prob,u0=res.u)\nres = Optimization.solve(prob,Adam(0.001);callback = callback,maxiters=2500)","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Finally we inspect the solution:","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"phi = discretization.phi\nts,xs,ys = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]\nu_real = [analytic_sol_func(t,x,y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi([t, x, y], res.u))) for t in ts for x in xs for y in ys]\n\nusing Plots\nusing Printf\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t,x,y) for x in xs for y in ys], (length(xs),length(ys)))\n        u_predict = reshape([Array(phi([t, x, y], res.u))[1] for x in xs for y in ys], length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict, t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real,st=:surface, label=\"\", title=title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st=:contourf,label=\"\", title=title)\n        plot(p1,p2,p3)\n    end\n    gif(anim,\"3pde.gif\", fps=10)\nend\n\nplot_(res)","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"(Image: 3pde)","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/#Performance-benchmarks","page":"Using GPUs","title":"Performance benchmarks","text":"","category":"section"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Here are some performance benchmarks for 2d-pde with various number of input points and the number of neurons in the hidden layer, measuring the time for 100 iterations. Сomparing runtime with GPU and CPU.","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"\njulia> CUDA.device()\nCuDevice(0): Tesla P100-PCIE-16GB\n","category":"page"},{"location":"modules/NeuralPDE/tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"(Image: image)","category":"page"},{"location":"modules/NeuralPDE/examples/heterogeneous/#PDEs-with-Dependent-Variables-on-Heterogeneous-Domains","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"","category":"section"},{"location":"modules/NeuralPDE/examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"A differential equation is said to have heterogeneous domains when its dependent variables depend on different independent variables:","category":"page"},{"location":"modules/NeuralPDE/examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"u(x) + w(x v) = fracpartial w(x v)partial w","category":"page"},{"location":"modules/NeuralPDE/examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"Here, we write an arbitrary heterogeneous system:","category":"page"},{"location":"modules/NeuralPDE/examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables p(..) q(..) r(..) s(..)\nDx = Differential(x)\nDy = Differential(y)\n\n# 2D PDE\neq  = p(x) + q(y) + Dx(r(x, y)) + Dy(s(y, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [p(1) ~ 0.f0, q(-1) ~ 0.0f0,\n       r(x, -1) ~ 0.f0, r(1, y) ~ 0.0f0,\n       s(y, 1) ~ 0.0f0, s(-1, x) ~ 0.0f0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n           y ∈ Interval(0.0, 1.0)]\n\nnumhid = 3\nchains = [[Lux.Chain(Dense(1, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ), Dense(numhid, 1)) for i in 1:2];\n                        [Lux.Chain(Dense(2, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ), Dense(numhid, 1)) for i in 1:2]]\ndiscretization = NeuralPDE.PhysicsInformedNN(chains, QuadratureTraining())\n\n@named pde_system = PDESystem(eq, bcs, domains, [x,y], [p(x), q(y), r(x, y), s(y, x)])\nprob = SciMLBase.discretize(pde_system, discretization)\n\ncallback = function (p,l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters=100)","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Notes-on-Algorithms","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"This page is a supplemental page which details some facts about the chosen algorithms, why some I took the time to make optimized versions for, and for others why they were ignored.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Explicit-Runge-Kutta-ODE-Algorithms","page":"Notes on Algorithms","title":"Explicit Runge-Kutta ODE Algorithms","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"From what I can tell, this is by far the most comprehensive comparison of Explicit Runge-Kutta ODE algorithms that you'll find.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Implementations","page":"Notes on Algorithms","title":"Implementations","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The different implementations have been benchmarked against each other. The efficiency was calculated by weighing both the time and error on classic test problems. To make clear distinctions, solver options were tweaked to many different settings, including:","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Matching errors\nMatching runtimes\nMatching settings\nLow/High tolerance","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The DifferentialEquations.jl implementations of the explicit Runge-Kutta solvers are by a good margin the most efficient implementations of the given algorithms. They utilize many extra tricks, nice caching, and threading if available, to vastly outperform the other methods in terms of efficiency (even with threading disabled). :DP5 performs much better than :dopri5, which vastly outperform ode45 (whose stepsize algorithm tends to have issues on some quasi-stiff problems). :DP8 performs better than dop853 in some cases, worse in others. Both vastly outperform ode78.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"For this reason, the DifferentialEquations.jl non-stiff algorithms are the recommended implementations. ODEInterface non-stiff algorithms are only recommended for historical purposes (i.e. to match previous results) or to try dop853 on a problem (work is being to find out what the difference is and squash the competition here!). The ODE.jl algorithms are not recommended for any serious use (the package is essentially deprecated: it's slow, gets high error, the timestepping algorithm is not robust, and doesn't implement many methods).","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-4","page":"Notes on Algorithms","title":"Order 4-","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"At this stage, coefficient of the truncation error seems to win out, or you are willing to live with low tolerance anyways. Thus Bogacki-Shampine is the clear winner in this category because at order 2/3 with FASL it has minimal numbers of function evaluations but also is stable enough to step as needed. All other methods don't compare because of the FASL property boosting the order and thus the stability (for low orders, it pretty much holds that higher order = higher stability (for optimal number of steps), which is not true as we go higher), making it more stable and have less error for lower numbers of function evaluations than the others in this category.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-5","page":"Notes on Algorithms","title":"Order 5","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"[Note that for all of these Peter Stone's modifications do not seem to be helpful since, although they lower the truncation error, they also modify the stability region in ways that can be worrisome (mostly they shrink the stability in the complex axis near the origin, making the problems not as suitable for a \"general purpose default\" like one would hope with a 4/5 solver)]","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The \"clear choice\" is the Dormand-Prince 4/5 pair. This is the pair which is used by default as ode45 in MATLAB, and serves similar functions in scipy, ODE.jl, etc. The golden standard implementation is Hairer's DOPRI5 (offered by ODEInterface.jl). After optimizations, DifferentialEquations.jl's native DP5 solver is much more efficient (between 4x-400x) than DOPRI5's, with various design choices factoring into this (which are documented in the benchmarks). This is pre-threading, and within method threading will likely be at least doubled or tripled when threading is enabled. Thus it's clear that the reference implementation to try other methods against is the DifferentialEquations.jl DP5 method.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"It's obvious that anything before Dormand-Prince 4/5's pair is simply not as good because of the optimizations on the local truncation error coefficient and the fact that FASL schemes essentially have one less function evaluation. So the previous algorithms were implemented as tableaus for the historical reasons but dealt with no further. These methods include the Runge, Cassity, Butcher, Fehlburg, Lawson, Luther and Konen, and Kutta schemes.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The next set of schemes are the Papakostas-Papageorgiou schemes. The problem is that they don't really get the much lower on the error than DP5, but also have wacky stability near the origin.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Tsitouras's looks to be a good match against DP5 as a 6-stage scheme to take on DP5. Its stability is similar to DP5 but its first error term is an order of magnitude smaller. More tests will likely determine that this is much better than DP5 in accordance with his paper.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Lastly, there are the 7-stage schemes. The more recent one is due to Sharp and Smart, but I am ignoring this because its error term is almost an order of magnitude larger than the BS pair, and its stability region is wonky near the origin. Its only plus over the BS pair is that it has a slightly larger stability in the real axis, which is not important when paired with adaptive stepping and for use on non-stiff problems.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"That leaves us with the Bogacki-Shampine pair. This pair gets more than an order of magnitude lower truncation error, enhanced complex stability, and two error estimators to make it more robust. In fact, this is the default which is chosen in Mathematica. Its downside is that since it is an 8-stage scheme, it requires an additional function evaluation.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Further tests will likely narrow this down to Tsitouras vs Bogacki-Shampine. Who will come out on top? Who knows.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-6","page":"Notes on Algorithms","title":"Order 6","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Sharp-Verner has bad complex stability near the origin. I don't like any of the Peter Stone modifications here. Butcher and Chummund methods have stability issues near the origin as well. Huta's method has too high of an error coefficient. Verner's 1991 has bad complex stability. Same as the most robust. The Verner \"most efficient\" has really good stability and error coefficient. In fact, nothing is even close except for Tsitouras' method. The DP method is two orders of magnitude higher in error coefficient than Verner. The Luther methods have too much error. Same as Tsitouras-Papakostas and  M. Tanaka, K. Kasuga, S. Yamashita and H. Yazaki.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Without a doubt the winner is the Verner \"most efficient\".","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-7","page":"Notes on Algorithms","title":"Order 7","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The Enright-Verner and other Verner methods all have stability issues near the origin in the complex plane and higher error coefficients. Sharp and Smart have higher error coefficients. Peter Stone's methods all have higher error. It's very clear that the best here is the Tanaka-Yamashita (efficient, not the stable) method by far.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-8","page":"Notes on Algorithms","title":"Order 8","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The Cooper-Verner methods do not have an error estimate and so no adaptive timestepping can be done. This is a deal-breaker. Going into this one would think that the clear winner would be Dormand-Prince 8. But that's not the case. However, that's comparing the classical 1981 DP87. Notice that the code for Dop853 is based off of the 1989 paper which has different coefficients (and currently I have no analysis for this).","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The other methods include Verner's Maple dverk78 which is bested in both stability and error coefficient by Enright-Verner which is bested by Tsitouras-Papakostas.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Thus the final showdown is between DP853 vs the Tsitouras-Papakostas pair.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-9","page":"Notes on Algorithms","title":"Order 9","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The Tsitouras scheme and the Sharp scheme have funky stability near the origin. Verner's schemes are much safer, and with similar error. They clearly dominate this category.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-10","page":"Notes on Algorithms","title":"Order 10","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Curtis' scheme has more function evaluations than needed, and Peter Stone's modification reduces the truncation error by a lot but adds three more function evaluations. Thus Hairer's 17 stage scheme (whose error and stability is similar to Curtis') is clearly better. Once again Peter Stone's modification adds three steps but does not reduce the truncation error here, so the unmodified version does better.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Tom Baker's method increases the stability region to something which is more than necessary but adds 4 function evaluations to do so (without lowering the error very much). Ono's scheme minimizes the error more than Hairer's here, with all else being basically the same. The Peter Stone methods add a lot of function evaluations (5+) and so they would only be useful in the case where the function evaluations are quick yet you still want really small error. Even then I'm not convinced they are better than the other methods, or better than the higher order methods which use less steps. The stability is only okay.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The Feagin scheme is fine, but with more error and less stability than the Hairer scheme. Thus it seems clear that Hairer's method dominates this category. However, that's only because it does not include an error estimate. Feagin's scheme is close in error and stability, but includes an error estimate which can be used for adaptivity, making it the choice in this category.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-11","page":"Notes on Algorithms","title":"Order 11","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"The order 11 schemes are due to Tom Baker at the University of Teeside. They have a nice sparsity pattern and receive slightly lower truncation error coefficents than the Feagin, but Feagin's dominates by being \"almost order 13\" anyways so while a nice try the order 11 scheme is likely overwhelmed in any case where it would be useful.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-12","page":"Notes on Algorithms","title":"Order 12","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"Here there are the Feagin schemes and Ono's scheme. Ono's scheme gets horrible stability with more error and so it's not in the running. Peter Stone's modifications do not make a substantive change, and where they do they get rid of the nice property that the Feagin 12 method satisfies many of the higher order conditions as well, making it look even higher order on some problems. Thus the standard Feagin 12 seems to win out in this category.","category":"page"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/#Order-14","page":"Notes on Algorithms","title":"Order 14","text":"","category":"section"},{"location":"modules/DiffEqDevDocs/internals/notes_on_algorithms/","page":"Notes on Algorithms","title":"Notes on Algorithms","text":"In this category there is just the Feagin. Peter Stone's modification barely changes anything in the analysis so I did not even attempt it.","category":"page"},{"location":"modules/SciMLSensitivity/bayesian/turing_bayesian/#Bayesian-Estimation-of-Differential-Equations-with-Probabilistic-Programming","page":"Bayesian Estimation of Differential Equations with Probabilistic Programming","title":"Bayesian Estimation of Differential Equations with Probabilistic Programming","text":"","category":"section"},{"location":"modules/SciMLSensitivity/bayesian/turing_bayesian/","page":"Bayesian Estimation of Differential Equations with Probabilistic Programming","title":"Bayesian Estimation of Differential Equations with Probabilistic Programming","text":"For a good overview of how to use the tools of SciML in conjunction with the Turing.jl probabilistic programming language, see the Bayesian Differential Equation Tutorial.","category":"page"},{"location":"modules/DiffEqFlux/utilities/MultipleShooting/#Multiple-Shooting-Functionality","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"","category":"section"},{"location":"modules/DiffEqFlux/utilities/MultipleShooting/","page":"Multiple Shooting Functionality","title":"Multiple Shooting Functionality","text":"multiple_shoot","category":"page"},{"location":"modules/DiffEqFlux/utilities/MultipleShooting/#DiffEqFlux.multiple_shoot","page":"Multiple Shooting Functionality","title":"DiffEqFlux.multiple_shoot","text":"Returns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nmultiple_shoot(p, ode_data, tsteps, prob, loss_function, solver, group_size;\n               continuity_term=100, kwargs...)\nmultiple_shoot(p, ode_data, tsteps, prob, loss_function, continuity_loss, solver, group_size;\n               continuity_term=100, kwargs...)\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data: Original Data to be modelled.\ntsteps: Timesteps on which ode_data was calculated.\nprob: ODE problem that the Neural Network attempts to solve.\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss   between them.   If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout different groups.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and   Common Solver Arguments   documentation for more details. Note: The parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\nReturns a total loss after trying a 'Direct multiple shooting' on ODE data and an array of predictions from each of the groups (smaller intervals). In Direct Multiple Shooting, the Neural Network divides the interval into smaller intervals and solves for them separately. The default continuity term is 100, implying any losses arising from the non-continuity of 2 different groups will be scaled by 100.\n\nmultiple_shoot(p, ode_data_ensemble, tsteps, ensemble_prob, ensemble_alg, loss_function, solver,\n                group_size; continuity_term=100, trajectories)\nmultiple_shoot(p, ode_data_ensemble, tsteps, ensemble_prob, ensemble_alg, loss_function,\n                continuity_loss, solver, group_size; continuity_term=100, trajectories)\n\nArguments:\n\np: The parameters of the Neural Network to be trained.\node_data_ensemble: Original Data to be modelled. Batches (or equivalently \"trajectories\") are located in the third dimension.\ntsteps: Timesteps on which ode_data_ensemble was calculated.\nensemble_prob: Ensemble problem that the Neural Network attempts to solve.\nensemble_alg: Ensemble algorithm, e.g. EnsembleThreads()\nloss_function: Any arbitrary function to calculate loss.\ncontinuity_loss: Function that takes states hatu_end of group k and\n\nu_0 of group k+1 as input and calculates prediction continuity loss   between them.   If no custom continuity_loss is specified, sum(abs, û_end - u_0) is used.\n\nsolver: ODE Solver algorithm.\ngroup_size: The group size achieved after splitting the ode_data into equal sizes.\ncontinuity_term: Weight term to ensure continuity of predictions throughout\n\ndifferent groups.\n\ntrajectories: number of trajectories for ensemble_prob.\nkwargs: Additional arguments splatted to the ODE solver. Refer to the\n\nLocal Sensitivity Analysis and   Common Solver Arguments   documentation for more details. Note: The parameter 'continuity_term' should be a relatively big number to enforce a large penalty whenever the last point of any group doesn't coincide with the first point of next group.\n\n\n\n\n\n","category":"function"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/#Handling-Exogenous-Input-Signals","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"The key to using exogeneous input signals is the same as in the rest of the SciML universe: just use the function in the definition of the differential equation. For example, if it's a standard differential equation, you can use the form","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"I(t) = t^2\n\nfunction f(du,u,p,t)\n  du[1] = I(t)\n  du[2] = u[1]\nend","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"so that I(t) is an exogenous input signal into f. Another form that could be useful is a closure. For example:","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"function f(du,u,p,t,I)\n  du[1] = I(t)\n  du[2] = u[1]\nend\n\n_f(du,u,p,t) = f(du,u,p,t,x -> x^2)","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"which encloses an extra argument into f so that _f is now the interface-compliant differential equation definition.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"Note that you can also learn what the exogenous equation is from data. For an example on how to do this, you can use the Optimal Control Example which shows how to parameterize a u(t) by a universal function and learn that from data.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/#Example-of-a-Neural-ODE-with-Exogenous-Input","page":"Handling Exogenous Input Signals","title":"Example of a Neural ODE with Exogenous Input","text":"","category":"section"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"In the following example, a discrete exogenous input signal ex is defined and used as an input into the neural network of a neural ODE system.","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"using DifferentialEquations, Lux, DiffEqFlux, Optimization, OptimizationPolyalgorithms, OptimizationFlux, Plots, Random\n\nrng = Random.default_rng()\ntspan = (0.1f0, Float32(10.0))\ntsteps = range(tspan[1], tspan[2], length = 100)\nt_vec = collect(tsteps)\nex = vec(ones(Float32,length(tsteps), 1))\nf(x) = (atan(8.0 * x - 4.0) + atan(4.0)) / (2.0 * atan(4.0))\n\nfunction hammerstein_system(u)\n    y= zeros(size(u))\n    for k in 2:length(u)\n        y[k] = 0.2 * f(u[k-1]) + 0.8 * y[k-1]\n    end\n    return y\nend\n\ny = Float32.(hammerstein_system(ex))\nplot(collect(tsteps), y, ticks=:native)\n\nnn_model = Lux.Chain(Lux.Dense(2,8, tanh), Lux.Dense(8, 1))\np_model,st = Lux.setup(rng, nn_model)\n\nu0 = Float32.([0.0])\n\nfunction dudt(u, p, t)\n    global st\n    #input_val = u_vals[Int(round(t*10)+1)]\n    out,st = nn_model(vcat(u[1], ex[Int(round(10*0.1))]), p, st)\n    return out\nend\n\nprob = ODEProblem(dudt,u0,tspan,nothing)\n\nfunction predict_neuralode(p)\n    _prob = remake(prob,p=p)\n    Array(solve(_prob, Tsit5(), saveat=tsteps, abstol = 1e-8, reltol = 1e-6))\nend\n\nfunction loss(p)\n    sol = predict_neuralode(p)\n    N = length(sol)\n    return sum(abs2.(y[1:N] .- sol'))/N\nend\n\nadtype = Optimization.AutoZygote()\noptf = Optimization.OptimizationFunction((x,p)->loss(x), adtype)\noptprob = Optimization.OptimizationProblem(optf, Lux.ComponentArray(p_model))\n\nres0 = Optimization.solve(optprob, PolyOpt(),maxiters=100)\n\nsol = predict_neuralode(res0.u)\nplot(tsteps,sol')\nN = length(sol)\nscatter!(tsteps,y[1:N])","category":"page"},{"location":"modules/SciMLSensitivity/ode_fitting/exogenous_input/","page":"Handling Exogenous Input Signals","title":"Handling Exogenous Input Signals","text":"(Image: )","category":"page"},{"location":"highlevels/inverse_problems/#parameter_estimation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"Parameter estimation for models and equations, also known as dynamic data analysis, solving the inverse problem, or Bayesian posterior estimation (when done probabilistically), is provided by the SciML tools for the equations in its set. In this introduction, we briefly present the relevant packages that facilitate parameter estimation, namely:","category":"page"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"SciMLSensitivity.jl\nDiffEqFlux.jl\nTuring.jl\nDataDrivenDiffEq.jl\nDiffEqParamEstim.jl\nDiffEqBayes.jl","category":"page"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"We also provide information regarding the respective strengths of these packages so that you can easily decide which one suits your needs best.","category":"page"},{"location":"highlevels/inverse_problems/#SciMLSensitivity.jl:-Local-Sensitivity-Analysis-and-Automatic-Differentiation-Support-for-Solvers","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"SciMLSensitivity.jl: Local Sensitivity Analysis and Automatic Differentiation Support for Solvers","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"SciMLSensitivity.jl is the system for local sensitivity analysis which all other inverse problem methods rely on. This package defines the interactions between the equation solvers and automatic differentiation, defining fast overloads for forward and adjoint (reverse) sensitivity analysis for fast gradient and Jacobian calculations with respect to model inputs. Its documentation covers how to use direct differentiation of equation solvers in conjunction with tools like Optimization.jl to perform model callibration of ODEs against data, PDE-constrained optimization, nonlinear optimal controls analysis, and much more. As a lower level tool, this library is very versitile, feature-rich, and high-performance, giving all of the tools required but not directly providing a higher level interface.","category":"page"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"note: Note\nSensitivity analysis is kept in a separate library from the solvers (SciMLSensitivity.jl), in order to not require all equation solvers to have a dependency on all automatic differentiation libraries. If automatic differentiation is applied to a solver library without importing SciMLSensitivity.jl, an error is thrown letting the user know to import SciMLSensitivity.jl for the functionality to exist.","category":"page"},{"location":"highlevels/inverse_problems/#DataDrivenDiffEq.jl:-Data-Driven-Modeling-and-Equation-Discovery","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"DataDrivenDiffEq.jl: Data-Driven Modeling and Equation Discovery","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"The distinguishing feature of this package is that its ultimate goal is to identify the differential equation model that generated the input data. Depending on the user's needs, the package can provide structural identification of a given differential equation (output in a symbolic form) or structural estimation (output as a function for prediction purposes).","category":"page"},{"location":"highlevels/inverse_problems/#DiffEqParamEstim.jl:-Simplified-Parameter-Estimation-Interface","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"DiffEqParamEstim.jl: Simplified Parameter Estimation Interface","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"This package is for simplified parameter estimation. While not as flexible of a system like DiffEqFlux.jl, it provides ready-made functions for doing standard optmization procedures like L2 fitting and MAP estimates. Among other features, it allows for the optimization of parameters in ODEs, stochastic problems, and delay differential equations.","category":"page"},{"location":"highlevels/inverse_problems/#DiffEqBayes.jl:-Simplified-Bayesian-Estimation-Interface","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"DiffEqBayes.jl: Simplified Bayesian Estimation Interface","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"As the name suggests, this package has been designed to provide the estimation of differential equations parameters by means of Bayesian methods. It works in conjunction with Turing.jl,  CmdStan.jl,  DynamicHMC.jl, and  ApproxBayes.jl. While not as flexible as direct usage of DiffEqFlux.jl or Turing.jl, DiffEqBayes.jl can be an approachable interface for those not familiar with Bayesian estimation, and provides a nice way to use Stan from pure Julia.","category":"page"},{"location":"highlevels/inverse_problems/#Third-Party-Tools-of-Note","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Third Party Tools of Note","text":"","category":"section"},{"location":"highlevels/inverse_problems/#Turing.jl:-A-Flexible-Probabilistic-Programming-Language-for-Bayesian-Analysis","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Turing.jl: A Flexible Probabilistic Programming Language for Bayesian Analysis","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"In the context of differential equations and parameter estimation, Turing.jl allows for a Bayesian estimation of differential equations (used in conjunction with the high-level package DiffEqBayes.jl). For more examples on combining Turing.jl with DiffEqBayes.jl, see the documentation below. It is important to note that Turing.jl can also perform Bayesian estimation without relying on DiffEqBayes.jl (for an example, consult  this tutorial).","category":"page"},{"location":"highlevels/inverse_problems/#Topopt.jl:-Topology-Optimization-in-Julia","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Topopt.jl: Topology Optimization in Julia","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"Topopt.jl solves topology optimization problems which are inverse problems on partial differential equations, solving for an optimal domain.","category":"page"},{"location":"highlevels/inverse_problems/#Recommended-Automatic-Differentiation-Libraries","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Recommended Automatic Differentiation Libraries","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"Solving inverse problems commonly requires using automatic differentiation (AD). SciML includes extensive support for automatic differentiation throughout its solvers, though some AD libraries are more tested than others. The following libraries are the current recommendations of the SciML developers.","category":"page"},{"location":"highlevels/inverse_problems/#ForwardDiff.jl:-Operator-Overloading-Forward-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"ForwardDiff.jl: Operator-Overloading Forward Mode Automatic Differentiation","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"ForwardDiff.jl is a library for operator-overloading based forward-mode automatic differentiation. It's commonly used as the default method for generating Jacobians throughout the SciML solver libraries.","category":"page"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"note: Note\nBecause ForwardDiff.jl uses an operator overloading approach, uses of ForwardDiff.jl require that any caches for non-allocating mutating code allows for Dual numbers. To allow such code to be ForwardDiff.jl-compatible, see PreallocationTools.jl.","category":"page"},{"location":"highlevels/inverse_problems/#Enzyme.jl:-LLVM-Level-Forward-and-Reverse-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Enzyme.jl: LLVM-Level Forward and Reverse Mode Automatic Differentiation","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"Enzyme.jl is an LLVM-level AD library for forward and reverse automatic differentiation. It supports many features required for high performance, such as being able to differentiate mutating and interleave compiler optimization with the AD passes. However, it does not support all of the Julia runtime, and thus some code with many dynamic behaviors and garbage collection (GC) invocations can be incompatible with Enzyme. Enzyme.jl is quickly becoming the new standard AD for SciML.","category":"page"},{"location":"highlevels/inverse_problems/#Zygote.jl:-Julia-Level-Source-to-Source-Reverse-Mode-Automatic-Differentiation","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Zygote.jl: Julia-Level Source-to-Source Reverse Mode Automatic Differentiation","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"Zygote.jl is the current standard user-level reverse-mode automatic differentiation library for the SciML solvers. User-level means that many library tutorials, like in   SciMLSensitivity.jl and DiffEqFlux.jl, are written showcase user code using Zygote.jl. This is because  Zygote.jl is the AD engine associated with the Flux machine learning library. However, Zygote.jl has many limitations which limits its performance in equation solver contexts, such as an inability to handle mutation and introducing many small allocations and type-instabilities. For this reason, the SciML equation solvers include define differentiation overloads using ChainRules.jl, meaning that the equation solvers tend to not use Zygote.jl internally even if the user code uses Zygote.gradient. In this manner, the speed and performance of more advanced techniques can be preserved while using the Julia standard.","category":"page"},{"location":"highlevels/inverse_problems/#FiniteDiff.jl:-Fast-Finite-Difference-Approximations","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"FiniteDiff.jl: Fast Finite Difference Approximations","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"FiniteDiff.jl is the preferred fallback library for numerical differentiation and is commonly used by SciML solver libraries when automatic differentiation is disabled.","category":"page"},{"location":"highlevels/inverse_problems/#SparseDiffTools.jl:-Tools-for-Fast-Automatic-Differentiation-with-Sparse-Operators","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"SparseDiffTools.jl: Tools for Fast Automatic Differentiation with Sparse Operators","text":"","category":"section"},{"location":"highlevels/inverse_problems/","page":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","title":"Parameter Estimation, Bayesian Analysis, and Inverse Problems Overview","text":"SparseDiffTools.jl is a library for sparse automatic differentiation. It's used internally by many of the SciML equation solver libraries, which explicitly expose interfaces for colorvec color vectors generated by SparseDiffTools.jl's methods. SparseDiffTools.jl also includes many features useful to users, such as operators for matrix-free Jacobian-vector and Hessian-vector products.","category":"page"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#Classic-Noise-Processes","page":"Classic Noise Processes","title":"Classic Noise Processes","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/noise_processes/","page":"Classic Noise Processes","title":"Classic Noise Processes","text":"This section describes the available NoiseProcess types. Note that all keyword arguments are splatted into the NoiseProcess constructor, and thus options like reset are available on the pre-built processes.","category":"page"},{"location":"modules/DiffEqNoiseProcess/noise_processes/","page":"Classic Noise Processes","title":"Classic Noise Processes","text":"WienerProcess\nWienerProcess!\nRealWienerProcess\nRealWienerProcess!\nOrnsteinUhlenbeckProcess\nOrnsteinUhlenbeckProcess!\nGeometricBrownianMotionProcess\nGeometricBrownianMotionProcess!\nCorrelatedWienerProcess\nCorrelatedWienerProcess!\nSimpleWienerProcess\nSimpleWienerProcess!\nCompoundPoissonProcess\nCompoundPoissonProcess!","category":"page"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.WienerProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.WienerProcess","text":"The WienerProcess, also known as Brownian motion, or the noise in the Langevin equation, is the stationary process with white noise increments and a distribution N(0,dt). The constructor is:\n\nWienerProcess(t0,W0,Z0=nothing;kwargs...)\nWienerProcess!(t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.WienerProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.WienerProcess!","text":"The WienerProcess, also known as Brownian motion, or the noise in the Langevin equation, is the stationary process with white noise increments and a distribution N(0,dt). The constructor is:\n\nWienerProcess(t0,W0,Z0=nothing;kwargs...)\nWienerProcess!(t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.RealWienerProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.RealWienerProcess","text":"The RealWienerProcess is a Brownian motion that is forced to be real-valued. While the normal WienerProcess becomes complex valued if W0 is complex, this verion is real valued for when you want to, for example, solve an SDE defined by complex numbers where the noise is in the reals.\n\nRealWienerProcess(t0,W0,Z0=nothing;kwargs...)\nRealWienerProcess!(t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.RealWienerProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.RealWienerProcess!","text":"The RealWienerProcess is a Brownian motion that is forced to be real-valued. While the normal WienerProcess becomes complex valued if W0 is complex, this verion is real valued for when you want to, for example, solve an SDE defined by complex numbers where the noise is in the reals.\n\nRealWienerProcess(t0,W0,Z0=nothing;kwargs...)\nRealWienerProcess!(t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.OrnsteinUhlenbeckProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.OrnsteinUhlenbeckProcess","text":"a Ornstein-Uhlenbeck process which is a Wiener process defined by the stochastic differential equation\n\ndX_t = theta (mu - X_t) dt + sigma dW_t\n\nThe OrnsteinUhlenbeckProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). The constructor is:\n\nOrnsteinUhlenbeckProcess(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)\nOrnsteinUhlenbeckProcess!(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.OrnsteinUhlenbeckProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.OrnsteinUhlenbeckProcess!","text":"A Ornstein-Uhlenbeck process which is a Wiener process defined by the stochastic differential equation\n\ndX_t = theta (mu - X_t) dt + sigma dW_t\n\nThe OrnsteinUhlenbeckProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). The constructor is:\n\nOrnsteinUhlenbeckProcess(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)\nOrnsteinUhlenbeckProcess!(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.GeometricBrownianMotionProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.GeometricBrownianMotionProcess","text":"A GeometricBrownianMotion process is a Wiener process with constant drift μ and constant diffusion σ. I.e. this is the solution of the stochastic differential equation\n\ndX_t = mu X_t dt + sigma X_t dW_t\n\nThe GeometricBrownianMotionProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). It can be back interpolated exactly as well. The constructor is:\n\nGeometricBrownianMotionProcess(μ,σ,t0,W0,Z0=nothing;kwargs...)\nGeometricBrownianMotionProcess!(μ,σ,t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.GeometricBrownianMotionProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.GeometricBrownianMotionProcess!","text":"A GeometricBrownianMotion process is a Wiener process with constant drift μ and constant diffusion σ. I.e. this is the solution of the stochastic differential equation\n\ndX_t = mu X_t dt + sigma X_t dW_t\n\nThe GeometricBrownianMotionProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). It can be back interpolated exactly as well. The constructor is:\n\nGeometricBrownianMotionProcess(μ,σ,t0,W0,Z0=nothing;kwargs...)\nGeometricBrownianMotionProcess!(μ,σ,t0,W0,Z0=nothing;kwargs...)\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.CorrelatedWienerProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.CorrelatedWienerProcess","text":"One can define a CorrelatedWienerProcess which is a Wiener process with correlations between the Wiener processes. The constructor is:\n\nCorrelatedWienerProcess(Γ,t0,W0,Z0=nothing;kwargs...)\nCorrelatedWienerProcess!(Γ,t0,W0,Z0=nothing;kwargs...)\n\nwhere Γ is the constant covariance matrix.\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.CorrelatedWienerProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.CorrelatedWienerProcess!","text":"One can define a CorrelatedWienerProcess which is a Wiener process with correlations between the Wiener processes. The constructor is:\n\nCorrelatedWienerProcess(Γ,t0,W0,Z0=nothing;kwargs...)\nCorrelatedWienerProcess!(Γ,t0,W0,Z0=nothing;kwargs...)\n\nwhere Γ is the constant covariance matrix.\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.SimpleWienerProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.SimpleWienerProcess","text":"The SimpleWienerProcess, also known as Brownian motion, or the noise in the Langevin equation, is the stationary process with white noise increments and a distribution N(0,dt). The constructor is:\n\nSimpleWienerProcess(t0,W0,Z0=nothing;kwargs...)\nSimpleWienerProcess(t0,W0,Z0=nothing;kwargs...)\n\nUnlike WienerProcess, this uses the SimpleNoiseProcess and thus does not support adaptivity, but is slightly more lightweight.\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.SimpleWienerProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.SimpleWienerProcess!","text":"The SimpleWienerProcess, also known as Brownian motion, or the noise in the Langevin equation, is the stationary process with white noise increments and a distribution N(0,dt). The constructor is:\n\nSimpleWienerProcess(t0,W0,Z0=nothing;kwargs...)\nSimpleWienerProcess(t0,W0,Z0=nothing;kwargs...)\n\nUnlike WienerProcess, this uses the SimpleNoiseProcess and thus does not support adaptivity, but is slightly more lightweight.\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.CompoundPoissonProcess","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.CompoundPoissonProcess","text":"https://www.math.wisc.edu/~anderson/papers/AndPostleap.pdf Incorporating postleap checks in tau-leaping J. Chem. Phys. 128, 054103 (2008); https://doi.org/10.1063/1.2819665\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.CompoundPoissonProcess!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.CompoundPoissonProcess!","text":"https://www.math.wisc.edu/~anderson/papers/AndPostleap.pdf Incorporating postleap checks in tau-leaping J. Chem. Phys. 128, 054103 (2008); https://doi.org/10.1063/1.2819665\n\n\n\n\n\n","category":"type"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#Bridges","page":"Classic Noise Processes","title":"Bridges","text":"","category":"section"},{"location":"modules/DiffEqNoiseProcess/noise_processes/","page":"Classic Noise Processes","title":"Classic Noise Processes","text":"BrownianBridge\nBrownianBridge!\nDiffEqNoiseProcess.GeometricBrownianBridge\nDiffEqNoiseProcess.GeometricBrownianBridge!\nDiffEqNoiseProcess.CompoundPoissonBridge\nDiffEqNoiseProcess.CompoundPoissonBridge!","category":"page"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.BrownianBridge","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.BrownianBridge","text":"A BrownianBridge process is a Wiener process with a pre-defined start and end value. This process is distribution exact and back be back interpolated exactly as well. The constructor is:\n\nBrownianBridge(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)\nBrownianBridge!(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)\n\nwhere W(t0)=W₀, W(tend)=Wend, and likewise for the Z process if defined.\n\n\n\n","category":"function"},{"location":"modules/DiffEqNoiseProcess/noise_processes/#DiffEqNoiseProcess.BrownianBridge!","page":"Classic Noise Processes","title":"DiffEqNoiseProcess.BrownianBridge!","text":"A BrownianBridge process is a Wiener process with a pre-defined start and end value. This process is distribution exact and back be back interpolated exactly as well. The constructor is:\n\nBrownianBridge(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)\nBrownianBridge!(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)\n\nwhere W(t0)=W₀, W(tend)=Wend, and likewise for the Z process if defined.\n\n\n\n","category":"function"},{"location":"modules/MethodOfLines/curvilinear_grids/#Curvilinear-Grids","page":"Curvilinear Grids","title":"Curvilinear Grids","text":"","category":"section"},{"location":"modules/MethodOfLines/curvilinear_grids/","page":"Curvilinear Grids","title":"Curvilinear Grids","text":"Curvilinear grids can be achieved via a change of variables. See this post on StackExchange for more.","category":"page"},{"location":"modules/NeuralPDE/manual/nnrode/#Random-Ordinary-Differential-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"modules/NeuralPDE/manual/nnrode/","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"TODO","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"using PolyChaos, LinearAlgebra\nmy_f(t) = t^2\na, b = 1.23, 3.45 # shape parameters of Jacobi weight\nint_exact = 0.353897; # reference value \nN = 4\nα, β = rm_jacobi(N+1,a,b)\nn_gauss, w_gauss = gauss(N,α,β)\nint_gauss = dot(w_gauss, my_f.(n_gauss))\nprint(\"first point:\\t $(n_gauss[1])\\n\")\nprint(\"end point:\\t $(n_gauss[end])\\n\")\nprint(\"error Gauss:\\t $(int_gauss - int_exact)\\n\")\nn_radau, w_radau = radau(N-1, α, β, 1.)\nint_radau = dot(w_radau, my_f.(n_radau))\nprint(\"first point:\\t $(n_radau[1])\\n\")\nprint(\"end point:\\t $(n_radau[end])\\n\")\nprint(\"error Radau:\\t $(int_radau - int_exact)\")\nn_lob, w_lob = lobatto(N-2, α, β, -1., 1.)\nint_lob = dot(w_lob, my_f.(n_lob))\nprint(\"first point:\\t $(n_lob[1])\\n\")\nprint(\"end point:\\t $(n_lob[end])\\n\")\nprint(\"error Lobatto:\\t $(int_lob - int_exact)\")\nn_fej, w_fej = fejer(N)\nint_fej = dot(w_fej, my_f.(n_fej).*(1 .- n_fej).^a.*(1 .+ n_fej).^b)\nprint(\"first point:\\t $(n_fej[1])\\n\")\nprint(\"end point:\\t $(n_fej[end])\\n\")\nprint(\"error Fejer:\\t $(int_fej - int_exact)\")\nn_fej2, w_fej2 = fejer2(N)\nint_fej2 = dot(w_fej2, my_f.(n_fej2).*(1 .- n_fej2).^a.*(1 .+ n_fej2).^b)\nprint(\"first point:\\t $(n_fej2[1])\\n\")\nprint(\"end point:\\t $(n_fej2[end])\\n\")\nprint(\"error Fejer2:\\t $(int_fej2 - int_exact)\")\nn_cc, w_cc = clenshaw_curtis(N)\nint_cc = dot(w_cc, my_f.(n_cc).*(1 .- n_cc).^a.*(1 .+ n_cc).^b)\nprint(\"first point:\\t\\t $(n_cc[1])\\n\")\nprint(\"end point:\\t\\t $(n_cc[end])\\n\")\nprint(\"error Clenshaw-Curtis:\\t $(int_cc - int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#QuadratureRules","page":"Quadrature Rules","title":"Quadrature Rules","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"In this tutorial we investigate how recurrence coefficients of orthogonal polynomials lead to quadrature rules.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"We want to solve the integral","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"I = int_-1^1 f(t) w(t) mathrmd t","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"with the weight function","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"w(t) = (1-t)^a (1+t)^b","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"for all t in -1 1 and a b  -1. For the function f we choose","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"f(t) = t^2","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"To solve the integral we do the following:","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Choose number of nodes N;\nGenerate recurrence coefficients;\nGenerate quadrature rule from those recurrence coefficients.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"We will compare Gauss quadrature to Gauss-Radau quadrature and Gauss-Lobatto quadrature.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Make sure to check out this tutorial too.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Let's begin:","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"using PolyChaos, LinearAlgebra\nmy_f(t) = t^2\na, b = 1.23, 3.45 # shape parameters of Jacobi weight\nint_exact = 0.353897; # reference value ","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Now we compute N recurrence coefficients.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"N = 4\nα, β = rm_jacobi(N+1, a, b)","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Gauss","page":"Quadrature Rules","title":"Gauss","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"The first quadrature rule is Gauss quadrature. This method goes back to Golub and Welsch.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_gauss, w_gauss = gauss(N, α, β)\nint_gauss = dot(w_gauss, my_f.(n_gauss))\nprint(\"first point:\\t $(n_gauss[1])\\n\")\nprint(\"end point:\\t $(n_gauss[end])\\n\")\nprint(\"error Gauss:\\t $(int_gauss - int_exact)\\n\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Since Gauss quadrature has a degree of exactness of 2N-1, the value of the integral is exact.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Gauss-Radau","page":"Quadrature Rules","title":"Gauss-Radau","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Gauss-Radau quadrature is a variant of Gauss quadrature that allows to specify a value of a node that has to be included. We choose to include the right end point t = 10.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_radau, w_radau = radau(N-1, α, β, 1.)\nint_radau = dot(w_radau, my_f.(n_radau))\nprint(\"first point:\\t $(n_radau[1])\\n\")\nprint(\"end point:\\t $(n_radau[end])\\n\")\nprint(\"error Radau:\\t $(int_radau - int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Gauss-Lobatto","page":"Quadrature Rules","title":"Gauss-Lobatto","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Next, we look at Gauss-Lobatto quadrature, which allows to include two points. We choose to include the left and end point of the interval, which are t in -10 10.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_lob, w_lob = lobatto(N-2, α, β, -1., 1.)\nint_lob = dot(w_lob, my_f.(n_lob))\nprint(\"first point:\\t $(n_lob[1])\\n\")\nprint(\"end point:\\t $(n_lob[end])\\n\")\nprint(\"error Lobatto:\\t $(int_lob - int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"There are other quadratures that we subsume as all-purpose quadrature rules. These include Fejér's first and second rule, and Clenshaw-Curtis quadrature.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Fejér's-First-Rule","page":"Quadrature Rules","title":"Fejér's First Rule","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Fejér's first rule does not include the end points of the interval.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_fej, w_fej = fejer(N)\nint_fej = dot(w_fej, my_f.(n_fej).*(1 .- n_fej).^a.*(1 .+ n_fej).^b)\nprint(\"first point:\\t $(n_fej[1])\\n\")\nprint(\"end point:\\t $(n_fej[end])\\n\")\nprint(\"error Fejer:\\t $(int_fej-int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Fejér's-Second-Rule","page":"Quadrature Rules","title":"Fejér's Second Rule","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Fejér's second rule does include the end points of the interval.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_fej2, w_fej2 = fejer2(N)\nint_fej2 = dot(w_fej2, my_f.(n_fej2).*(1 .- n_fej2).^a.*(1 .+ n_fej2).^b)\nprint(\"first point:\\t $(n_fej2[1])\\n\")\nprint(\"end point:\\t $(n_fej2[end])\\n\")\nprint(\"error Fejer2:\\t $(int_fej2 - int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/#Clenshaw-Curtis","page":"Quadrature Rules","title":"Clenshaw-Curtis","text":"","category":"section"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"Clenshaw-Curtis quadrature is similar to Féjer's second rule, as in it includes the end points of the integration interval. For the same number of nodes it is also more accurate than Féjer's rules, generally speaking.","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"n_cc, w_cc = clenshaw_curtis(N)\nint_cc = dot(w_cc, my_f.(n_cc).*(1 .- n_cc).^a.*(1 .+ n_cc).^b)\nprint(\"first point:\\t\\t $(n_cc[1])\\n\")\nprint(\"end point:\\t\\t $(n_cc[end])\\n\")\nprint(\"error Clenshaw-Curtis:\\t $(int_cc - int_exact)\")","category":"page"},{"location":"modules/PolyChaos/quadrature_rules/","page":"Quadrature Rules","title":"Quadrature Rules","text":"As we can see, for the same number of nodes N, the quadrature rules based on the recurrence coefficients can greatly outperform the all-purpose quadratures. So, whenever possible, use quadrature rules based on recurrence coefficients of the orthogonal polynomials relative to the underlying measure. Make sure to check out this tutorial too.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#jump_solve","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"solve(prob::JumpProblem,alg;kwargs)","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#Recommended-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"A JumpProblem(prob,aggregator,jumps...) come in two forms. The first major form is if it does not have a RegularJump. In this case, it can be solved with any integrator on  prob. However, in the case of a pure JumpProblem (a JumpProblem over a  DiscreteProblem), there are special algorithms available.  The SSAStepper() is an efficient streamlined algorithm for running the  aggregator version of the SSA for pure ConstantRateJump and/or MassActionJump problems. However, it is not compatible with event handling. If events are necessary, then FunctionMap does well.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"If there is a RegularJump, then specific methods must be used. The current recommended method is TauLeaping if you need adaptivity, events, etc. If you just need the most barebones fixed time step leaping method, then SimpleTauLeaping can have performance benefits.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#Special-Methods-for-Pure-Jump-Problems","page":"Jump Problem and Jump Diffusion Solvers","title":"Special Methods for Pure Jump Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"If you are using jumps with a differential equations, use the same methods as in the case of the differential equation solving. However, the following algorithms are optimized for pure jump problems.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#JumpProcesses.jl","page":"Jump Problem and Jump Diffusion Solvers","title":"JumpProcesses.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"SSAStepper: a stepping algorithm for pure ConstantRateJump and/or MassActionJump JumpProblems. Supports handling of DiscreteCallback and saving controls like saveat.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#RegularJump-Compatible-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"RegularJump Compatible Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#StochasticDiffEq.jl","page":"Jump Problem and Jump Diffusion Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"These methods support mixing with event handling, other jump types, and all of the features of the normal differential equation solvers.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"TauLeaping: an adaptive tau-leaping algorithm with post-leap estimates.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#JumpProcesses.jl-2","page":"Jump Problem and Jump Diffusion Solvers","title":"JumpProcesses.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"SimpleTauLeaping: a tau-leaping algorithm for pure RegularJump JumpProblems. Requires a choice of dt.\nRegularSSA: a version of SSA for pure RegularJump JumpProblems.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#Regular-Jump-Diffusion-Compatible-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"Regular Jump Diffusion Compatible Methods","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"Regular jump diffusions are JumpProblems where the internal problem is an SDEProblem and the jump process has designed a regular jump.","category":"page"},{"location":"modules/DiffEqDocs/solvers/jump_solve/#StochasticDiffEq.jl-2","page":"Jump Problem and Jump Diffusion Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"modules/DiffEqDocs/solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"EM: Explicit Euler-Maruyama.\nImplicitEM: Implicit Euler-Maruyama. See the SDE solvers page for more details.","category":"page"},{"location":"modules/LinearSolve/advanced/developing/#Developing-New-Linear-Solvers","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"","category":"section"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Developing new or custom linear solvers for the SciML interface can be done in one of two ways:","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"You can either create a completely new set of dispatches for init and solve.\nYou can extend LinearSolve.jl's internal mechanisms.","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"For developer ease, we highly recommend (2) as that will automatically make the caching API work. Thus this is the documentation for how to do that.","category":"page"},{"location":"modules/LinearSolve/advanced/developing/#Developing-New-Linear-Solvers-with-LinearSolve.jl-Primitives","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers with LinearSolve.jl Primitives","text":"","category":"section"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"Let's create a new wrapper for a simple LU-factorization which uses only the basic machinery. A simplified version is:","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"struct MyLUFactorization{P} <: SciMLBase.AbstractLinearAlgorithm end\r\n\r\ninit_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose) = lu!(convert(AbstractMatrix,A))\r\n\r\nfunction SciMLBase.solve(cache::LinearCache, alg::MyLUFactorization; kwargs...)\r\n    if cache.isfresh\r\n        A = convert(AbstractMatrix,A)\r\n        fact = lu!(A)\r\n        cache = set_cacheval(cache, fact)\r\n    end\r\n    y = ldiv!(cache.u, cache.cacheval, cache.b)\r\n    SciMLBase.build_linear_solution(alg,y,nothing,cache)\r\nend","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"The way this works is as follows. LinearSolve.jl has a LinearCache that everything shares (this is what gives most of the ease of use). However, many algorithms need to cache their own things, and so there's one value cacheval that is for the algorithms to modify. The function:","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"init_cacheval(alg::MyLUFactorization, A, b, u, Pl, Pr, maxiters, abstol, reltol, verbose)","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"is what is called at init time to create the first cacheval. Note that this should match the type of the cache later used in solve as many algorithms, like those in OrdinaryDiffEq.jl, expect type-groundedness in the linear solver definitions. While there are cheaper ways to obtain this type for LU factorizations (specifically, ArrayInterfaceCore.lu_instance(A)), for a demonstration this just performs an LU-factorization to get an LU{T, Matrix{T}} which it puts into the cacheval so its typed for future use.","category":"page"},{"location":"modules/LinearSolve/advanced/developing/","page":"Developing New Linear Solvers","title":"Developing New Linear Solvers","text":"After the init_cacheval, the only thing left to do is to define SciMLBase.solve(cache::LinearCache, alg::MyLUFactorization). Many algorithms may use a lazy matrix-free representation of the operator A. Thus if the algorithm requires a concrete matrix, like LU-factorization does, the algorithm should convert(AbstractMatrix,cache.A). The flag cache.isfresh states whether A has changed since the last solve. Since we only need to factorize when A is new, the factorization part of the algorithm is done in a if cache.isfresh. cache = set_cacheval(cache, fact) puts the new factorization into the cache so it's updated for future solves. Then y = ldiv!(cache.u, cache.cacheval, cache.b) performs the solve and a linear solution is returned via SciMLBase.build_linear_solution(alg,y,nothing,cache).","category":"page"},{"location":"modules/LabelledArrays/SLArrays/#SLArrays","page":"SLArrays","title":"SLArrays","text":"","category":"section"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"The SLArray and SLVector macros create static LabelledArrays. First the user would create the array type, then use that constructor to generate instances of the labelled array.","category":"page"},{"location":"modules/LabelledArrays/SLArrays/#@SLArray-and-@SLVector-macros","page":"SLArrays","title":"@SLArray and @SLVector macros","text":"","category":"section"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"Macro constructors are convenient for building most SLArray objects. An  @SLArray may be of arbitrary dimension while an @SLVector is a  one dimensional array. ","category":"page"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"@SLArray\n@SLVector","category":"page"},{"location":"modules/LabelledArrays/SLArrays/#LabelledArrays.@SLArray","page":"SLArrays","title":"LabelledArrays.@SLArray","text":"@SLArray Size Names\n@SLArray Eltype Size Names\n\nThe macro creates a labelled static vector with element type ElType, names from Names, and size from Size. If no eltype is given, then the eltype is determined from the arguments in the constructor.\n\nFor example:\n\nABCD = @SLArray (2,2) (:a,:b,:c,:d)\nx = ABCD(1.0, 2.5, 3.0, 5.0)\nx.a == 1.0\nx.b == 2.5\nx.c == x[3]\nx.d == x[2,2]\nEFG = @SLArray (2,2) (e=1:3, f=4, g=2:4)\ny = EFG(1.0,2.5,3.0,5.0)\nEFG = @SLArray (2,2) (e=(2, :), f=4, g=2:4)\n\nUsers can also specify the indices directly.\n\njulia> EFG = @SLArray (2,2) (e=1:3, f=4, g=2:4);\njulia> y = EFG(1.0,2.5,3.0,5.0)\n2×2 SLArray{Tuple{2,2},Float64,2,4,(e = 1:3, f = 4, g = 2:4)}:\n 1.0  3.0\n 2.5  5.0\n\njulia> y.g\n3-element view(reshape(::StaticArrays.SArray{Tuple{2,2},Float64,2,4}, 4), 2:4) with eltype Float64:\n 2.5\n 3.0\n 5.0\n\njulia> Arr = @SLArray (2, 2) (a = (2, :), b = 3);\njulia> z = Arr(1, 2, 3, 4);\njulia> z.a\n2-element view(::StaticArrays.SArray{Tuple{2,2},Int64,2,4}, 2, :) with eltype Int64:\n 2\n 4\n\n\n\n\n\n","category":"macro"},{"location":"modules/LabelledArrays/SLArrays/#LabelledArrays.@SLVector","page":"SLArrays","title":"LabelledArrays.@SLVector","text":"@SLVector Names\n@SLVector Eltype Names\n\nThe macro creates a labelled static vector with element type ElType, and names from Names. If no eltype is given, then the eltype is determined from the values in the constructor. The array size is found from the input data. \n\nFor example:\n\nABC = @SLVector (:a,:b,:c)\nx = ABC(1.0,2.5,3.0)\nx.a == 1.0\nx.b == 2.5\nx.c == x[3]\n\n\n\n\n\n","category":"macro"},{"location":"modules/LabelledArrays/SLArrays/#SLArray-and-SLVector-constructors","page":"SLArrays","title":"SLArray and SLVector constructors","text":"","category":"section"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"Alternatively, users can construct a static labelled array using the SLVector and SLArrays constructors by writing out the entries as keyword arguments:","category":"page"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"SLArray\nSLVector","category":"page"},{"location":"modules/LabelledArrays/SLArrays/#LabelledArrays.SLArray","page":"SLArrays","title":"LabelledArrays.SLArray","text":"SLArray{::Tuple}(::NamedTuple)\nSLArray{::Tuple}(kwargs)\n\nThese are the standard constructors for SLArray. For general N-dimensional  labelled arrays, users need to specify the size (Tuple{dim1,dim2,...}) in the type parameter to the SLArray constructor:\n\njulia> SLArray{Tuple{2,2}}((a=1, b=2, c=3, d=4))\n2×2 SLArray{Tuple{2, 2}, Int64, 2, 4, (:a, :b, :c, :d)} with indices SOneTo(2)×SOneTo(2):\n :a => 1  :c => 3\n :b => 2  :d => 4\n\njulia> SLArray{Tuple{2,2}}(a=1, b=2, c=3, d=4)\n 2×2 SLArray{Tuple{2,2},2,(:a, :b, :c, :d),Int64}:\n 1  3\n 2  4\n\nConstructing copies with some changed elements is supported by a keyword constructor whose first argument is the source and whose additional keyword arguments indicate the changes.\n\njulia> ABCD = @SLArray (2,2) (:a,:b,:c,:d);\njulia> B = ABCD(1,2,3,4);\njulia> B2 = SLArray(B; c=30 )\n2×2 SLArray{Tuple{2,2},Int64,2,4,(:a, :b, :c, :d)}:\n 1  30\n 2   4\n\nAdditional examples:\n\nSLArray{Tuple{2,2}}((a=1, b=2, c=3, d=4))\n\n\n\n\n\nSLVector(v1::SLArray; kwargs...)\n\nCreates a copy of v1 with corresponding items in kwargs replaced.\n\nFor example:\n\nABCD = @SLArray (2,2) (:a,:b,:c,:d);\nB = ABCD(1,2,3,4);\nB2 = SLArray(B; c=30 )\n\n\n\n\n\n","category":"type"},{"location":"modules/LabelledArrays/SLArrays/#LabelledArrays.SLVector","page":"SLArrays","title":"LabelledArrays.SLVector","text":"SLVector(::NamedTuple)\nSLVector(kwargs)\n\nThe standard constructors for SLArray.\n\njulia> SLVector(a=1, b=2, c=3)\n3-element SLArray{Tuple{3},1,(:a, :b, :c),Int64}:\n 1\n 2\n 3\n\nConstructing copies with some items changed is supported by a keyword constructor whose first argument is the source and whose additional keyword arguments indicate the changes.\n\njulia> v1 = SLVector(a=1.1, b=2.2, c=3.3);\njulia> v2 = SLVector(v1; b=20.20, c=30.30 )\n3-element SLArray{Tuple{3},Float64,1,3,(:a, :b, :c)}:\n  1.1\n 20.2\n 30.3\n\nAdditional examples:\n\nSLVector((a=1, b=2)) \nSLVector(a=1, b=2) \n\n\n\n\n\nSLVector(v1::SLArray; kwargs...)\n\nCreates a 1D copy of v1 with corresponding items in kwargs replaced.\n\nFor example:\n\nz = SLVector(a=1, b=2, c=3);\nz2 = SLVector(z; c=30)\n\n\n\n\n\n","category":"function"},{"location":"modules/LabelledArrays/SLArrays/#Manipulating-SLArrays-and-SLVectors","page":"SLArrays","title":"Manipulating SLArrays and SLVectors","text":"","category":"section"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"Users may want a list of the labels or keys in an SLArray or SLVector. The symbols(::SLArray) function returns a tuple of array labels.","category":"page"},{"location":"modules/LabelledArrays/SLArrays/","page":"SLArrays","title":"SLArrays","text":"symbols(::SLArray)","category":"page"},{"location":"modules/LabelledArrays/SLArrays/#LabelledArrays.symbols-Tuple{SLArray}","page":"SLArrays","title":"LabelledArrays.symbols","text":"symbols(::SLArray)\n\nReturns the labels of the SLArray .\n\nFor example:\n\njulia> z = SLVector(a=1, b=2, c=3)\n3-element SLArray{Tuple{3}, Int64, 1, 3, (:a, :b, :c)} with indices SOneTo(3):\n :a => 1\n :b => 2\n :c => 3\n\njulia> symbols(z)\n(:a, :b, :c)\n\n\n\n\n\nsymbols(::LArray)\n\nReturns the labels of the LArray .\n\nFor example:\n\njulia julia> z = @LVector Float64 (:a, :b, :c, :d); julia> symbols(z) (:a, :b, :c, :d)`\n\n\n\n\n\n","category":"method"},{"location":"modules/DiffEqDocs/types/discrete_types/#Discrete-Problems","page":"Discrete Problems","title":"Discrete Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"DiscreteProblem\nDiscreteFunction","category":"page"},{"location":"modules/DiffEqDocs/types/discrete_types/#SciMLBase.DiscreteProblem","page":"Discrete Problems","title":"SciMLBase.DiscreteProblem","text":"Defines a discrete dynamical system problem. Documentation Page: https://diffeq.sciml.ai/stable/types/discrete_types/\n\nMathematical Specification of a Discrete Problem\n\nTo define an Discrete Problem, you simply need to give the function f and the initial condition u_0 which define a function map:\n\nu_n+1 = f(u_npt_n+1)\n\nf should be specified as f(un,p,t) (or in-place as f(unp1,un,p,t)), and u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. u_n+1 only depends on the previous iteration u_n and t_n+1. The default t_n+1 of FunctionMap is t_n = t_0 + n*dt (with dt=1 being the default). For continuous-time Markov chains this is the time at which the change is occuring.\n\nNote that if the discrete solver is set to have scale_by_time=true, then the problem is interpreted as the map:\n\nu_n+1 = u_n + dt f(u_npt_n+1)\n\nProblem Type\n\nConstructors\n\nDiscreteProblem{isinplace}(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace}(u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the identity map.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the map.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nNote About Timing\n\nNote that if no dt and not tstops is given, it's assumed that dt=1 and thus tspan=(0,n) will solve for n iterations. If in the solver dt is given, then the number of iterations will change. And if tstops is not empty, the solver will revert to the standard behavior of fixed timestep methods, which is \"step to each tstop\".\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/discrete_types/#SciMLBase.DiscreteFunction","page":"Discrete Problems","title":"SciMLBase.DiscreteFunction","text":"DiscreteFunction{iip,F,Ta,S,O} <: AbstractDiscreteFunction{iip}\n\nA representation of an discrete dynamical system f, defined by:\n\nu_n+1 = f(upt_n+1)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDiscreteFunction{iip,recompile}(f;\n                                analytic=nothing,\n                                syms=nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DiscreteFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/discrete_types/#Solution-Type","page":"Discrete Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"DiscreteProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"page"},{"location":"modules/Optimization/API/solve/#Common-Solver-Options-(Solve-Keyword-Arguments)","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"","category":"section"},{"location":"modules/Optimization/API/solve/","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"solve(::OptimizationProblem,::Any)","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/#dynamical_prob","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"DynamicalODEProblem\nSecondOrderODEProblem\nDynamicalODEFunction","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/#SciMLBase.DynamicalODEProblem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.DynamicalODEProblem","text":"Defines an dynamical ordinary differential equation (ODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/dynamical_types/\n\nDynamical ordinary differential equations, such as those arising from the definition of a Hamiltonian system or a second order ODE, have a special structure that can be utilized in the solution of the differential equation. On this page we describe how to define second order differential equations for their efficient numerical solution.\n\nMathematical Specification of a Dynamical ODE Problem\n\nThese algorithms require a Partitioned ODE of the form:\n\nfracdvdt = f_1(ut) \nfracdudt = f_2(v) \n\nThis is a Partitioned ODE partitioned into two groups, so the functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of u and t. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent but the kinetic energy is only dependent on v.\n\nNote that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed make note of this requirement with \"Requires quadratic kinetic energy\".\n\nConstructor\n\nDynamicalODEProblem(f::DynamicalODEFunction,v0,u0,tspan,p=NullParameters();kwargs...)\nDynamicalODEProblem{isinplace}(f1,f2,v0,u0,tspan,p=NullParameters();kwargs...)\n\nDefines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf1 and f2: The functions in the ODE.\nv0 and u0: The initial conditions.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dynamical_types/#SciMLBase.SecondOrderODEProblem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.SecondOrderODEProblem","text":"Defines a second order ordinary differential equation (ODE) problem. Documentation Page: https://diffeq.sciml.ai/stable/types/dynamical_types/\n\nMathematical Specification of a 2nd Order ODE Problem\n\nTo define a 2nd Order ODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nu = f(uupt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFrom this form, a dynamical ODE:\n\nv = f(vupt) \nu = v \n\nis generated.\n\nConstructors\n\nSecondOrderODEProblem{isinplace}(f,du0,u0,tspan,callback=CallbackSet())\n\nDefines the ODE with the specified functions.\n\nFields\n\nf: The function for the second derivative.\ndu0: The initial derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ncallback: A callback to be applied to every solver which uses the problem. Defaults to nothing.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dynamical_types/#SciMLBase.DynamicalODEFunction","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.DynamicalODEFunction","text":"DynamicalODEFunction{iip,F1,F2,TMM,Ta,Tt,TJ,JVP,VJP,JP,SP,TW,TWt,TPJ,S,O,TCV} <: AbstractODEFunction{iip}\n\nA representation of an ODE function f, defined by:\n\nM fracdudt = f(upt)\n\nas a partitioned ODE:\n\nM_1 fracdudt = f_1(upt)\nM_2 fracdudt = f_2(upt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDynamicalODEFunction{iip,recompile}(f1,f2;\n                                    mass_matrix=I,\n                                    analytic=nothing,\n                                    tgrad=nothing,\n                                    jac=nothing,\n                                    jvp=nothing,\n                                    vjp=nothing,\n                                    jac_prototype=nothing,\n                                    sparsity=jac_prototype,\n                                    paramjac = nothing,\n                                    syms = nothing,\n                                    indepsym = nothing,\n                                    colorvec = nothing)\n\nNote that only the functions f_i themselves are required. These functions should be given as f_i!(du,u,p,t) or du = f_i(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M_i represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://diffeq.sciml.ai/stable/solvers/daesolve/. Must be an AbstractArray or an AbstractSciMLOperator. Should be given as a tuple of mass matrices, i.e. `(M1, M_2)` for the mass matrices of equations 1 and 2 respectively.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracpartial f(upt)partial t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ast v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\nsyms: the symbol names for the elements of the equation. This should match u0 in size. For example, if u0 = [0.0,1.0] and syms = [:x, :y], this will apply a canonical naming to the values, allowing sol[:x] in the solution and automatically naming values in plots.\nindepsym: the canonical naming for the independent variable. Defaults to nothing, which internally uses t as the representation in any plots.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nrecompile: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DynamicalODEFunction type directly match the names of the inputs.\n\n\n\n","category":"type"},{"location":"modules/DiffEqDocs/types/dynamical_types/#Solution-Type","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Dynamical ODE solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/#Hamiltonian-Problems","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Hamiltonian Problems","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"HamiltonianProblems are provided by DiffEqPhysics.jl and provide an easy way to define equations of motion from the corresponding Hamiltonian. To define a HamiltonianProblem one only needs to specify the Hamiltonian:","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"H(pq)","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"and autodifferentiation (via ForwardDiff.jl) will create the appropriate equations.","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/#Constructors","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Constructors","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"HamiltonianProblem{T}(H,p0,q0,tspan,param=nothing;kwargs...)","category":"page"},{"location":"modules/DiffEqDocs/types/dynamical_types/#Fields","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Fields","text":"","category":"section"},{"location":"modules/DiffEqDocs/types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"H: The Hamiltonian H(p,q,params) which returns a scalar.\np0: The initial momentums.\nq0: The initial positions.\ntspan: The timespan for the problem.\nparam: Defaults to nothing. param will be passed to H's params. ","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#MathOptInterface.jl","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"MathOptInterface is Julia abstration layer to interface with variety of mathematical optimization solvers.","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#Installation:-OptimizationMOI.jl","page":"MathOptInterface.jl","title":"Installation: OptimizationMOI.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"To use this package, install the OptimizationMOI package:","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"import Pkg; Pkg.add(\"OptimizationMOI\")","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#Details","page":"MathOptInterface.jl","title":"Details","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"As of now, the Optimization interface to MathOptInterface implements only the maxtime common keyword argument. ","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"An optimizer which supports the MathOptInterface API can be called be called directly if no optimizer options have to be defined. ","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"For example using the Ipopt.jl optimizer:","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"sol = solve(prob, Ipopt.Optimizer())","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"The optimizer options are handled in one of two ways. They can either be set via Optimization.MOI.OptimizerWithAttributes() or as keyword argument to solve. ","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"For example using the Ipopt.jl optimizer:","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"opt = Optimization.MOI.OptimizerWithAttributes(Ipopt.Optimizer, \"option_name\" => option_value, ...)\nsol = solve(prob, opt)\n\nsol = solve(prob,  Ipopt.Optimizer(); option_name = option_value, ...)","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#Optimizers","page":"MathOptInterface.jl","title":"Optimizers","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#Ipopt.jl-(MathOptInterface)","page":"MathOptInterface.jl","title":"Ipopt.jl (MathOptInterface)","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"Ipopt.Optimizer\nThe full list of optimizer options can be found in the Ipopt Documentation","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#KNITRO.jl-(MathOptInterface)","page":"MathOptInterface.jl","title":"KNITRO.jl (MathOptInterface)","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"KNITRO.Optimizer\nThe full list of optimizer options can be found in the KNITRO Documentation","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/#Juniper.jl-(MathOptInterface)","page":"MathOptInterface.jl","title":"Juniper.jl (MathOptInterface)","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"Juniper.Optimizer\nJuniper requires a nonlinear optimizer to be set via the nl_solver option, which must be a MathOptInterface-based optimizer. See the Juniper documentation for more detail.","category":"page"},{"location":"modules/Optimization/optimization_packages/mathoptinterface/","page":"MathOptInterface.jl","title":"MathOptInterface.jl","text":"using Optimization, ForwardDiff\nrosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\n_p  = [1.0, 100.0]\n\nf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\nprob = Optimization.OptimizationProblem(f, x0, _p)\n\nusing Juniper, Ipopt\nopt = Optimization.MOI.OptimizerWithAttributes(\n    Juniper.Optimizer,\n    \"nl_solver\"=>Optimization.MOI.OptimizerWithAttributes(Ipopt.Optimizer, \"print_level\"=>0),\n)\nsol = solve(prob, opt)","category":"page"},{"location":"modules/COLPRAC/#ColPrac:-Contributor's-Guide-on-Collaborative-Practices-for-Community-Packages","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"This document describes some best practices for collaborating on repositories. Following these practices makes it easier for contributors (new and old) to understand what is expected of them. It should be linked to in the README.md.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"There are many good practices that this document does not cover. These include other members of the wider community reviewing pull requests (PRs) they are interested in, and maintainers encouraging and supporting people who open issues to make PRs to solve them. This document facilitates these other good practices by clarifying what can seem a mysterious process to those who are unfamiliar with it.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"This document is also only intended for community practices, it is not suitable for solo projects with one maintainer.","category":"page"},{"location":"modules/COLPRAC/#Community-Standards","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Community Standards","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Interactions with people in the community must always follow the community standards,","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"including in pull requests, issues, and discussions.","category":"page"},{"location":"modules/COLPRAC/#Contributing-PRs","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Contributing PRs","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"PRs should match the existing code style present in the file.\nPRs affecting the public API, including adding new features, must update the public documentation.\nComments and (possibly internal) docstrings should make the code accessible.\nPRs that change code must have appropriate tests.\nChanges to the code must be made via PR, not pushing to master.","category":"page"},{"location":"modules/COLPRAC/#Reviewing,-Approving-and-Merging-PRs","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Reviewing, Approving and Merging PRs","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"PRs must have 1 approval before they are merged.\nPR authors should not approve their own PRs.\nPRs should pass CI tests before being merged.\nPRs by people without merge rights must have approval from someone who has merge rights (who will usually then merge the PR).\nPRs by people with merge rights must have approval from someone else, who may or may not have merge rights (and then may merge their own PR).\nPRs by people with merge rights should not be merged by people other than the author (just approved).","category":"page"},{"location":"modules/COLPRAC/#Releases","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Releases","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"A release should be made as soon as possible after a bugfix PR is merged.\nCare and consideration should be given as to when to make a breaking release.\nIf the repository is in a state where there are unreleased changes for an extended period of time in preparation for a release, then the version in the Project.toml should be set to the version number of the intended release, with the -DEV suffix.\nThe person who merged the PR should register the new release of the package.","category":"page"},{"location":"modules/COLPRAC/#Becoming-a-Collaborator-(gaining-merge-rights)","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Becoming a Collaborator (gaining merge rights)","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Collaborator merge rights are typically assigned at an Organisational level for all repositories in a GitHub organisation, or at a Team level for a subset of repositories.\nBefore becoming a collaborator it is usual to:\ncontribute several PRs,\nreview constructively and kindly several PRs,\ncontribute meaningfully to several discussions on issues.\nYou may ask to be added as a collaborator.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"It is not rude to ask.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"","category":"page"},{"location":"modules/COLPRAC/#ColPrac:-Further-Guidance","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Further Guidance","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"This page offers some further guidance on conventions that can be helpful when collaborating on projects. This is an expansion on the Collaborative Practices, with more details and extra guidance. Anything detailed here should be considered less important than the main Collaborative Practices.","category":"page"},{"location":"modules/COLPRAC/#Guidance-on-contributing-PRs","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Guidance on contributing PRs","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"You should usually open an issue about a bug or possible improvement before opening a PR with a solution.\nPRs should do a single thing, so that they are easier to review.\nFor example, fix one bug, or update compatibility, rather than fixing a bunch of bugs and updating compatibility and adding a new feature.\nPRs should add tests which cover the new or fixed functionality.\nPRs that move code should not also change code, so that they are easier to review.\nIf only moving code, review for correctness is not required.\nIf only changing code, then the diff makes it clear what lines have changed.\nPRs with large improvements to style should not also change functionality.\nThis is to avoid making large diffs that are not the focus of the PR.\nWhile it is often helpful to fix a few typos in comments on the way past, it is different to using a regex or formatter on the whole project to fix spacing around operators.\nPRs introducing breaking changes should make this clear when opening the PR.\nYou should not push commits which commented-out tests.\nIf pushing a commit for which a test is broken, use the @test_broken macro.\nCommenting out tests while developing locally is okay, but committing a commented-out test increases the risk of it silently not being run when it should be.\nYou should not squash down commits while review is still on-going.\nSquashing commits prevents the reviewer being able to see what commits are added since the last review.\nYou should help review your PRs, even though you cannot approve your own PRs.\nFor instance, start the review process by commenting on why certain bits of the code changed, or highlighting places where you would particularly like reviewer feedback.","category":"page"},{"location":"modules/COLPRAC/#Guidance-on-reviewing,-approving-and-merging-PRs","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Guidance on reviewing, approving and merging PRs","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Review comments should be phrased as questions, as it shows you are open to new ideas.\nFor instance, “Why did you change this to X? Doesn’t that prevent Y?” rather than “You should not have changed this, it will prevent Y”.\nSmall review suggestions, such as typo fixes, should make use of the suggested change feature.\nThis makes it easier and more likely for all the smaller changes to be made.\nReviewers should continue acting as a reviewer until the PR is merged.","category":"page"},{"location":"modules/COLPRAC/#Guidance-on-Package-Releases","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Guidance on Package Releases","text":"","category":"section"},{"location":"modules/COLPRAC/#Incrementing-the-package-version","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Incrementing the package version","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Follow the extension of SemVer 2.0 encoded in Julia package manager Pkg.jl.\nFor a version number X.Y.Z, with Major version X, Minor version Y, Patch version Z:\nPost-1.0.0: for breaking changes increment X, for non-breaking new features increment Y, for bug-fixes increment Z.\nPre-1.0.0: for breaking changes increment Y, for non-breaking (feature or bug-fix) increment Z.\nIntroducing deprecations is not breaking; removing deprecations is breaking.\nThere is a cost to making breaking releases - downstream packages have to add support for the new version - so there has to be a bigger benefit to making breaking changes.","category":"page"},{"location":"modules/COLPRAC/#Unreleased-Changes-and-DEV","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Unreleased Changes and -DEV","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Following the Collaborative Practices, when there are unreleased changes in the repository for an extended period of time the version number in the Project.toml should be suffixed with -DEV. This makes it clear that there are unreleased changes. Which is useful for many things, including quickly understanding why a bug is still occurring, and working out if a bugfix may need to be backported.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Some more details on the use of -DEV.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"After/during/before the PR making the first change of the release, the version number in the Project.toml should be changed to the intended release number should suffixed -DEV.\nFor instance, if the current version is 0.6.3, then the PR making the breaking change could bump it to 0.7.0-DEV.\nThings are more complex if a breaking change is made after the version has been suffixed with -DEV for a non-breaking change.\nThis should be rare since non-breaking changes should be released as soon as possible.\nIf it does occur the following rule applies: if all version numbers to the right of the digit you would increment are zero, then you do not need to change the version; otherwise you do.\nFollow-up PRs can then be made which do not need to increment the version.\nOnce all the follow-up changes have been made, we can make a PR to drop the -DEV suffix and make a new release once this PR is merged.\nNote that locally when using pkg”dev Foo...” to install particular unreleased versions to an environment, Pkg ignores suffixes to the version number.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Pkg treats 0.7.0-DEV identically to 0.7.0. This means you can update the [compat] section of a group of packages and test them together.","category":"page"},{"location":"modules/COLPRAC/#Changing-dependency-compatibility","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Changing dependency compatibility","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Generally changing dependency compatibility should be a non-breaking feature.\ni.e. pre-1.0 change patch version number, post-1.0 change the minor version number.\nFor instance, adding or removing compatibility with a particular version of a current dependency, which may or may not require internal code changes.\nThis also applies when adding or removing packages as dependencies.\nThe new feature in question is the ability to use with a different set of packages.\nChanging a dependency to resolve a bug is a bug-fix.\ni.e. pre/post-1.0 change patch version number.\nFor instance, if a bug in a downstream dependency is causing a problem in your package restricting compat to not allow that version would be a bug-fix.\nChanging compatibility with dependencies may be a breaking release, if it breaks the user-facing interface.   That is to say if the dependency’s API leaks into your API.   There are three ways that this can happen:\nReexporting a function that has changed.\nReturning an object of a type that’s behaviour has changed.\nSubtyping an object that has changed.","category":"page"},{"location":"modules/COLPRAC/#Dropping-support-for-earlier-versions-of-Julia","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Dropping support for earlier versions of Julia","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Changing Julia version compatibility must be a non-breaking feature.\nIt cannot alone be breaking, since Julia versions that are now unsupported will just never see this newer package release.\nTagging the change as a Minor release makes it possible to release backported bug fixes for users stuck on the old Julia version.   For instance, if the current release is 5.4.0 then we can still go back and release 5.3.1.\nDropping support for earlier versions of Julia has a cost - it prevents users on those versions, such as the Long-Term Support version, from using newer releases of your package - so there should usually be a compelling reason to drop support.","category":"page"},{"location":"modules/COLPRAC/#Accidental-breaking-releases","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Accidental breaking releases","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Do not panic, these things sometimes slip through.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"It is important to fix it as soon as possible, as otherwise people start using the breaking change, and reverting it later causes more problems (c.f. Murphy's law).","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"To fix it:","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Make a PR which reverts the PR that made the breaking change.\nBump the Patch version number in the Project.toml.  It was a bug that a breaking API change was made, so a Patch release is correct to fix it.\nMerge the PR and release the new version.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Once the change is reverted you can take stock and decide what to do. There are generally 2 options:","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Make a new PR to reimplement the feature in a non-breaking way.\nMake a new PR which reverts the reversion, bump the version number to signify it as breaking, and release the new breaking version.","category":"page"},{"location":"modules/COLPRAC/#**Example**","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Example","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Consider a package which is currently on v1.14.2. I made a PR to add a new feature and tagged release v1.15.0. The next evening, we get bug reports that the new feature actually broke lots of real uses.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Maybe I changed what I thought was an internal function, but one that was actually part of the public API; maybe I accidentally changed the return type, and that was something people depended on. Whatever it was, I broke it, and this was not caught in code review.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"To fix it, I revert the change, and then tag release v1.15.1. Hopefully, I also can add a test to prevent that part of the API being broken by mistake.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Now I look at my change again. If I can add the same functionality in a non-breaking way - for example, make a new internal function for my use - then I would do so and tag v1.15.2 or v1.16.0 depending on what had to change. If I cannot make an equivalent non-breaking change, then I would have to make the breaking change and tag v2.0.0.","category":"page"},{"location":"modules/COLPRAC/#Accidental-support-for-an-unsupported-dependency","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Accidental support for an unsupported dependency","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Say you were updating PackageA to support a new version of a dependency, PackageB. For example, you want PackageA v1.1.0 to support PackageB v0.5 and to discontinue supporting v0.4. But say you forgot to remove the compatibility for v0.4, which now no longer works, but other downstream packages that only use v0.4 are now pulling in PackageA v1.1.0 and getting errors.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Simply releasing a patch for PackageA (v1.1.1) that removes support for v0.4 won't work in this instance because downstream packages will continue to pull in v1.1.0. It might seem sufficient to just pin the downstream packages to use v1.0.0 but there may be a lot of them to fix and you can't be certain you're aware of them all. It also does nothing to prevent new compatibility issues arising in future.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"To fix this, you should still release a patch of PackageA (v1.1.1) that removes support for v0.4 of PackageB but you should then mark v1.1.0 of PackageA as broken in the registry. To do this, simply make a PR to the the registry adding yanked = true to the Version.toml file under the version causing issues (in this case v1.1.0). This marks the release as broken and prevents it from being used by any package from then on.","category":"page"},{"location":"modules/COLPRAC/#Guidance-on-automatically-enforcing-guidelines","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Guidance on automatically enforcing guidelines","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Many of these guidelines can and should be enforced automatically.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"GitHub: Defining the mergeability of pull requests\nBitbucket: Suggest or require checks before a merge\nGitLab: Status checks that are required to allow a merge requested [WIP]","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"","category":"page"},{"location":"modules/COLPRAC/#Changes-that-are-not-considered-breaking","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Changes that are not considered breaking","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Everything on this list can, in theory, break users' code. See XKCD#1172. However, we consider changes to these things to be non-breaking from the perspective of package versioning.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"Bugs: We may make backwards incompatible behavior changes if the current implementation is clearly broken, that is, if it contradicts the documentation or if a well-understood behavior is not properly implemented due to a bug.\nInternal changes: Non-public API may be changed or removed.   The public API is all exported symbols, plus any unexported symbols that are explicitly documented as part of the public API, for instance documented as part of standard usage in the README or hosted documentation.\nException behavior:\nExceptions may be replaced with non-error behavior.\nFor instance, we may change a function to compute a result instead of raising an exception, even if that error is documented.\nError message text may change.\nException types may change unless the exception type for a specific error condition is specified in the documentation.\nFloating point numerical details: The specific floating point values may change at any time.   Users should rely only on approximate accuracy, numerical stability, or statistical properties, not on the specific bits computed.\nNew exports: Adding a new export is never considered breaking.   However, one should consider carefully before exporting a commonly used name that might clash with an existing name (especially, if clashing with Base).\nNew supertypes:\nA new supertype may be added to an existing hierarchy.\nThat is, changing A <: B to A <: B <: C or A <: C <: B.   This includes adding a supertype to something without one, i.e. with supertype Any.\nA Union constant may be replaced by an abstract type that covers all elements of the union.\nChanges to the string representation: The output of print/string or show/repr on a type may change at any time.   Users should not depend on the exact text, but rather on the meaning of the text.   Changing the string representation often breaks downstream packages tests, because it is hard to write test-cases that depend only on meaning (though unit tests with mocking can be shielded from this kind of breaking).","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"(This guidance on non-breaking changes is inspired by https://www.tensorflow.org/guide/versions.)","category":"page"},{"location":"modules/COLPRAC/#Appendix:","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Appendix:","text":"","category":"section"},{"location":"modules/COLPRAC/#Marking-a-Repository-as-following-ColPrac:","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"Marking a Repository as following ColPrac:","text":"","category":"section"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"As mentioned at the top, community repositories following ColPrac, should link to it in their README.md. One way to do that is with a GitHub badge.","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"(Image: ColPrac: Contributor's Guide on Collaborative Practices for Community Packages)","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"[![ColPrac: Contributor's Guide on Collaborative Practices for Community Packages](https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet)](https://github.com/SciML/ColPrac)","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"In many-cases ColPrac serves in the places of a CONTRIBUTING.md, having all the common guidance that you would otherwise put there. If your package has its own CONTRIBUTING.md then you should also link to ColPrac there, and indicate how the contents of ColPrac relates to the CONTRIBUTING.md. For example by stating:","category":"page"},{"location":"modules/COLPRAC/","page":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","title":"ColPrac: Contributor's Guide on Collaborative Practices for Community Packages","text":"We follow the ColPrac guide for collaborative practices. New contributors should make sure to read that guide. Below are some additional practices we follow.","category":"page"},{"location":"modules/StructuralIdentifiability/utils/wronskian/#Wronskian-Tools","page":"Wronskian Tools","title":"Wronskian Tools","text":"","category":"section"},{"location":"modules/StructuralIdentifiability/utils/wronskian/","page":"Wronskian Tools","title":"Wronskian Tools","text":"Modules = [StructuralIdentifiability]\nPages   = [\"wronskian.jl\"]","category":"page"},{"location":"modules/StructuralIdentifiability/utils/wronskian/#StructuralIdentifiability.get_max_below-Tuple{StructuralIdentifiability.ExpVectTrie, Vector{Int64}}","page":"Wronskian Tools","title":"StructuralIdentifiability.get_max_below","text":"get_max_below(t, vect)\n\nInput:\n\nt - a trie with exponent vectors\nvect - yet another exponent vector\n\nOutput: \n\na pair (d, v) where v is a vector in the trie which is componenwise ≤ vect and the difference d is as small as possible\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/wronskian/#StructuralIdentifiability.massive_eval-Tuple{Any, Any}","page":"Wronskian Tools","title":"StructuralIdentifiability.massive_eval","text":"massive_eval(polys, eval_dict)\n\nInput:\n\npolys - a list of polynomials\neval_dict - dictionary from variables to the values. Missing values are treated as zeroes\n\nOutput: \n\na list of values of the polynomials\n\nEvaluates a list of polynomails at a point. Assumes that multiplications are relatively expensive (like in truncated power series) so all the monomials are precomputed first and the values of monomials of lower degree are cached and used to compute the values of the monomials of higher degree\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/wronskian/#StructuralIdentifiability.monomial_compress-Tuple{Any, ODE}","page":"Wronskian Tools","title":"StructuralIdentifiability.monomial_compress","text":"monomial_compress(io_equation, ode)\n\nCompresses an input-output equation for the rank computation Input: \n\nio_equation - input-output equation\node - the corresponding ODE model\n\nOutput: \n\npair (coeffs, terms) such that:\nsum of coeffs[i] * terms[i] = io_equation\ncoeffs involve only parameters, terms involve only inputs and outputs\nlength of the representation is the smallest possible\n\n\n\n\n\n","category":"method"},{"location":"modules/StructuralIdentifiability/utils/wronskian/#StructuralIdentifiability.wronskian-Union{Tuple{P}, Tuple{Dict{P, P}, ODE{P}}} where P<:AbstractAlgebra.MPolyElem","page":"Wronskian Tools","title":"StructuralIdentifiability.wronskian","text":"wronskian(io_equations, ode)\n\nInput:\n\nio_equations - a set of io-equations in the form of the Dict as returned by find_ioequations\node - the ODE object\n\nOutput: \n\na list of wronskians evaluated at a point modulo prime\n\nComputes the wronskians of io_equations\n\n\n\n\n\n","category":"method"},{"location":"modules/Optimization/optimization_packages/nomad/#NOMAD.jl","page":"NOMAD.jl","title":"NOMAD.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"NOMAD is Julia package interfacing to NOMAD,which is a C++ implementation of the Mesh Adaptive Direct Search algorithm (MADS), designed for difficult blackbox optimization problems. These problems occur when the functions defining the objective and constraints are the result of costly computer simulations. NOMAD.jl documentation","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"The NOMAD algorithm is called by NOMADOpt()","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/#Installation:-OptimizationNOMAD.jl","page":"NOMAD.jl","title":"Installation: OptimizationNOMAD.jl","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"To use this package, install the OptimizationNOMAD package:","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"import Pkg; Pkg.add(\"OptimizationNOMAD\")","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/#Global-Optimizer","page":"NOMAD.jl","title":"Global Optimizer","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nomad/#Without-Constraint-Equations","page":"NOMAD.jl","title":"Without Constraint Equations","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"The method in NOMAD is performing global optimization on problems both with and without constraint equations. Currently however, linear and nonlinear constraints  defined in Optimization are not passed.","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"NOMAD works both with and without lower and upper boxconstraints set by lb and ub in the OptimizationProblem.","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/#Examples","page":"NOMAD.jl","title":"Examples","text":"","category":"section"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"The Rosenbrock function can optimized using the NOMADOpt() with and without boxcontraints as follows:","category":"page"},{"location":"modules/Optimization/optimization_packages/nomad/","page":"NOMAD.jl","title":"NOMAD.jl","text":"rosenbrock(x, p) =  (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2\nx0 = zeros(2)\np  = [1.0, 100.0]\nf = OptimizationFunction(rosenbrock)\n\nprob = OptimizationProblem(f, x0, _p)\nsol = Optimization.solve(prob,NOMADOpt())\n\nprob = OptimizationProblem(f, x0, _p, lb = [-1.0,-1.0], ub = [1.5,1.5])\nsol = Optimization.solve(prob,NOMADOpt())","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/#ModelingToolkitStandardLibrary.jl","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"ModelingToolkitStandardLibrary.jl is a standard library for the  ModelingToolkit acasual modeling system.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/#Installation","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"Installation","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"Assuming that you already have Julia correctly installed, it suffices to import ModelingToolkitStandardLibrary.jl in the standard way:","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"import Pkg; Pkg.add(\"ModelingToolkitStandardLibrary\")","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/#Tutorials","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"Tutorials","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"RC Circuit\nCustom Component","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/#Libraries","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"Libraries","text":"","category":"section"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"The following are the constituant libraries of the ModelingToolkit Standard Library.","category":"page"},{"location":"modules/ModelingToolkitStandardLibrary/","page":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","title":"ModelingToolkitStandardLibrary.jl: A Standard Library for ModelingToolkit","text":"Basic Blocks\nMechanical Components\nElectrical Components\nMagnetic Components\nThermal Components","category":"page"},{"location":"modules/Surrogates/samples/#Samples","page":"Samples","title":"Samples","text":"","category":"section"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Sampling methods are provided by the QuasiMonteCarlo package.","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"The syntax for sampling in an interval or region is the following:","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,S::SamplingAlgorithm)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"where lb and ub are, respectively, the lower and upper bounds. There are many sampling algorithms to choose from:","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Grid sample","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"GridSample{T}\nsample(n,lb,ub,S::GridSample)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Uniform sample","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::UniformSample)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Sobol sample","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::SobolSample)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Latin Hypercube sample","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"sample(n,lb,ub,::LatinHypercubeSample)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Low Discrepancy sample","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"LowDiscrepancySample{T}\nsample(n,lb,ub,S::LowDiscrepancySample)","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Sample on section","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"SectionSample\nsample(n,lb,ub,S::SectionSample)","category":"page"},{"location":"modules/Surrogates/samples/#Adding-a-new-sampling-method","page":"Samples","title":"Adding a new sampling method","text":"","category":"section"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Adding a new sampling method is a two- step process:","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Adding a new SamplingAlgorithm type\nOverloading the sample function with the new type.","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"Example","category":"page"},{"location":"modules/Surrogates/samples/","page":"Samples","title":"Samples","text":"struct NewAmazingSamplingAlgorithm{OPTIONAL} <: QuasiMonteCarlo.SamplingAlgorithm end\n\nfunction sample(n,lb,ub,::NewAmazingSamplingAlgorithm)\n    if lb is  Number\n        ...\n        return x\n    else\n        ...\n        return Tuple.(x)\n    end\nend","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#uncertainty_quantification","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Uncertainty quantification allows a user to identify the uncertainty associated with the numerical approximation given by DifferentialEquations.jl. This page describes the different methods available for quantifying such uncertainties. Note that this requires one of the native Julia solvers like OrdinaryDiffEq.jl, StochasticDiffEq.jl, or DelayDiffEq.jl.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#Installation","page":"Uncertainty Quantification","title":"Installation","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqUncertainty.jl:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"]add DiffEqUncertainty\nusing DiffEqUncertainty","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#ProbInts","page":"Uncertainty Quantification","title":"ProbInts","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"The ProbInts method for uncertainty quantification involves the transformation of an ODE into an associated SDE where the noise is related to the timesteps and the order of the algorithm. This is implemented into the DiffEq system via a callback function. The first form is:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ProbIntsUncertainty(σ,order,save=true)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"σ is the noise scaling factor and order is the order of the algorithm. save is for choosing whether this callback should control the saving behavior. Generally this is true unless one is stacking callbacks in a CallbackSet. It is recommended that σ is representative of the size of the errors in a single step of the equation.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"If you are using an adaptive algorithm, the callback","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"AdaptiveProbIntsUncertainty(order,save=true)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"determines the noise scaling automatically using an internal error estimate.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#Example-1:-FitzHugh-Nagumo","page":"Uncertainty Quantification","title":"Example 1: FitzHugh-Nagumo","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"In this example we will determine our uncertainty when solving the FitzHugh-Nagumo model with the Euler() method. We define the FitzHugh-Nagumo model:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"function fitz(du,u,p,t)\n  V,R = u\n  a,b,c = p\n  du[1] = c*(V - V^3/3 + R)\n  du[2] = -(1/c)*(V -  a - b*R)\nend\nu0 = [-1.0;1.0]\ntspan = (0.0,20.0)\np = (0.2,0.2,3.0)\nprob = ODEProblem(fitz,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Now we define the ProbInts callback. In this case, our method is the Euler method and thus it is order 1. For the noise scaling, we will try a few different values and see how it changes. For σ=0.2, we define the callback as:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.2,1)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"This is akin to having an error of approximately 0.2 at each step. We now build and solve a EnsembleProblem for 100 trajectories:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/10)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Now we can plot the resulting Monte Carlo solution:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"using Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_02)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"If we increase the amount of error, we see that some parts of the equation have less uncertainty than others. For example, at σ=0.5:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.5,1)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/10)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_05)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"But at this amount of noise, we can see how we contract to the true solution by decreasing dt:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.5,1)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/100)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_lowh)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#Example-2:-Adaptive-ProbInts-on-FitzHugh-Nagumo","page":"Uncertainty Quantification","title":"Example 2: Adaptive ProbInts on FitzHugh-Nagumo","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"While the first example is academic and shows how the ProbInts method scales, the fact that one should have some idea of the error in order to calibrate σ can lead to complications. Thus the more useful method in many cases is the AdaptiveProbIntsUncertainty version. In this version, no σ is required since this is calculated using an internal error estimate. Thus this gives an accurate representation of the possible error without user input.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Let's try this with the order 5 Tsit5() method on the same problem as before:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_adaptive_default)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"In this case, we see that the default tolerances give us a very good solution. However, if we increase the tolerance a lot:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb,abstol=1e-3,reltol=1e-1)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_adaptive_default)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"we can see that the moments just after the rise can be uncertain.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/#Example-3:-Adaptive-ProbInts-on-the-Lorenz-Attractor","page":"Uncertainty Quantification","title":"Example 3: Adaptive ProbInts on the Lorenz Attractor","text":"","category":"section"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"One very good use of uncertainty quantification is on chaotic models. Chaotic equations diverge from the true solution according to the error exponentially. This means that as time goes on, you get further and further from the solution. The ProbInts method can help diagnose how much of the timeseries is reliable.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"As in the previous example, we first define the model:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"function g(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,30.0)\np = [10.0,28.0,8/3]\nprob = ODEProblem(g,u0,tspan,p)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"and then we build the ProbInts type. Let's use the order 5 Tsit5 again.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Then we solve the MonteCarloProblem","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_chaos)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Here we see that by t about 22 we start to receive strong deviations from the \"true\" solution. We can increase the amount of time before error explosion by using a higher order method with stricter tolerances:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"tspan = (0.0,40.0)\nprob = ODEProblem(g,u0,tspan,p)\ncb = AdaptiveProbIntsUncertainty(7)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Vern7(),trajectories=100,callback=cb,reltol=1e-6)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_high_order)","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"we see that we can extend the amount of time until we deviate strongly from the \"true\" solution. Of course, for a chaotic system like the Lorenz one presented here, it is impossible to follow the true solution for long times, due to the fact that the system is chaotic and unavoidable deviations due to the numerical precision of a computer get amplified exponentially.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"However, not all hope is lost. The shadowing theorem is a strong statement for having confidence in numerical evolution of chaotic systems:","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Although a numerically computed chaotic trajectory diverges exponentially from the true trajectory with the same initial coordinates, there exists an errorless trajectory with a slightly different initial condition that stays near (\"shadows\") the numerically computed one.","category":"page"},{"location":"modules/DiffEqDocs/analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"For more info on the shadowing theorem, please see the book Chaos in Dynamical Systems by E. Ott.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/#Example:-Nice-DiffEq-Syntax-Without-A-DSL","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"","category":"section"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"Users of the SciML ecosystem are often solving large models with complicated states and/or hundreds or thousands of parameters. These models are implemented using arrays, and those arrays have traditionally been indexed by integers,  such as p[1] or p[1:5]. Numerical indexing is wonderful for small  models, but can quickly cause problems as models become bigger. It is easy to  forget which index corresponds to which reaction rate or which diffusion coeffient.  This confusion can lead to difficult to debug problems in a user's code. LabelledArrays  can make an important difference here. It is much easier to build a model using parameter  references such as p.rate_nacl or p.probability_birth, instead  of p[26] or p[1026]. Labelled arrays make both the development and debugging of models  much faster.  ","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"LabelledArrays.jl are a way to get DSL-like syntax without a macro. In this case, we can solve differential equations with labelled components by making use of labelled arrays, and always refer to the components by name instead of index.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"One key caveat is that users do not need to sacrifice performance when using  labelled arrays. Labelled arrays are as performant as traditional numerically  indexed arrays.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"Let's solve the Lorenz equation using an LVectors. LVectors are  mutable, hence we can use the non-allocating form of the OrdinaryDiffEq  API.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"using LabelledArrays, OrdinaryDiffEq\n\nfunction lorenz_f!(du,u,p,t)\n  du.x = p.σ*(u.y-u.x)\n  du.y = u.x*(p.ρ-u.z) - u.y\n  du.z = u.x*u.y - p.β*u.z\nend\n\nu0 = @LArray [1.0,0.0,0.0] (:x,:y,:z)\np = @LArray [10.0, 28.0, 8/3]  (:σ,:ρ,:β)\ntspan = (0.0,10.0)\nprob = ODEProblem(lorenz_f!,u0,tspan,p)\nsol = solve(prob,Tsit5())\n# Now the solution can be indexed as .x/y/z as well!\nsol[10].x","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"In the example above, we used an LArray to define the intial state u0 as well as the parameter vector p. The reminder of the ODE solution steps are are no different that the original DifferentialEquations tutorials.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"Alternatively, we can use an immutable SLVector to  implement the same equation. In this case, we need to  use the allocating form of the OrdinaryDiffEq API when defining our model equation.","category":"page"},{"location":"modules/LabelledArrays/Example_dsl/","page":"Example: Nice DiffEq Syntax Without A DSL","title":"Example: Nice DiffEq Syntax Without A DSL","text":"LorenzVector = @SLVector (:x,:y,:z)\nLorenzParameterVector = @SLVector (:σ,:ρ,:β)\n\nfunction f(u,p,t)\n  x = p.σ*(u.y-u.x)\n  y = u.x*(p.ρ-u.z) - u.y\n  z = u.x*u.y - p.β*u.z\n  LorenzVector(x,y,z)\nend\n\nu0 = LorenzVector(1.0,0.0,0.0)\np = LorenzParameterVector(10.0,28.0,8/3)\ntspan = (0.0,10.0)\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())","category":"page"},{"location":"modules/DiffEqFlux/layers/SplineLayer/#Spline-Layer","page":"Spline Layer","title":"Spline Layer","text":"","category":"section"},{"location":"modules/DiffEqFlux/layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Constructs a Spline Layer. At a high-level, it performs the following:","category":"page"},{"location":"modules/DiffEqFlux/layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"Takes as input a one-dimensional training dataset, a time span, a time step and an interpolation method.\nDuring training, adjusts the values of the function at multiples of the time-step such that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.","category":"page"},{"location":"modules/DiffEqFlux/layers/SplineLayer/","page":"Spline Layer","title":"Spline Layer","text":"SplineLayer","category":"page"},{"location":"modules/DiffEqFlux/layers/SplineLayer/#DiffEqFlux.SplineLayer","page":"Spline Layer","title":"DiffEqFlux.SplineLayer","text":"Constructs a Spline Layer. At a high-level, it performs the following:\n\nTakes as input a one-dimensional training dataset, a time span, a time step and\n\nan interpolation method.\n\nDuring training, adjusts the values of the function at multiples of the time-step\n\nsuch that the curve interpolated through these points has minimum loss on the corresponding one-dimensional dataset.\n\nSplineLayer(time_span,time_step,spline_basis,saved_points=nothing)\n\nArguments:\n\ntime_span: Tuple of real numbers corresponding to the time span.\ntime_step: Real number corresponding to the time step.\nspline_basis: Interpolation method to be used yb the basis (current supported interpolation methods: ConstantInterpolation, LinearInterpolation, QuadraticInterpolation, QuadraticSpline, CubicSpline).\n'saved_points': values of the function at multiples of the time step. Initialized by default\n\nto a random vector sampled from the unit normal.\n\n\n\n\n\n","category":"type"}]
}
